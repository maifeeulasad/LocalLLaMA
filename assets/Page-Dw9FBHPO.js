import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"llama : add high-throughput mode by ggerganov · Pull Request #14363 · ggml-org/llama.cpp","link_flair_richtext":[{"e":"text","t":"News"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrmxn7","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":"#bbbdbf","ups":88,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_152zyn72n4","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"News","can_mod_post":false,"score":88,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=39e27648161efabef59abd39787fd6f8222c7969","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751646416,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"github.com","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://github.com/ggml-org/llama.cpp/pull/14363","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?auto=webp&amp;s=b075ebe4728db7369f2baf8fe5f07feb026d677f","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2f754516aec2c21e4f5375196c6d7bba0b657d3","width":108,"height":54},{"url":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f50c6c3d42adc6f5e7c7537d79d13142c6f96a5e","width":216,"height":108},{"url":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ee740e6978d6728572a5a26bb4282a2e0bcf4ea","width":320,"height":160},{"url":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6fff29b1956872ed77766a108a404db46b43026","width":640,"height":320},{"url":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20807a8fd9c08388e57b320daad60921fe93ecce","width":960,"height":480},{"url":"https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fb818ec45c4aa13e22ce4fcf051a5dddea654c9","width":1080,"height":540}],"variants":{},"id":"IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#cc3600","id":"1lrmxn7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"LinkSea8324","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/","stickied":false,"url":"https://github.com/ggml-org/llama.cpp/pull/14363","subreddit_subscribers":494987,"created_utc":1751646416,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1c0fkw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LinkSea8324","can_mod_post":false,"created_utc":1751647727,"send_replies":true,"parent_id":"t1_n1bzti0","score":20,"author_fullname":"t2_152zyn72n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly, this is the only reason we moved to vLLM for server production.\\n\\n(Well now there is also Dual Chunked attention but that's another story)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c0fkw","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly, this is the only reason we moved to vLLM for server production.&lt;/p&gt;\\n\\n&lt;p&gt;(Well now there is also Dual Chunked attention but that&amp;#39;s another story)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrmxn7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1c0fkw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751647727,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cj0qt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1cenw0","score":6,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unfortunately not. That feature would be quite helpful for benchmarking and other bulk tasks. A feature to continue token generation at the previously set context limit was added. This then helps to maximize speed for batch loads of greatly varying sizes - sort of manual emulation of paged attention in a multi-pass scenario. This doesn't work with interactive requests though.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1cj0qt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately not. That feature would be quite helpful for benchmarking and other bulk tasks. A feature to continue token generation at the previously set context limit was added. This then helps to maximize speed for batch loads of greatly varying sizes - sort of manual emulation of paged attention in a multi-pass scenario. This doesn&amp;#39;t work with interactive requests though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrmxn7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1cj0qt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751653377,"author_flair_text":null,"treatment_tags":[],"created_utc":1751653377,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cenw0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"its_just_andy","can_mod_post":false,"created_utc":1751652001,"send_replies":true,"parent_id":"t1_n1bzti0","score":5,"author_fullname":"t2_1q67g89k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"does llama cpp have any concept of 'paged attention', or similar? something that shares a kv cache dynamically between multiple user requests, instead of partitioning the gpu memory per stream?\\n\\nI recall that it does not and doesn't have plans to add it which is fair, but just wondering if anything changed","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cenw0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;does llama cpp have any concept of &amp;#39;paged attention&amp;#39;, or similar? something that shares a kv cache dynamically between multiple user requests, instead of partitioning the gpu memory per stream?&lt;/p&gt;\\n\\n&lt;p&gt;I recall that it does not and doesn&amp;#39;t have plans to add it which is fair, but just wondering if anything changed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrmxn7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1cenw0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751652001,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"0e2e7958-9549-11ee-a999-027a9c984b05","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dj1ck","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dfteq","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From a quick look this change seems independent. I thus assume it'll work with --cb, which is nice since --cb is what I've been using extensively for quite a while.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1dj1ck","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From a quick look this change seems independent. I thus assume it&amp;#39;ll work with --cb, which is nice since --cb is what I&amp;#39;ve been using extensively for quite a while.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrmxn7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1dj1ck/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751665208,"author_flair_text":null,"treatment_tags":[],"created_utc":1751665208,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dfteq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"noneabove1182","can_mod_post":false,"created_utc":1751664088,"send_replies":true,"parent_id":"t1_n1bzti0","score":3,"author_fullname":"t2_7quep","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you know if this applies to continuous batching? One of my favourite recent discoveries was that you could just hammer an endpoint without having to batch the requests ahead of time and still get a chunk of performance increase ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dfteq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Bartowski"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you know if this applies to continuous batching? One of my favourite recent discoveries was that you could just hammer an endpoint without having to batch the requests ahead of time and still get a chunk of performance increase &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrmxn7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1dfteq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751664088,"author_flair_text":"Bartowski","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#889bdb","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1bzti0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1751647539,"send_replies":true,"parent_id":"t3_1lrmxn7","score":67,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The high-throughput mode increases prompt processing and token generation speed *a lot*, when activated with \`--attn-streams\`. This only applies to parallel processing though, like done for benchmarking and larger batch workloads. \\"Single user\\" performance remains unaffected. In any case, this brings llama.cpp closer to the vLLM performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bzti0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The high-throughput mode increases prompt processing and token generation speed &lt;em&gt;a lot&lt;/em&gt;, when activated with &lt;code&gt;--attn-streams&lt;/code&gt;. This only applies to parallel processing though, like done for benchmarking and larger batch workloads. &amp;quot;Single user&amp;quot; performance remains unaffected. In any case, this brings llama.cpp closer to the vLLM performance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1bzti0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751647539,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrmxn7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":67}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1hdtup","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MatterMean5176","can_mod_post":false,"created_utc":1751728975,"send_replies":true,"parent_id":"t3_1lrmxn7","score":0,"author_fullname":"t2_1ju039btvf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hi","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1hdtup","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1hdtup/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751728975,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrmxn7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dekps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1751663662,"send_replies":true,"parent_id":"t1_n1d8fg4","score":3,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Batch processing parallel requests eats up even more RAM than a single session - maybe not the best idea when running a Q2\\\\_XXS and additional RAM should rather be used for a slightly larger and more capable quant.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dekps","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Batch processing parallel requests eats up even more RAM than a single session - maybe not the best idea when running a Q2_XXS and additional RAM should rather be used for a slightly larger and more capable quant.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrmxn7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1dekps/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751663662,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1d8fg4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751661648,"send_replies":true,"parent_id":"t3_1lrmxn7","score":0,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wonder if ik\\\\_llama supports this. Imagine running deepseek-R1 on 128GB of RAM and a 3060 at usable speeds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d8fg4","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if ik_llama supports this. Imagine running deepseek-R1 on 128GB of RAM and a 3060 at usable speeds.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1d8fg4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751661648,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lrmxn7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1c0tbm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Conversation9561","can_mod_post":false,"created_utc":1751647840,"send_replies":true,"parent_id":"t3_1lrmxn7","score":-1,"author_fullname":"t2_jqxb4pte","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wonder if this will make llama.cpp speeds on par with MLX on Mac devices.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c0tbm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if this will make llama.cpp speeds on par with MLX on Mac devices.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/n1c0tbm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751647840,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrmxn7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
