import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have been experimenting building my own UI and having it load and run some Llama models. I have an RTX 4080 (16GB VRAM) and I run the Llama 3.1 13B at 50 tokens/s. I was unable to get Llama 4 17B to run any faster than 0.2 Tokens/s.\\n\\nLlama 3.1 13B is not up to my tasks other than being a standard chatbot. Llama 4 17B gave me some actual good reasoning and completed my tests, but the speed is too slow.\\n\\nI see people on reddit say something along the line \\"You don't need to load the entire model into VRAM, there are many ways to do it as long as you are okay with tokens/s at your read speed\\" and went on suggesting a 32B model on a 4080 to the guy. How?\\n\\nAm I able to load a 32B on my system and have it generate text at read speed (Read speed is relative) but certainly faster than 0.2 tokens/s.\\n\\nMy system:\\n\\n64GB RAM  \\nRyzen 5900X  \\nRTX 4080 (16GB)  \\n\\n\\n  \\nMy goal is to have 2-3 models to switch between. One for generic chatbot stuff, one for high reasoning and one for coding. Al tough, chatbot stuff and reasoning could be one model.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need help","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lx4mad","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_urx6gulz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752233931,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have been experimenting building my own UI and having it load and run some Llama models. I have an RTX 4080 (16GB VRAM) and I run the Llama 3.1 13B at 50 tokens/s. I was unable to get Llama 4 17B to run any faster than 0.2 Tokens/s.&lt;/p&gt;\\n\\n&lt;p&gt;Llama 3.1 13B is not up to my tasks other than being a standard chatbot. Llama 4 17B gave me some actual good reasoning and completed my tests, but the speed is too slow.&lt;/p&gt;\\n\\n&lt;p&gt;I see people on reddit say something along the line &amp;quot;You don&amp;#39;t need to load the entire model into VRAM, there are many ways to do it as long as you are okay with tokens/s at your read speed&amp;quot; and went on suggesting a 32B model on a 4080 to the guy. How?&lt;/p&gt;\\n\\n&lt;p&gt;Am I able to load a 32B on my system and have it generate text at read speed (Read speed is relative) but certainly faster than 0.2 tokens/s.&lt;/p&gt;\\n\\n&lt;p&gt;My system:&lt;/p&gt;\\n\\n&lt;p&gt;64GB RAM&lt;br/&gt;\\nRyzen 5900X&lt;br/&gt;\\nRTX 4080 (16GB)  &lt;/p&gt;\\n\\n&lt;p&gt;My goal is to have 2-3 models to switch between. One for generic chatbot stuff, one for high reasoning and one for coding. Al tough, chatbot stuff and reasoning could be one model.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lx4mad","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Aelexi93","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lx4mad/need_help/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lx4mad/need_help/","subreddit_subscribers":497826,"created_utc":1752233931,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j95pk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2j8bn9","score":1,"author_fullname":"t2_pfi81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, use your gpu. try llama-server or lmstudio for testing. make sure u use cuda runtime","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j95pk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, use your gpu. try llama-server or lmstudio for testing. make sure u use cuda runtime&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4mad","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4mad/need_help/n2j95pk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752235162,"author_flair_text":null,"treatment_tags":[],"created_utc":1752235162,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j8bn9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Aelexi93","can_mod_post":false,"created_utc":1752234833,"send_replies":true,"parent_id":"t1_n2j7ufv","score":1,"author_fullname":"t2_urx6gulz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No wonder it gave me good answers! So 24B is certainly bigger than 17B, am I able to load it in  a way that yields more than 0.2 tokens/s?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j8bn9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No wonder it gave me good answers! So 24B is certainly bigger than 17B, am I able to load it in  a way that yields more than 0.2 tokens/s?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4mad","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4mad/need_help/n2j8bn9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234833,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j7ufv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752234642,"send_replies":true,"parent_id":"t3_1lx4mad","score":1,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama 4 17B is not 17b big. its 17b active parameters. Its either 100b or 400b depending on which one u actually one.  \\nUse a 32b model at q3 and it will be loadable fully into vram. Will be tight though, and quality wont be insane.  \\nI'd suggest you look at 24b models at q4 and ull be just fine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j7ufv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 4 17B is not 17b big. its 17b active parameters. Its either 100b or 400b depending on which one u actually one.&lt;br/&gt;\\nUse a 32b model at q3 and it will be loadable fully into vram. Will be tight though, and quality wont be insane.&lt;br/&gt;\\nI&amp;#39;d suggest you look at 24b models at q4 and ull be just fine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4mad/need_help/n2j7ufv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234642,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4mad","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j8da0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Aelexi93","can_mod_post":false,"created_utc":1752234850,"send_replies":true,"parent_id":"t1_n2j7v14","score":1,"author_fullname":"t2_urx6gulz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will look into it, thanks for reply!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j8da0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will look into it, thanks for reply!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4mad","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4mad/need_help/n2j8da0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234850,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j7v14","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752234648,"send_replies":true,"parent_id":"t3_1lx4mad","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try Mistral Small 3.2 for non-coding and qwen 2.5 coder 14b for coding","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j7v14","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try Mistral Small 3.2 for non-coding and qwen 2.5 coder 14b for coding&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4mad/need_help/n2j7v14/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234648,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4mad","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2joqbe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Longjumpingfish0403","can_mod_post":false,"created_utc":1752240603,"send_replies":true,"parent_id":"t3_1lx4mad","score":1,"author_fullname":"t2_jarttha4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For running larger models like a 32B on your RTX 4080, you might want to explore offloading parts of the model to your CPU to balance the load, which can be done through techniques like model parallelism. Also, using an optimized library like Hugging Face’s Transformers or DeepSpeed can help manage your resources better. Check if they offer features like quantization or mixed precision to reduce memory usage and boost speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2joqbe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For running larger models like a 32B on your RTX 4080, you might want to explore offloading parts of the model to your CPU to balance the load, which can be done through techniques like model parallelism. Also, using an optimized library like Hugging Face’s Transformers or DeepSpeed can help manage your resources better. Check if they offer features like quantization or mixed precision to reduce memory usage and boost speed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4mad/need_help/n2joqbe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4mad","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:a});export{r as default};
