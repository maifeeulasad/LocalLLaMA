import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey Everyone, I’ve been working on a project called Fissure. It is a personal AI server that runs on your own machine. The goal is to make it pretty simple to run a local LLM, access it from your phone or laptop, and keep everything private.   \\n  \\nRight now I’m focused on the desktop app, which runs your chosen model through Ollama and sets up remote access using Tailscale(have been looking into alternatives). Once it's running, you get a private Tailscale url you can plug into a mobile app or anything that can hit an api, and this works across devices as long as they’re on your Tailscale network. Its still pretty early but I’m trying to make this easier than putting together a bunch of tools, more of a one click or couple clicks to set up and get going.  \\n  \\nI'm curious if anyone else would find this useful, and what features or pain points you’d want solved. Id really appreciate any feedback or thoughts.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local AI server with Ollama and Tailscale integration looking for feedback","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwvrev","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.44,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1oqtfoffx3","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752201992,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey Everyone, I’ve been working on a project called Fissure. It is a personal AI server that runs on your own machine. The goal is to make it pretty simple to run a local LLM, access it from your phone or laptop, and keep everything private.   &lt;/p&gt;\\n\\n&lt;p&gt;Right now I’m focused on the desktop app, which runs your chosen model through Ollama and sets up remote access using Tailscale(have been looking into alternatives). Once it&amp;#39;s running, you get a private Tailscale url you can plug into a mobile app or anything that can hit an api, and this works across devices as long as they’re on your Tailscale network. Its still pretty early but I’m trying to make this easier than putting together a bunch of tools, more of a one click or couple clicks to set up and get going.  &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m curious if anyone else would find this useful, and what features or pain points you’d want solved. Id really appreciate any feedback or thoughts.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lwvrev","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Remarkable-Stay-2193","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/","subreddit_subscribers":497504,"created_utc":1752201992,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2k1xhw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Remarkable-Stay-2193","can_mod_post":false,"created_utc":1752244583,"send_replies":true,"parent_id":"t1_n2ih94v","score":1,"author_fullname":"t2_1oqtfoffx3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s a good point, a Docker setup can definitely handle this. I’m just trying to take that same thing and package it into something more \\"out of the box\\" and ready to use, especially for those who don’t want to deal with docker or containers.\\n\\nAnd thank you, I've not heard of tsdproxy before.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2k1xhw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s a good point, a Docker setup can definitely handle this. I’m just trying to take that same thing and package it into something more &amp;quot;out of the box&amp;quot; and ready to use, especially for those who don’t want to deal with docker or containers.&lt;/p&gt;\\n\\n&lt;p&gt;And thank you, I&amp;#39;ve not heard of tsdproxy before.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwvrev","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/n2k1xhw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752244583,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ih94v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"coolkat2103","can_mod_post":false,"created_utc":1752220943,"send_replies":true,"parent_id":"t3_1lwvrev","score":2,"author_fullname":"t2_jfzhvuv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It can all be done in one docker file. Openwebui with ollama and something like tsdproxy all in one docker compose file","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ih94v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It can all be done in one docker file. Openwebui with ollama and something like tsdproxy all in one docker compose file&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/n2ih94v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220943,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwvrev","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2k0epw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Remarkable-Stay-2193","can_mod_post":false,"created_utc":1752244142,"send_replies":true,"parent_id":"t1_n2hqfhn","score":1,"author_fullname":"t2_1oqtfoffx3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama gives you a model and API. This wraps it with Tailscale and cuts down some setup complexity.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2k0epw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama gives you a model and API. This wraps it with Tailscale and cuts down some setup complexity.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwvrev","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/n2k0epw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752244142,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hqfhn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xFatWhiteMan","can_mod_post":false,"created_utc":1752207332,"send_replies":true,"parent_id":"t3_1lwvrev","score":2,"author_fullname":"t2_pz2dkuedp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What functionality does it add over ollama ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hqfhn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What functionality does it add over ollama ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/n2hqfhn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752207332,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwvrev","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hssfw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hawk_7979","can_mod_post":false,"created_utc":1752208360,"send_replies":true,"parent_id":"t3_1lwvrev","score":2,"author_fullname":"t2_85eeoiqtd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can use Tailscale internal tool to expose API to your tailnet securely using Tailscale serve.\\n\\nhttps://tailscale.com/kb/1312/serve\\n\\nJust one line command to setup and get going","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hssfw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can use Tailscale internal tool to expose API to your tailnet securely using Tailscale serve.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://tailscale.com/kb/1312/serve\\"&gt;https://tailscale.com/kb/1312/serve&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Just one line command to setup and get going&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwvrev/local_ai_server_with_ollama_and_tailscale/n2hssfw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752208360,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwvrev","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
