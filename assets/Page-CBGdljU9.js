import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Here\'s a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.\\n\\nThis process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.\\n\\nI\'m sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.\\n\\n\\\\1. Clone the official LiteLLM repo:\\n\\n```sh\\ngit clone https://github.com/BerriAI/litellm.git\\ncd litellm\\n```\\n\\n\\\\2. Create an `.env` file with your OpenRouter API key (make sure to insert your own API key!):\\n\\n```sh\\ncat &lt;&lt;\\\\EOF &gt;.env\\nLITELLM_MASTER_KEY = \\"sk-1234\\"\\n\\n# OpenRouter\\nOPENROUTER_API_KEY = \\"sk-or-v1-â€¦\\" # ðŸš©\\nEOF\\n```\\n\\n\\\\3. Create a `config.yaml` file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):\\n\\n```sh\\ncat &lt;&lt;\\\\EOF &gt;config.yaml\\nmodel_list:\\n  - model_name: \\"anthropic/*\\"\\n    litellm_params:\\n      model: \\"openrouter/qwen/qwen3-coder\\" # Qwen/Qwen3-Coder-480B-A35B-Instruct\\n      max_tokens: 65536\\n      repetition_penalty: 1.05\\n      temperature: 0.7\\n      top_k: 20\\n      top_p: 0.8\\nEOF\\n```\\n\\n\\\\4. Create a `docker-compose.yml` file that loads `config.yaml` (it\'s easier to just create a finished one with all the required changes than to edit the original file):\\n\\n```sh\\ncat &lt;&lt;\\\\EOF &gt;docker-compose.yml\\nservices:\\n  litellm:\\n    build:\\n      context: .\\n      args:\\n        target: runtime\\n    ############################################################################\\n    command:\\n      - \\"--config=/app/config.yaml\\"\\n    container_name: litellm\\n    hostname: litellm\\n    image: ghcr.io/berriai/litellm:main-stable\\n    restart: unless-stopped\\n    volumes:\\n      - ./config.yaml:/app/config.yaml\\n    ############################################################################\\n    ports:\\n      - \\"4000:4000\\" # Map the container port to the host, change the host port if necessary\\n    environment:\\n      DATABASE_URL: \\"postgresql://llmproxy:dbpassword9090@db:5432/litellm\\"\\n      STORE_MODEL_IN_DB: \\"True\\" # allows adding models to proxy via UI\\n    env_file:\\n      - .env # Load local .env file\\n    depends_on:\\n      - db  # Indicates that this service depends on the \'db\' service, ensuring \'db\' starts first\\n    healthcheck:  # Defines the health check configuration for the container\\n      test: [ \\"CMD-SHELL\\", \\"wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1\\" ]  # Command to execute for health check\\n      interval: 30s  # Perform health check every 30 seconds\\n      timeout: 10s   # Health check command times out after 10 seconds\\n      retries: 3     # Retry up to 3 times if health check fails\\n      start_period: 40s  # Wait 40 seconds after container start before beginning health checks\\n\\n  db:\\n    image: postgres:16\\n    restart: always\\n    container_name: litellm_db\\n    environment:\\n      POSTGRES_DB: litellm\\n      POSTGRES_USER: llmproxy\\n      POSTGRES_PASSWORD: dbpassword9090\\n    ports:\\n      - \\"5432:5432\\"\\n    volumes:\\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\\n    healthcheck:\\n      test: [\\"CMD-SHELL\\", \\"pg_isready -d litellm -U llmproxy\\"]\\n      interval: 1s\\n      timeout: 5s\\n      retries: 10\\n\\nvolumes:\\n  postgres_data:\\n    name: litellm_postgres_data # Named volume for Postgres data persistence\\nEOF\\n```\\n\\n\\\\5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):\\n\\n```sh\\ndocker compose up -d --build\\n```\\n\\n\\\\6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (`.zshrc`, `.bashrc`, etc.) for persistence):\\n\\n```sh\\nexport ANTHROPIC_AUTH_TOKEN=sk-1234\\nexport ANTHROPIC_BASE_URL=http://localhost:4000\\nexport ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\\nexport ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\\nexport CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\\n```\\n\\n\\\\7. Start Claude Code and it\'ll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the `/model` command that it\'s using a custom model):\\n\\n```sh\\nclaude\\n```\\n\\n\\\\8. Optional: Add an alias to your shell profile (`.zshrc`, `.bashrc`, etc.) to make it easier to use (e.g. `qlaude` for \\"Claude with Qwen\\"):\\n\\n```sh\\nalias qlaude=\'ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude\'\\n```\\n\\nHave fun and happy coding!\\n\\nPS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"HOWTO: Use Qwen3-Coder (or any other LLM) with Claude Code (via LiteLLM)","link_flair_richtext":[{"e":"text","t":"Tutorial | Guide"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":94,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7ci3s","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":null,"ups":80,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_th129","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Tutorial | Guide","can_mod_post":false,"score":80,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/u6BYojitjaMwq8K3EQg_ygO8c52qAwKJff5aywSVUks.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1753284943,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.&lt;/p&gt;\\n\\n&lt;p&gt;This process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.&lt;/p&gt;\\n\\n&lt;p&gt;\\\\1. Clone the official LiteLLM repo:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;sh\\ngit clone https://github.com/BerriAI/litellm.git\\ncd litellm\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;\\\\2. Create an &lt;code&gt;.env&lt;/code&gt; file with your OpenRouter API key (make sure to insert your own API key!):&lt;/p&gt;\\n\\n&lt;p&gt;```sh\\ncat &amp;lt;&amp;lt;\\\\EOF &amp;gt;.env\\nLITELLM_MASTER_KEY = &amp;quot;sk-1234&amp;quot;&lt;/p&gt;\\n\\n&lt;h1&gt;OpenRouter&lt;/h1&gt;\\n\\n&lt;p&gt;OPENROUTER_API_KEY = &amp;quot;sk-or-v1-â€¦&amp;quot; # ðŸš©\\nEOF\\n```&lt;/p&gt;\\n\\n&lt;p&gt;\\\\3. Create a &lt;code&gt;config.yaml&lt;/code&gt; file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;sh\\ncat &amp;lt;&amp;lt;\\\\EOF &amp;gt;config.yaml\\nmodel_list:\\n  - model_name: &amp;quot;anthropic/*&amp;quot;\\n    litellm_params:\\n      model: &amp;quot;openrouter/qwen/qwen3-coder&amp;quot; # Qwen/Qwen3-Coder-480B-A35B-Instruct\\n      max_tokens: 65536\\n      repetition_penalty: 1.05\\n      temperature: 0.7\\n      top_k: 20\\n      top_p: 0.8\\nEOF\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;\\\\4. Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file that loads &lt;code&gt;config.yaml&lt;/code&gt; (it&amp;#39;s easier to just create a finished one with all the required changes than to edit the original file):&lt;/p&gt;\\n\\n&lt;p&gt;```sh\\ncat &amp;lt;&amp;lt;\\\\EOF &amp;gt;docker-compose.yml\\nservices:\\n  litellm:\\n    build:\\n      context: .\\n      args:\\n        target: runtime\\n    ############################################################################\\n    command:\\n      - &amp;quot;--config=/app/config.yaml&amp;quot;\\n    container_name: litellm\\n    hostname: litellm\\n    image: ghcr.io/berriai/litellm:main-stable\\n    restart: unless-stopped\\n    volumes:\\n      - ./config.yaml:/app/config.yaml\\n    ############################################################################\\n    ports:\\n      - &amp;quot;4000:4000&amp;quot; # Map the container port to the host, change the host port if necessary\\n    environment:\\n      DATABASE_URL: &amp;quot;postgresql://llmproxy:dbpassword9090@db:5432/litellm&amp;quot;\\n      STORE_MODEL_IN_DB: &amp;quot;True&amp;quot; # allows adding models to proxy via UI\\n    env_file:\\n      - .env # Load local .env file\\n    depends_on:\\n      - db  # Indicates that this service depends on the &amp;#39;db&amp;#39; service, ensuring &amp;#39;db&amp;#39; starts first\\n    healthcheck:  # Defines the health check configuration for the container\\n      test: [ &amp;quot;CMD-SHELL&amp;quot;, &amp;quot;wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1&amp;quot; ]  # Command to execute for health check\\n      interval: 30s  # Perform health check every 30 seconds\\n      timeout: 10s   # Health check command times out after 10 seconds\\n      retries: 3     # Retry up to 3 times if health check fails\\n      start_period: 40s  # Wait 40 seconds after container start before beginning health checks&lt;/p&gt;\\n\\n&lt;p&gt;db:\\n    image: postgres:16\\n    restart: always\\n    container_name: litellm_db\\n    environment:\\n      POSTGRES_DB: litellm\\n      POSTGRES_USER: llmproxy\\n      POSTGRES_PASSWORD: dbpassword9090\\n    ports:\\n      - &amp;quot;5432:5432&amp;quot;\\n    volumes:\\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\\n    healthcheck:\\n      test: [&amp;quot;CMD-SHELL&amp;quot;, &amp;quot;pg_isready -d litellm -U llmproxy&amp;quot;]\\n      interval: 1s\\n      timeout: 5s\\n      retries: 10&lt;/p&gt;\\n\\n&lt;p&gt;volumes:\\n  postgres_data:\\n    name: litellm_postgres_data # Named volume for Postgres data persistence\\nEOF\\n```&lt;/p&gt;\\n\\n&lt;p&gt;\\\\5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;sh\\ndocker compose up -d --build\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;\\\\6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) for persistence):&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;sh\\nexport ANTHROPIC_AUTH_TOKEN=sk-1234\\nexport ANTHROPIC_BASE_URL=http://localhost:4000\\nexport ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\\nexport ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\\nexport CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;\\\\7. Start Claude Code and it&amp;#39;ll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the &lt;code&gt;/model&lt;/code&gt; command that it&amp;#39;s using a custom model):&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;sh\\nclaude\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;\\\\8. Optional: Add an alias to your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) to make it easier to use (e.g. &lt;code&gt;qlaude&lt;/code&gt; for &amp;quot;Claude with Qwen&amp;quot;):&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;sh\\nalias qlaude=&amp;#39;ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude&amp;#39;\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Have fun and happy coding!&lt;/p&gt;\\n\\n&lt;p&gt;PS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/5p7u0le68nef1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/5p7u0le68nef1.png?auto=webp&amp;s=d6ee6eb61c270bf74164a4529655ea608dd7d761","width":1682,"height":1130},"resolutions":[{"url":"https://preview.redd.it/5p7u0le68nef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2f9ecbfadf0a585c661b7818dc4d782fd8cb3f3","width":108,"height":72},{"url":"https://preview.redd.it/5p7u0le68nef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5ed170eaa8c71b609cea8209eb4a660ef943d7d","width":216,"height":145},{"url":"https://preview.redd.it/5p7u0le68nef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d361e88e5a29d401767472fe3a197448482be18","width":320,"height":214},{"url":"https://preview.redd.it/5p7u0le68nef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=052b75b74825ad0e1f536d20305b7b06a2c2db8c","width":640,"height":429},{"url":"https://preview.redd.it/5p7u0le68nef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e30fe50b638eda8e2b345142bbdcf622a0cdff2b","width":960,"height":644},{"url":"https://preview.redd.it/5p7u0le68nef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ddb6221e7b7cb0abb8d800206489d39dee05e42","width":1080,"height":725}],"variants":{},"id":"mIuYvF0JIlHjZcMZ6w4mc0OQRmovAhW7m8mjpQFvvxs"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"449b05a6-bf8e-11ed-b4bd-66961e47bd50","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#0079d3","id":"1m7ci3s","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"WolframRavenwolf","discussion_type":null,"num_comments":17,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/","stickied":false,"url":"https://i.redd.it/5p7u0le68nef1.png","subreddit_subscribers":503757,"created_utc":1753284943,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4svqh3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WolframRavenwolf","can_mod_post":false,"created_utc":1753311481,"send_replies":true,"parent_id":"t1_n4siutw","score":3,"author_fullname":"t2_th129","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The consensus among my AI Engineer colleagues is that Claude Code is the best AI code assistant, thanks to the powerful combination of Claude 3 Sonnet, Opus, and the app itself. However, it\'s quite expensive, so being able to use Qwen3-Coder inside the familiar interface is an interesting alternative. This approach allows anyone to experiment with it and find out for themselves how well it suits their needs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4svqh3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The consensus among my AI Engineer colleagues is that Claude Code is the best AI code assistant, thanks to the powerful combination of Claude 3 Sonnet, Opus, and the app itself. However, it&amp;#39;s quite expensive, so being able to use Qwen3-Coder inside the familiar interface is an interesting alternative. This approach allows anyone to experiment with it and find out for themselves how well it suits their needs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4svqh3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753311481,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4siutw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"created_utc":1753307394,"send_replies":true,"parent_id":"t3_1m7ci3s","score":3,"author_fullname":"t2_g8xg6sut","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I thought qwen has their own CLI?  Is Claude code better?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4siutw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I thought qwen has their own CLI?  Is Claude code better?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4siutw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753307394,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ci3s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4svj3h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CtrlAltDelve","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4suf9n","score":2,"author_fullname":"t2_1f1tptkzcs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Whoops, good catch. I went ahead and fixed that :)\\n\\nFor what it\'s worth in the future, the annoying thing about Old Reddit is that it doesn\'t use backticks for code blocks. It uses indentation with four spaces per line that you want to be part of a code block. So I 100% used an LLM to convert. Your backtick code blocks into indentation code blocks.\\n\\nBut I think honestly a Github Gist is a better idea anyway :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4svj3h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whoops, good catch. I went ahead and fixed that :)&lt;/p&gt;\\n\\n&lt;p&gt;For what it&amp;#39;s worth in the future, the annoying thing about Old Reddit is that it doesn&amp;#39;t use backticks for code blocks. It uses indentation with four spaces per line that you want to be part of a code block. So I 100% used an LLM to convert. Your backtick code blocks into indentation code blocks.&lt;/p&gt;\\n\\n&lt;p&gt;But I think honestly a Github Gist is a better idea anyway :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4svj3h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753311418,"author_flair_text":null,"treatment_tags":[],"created_utc":1753311418,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4suf9n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WolframRavenwolf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4s5vmm","score":2,"author_fullname":"t2_th129","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, great idea!\\n\\nBy the way, the git clone URL got messed up and turned into a Markdown link inside the code block. Other than that, it looks good to me.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4suf9n","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, great idea!&lt;/p&gt;\\n\\n&lt;p&gt;By the way, the git clone URL got messed up and turned into a Markdown link inside the code block. Other than that, it looks good to me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4suf9n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753311064,"author_flair_text":null,"treatment_tags":[],"created_utc":1753311064,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s5vmm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CtrlAltDelve","can_mod_post":false,"created_utc":1753303616,"send_replies":true,"parent_id":"t1_n4qf9a9","score":3,"author_fullname":"t2_1f1tptkzcs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Reformatted for Old Reddit users :)\\n\\n---\\n\\nHere\'s a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.\\n\\nThis process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.\\n\\nI\'m sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.\\n\\n**1. Clone the official LiteLLM repo:**\\n\\n\\n    git clone https://github.com/BerriAI/litellm.git\\n    cd litellm\\n\\n\\n**2. Create an `.env` file with your OpenRouter API key (make sure to insert your own API key\\\\!):**\\n\\n\\n    cat &lt;&lt;\\\\EOF &gt;.env\\n    LITELLM_MASTER_KEY = \\"sk-1234\\"\\n    # OpenRouter\\n    OPENROUTER_API_KEY = \\"sk-or-v1-â€¦\\" # ðŸš©\\n    EOF\\n\\n\\n**3. Create a `config.yaml` file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):**\\n\\n\\n    cat &lt;&lt;\\\\EOF &gt;config.yaml\\n    model_list:\\n      - model_name: \\"anthropic/*\\"\\n        litellm_params:\\n          model: \\"openrouter/qwen/qwen3-coder\\" # Qwen/Qwen3-Coder-480B-A35B-Instruct\\n          max_tokens: 65536\\n          repetition_penalty: 1.05\\n          temperature: 0.7\\n          top_k: 20\\n          top_p: 0.8\\n    EOF\\n\\n\\n**4. Create a `docker-compose.yml` file that loads `config.yaml` (it\'s easier to just create a finished one with all the required changes than to edit the original file):**\\n\\n\\n    cat &lt;&lt;\\\\EOF &gt;docker-compose.yml\\n    services:\\n      litellm:\\n        build:\\n          context: .\\n          args:\\n            target: runtime\\n        ############################################################################\\n        command:\\n          - \\"--config=/app/config.yaml\\"\\n        container_name: litellm\\n        hostname: litellm\\n        image: ghcr.io/berriai/litellm:main-stable\\n        restart: unless-stopped\\n        volumes:\\n          - ./config.yaml:/app/config.yaml\\n        ############################################################################\\n        ports:\\n          - \\"4000:4000\\" # Map the container port to the host, change the host port if necessary\\n        environment:\\n          DATABASE_URL: \\"postgresql://llmproxy:dbpassword9090@db:5432/litellm\\"\\n          STORE_MODEL_IN_DB: \\"True\\" # allows adding models to proxy via UI\\n        env_file:\\n          - .env # Load local .env file\\n        depends_on:\\n          - db # Indicates that this service depends on the \'db\' service, ensuring \'db\' starts first\\n        healthcheck: # Defines the health check configuration for the container\\n          test: [ \\"CMD-SHELL\\", \\"wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1\\" ] # Command to execute for health check\\n          interval: 30s # Perform health check every 30 seconds\\n          timeout: 10s # Health check command times out after 10 seconds\\n          retries: 3 # Retry up to 3 times if health check fails\\n          start_period: 40s # Wait 40 seconds after container start before beginning health checks\\n    \\n      db:\\n        image: postgres:16\\n        restart: always\\n        container_name: litellm_db\\n        environment:\\n          POSTGRES_DB: litellm\\n          POSTGRES_USER: llmproxy\\n          POSTGRES_PASSWORD: dbpassword9090\\n        ports:\\n          - \\"5432:5432\\"\\n        volumes:\\n          - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\\n        healthcheck:\\n          test: [\\"CMD-SHELL\\", \\"pg_isready -d litellm -U llmproxy\\"]\\n          interval: 1s\\n          timeout: 5s\\n          retries: 10\\n    \\n    volumes:\\n      postgres_data:\\n        name: litellm_postgres_data # Named volume for Postgres data persistence\\n    EOF\\n\\n\\n**5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):**\\n\\n\\n    docker compose up -d --build\\n\\n\\n**6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (`.zshrc`, `.bashrc`, etc.) for persistence):**\\n\\n\\n    export ANTHROPIC_AUTH_TOKEN=sk-1234\\n    export ANTHROPIC_BASE_URL=http://localhost:4000\\n    export ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\\n    export ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\\n    export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\\n\\n\\n**7. Start Claude Code and it\'ll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the `/model` command that it\'s using a custom model):**\\n\\n\\n    claude\\n\\n\\n**8. Optional: Add an alias to your shell profile (`.zshrc`, `.bashrc`, etc.) to make it easier to use (e.g. `qlaude` for \\"Claude with Qwen\\"):**\\n\\n\\n    alias qlaude=\'ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude\'\\n\\n\\nHave fun and happy coding!\\n\\nPS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.","edited":1753311370,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s5vmm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reformatted for Old Reddit users :)&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Here&amp;#39;s a simple way for Claude Code users to switch from the costly Claude models to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine.&lt;/p&gt;\\n\\n&lt;p&gt;This process is quite universal and can be easily adapted to suit your needs. Feel free to explore other models (including local ones) as well as different providers and coding agents.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m sharing what works for me. This guide is set up so you can just copy and paste the commands into your terminal.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;1. Clone the official LiteLLM repo:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/BerriAI/litellm.git\\ncd litellm\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;2. Create an &lt;code&gt;.env&lt;/code&gt; file with your OpenRouter API key (make sure to insert your own API key!):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;\\\\EOF &amp;gt;.env\\nLITELLM_MASTER_KEY = &amp;quot;sk-1234&amp;quot;\\n# OpenRouter\\nOPENROUTER_API_KEY = &amp;quot;sk-or-v1-â€¦&amp;quot; # ðŸš©\\nEOF\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;3. Create a &lt;code&gt;config.yaml&lt;/code&gt; file that replaces Anthropic models with Qwen3-Coder (with all the recommended parameters):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;\\\\EOF &amp;gt;config.yaml\\nmodel_list:\\n  - model_name: &amp;quot;anthropic/*&amp;quot;\\n    litellm_params:\\n      model: &amp;quot;openrouter/qwen/qwen3-coder&amp;quot; # Qwen/Qwen3-Coder-480B-A35B-Instruct\\n      max_tokens: 65536\\n      repetition_penalty: 1.05\\n      temperature: 0.7\\n      top_k: 20\\n      top_p: 0.8\\nEOF\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;4. Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file that loads &lt;code&gt;config.yaml&lt;/code&gt; (it&amp;#39;s easier to just create a finished one with all the required changes than to edit the original file):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;\\\\EOF &amp;gt;docker-compose.yml\\nservices:\\n  litellm:\\n    build:\\n      context: .\\n      args:\\n        target: runtime\\n    ############################################################################\\n    command:\\n      - &amp;quot;--config=/app/config.yaml&amp;quot;\\n    container_name: litellm\\n    hostname: litellm\\n    image: ghcr.io/berriai/litellm:main-stable\\n    restart: unless-stopped\\n    volumes:\\n      - ./config.yaml:/app/config.yaml\\n    ############################################################################\\n    ports:\\n      - &amp;quot;4000:4000&amp;quot; # Map the container port to the host, change the host port if necessary\\n    environment:\\n      DATABASE_URL: &amp;quot;postgresql://llmproxy:dbpassword9090@db:5432/litellm&amp;quot;\\n      STORE_MODEL_IN_DB: &amp;quot;True&amp;quot; # allows adding models to proxy via UI\\n    env_file:\\n      - .env # Load local .env file\\n    depends_on:\\n      - db # Indicates that this service depends on the &amp;#39;db&amp;#39; service, ensuring &amp;#39;db&amp;#39; starts first\\n    healthcheck: # Defines the health check configuration for the container\\n      test: [ &amp;quot;CMD-SHELL&amp;quot;, &amp;quot;wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1&amp;quot; ] # Command to execute for health check\\n      interval: 30s # Perform health check every 30 seconds\\n      timeout: 10s # Health check command times out after 10 seconds\\n      retries: 3 # Retry up to 3 times if health check fails\\n      start_period: 40s # Wait 40 seconds after container start before beginning health checks\\n\\n  db:\\n    image: postgres:16\\n    restart: always\\n    container_name: litellm_db\\n    environment:\\n      POSTGRES_DB: litellm\\n      POSTGRES_USER: llmproxy\\n      POSTGRES_PASSWORD: dbpassword9090\\n    ports:\\n      - &amp;quot;5432:5432&amp;quot;\\n    volumes:\\n      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts\\n    healthcheck:\\n      test: [&amp;quot;CMD-SHELL&amp;quot;, &amp;quot;pg_isready -d litellm -U llmproxy&amp;quot;]\\n      interval: 1s\\n      timeout: 5s\\n      retries: 10\\n\\nvolumes:\\n  postgres_data:\\n    name: litellm_postgres_data # Named volume for Postgres data persistence\\nEOF\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;docker compose up -d --build\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLLM (remember to execute this before starting Claude Code or include it in your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) for persistence):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;export ANTHROPIC_AUTH_TOKEN=sk-1234\\nexport ANTHROPIC_BASE_URL=http://localhost:4000\\nexport ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder\\nexport ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder\\nexport CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry, error reporting, and auto-updates\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;7. Start Claude Code and it&amp;#39;ll use Qwen3-Coder via OpenRouter instead of the expensive Claude models (you can check with the &lt;code&gt;/model&lt;/code&gt; command that it&amp;#39;s using a custom model):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;claude\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;8. Optional: Add an alias to your shell profile (&lt;code&gt;.zshrc&lt;/code&gt;, &lt;code&gt;.bashrc&lt;/code&gt;, etc.) to make it easier to use (e.g. &lt;code&gt;qlaude&lt;/code&gt; for &amp;quot;Claude with Qwen&amp;quot;):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;alias qlaude=&amp;#39;ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder claude&amp;#39;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Have fun and happy coding!&lt;/p&gt;\\n\\n&lt;p&gt;PS: There are other ways to do this using dedicated Claude Code proxies, of which there are quite a few on GitHub. Before implementing this with LiteLLM, I reviewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track record and a large user base, which is why I chose LiteLLM. Open Source offers many options, so feel free to explore other projects and find what works best for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4s5vmm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753303616,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qf9a9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"WolframRavenwolf","can_mod_post":false,"created_utc":1753286280,"send_replies":true,"parent_id":"t3_1m7ci3s","score":5,"author_fullname":"t2_th129","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Old Reddit doesn\'t display the Markdown code blocks correctly. Please use New Reddit or check out the Gist I posted here: [https://gist.github.com/WolframRavenwolf/0ee85a65b10e1a442e4bf65f848d6b01](https://gist.github.com/WolframRavenwolf/0ee85a65b10e1a442e4bf65f848d6b01)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qf9a9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Old Reddit doesn&amp;#39;t display the Markdown code blocks correctly. Please use New Reddit or check out the Gist I posted here: &lt;a href=\\"https://gist.github.com/WolframRavenwolf/0ee85a65b10e1a442e4bf65f848d6b01\\"&gt;https://gist.github.com/WolframRavenwolf/0ee85a65b10e1a442e4bf65f848d6b01&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4qf9a9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753286280,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ci3s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vifgo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WolframRavenwolf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4uqiys","score":1,"author_fullname":"t2_th129","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure, if you don\'t mind sending your prompts and code to China. Which isn\'t bad per se, just something to be aware of! Also ensure you have permission when working on an employer\'s codebase, just as you would with any other online service you use.\\n\\nI also haven\'t seen a clear note on whether these alternatives use the recommended inference settings. Since these settings depend on the model, they need to be configured somewhere. With the LiteLLM solution, you have them in your config, allowing you to change them anytime, especially when using a different model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vifgo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, if you don&amp;#39;t mind sending your prompts and code to China. Which isn&amp;#39;t bad per se, just something to be aware of! Also ensure you have permission when working on an employer&amp;#39;s codebase, just as you would with any other online service you use.&lt;/p&gt;\\n\\n&lt;p&gt;I also haven&amp;#39;t seen a clear note on whether these alternatives use the recommended inference settings. Since these settings depend on the model, they need to be configured somewhere. With the LiteLLM solution, you have them in your config, allowing you to change them anytime, especially when using a different model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4vifgo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753352763,"author_flair_text":null,"treatment_tags":[],"created_utc":1753352763,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uqiys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sb6_6_6_6","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4qlisg","score":2,"author_fullname":"t2_dsfdrhveo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can use it directly with Alibaba Cloud and Claude Code. This screenshot shows their setup for Claude Code.\\n\\nhttps://preview.redd.it/i21p37kzjref1.jpeg?width=1143&amp;format=pjpg&amp;auto=webp&amp;s=efe8d8f6d00ef9473e96818c0f79c380dbbd8197","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4uqiys","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can use it directly with Alibaba Cloud and Claude Code. This screenshot shows their setup for Claude Code.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/i21p37kzjref1.jpeg?width=1143&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efe8d8f6d00ef9473e96818c0f79c380dbbd8197\\"&gt;https://preview.redd.it/i21p37kzjref1.jpeg?width=1143&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efe8d8f6d00ef9473e96818c0f79c380dbbd8197&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4uqiys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753337319,"media_metadata":{"i21p37kzjref1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":98,"x":108,"u":"https://preview.redd.it/i21p37kzjref1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0328ae2c22cc8d5a4d0d37b34544a48c418f4de8"},{"y":196,"x":216,"u":"https://preview.redd.it/i21p37kzjref1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a14000c62ad3c3de9bc4ff2c8c6c8c0e67a8b458"},{"y":291,"x":320,"u":"https://preview.redd.it/i21p37kzjref1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7faccbcc3ce89648ca630b1c2f44773ff3f3112"},{"y":582,"x":640,"u":"https://preview.redd.it/i21p37kzjref1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=06f00ddcbcc533f3e5ee5827d149d57da273128d"},{"y":874,"x":960,"u":"https://preview.redd.it/i21p37kzjref1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=437053e0d80aa62ef6e40797c67f13a615a95d92"},{"y":983,"x":1080,"u":"https://preview.redd.it/i21p37kzjref1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f08a3f777d167a327f9a66b16269658873b8a0f8"}],"s":{"y":1041,"x":1143,"u":"https://preview.redd.it/i21p37kzjref1.jpeg?width=1143&amp;format=pjpg&amp;auto=webp&amp;s=efe8d8f6d00ef9473e96818c0f79c380dbbd8197"},"id":"i21p37kzjref1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1753337319,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qlisg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"WolframRavenwolf","can_mod_post":false,"created_utc":1753288031,"send_replies":true,"parent_id":"t1_n4qfjqf","score":4,"author_fullname":"t2_th129","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I currently have two 3090 GPUs with a total of 48 GB VRAM, so I\'m running Qwen3-Coder via OpenRouter for now. Qwen will soon release a smaller version, which could be a local alternative. Then it\'s just a matter of changing the model config in LiteLLM to point to a local OpenAI-compatible API endpoint.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qlisg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I currently have two 3090 GPUs with a total of 48 GB VRAM, so I&amp;#39;m running Qwen3-Coder via OpenRouter for now. Qwen will soon release a smaller version, which could be a local alternative. Then it&amp;#39;s just a matter of changing the model config in LiteLLM to point to a local OpenAI-compatible API endpoint.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4qlisg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753288031,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qfjqf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"orliesaurus","can_mod_post":false,"created_utc":1753286359,"send_replies":true,"parent_id":"t3_1m7ci3s","score":3,"author_fullname":"t2_czrsv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey Wolfram, thank you so much for sharing, this is a nice step by step write up. what GPU are you running this on?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qfjqf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey Wolfram, thank you so much for sharing, this is a nice step by step write up. what GPU are you running this on?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4qfjqf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753286359,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ci3s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4r8h6w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Acrobatic_Cat_3448","can_mod_post":false,"created_utc":1753294239,"send_replies":true,"parent_id":"t3_1m7ci3s","score":2,"author_fullname":"t2_133m0xy6vg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks. Curious - how does it fare vs aider?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4r8h6w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. Curious - how does it fare vs aider?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4r8h6w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753294239,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ci3s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vgl4y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WolframRavenwolf","can_mod_post":false,"created_utc":1753351814,"send_replies":true,"parent_id":"t1_n4ul6ph","score":1,"author_fullname":"t2_th129","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure. Just append \\":free\\" to the model name in `config.yaml`:\\n\\n    model: \\"openrouter/qwen/qwen3-coder:free\\" # Qwen/Qwen3-Coder-480B-A35B-Instruct\\n\\nJust be aware of rate limits and privacy implications: Free endpoints may log, retain, or train on your prompts/code.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vgl4y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure. Just append &amp;quot;:free&amp;quot; to the model name in &lt;code&gt;config.yaml&lt;/code&gt;:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;model: &amp;quot;openrouter/qwen/qwen3-coder:free&amp;quot; # Qwen/Qwen3-Coder-480B-A35B-Instruct\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Just be aware of rate limits and privacy implications: Free endpoints may log, retain, or train on your prompts/code.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4vgl4y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753351814,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ul6ph","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"First-Ad7059","can_mod_post":false,"created_utc":1753334594,"send_replies":true,"parent_id":"t3_1m7ci3s","score":2,"author_fullname":"t2_7108luqsr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"can i also use the free qwen version if yes then what will be the config file??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ul6ph","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;can i also use the free qwen version if yes then what will be the config file??&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4ul6ph/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753334594,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ci3s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vj2t2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WolframRavenwolf","can_mod_post":false,"created_utc":1753353095,"send_replies":true,"parent_id":"t1_n4v0yq3","score":1,"author_fullname":"t2_th129","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, that\'s very helpful information! Editing your IDE\'s terminal settings isn\'t necessary if you set the environment variables globally in your shell profile, but it\'s a perfect solution when you want to avoid that kind of persistence yet still wish to use the Claude button in your IDE.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vj2t2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, that&amp;#39;s very helpful information! Editing your IDE&amp;#39;s terminal settings isn&amp;#39;t necessary if you set the environment variables globally in your shell profile, but it&amp;#39;s a perfect solution when you want to avoid that kind of persistence yet still wish to use the Claude button in your IDE.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4vj2t2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753353095,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4v0yq3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"krazzmann","can_mod_post":false,"created_utc":1753343022,"send_replies":true,"parent_id":"t3_1m7ci3s","score":3,"author_fullname":"t2_lb5zquls","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I actually installed litellm system wide with uv \\\\`uv tool installl litellm\\\\[proxy\\\\]\\\\`. Then you can also add it to your system init process to start it at boot time.\\n\\nIf you want to use the VS Code extension with this Qwen hack, then edit your VS Code settings.json and add :\\n\\n        \\"terminal.integrated.env.osx\\": {\\n            \\"ANTHROPIC_API_KEY\\": \\"sk-1234\\",\\n            \\"ANTHROPIC_BASE_URL\\": \\"http://localhost:4000\\",\\n            \\"ANTHROPIC_MODEL\\": \\"openrouter/qwen/qwen3-coder\\",\\n            \\"ANTHROPIC_SMALL_FAST_MODEL\\": \\"openrouter/qwen/qwen3-coder\\",\\n            \\"CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC\\": \\"1\\"\\n        }\\n\\n\\\\`terminal.integrated.env.linux\\\\` or \\\\`terminal.integrated.env.windows\\\\` respectively","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v0yq3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I actually installed litellm system wide with uv `uv tool installl litellm[proxy]`. Then you can also add it to your system init process to start it at boot time.&lt;/p&gt;\\n\\n&lt;p&gt;If you want to use the VS Code extension with this Qwen hack, then edit your VS Code settings.json and add :&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;    &amp;quot;terminal.integrated.env.osx&amp;quot;: {\\n        &amp;quot;ANTHROPIC_API_KEY&amp;quot;: &amp;quot;sk-1234&amp;quot;,\\n        &amp;quot;ANTHROPIC_BASE_URL&amp;quot;: &amp;quot;http://localhost:4000&amp;quot;,\\n        &amp;quot;ANTHROPIC_MODEL&amp;quot;: &amp;quot;openrouter/qwen/qwen3-coder&amp;quot;,\\n        &amp;quot;ANTHROPIC_SMALL_FAST_MODEL&amp;quot;: &amp;quot;openrouter/qwen/qwen3-coder&amp;quot;,\\n        &amp;quot;CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC&amp;quot;: &amp;quot;1&amp;quot;\\n    }\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;`terminal.integrated.env.linux` or `terminal.integrated.env.windows` respectively&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4v0yq3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753343022,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ci3s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vhqld","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WolframRavenwolf","can_mod_post":false,"created_utc":1753352412,"send_replies":true,"parent_id":"t1_n4tcaw8","score":1,"author_fullname":"t2_th129","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That\'s one of the dedicated Claude Code proxies on GitHub I mentioned in the PS. It doesn\'t seem to support the recommended inference parameters (temperature, top_k, top_p, etc.), which are specific to the model rather than the provider. This results in suboptimal settings. That\'s a key reason I chose LiteLLM, where you have complete control over these parameters.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vhqld","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s one of the dedicated Claude Code proxies on GitHub I mentioned in the PS. It doesn&amp;#39;t seem to support the recommended inference parameters (temperature, top_k, top_p, etc.), which are specific to the model rather than the provider. This results in suboptimal settings. That&amp;#39;s a key reason I chose LiteLLM, where you have complete control over these parameters.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ci3s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4vhqld/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753352412,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tcaw8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redditisunproductive","can_mod_post":false,"created_utc":1753317016,"send_replies":true,"parent_id":"t3_1m7ci3s","score":1,"author_fullname":"t2_19353jsswd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is easier: https://github.com/musistudio/claude-code-router","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tcaw8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is easier: &lt;a href=\\"https://github.com/musistudio/claude-code-router\\"&gt;https://github.com/musistudio/claude-code-router&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any_other_llm_with_claude/n4tcaw8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753317016,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ci3s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>e.jsx(t,{data:l});export{r as default};
