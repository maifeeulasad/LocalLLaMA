import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi there guys, hope you're having a good day!\\n\\nAfter latest improvements on ik llamacpp, [https://github.com/ikawrakow/ik\\\\_llama.cpp/commits/main/](https://github.com/ikawrakow/ik_llama.cpp/commits/main/), I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I'm testing.\\n\\nMy setup is:\\n\\n* AMD Ryzen 7 7800X3D\\n* 192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s\\n* 3 1600W PSUs (Corsair 1600i)\\n* AM5 MSI Carbon X670E\\n* 5090/5090 at PCIe X8/X8 5.0\\n* 4090/4090 at PCIe X4/X4 4.0\\n* 3090/3090 at PCIe X4/X4 4.0\\n* A6000 at PCIe X4 4.0.\\n* Fedora Linux 41 (instead of 42 just because I'm lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)\\n* SATA and USB-&gt;M2 Storage\\n\\nThe benchmarks are based on mostly, R1-0528, BUT it has the same size and it's quants on V3-0324 and TNG-R1T2-Chimera.\\n\\nI have tested the next models:\\n\\n* [unsloth](https://huggingface.co/unsloth) DeepSeek Q2\\\\_K\\\\_XL:\\n   * llm\\\\_load\\\\_print\\\\_meta: model size       = 233.852 GiB (2.994 BPW)\\n* [unsloth](https://huggingface.co/unsloth) DeepSeek IQ3\\\\_XXS:\\n   * llm\\\\_load\\\\_print\\\\_meta: model size       = 254.168 GiB (3.254 BPW)\\n* [unsloth](https://huggingface.co/unsloth) DeepSeek Q3\\\\_K\\\\_XL:\\n   * llm\\\\_load\\\\_print\\\\_meta: model size       = 275.576 GiB (3.528 BPW)\\n* [ubergarm](https://huggingface.co/ubergarm) DeepSeek IQ3\\\\_KS:\\n   * llm\\\\_load\\\\_print\\\\_meta: model size       = 281.463 GiB (3.598 BPW)\\n* [unsloth](https://huggingface.co/unsloth) DeepSeek IQ4\\\\_XS:\\n   * llm\\\\_load\\\\_print\\\\_meta: model size       = 333.130 GiB (4.264 BPW)\\n\\nEach model may have been tested on different formats. Q2\\\\_K\\\\_XL and IQ3\\\\_XXS has less info, but the rest have a lot more. So here we go!\\n\\n# unsloth DeepSeek Q2_K_XL\\n\\nRunning the model with:\\n\\n    ./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf' \\\\\\n    -c 32768 --no-mmap -ngl 999 \\\\\\n    -ot \\"blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0\\" \\\\\\n    -ot \\"blk.(8|9|10|11).ffn.=CUDA1\\" \\\\\\n    -ot \\"blk.(12|13|14|15).ffn.=CUDA2\\" \\\\\\n    -ot \\"blk.(16|17|18|19|20).ffn.=CUDA3\\" \\\\\\n    -ot \\"blk.(21|22|23|24).ffn.=CUDA4\\" \\\\\\n    -ot \\"blk.(25|26|27|28).ffn.=CUDA5\\" \\\\\\n    -ot \\"blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6\\" \\\\\\n    -ot exps=CPU \\\\\\n    -fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe\\n\\nI get:\\n\\nmain: n\\\\_kv\\\\_max = 32768, n\\\\_batch = 5120, n\\\\_ubatch = 5120, flash\\\\_attn = 1, n\\\\_gpu\\\\_layers = 999, n\\\\_threads = 8, n\\\\_threads\\\\_batch = 8\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  5120 |   1280 |      0 |   12.481 |   410.21 |  104.088 |    12.30 |\\n    |  5120 |   1280 |   5120 |   14.630 |   349.98 |  109.724 |    11.67 |\\n    |  5120 |   1280 |  10240 |   17.167 |   298.25 |  112.938 |    11.33 |\\n    |  5120 |   1280 |  15360 |   20.008 |   255.90 |  119.037 |    10.75 |\\n    |  5120 |   1280 |  20480 |   22.444 |   228.12 |  122.706 |    10.43 |\\n\\n[Perf comparison \\\\(ignore 4096 as I forgor to save the perf\\\\)](https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7)\\n\\nQ2\\\\_K\\\\_XL performs really good for a system like this! And it's performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it's at 3bpw.\\n\\n# unsloth DeepSeek IQ3_XXS\\n\\nRunning the model with:\\n\\n    ./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf' \\\\\\n    -c 32768 --no-mmap -ngl 999 \\\\\\n    -ot \\"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\\" \\\\\\n    -ot \\"blk.(7|8|9|10).ffn.=CUDA1\\" \\\\\\n    -ot \\"blk.(11|12|13|14).ffn.=CUDA2\\" \\\\\\n    -ot \\"blk.(15|16|17|18|19).ffn.=CUDA3\\" \\\\\\n    -ot \\"blk.(20|21|22|23).ffn.=CUDA4\\" \\\\\\n    -ot \\"blk.(24|25|26|27).ffn.=CUDA5\\" \\\\\\n    -ot \\"blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6\\" \\\\\\n    -ot exps=CPU \\\\\\n    -fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe\\n\\nI get\\n\\n    Small test for this one!\\n    \\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  4096 |   1024 |      0 |   10.671 |   383.83 |  117.496 |     8.72 |\\n    |  4096 |   1024 |   4096 |   11.322 |   361.77 |  120.192 |     8.52 |\\n\\nhttps://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=34fa15b35573c0d7ce936d9da953d4d483320902\\n\\nSorry on this one to have few data! IQ3\\\\_XXS quality is really good for it's size.\\n\\n# unsloth DeepSeek Q3_K_XL\\n\\nNow we enter a bigger territory. Note that you will notice Q3\\\\_K\\\\_XL being faster than IQ3\\\\_XXS, despite being bigger.\\n\\nRunning the faster PP one with:\\n\\n    ./llama-server -m '/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf' \\\\\\n    -c 32768 --no-mmap -ngl 999 \\\\\\n    -ot \\"blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0\\" \\\\\\n    -ot \\"blk.(8|9|10|11).ffn.=CUDA1\\" \\\\\\n    -ot \\"blk.(12|13|14|15).ffn.=CUDA2\\" \\\\\\n    -ot \\"blk.(16|17|18|19|20).ffn.=CUDA3\\" \\\\\\n    -ot \\"blk.(21|22|23).ffn.=CUDA4\\" \\\\\\n    -ot \\"blk.(24|25|26).ffn.=CUDA5\\" \\\\\\n    -ot \\"blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6\\" \\\\\\n    -ot exps=CPU \\\\\\n    -fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256\\n\\nResults look like this:\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  2560 |    640 |      0 |    9.781 |   261.72 |   65.367 |     9.79 |\\n    |  2560 |    640 |   2560 |   10.048 |   254.78 |   65.824 |     9.72 |\\n    |  2560 |    640 |   5120 |   10.625 |   240.93 |   66.134 |     9.68 |\\n    |  2560 |    640 |   7680 |   11.167 |   229.24 |   67.225 |     9.52 |\\n    |  2560 |    640 |  10240 |   12.268 |   208.68 |   67.475 |     9.49 |\\n    |  2560 |    640 |  12800 |   13.433 |   190.58 |   68.743 |     9.31 |\\n    |  2560 |    640 |  15360 |   14.564 |   175.78 |   69.585 |     9.20 |\\n    |  2560 |    640 |  17920 |   15.734 |   162.70 |   70.589 |     9.07 |\\n    |  2560 |    640 |  20480 |   16.889 |   151.58 |   72.524 |     8.82 |\\n    |  2560 |    640 |  23040 |   18.100 |   141.43 |   74.534 |     8.59 |\\n\\nWith more layers on GPU, but smaller batch size, I get\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  2048 |    512 |      0 |    9.017 |   227.12 |   50.612 |    10.12 |\\n    |  2048 |    512 |   2048 |    9.113 |   224.73 |   51.027 |    10.03 |\\n    |  2048 |    512 |   4096 |    9.436 |   217.05 |   51.864 |     9.87 |\\n    |  2048 |    512 |   6144 |    9.680 |   211.56 |   52.818 |     9.69 |\\n    |  2048 |    512 |   8192 |    9.984 |   205.12 |   53.354 |     9.60 |\\n    |  2048 |    512 |  10240 |   10.349 |   197.90 |   53.896 |     9.50 |\\n    |  2048 |    512 |  12288 |   10.936 |   187.27 |   54.600 |     9.38 |\\n    |  2048 |    512 |  14336 |   11.688 |   175.22 |   55.150 |     9.28 |\\n    |  2048 |    512 |  16384 |   12.419 |   164.91 |   55.852 |     9.17 |\\n    |  2048 |    512 |  18432 |   13.113 |   156.18 |   56.436 |     9.07 |\\n    |  2048 |    512 |  20480 |   13.871 |   147.65 |   56.823 |     9.01 |\\n    |  2048 |    512 |  22528 |   14.594 |   140.33 |   57.590 |     8.89 |\\n    |  2048 |    512 |  24576 |   15.335 |   133.55 |   58.278 |     8.79 |\\n    |  2048 |    512 |  26624 |   16.073 |   127.42 |   58.723 |     8.72 |\\n    |  2048 |    512 |  28672 |   16.794 |   121.95 |   59.553 |     8.60 |\\n    |  2048 |    512 |  30720 |   17.522 |   116.88 |   59.921 |     8.54 |\\n\\nAnd with less GPU layers on GPU, but higher batch size, I get\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  4096 |   1024 |      0 |   12.005 |   341.19 |  111.632 |     9.17 |\\n    |  4096 |   1024 |   4096 |   12.515 |   327.28 |  138.930 |     7.37 |\\n    |  4096 |   1024 |   8192 |   13.389 |   305.91 |  118.220 |     8.66 |\\n    |  4096 |   1024 |  12288 |   15.018 |   272.74 |  119.289 |     8.58 |\\n\\nSo then, performance for different batch sizes and layers, looks like this:\\n\\n[Higher ub\\\\/b is because I ended the test earlier!](https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1)\\n\\nSo you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.\\n\\n# ubergarm DeepSeek IQ3_KS ([TNG-R1T2-Chimera](https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF))\\n\\nThis one is really good! And it has some more optimizations that may apply more on iklcpp.\\n\\nRunning this one with:\\n\\n    ./llama-server -m '/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf' \\\\\\n    -c 32768 --no-mmap -ngl 999 \\\\\\n    -ot \\"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\\" \\\\\\n    -ot \\"blk.(7|8|9).ffn.=CUDA1\\" \\\\\\n    -ot \\"blk.(10|11|12).ffn.=CUDA2\\" \\\\\\n    -ot \\"blk.(13|14|15|16).ffn.=CUDA3\\" \\\\\\n    -ot \\"blk.(17|18|19).ffn.=CUDA4\\" \\\\\\n    -ot \\"blk.(20|21|22).ffn.=CUDA5\\" \\\\\\n    -ot \\"blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6\\" \\\\\\n    -ot exps=CPU \\\\\\n    -fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256\\n\\nI  get\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  6144 |   1536 |      0 |   15.406 |   398.81 |  174.929 |     8.78 |\\n    |  6144 |   1536 |   6144 |   18.289 |   335.94 |  180.393 |     8.51 |\\n    |  6144 |   1536 |  12288 |   22.229 |   276.39 |  186.113 |     8.25 |\\n    |  6144 |   1536 |  18432 |   24.533 |   250.44 |  191.037 |     8.04 |\\n    |  6144 |   1536 |  24576 |   28.122 |   218.48 |  196.268 |     7.83 |\\n\\nOr 8192 batch size/ubatch size, I get\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  8192 |   2048 |      0 |   20.147 |   406.61 |  232.476 |     8.81 |\\n    |  8192 |   2048 |   8192 |   26.009 |   314.97 |  242.648 |     8.44 |\\n    |  8192 |   2048 |  16384 |   32.628 |   251.07 |  253.309 |     8.09 |\\n    |  8192 |   2048 |  24576 |   39.010 |   210.00 |  264.415 |     7.75 |\\n\\nSo the graph looks like this\\n\\nhttps://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf\\n\\nAgain, this model is really good, and really fast! Totally recommended.\\n\\n# unsloth DeepSeek IQ4_XS\\n\\nAt this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.\\n\\nRunning this model with the best balance with:\\n\\n    ./llama-sweep-bench -m '/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf' -c 32768 --no-mmap -ngl 999 \\\\\\n    -ot \\"blk.(0|1|2|3|4|5|6).ffn.=CUDA0\\" \\\\\\n    -ot \\"blk.(7|8|9).ffn.=CUDA1\\" \\\\\\n    -ot \\"blk.(10|11|12).ffn.=CUDA2\\" \\\\\\n    -ot \\"blk.(13|14|15|16).ffn.=CUDA3\\" \\\\\\n    -ot \\"blk.(17|18|19).ffn.=CUDA4\\" \\\\\\n    -ot \\"blk.(20|21|22).ffn.=CUDA5\\" \\\\\\n    -ot \\"blk.(23|24|25|26|27|28|29).ffn.=CUDA6\\" \\\\\\n    -ot \\"blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1\\" \\\\\\n    -ot \\"blk.30.ffn_gate_exps.weight=CUDA1\\" \\\\\\n    -ot \\"blk.30.ffn_down_exps.weight=CUDA2\\" \\\\\\n    -ot \\"blk.30.ffn_up_exps.weight=CUDA4\\" \\\\\\n    -ot \\"blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5\\" \\\\\\n    -ot \\"blk.31.ffn_gate_exps.weight=CUDA5\\" \\\\\\n    -ot \\"blk.31.ffn_down_exps.weight=CUDA0\\" \\\\\\n    -ot \\"blk.31.ffn_up_exps.weight=CUDA3\\" \\\\\\n    -ot \\"blk.32.ffn_gate_exps.weight=CUDA1\\" \\\\\\n    -ot \\"blk.32.ffn_down_exps.weight=CUDA2\\" \\\\\\n    -ot exps=CPU \\\\\\n    -fa -mg 0 -ub 1024 -mla 1 -amb 256\\n\\nUsing 161GB of RAM and the GPUs totally maxed, I get\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  1024 |    256 |      0 |    9.336 |   109.69 |   31.102 |     8.23 |\\n    |  1024 |    256 |   1024 |    9.345 |   109.57 |   31.224 |     8.20 |\\n    |  1024 |    256 |   2048 |    9.392 |   109.03 |   31.193 |     8.21 |\\n    |  1024 |    256 |   3072 |    9.452 |   108.34 |   31.472 |     8.13 |\\n    |  1024 |    256 |   4096 |    9.540 |   107.34 |   31.623 |     8.10 |\\n    |  1024 |    256 |   5120 |    9.750 |   105.03 |   32.674 |     7.83 |\\n\\nRunning a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:\\n\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  1792 |    448 |      0 |   10.701 |   167.46 |   56.284 |     7.96 |\\n    |  1792 |    448 |   1792 |   10.729 |   167.02 |   56.638 |     7.91 |\\n    |  1792 |    448 |   3584 |   10.947 |   163.71 |   57.194 |     7.83 |\\n    |  1792 |    448 |   5376 |   11.099 |   161.46 |   58.003 |     7.72 |\\n    |  1792 |    448 |   7168 |   11.267 |   159.06 |   58.127 |     7.71 |\\n    |  1792 |    448 |   8960 |   11.450 |   156.51 |   58.697 |     7.63 |\\n    |  1792 |    448 |  10752 |   11.627 |   154.12 |   59.421 |     7.54 |\\n    |  1792 |    448 |  12544 |   11.809 |   151.75 |   59.686 |     7.51 |\\n    |  1792 |    448 |  14336 |   12.007 |   149.24 |   60.075 |     7.46 |\\n    |  1792 |    448 |  16128 |   12.251 |   146.27 |   60.624 |     7.39 |\\n    |  1792 |    448 |  17920 |   12.639 |   141.79 |   60.977 |     7.35 |\\n    |  1792 |    448 |  19712 |   13.113 |   136.66 |   61.481 |     7.29 |\\n    |  1792 |    448 |  21504 |   13.639 |   131.39 |   62.117 |     7.21 |\\n    |  1792 |    448 |  23296 |   14.184 |   126.34 |   62.393 |     7.18 |\\n\\nAnd there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:\\n\\nhttps://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9d374193172785b18b03858204abb37a0ac8b9fa\\n\\nAs you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.\\n\\n# Final comparison\\n\\nAn image comparing 1 of each in one image, looks like this\\n\\nhttps://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6\\n\\nI don't have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (\`3.2119 +/- 0.01697\` vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, **so these quants are quite good quality.**\\n\\nFor the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):\\n\\n* 90-95GB RAM on Q2\\\\_K\\\\_XL, rest on VRAM.\\n* 100-110GB RAM on IQ3\\\\_XXS, rest on VRAM.\\n* 115-140GB RAM on Q3\\\\_K\\\\_XL, rest on VRAM.\\n* 115-135GB RAM on IQ3\\\\_KS, rest on VRAM.\\n* 161-177GB RAM on IQ4\\\\_XS, rest on VRAM.\\n\\nSomeone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it's because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.\\n\\nFor DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8\\\\_0 ctx (I didn't use it here, but it lets me use 64K at q8 with the same config as 32K at f16).\\n\\nHope this post can help someone interested in these results, any question is welcome!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":72,"top_awarded_type":null,"hide_score":false,"media_metadata":{"rj0kip6gw3cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":56,"x":108,"u":"https://preview.redd.it/rj0kip6gw3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1f101fc6491542a1a0d5232c62e944c92274120"},{"y":112,"x":216,"u":"https://preview.redd.it/rj0kip6gw3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9511e2308f22e080b1ceecf696c988a7e9d44dde"},{"y":166,"x":320,"u":"https://preview.redd.it/rj0kip6gw3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc4b301b035b11bb121d463ba4d2cde5bd111d2e"},{"y":333,"x":640,"u":"https://preview.redd.it/rj0kip6gw3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=437e96f03df2f2d0622934a1195ec3771ca24181"},{"y":499,"x":960,"u":"https://preview.redd.it/rj0kip6gw3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6b3f9cb824b56c49dee448ac1916c538919b228"},{"y":561,"x":1080,"u":"https://preview.redd.it/rj0kip6gw3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18249a8e504568e0fdb251e3f559bc4576973e14"}],"s":{"y":1998,"x":3840,"u":"https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf"},"id":"rj0kip6gw3cf1"},"r9tt4pktt3cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":56,"x":108,"u":"https://preview.redd.it/r9tt4pktt3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d06714ed9be3cf158f64b49625f04d4d3c1f811"},{"y":112,"x":216,"u":"https://preview.redd.it/r9tt4pktt3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef051b2e1c39b6fd156b5c24ba9927d364fa613f"},{"y":166,"x":320,"u":"https://preview.redd.it/r9tt4pktt3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2bb45907231ce9906909fc31595392a8ea256940"},{"y":333,"x":640,"u":"https://preview.redd.it/r9tt4pktt3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4f664b75a1378a0d7f3c9ca85df7c3bd682e3863"},{"y":499,"x":960,"u":"https://preview.redd.it/r9tt4pktt3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e4006e3d3ab018f96cc70afdde829cacd8e17a7"},{"y":561,"x":1080,"u":"https://preview.redd.it/r9tt4pktt3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66b3ccced4271b1a7bad7f5174460be8221e4edc"}],"s":{"y":1998,"x":3840,"u":"https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7"},"id":"r9tt4pktt3cf1"},"dtrfsnabu3cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":56,"x":108,"u":"https://preview.redd.it/dtrfsnabu3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fc0140a716d4f3f1b0ceeaacd5a84bc2be68565"},{"y":112,"x":216,"u":"https://preview.redd.it/dtrfsnabu3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ba825367494d413ad942db2b001da7b0ead3a0c"},{"y":166,"x":320,"u":"https://preview.redd.it/dtrfsnabu3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fdad97203dc8fdcfaabe6027fe6692e4548aac60"},{"y":333,"x":640,"u":"https://preview.redd.it/dtrfsnabu3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46bee9fde8e9875eb11bed658996aa56ee543827"},{"y":499,"x":960,"u":"https://preview.redd.it/dtrfsnabu3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b648d4bb124f102339fb52c54397c79f215c38a4"},{"y":561,"x":1080,"u":"https://preview.redd.it/dtrfsnabu3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c1898a494375997a6904dfbbda045e69fec24a72"}],"s":{"y":1998,"x":3840,"u":"https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=34fa15b35573c0d7ce936d9da953d4d483320902"},"id":"dtrfsnabu3cf1"},"owlrn4cqx3cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":56,"x":108,"u":"https://preview.redd.it/owlrn4cqx3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bca3112656c90be4044439b52e00946321961046"},{"y":112,"x":216,"u":"https://preview.redd.it/owlrn4cqx3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e268313470f242e9afa03279fc22c8ac2b078da2"},{"y":166,"x":320,"u":"https://preview.redd.it/owlrn4cqx3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1451526e6065a4be1fa87a59be3f3b3d96e6496"},{"y":333,"x":640,"u":"https://preview.redd.it/owlrn4cqx3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bbd26aa16d1131f087e6e6dc7a3dcfccd748228"},{"y":499,"x":960,"u":"https://preview.redd.it/owlrn4cqx3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b5091dff9d48e6a04095f1c8d1d4e5045aca3aa"},{"y":561,"x":1080,"u":"https://preview.redd.it/owlrn4cqx3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30d20fa0f3120330480aeb16bbc72c15e2fbfd40"}],"s":{"y":1998,"x":3840,"u":"https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6"},"id":"owlrn4cqx3cf1"},"l9jswixxu3cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":56,"x":108,"u":"https://preview.redd.it/l9jswixxu3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98693343116b5932d4e4febfade79a9540bf5251"},{"y":112,"x":216,"u":"https://preview.redd.it/l9jswixxu3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c19aafa0adf244051402fb495f7ea3e83ce4d332"},{"y":166,"x":320,"u":"https://preview.redd.it/l9jswixxu3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88a4c3ce4e0a67659285ff944ea2caf076534397"},{"y":333,"x":640,"u":"https://preview.redd.it/l9jswixxu3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6a67f633cd95a5690c3eec22b4e533133bd30e9"},{"y":499,"x":960,"u":"https://preview.redd.it/l9jswixxu3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=47fd0b46a4f4552c164259485d6d18c3810490ec"},{"y":561,"x":1080,"u":"https://preview.redd.it/l9jswixxu3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f47e3de82b9f829542cf88b78ddc69d029a420f8"}],"s":{"y":1998,"x":3840,"u":"https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1"},"id":"l9jswixxu3cf1"},"r8xka0tcx3cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":56,"x":108,"u":"https://preview.redd.it/r8xka0tcx3cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=60c2c8b2ff94822236365cd794d77a9025518b37"},{"y":112,"x":216,"u":"https://preview.redd.it/r8xka0tcx3cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3923f4e719843f30db5fb36d50cd44f6c92cde1c"},{"y":166,"x":320,"u":"https://preview.redd.it/r8xka0tcx3cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=15850541050f33ee722c2a69833096aaee3fab2b"},{"y":333,"x":640,"u":"https://preview.redd.it/r8xka0tcx3cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=07def092ebeb76d951427fa69df4343132d82320"},{"y":499,"x":960,"u":"https://preview.redd.it/r8xka0tcx3cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c5bff32a3671665b53452ff08c02de89fa87d27"},{"y":561,"x":1080,"u":"https://preview.redd.it/r8xka0tcx3cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0554a466e15e1086df7bee957146b2ec82ca05b7"}],"s":{"y":1998,"x":3840,"u":"https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9d374193172785b18b03858204abb37a0ac8b9fa"},"id":"r8xka0tcx3cf1"}},"name":"t3_1lwnj5x","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":58,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","is_original_content":false,"author_fullname":"t2_j1kqr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":58,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/2-o5OWNt03DgmcUIElRxpmapK3b8mnf9fOYvDpwJaPg.jpg","edited":1752181687,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752179851,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi there guys, hope you&amp;#39;re having a good day!&lt;/p&gt;\\n\\n&lt;p&gt;After latest improvements on ik llamacpp, &lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp/commits/main/\\"&gt;https://github.com/ikawrakow/ik_llama.cpp/commits/main/&lt;/a&gt;, I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I&amp;#39;m testing.&lt;/p&gt;\\n\\n&lt;p&gt;My setup is:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt;\\n&lt;li&gt;192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s&lt;/li&gt;\\n&lt;li&gt;3 1600W PSUs (Corsair 1600i)&lt;/li&gt;\\n&lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt;\\n&lt;li&gt;5090/5090 at PCIe X8/X8 5.0&lt;/li&gt;\\n&lt;li&gt;4090/4090 at PCIe X4/X4 4.0&lt;/li&gt;\\n&lt;li&gt;3090/3090 at PCIe X4/X4 4.0&lt;/li&gt;\\n&lt;li&gt;A6000 at PCIe X4 4.0.&lt;/li&gt;\\n&lt;li&gt;Fedora Linux 41 (instead of 42 just because I&amp;#39;m lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)&lt;/li&gt;\\n&lt;li&gt;SATA and USB-&amp;gt;M2 Storage&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;The benchmarks are based on mostly, R1-0528, BUT it has the same size and it&amp;#39;s quants on V3-0324 and TNG-R1T2-Chimera.&lt;/p&gt;\\n\\n&lt;p&gt;I have tested the next models:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/unsloth\\"&gt;unsloth&lt;/a&gt; DeepSeek Q2_K_XL:\\n\\n&lt;ul&gt;\\n&lt;li&gt;llm_load_print_meta: model size       = 233.852 GiB (2.994 BPW)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/unsloth\\"&gt;unsloth&lt;/a&gt; DeepSeek IQ3_XXS:\\n\\n&lt;ul&gt;\\n&lt;li&gt;llm_load_print_meta: model size       = 254.168 GiB (3.254 BPW)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/unsloth\\"&gt;unsloth&lt;/a&gt; DeepSeek Q3_K_XL:\\n\\n&lt;ul&gt;\\n&lt;li&gt;llm_load_print_meta: model size       = 275.576 GiB (3.528 BPW)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/ubergarm\\"&gt;ubergarm&lt;/a&gt; DeepSeek IQ3_KS:\\n\\n&lt;ul&gt;\\n&lt;li&gt;llm_load_print_meta: model size       = 281.463 GiB (3.598 BPW)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/unsloth\\"&gt;unsloth&lt;/a&gt; DeepSeek IQ4_XS:\\n\\n&lt;ul&gt;\\n&lt;li&gt;llm_load_print_meta: model size       = 333.130 GiB (4.264 BPW)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Each model may have been tested on different formats. Q2_K_XL and IQ3_XXS has less info, but the rest have a lot more. So here we go!&lt;/p&gt;\\n\\n&lt;h1&gt;unsloth DeepSeek Q2_K_XL&lt;/h1&gt;\\n\\n&lt;p&gt;Running the model with:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf&amp;#39; \\\\\\n-c 32768 --no-mmap -ngl 999 \\\\\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \\\\\\n-ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \\\\\\n-ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \\\\\\n-ot &amp;quot;blk.(21|22|23|24).ffn.=CUDA4&amp;quot; \\\\\\n-ot &amp;quot;blk.(25|26|27|28).ffn.=CUDA5&amp;quot; \\\\\\n-ot &amp;quot;blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6&amp;quot; \\\\\\n-ot exps=CPU \\\\\\n-fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I get:&lt;/p&gt;\\n\\n&lt;p&gt;main: n_kv_max = 32768, n_batch = 5120, n_ubatch = 5120, flash_attn = 1, n_gpu_layers = 999, n_threads = 8, n_threads_batch = 8&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  5120 |   1280 |      0 |   12.481 |   410.21 |  104.088 |    12.30 |\\n|  5120 |   1280 |   5120 |   14.630 |   349.98 |  109.724 |    11.67 |\\n|  5120 |   1280 |  10240 |   17.167 |   298.25 |  112.938 |    11.33 |\\n|  5120 |   1280 |  15360 |   20.008 |   255.90 |  119.037 |    10.75 |\\n|  5120 |   1280 |  20480 |   22.444 |   228.12 |  122.706 |    10.43 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7\\"&gt;Perf comparison (ignore 4096 as I forgor to save the perf)&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Q2_K_XL performs really good for a system like this! And it&amp;#39;s performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it&amp;#39;s at 3bpw.&lt;/p&gt;\\n\\n&lt;h1&gt;unsloth DeepSeek IQ3_XXS&lt;/h1&gt;\\n\\n&lt;p&gt;Running the model with:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf&amp;#39; \\\\\\n-c 32768 --no-mmap -ngl 999 \\\\\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\\\\n-ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.(11|12|13|14).ffn.=CUDA2&amp;quot; \\\\\\n-ot &amp;quot;blk.(15|16|17|18|19).ffn.=CUDA3&amp;quot; \\\\\\n-ot &amp;quot;blk.(20|21|22|23).ffn.=CUDA4&amp;quot; \\\\\\n-ot &amp;quot;blk.(24|25|26|27).ffn.=CUDA5&amp;quot; \\\\\\n-ot &amp;quot;blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6&amp;quot; \\\\\\n-ot exps=CPU \\\\\\n-fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I get&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Small test for this one!\\n\\n|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  4096 |   1024 |      0 |   10.671 |   383.83 |  117.496 |     8.72 |\\n|  4096 |   1024 |   4096 |   11.322 |   361.77 |  120.192 |     8.52 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902\\"&gt;https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Sorry on this one to have few data! IQ3_XXS quality is really good for it&amp;#39;s size.&lt;/p&gt;\\n\\n&lt;h1&gt;unsloth DeepSeek Q3_K_XL&lt;/h1&gt;\\n\\n&lt;p&gt;Now we enter a bigger territory. Note that you will notice Q3_K_XL being faster than IQ3_XXS, despite being bigger.&lt;/p&gt;\\n\\n&lt;p&gt;Running the faster PP one with:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf&amp;#39; \\\\\\n-c 32768 --no-mmap -ngl 999 \\\\\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \\\\\\n-ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \\\\\\n-ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \\\\\\n-ot &amp;quot;blk.(21|22|23).ffn.=CUDA4&amp;quot; \\\\\\n-ot &amp;quot;blk.(24|25|26).ffn.=CUDA5&amp;quot; \\\\\\n-ot &amp;quot;blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6&amp;quot; \\\\\\n-ot exps=CPU \\\\\\n-fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Results look like this:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  2560 |    640 |      0 |    9.781 |   261.72 |   65.367 |     9.79 |\\n|  2560 |    640 |   2560 |   10.048 |   254.78 |   65.824 |     9.72 |\\n|  2560 |    640 |   5120 |   10.625 |   240.93 |   66.134 |     9.68 |\\n|  2560 |    640 |   7680 |   11.167 |   229.24 |   67.225 |     9.52 |\\n|  2560 |    640 |  10240 |   12.268 |   208.68 |   67.475 |     9.49 |\\n|  2560 |    640 |  12800 |   13.433 |   190.58 |   68.743 |     9.31 |\\n|  2560 |    640 |  15360 |   14.564 |   175.78 |   69.585 |     9.20 |\\n|  2560 |    640 |  17920 |   15.734 |   162.70 |   70.589 |     9.07 |\\n|  2560 |    640 |  20480 |   16.889 |   151.58 |   72.524 |     8.82 |\\n|  2560 |    640 |  23040 |   18.100 |   141.43 |   74.534 |     8.59 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;With more layers on GPU, but smaller batch size, I get&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  2048 |    512 |      0 |    9.017 |   227.12 |   50.612 |    10.12 |\\n|  2048 |    512 |   2048 |    9.113 |   224.73 |   51.027 |    10.03 |\\n|  2048 |    512 |   4096 |    9.436 |   217.05 |   51.864 |     9.87 |\\n|  2048 |    512 |   6144 |    9.680 |   211.56 |   52.818 |     9.69 |\\n|  2048 |    512 |   8192 |    9.984 |   205.12 |   53.354 |     9.60 |\\n|  2048 |    512 |  10240 |   10.349 |   197.90 |   53.896 |     9.50 |\\n|  2048 |    512 |  12288 |   10.936 |   187.27 |   54.600 |     9.38 |\\n|  2048 |    512 |  14336 |   11.688 |   175.22 |   55.150 |     9.28 |\\n|  2048 |    512 |  16384 |   12.419 |   164.91 |   55.852 |     9.17 |\\n|  2048 |    512 |  18432 |   13.113 |   156.18 |   56.436 |     9.07 |\\n|  2048 |    512 |  20480 |   13.871 |   147.65 |   56.823 |     9.01 |\\n|  2048 |    512 |  22528 |   14.594 |   140.33 |   57.590 |     8.89 |\\n|  2048 |    512 |  24576 |   15.335 |   133.55 |   58.278 |     8.79 |\\n|  2048 |    512 |  26624 |   16.073 |   127.42 |   58.723 |     8.72 |\\n|  2048 |    512 |  28672 |   16.794 |   121.95 |   59.553 |     8.60 |\\n|  2048 |    512 |  30720 |   17.522 |   116.88 |   59.921 |     8.54 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;And with less GPU layers on GPU, but higher batch size, I get&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  4096 |   1024 |      0 |   12.005 |   341.19 |  111.632 |     9.17 |\\n|  4096 |   1024 |   4096 |   12.515 |   327.28 |  138.930 |     7.37 |\\n|  4096 |   1024 |   8192 |   13.389 |   305.91 |  118.220 |     8.66 |\\n|  4096 |   1024 |  12288 |   15.018 |   272.74 |  119.289 |     8.58 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;So then, performance for different batch sizes and layers, looks like this:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1\\"&gt;Higher ub/b is because I ended the test earlier!&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;So you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.&lt;/p&gt;\\n\\n&lt;h1&gt;ubergarm DeepSeek IQ3_KS (&lt;a href=\\"https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF\\"&gt;TNG-R1T2-Chimera&lt;/a&gt;)&lt;/h1&gt;\\n\\n&lt;p&gt;This one is really good! And it has some more optimizations that may apply more on iklcpp.&lt;/p&gt;\\n\\n&lt;p&gt;Running this one with:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf&amp;#39; \\\\\\n-c 32768 --no-mmap -ngl 999 \\\\\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\\\\n-ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \\\\\\n-ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \\\\\\n-ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \\\\\\n-ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \\\\\\n-ot &amp;quot;blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6&amp;quot; \\\\\\n-ot exps=CPU \\\\\\n-fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I  get&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  6144 |   1536 |      0 |   15.406 |   398.81 |  174.929 |     8.78 |\\n|  6144 |   1536 |   6144 |   18.289 |   335.94 |  180.393 |     8.51 |\\n|  6144 |   1536 |  12288 |   22.229 |   276.39 |  186.113 |     8.25 |\\n|  6144 |   1536 |  18432 |   24.533 |   250.44 |  191.037 |     8.04 |\\n|  6144 |   1536 |  24576 |   28.122 |   218.48 |  196.268 |     7.83 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Or 8192 batch size/ubatch size, I get&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  8192 |   2048 |      0 |   20.147 |   406.61 |  232.476 |     8.81 |\\n|  8192 |   2048 |   8192 |   26.009 |   314.97 |  242.648 |     8.44 |\\n|  8192 |   2048 |  16384 |   32.628 |   251.07 |  253.309 |     8.09 |\\n|  8192 |   2048 |  24576 |   39.010 |   210.00 |  264.415 |     7.75 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;So the graph looks like this&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf\\"&gt;https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Again, this model is really good, and really fast! Totally recommended.&lt;/p&gt;\\n\\n&lt;h1&gt;unsloth DeepSeek IQ4_XS&lt;/h1&gt;\\n\\n&lt;p&gt;At this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.&lt;/p&gt;\\n\\n&lt;p&gt;Running this model with the best balance with:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./llama-sweep-bench -m &amp;#39;/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf&amp;#39; -c 32768 --no-mmap -ngl 999 \\\\\\n-ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \\\\\\n-ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \\\\\\n-ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \\\\\\n-ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \\\\\\n-ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \\\\\\n-ot &amp;quot;blk.(23|24|25|26|27|28|29).ffn.=CUDA6&amp;quot; \\\\\\n-ot &amp;quot;blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.30.ffn_gate_exps.weight=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.30.ffn_down_exps.weight=CUDA2&amp;quot; \\\\\\n-ot &amp;quot;blk.30.ffn_up_exps.weight=CUDA4&amp;quot; \\\\\\n-ot &amp;quot;blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \\\\\\n-ot &amp;quot;blk.31.ffn_gate_exps.weight=CUDA5&amp;quot; \\\\\\n-ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA0&amp;quot; \\\\\\n-ot &amp;quot;blk.31.ffn_up_exps.weight=CUDA3&amp;quot; \\\\\\n-ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \\\\\\n-ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA2&amp;quot; \\\\\\n-ot exps=CPU \\\\\\n-fa -mg 0 -ub 1024 -mla 1 -amb 256\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Using 161GB of RAM and the GPUs totally maxed, I get&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  1024 |    256 |      0 |    9.336 |   109.69 |   31.102 |     8.23 |\\n|  1024 |    256 |   1024 |    9.345 |   109.57 |   31.224 |     8.20 |\\n|  1024 |    256 |   2048 |    9.392 |   109.03 |   31.193 |     8.21 |\\n|  1024 |    256 |   3072 |    9.452 |   108.34 |   31.472 |     8.13 |\\n|  1024 |    256 |   4096 |    9.540 |   107.34 |   31.623 |     8.10 |\\n|  1024 |    256 |   5120 |    9.750 |   105.03 |   32.674 |     7.83 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Running a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  1792 |    448 |      0 |   10.701 |   167.46 |   56.284 |     7.96 |\\n|  1792 |    448 |   1792 |   10.729 |   167.02 |   56.638 |     7.91 |\\n|  1792 |    448 |   3584 |   10.947 |   163.71 |   57.194 |     7.83 |\\n|  1792 |    448 |   5376 |   11.099 |   161.46 |   58.003 |     7.72 |\\n|  1792 |    448 |   7168 |   11.267 |   159.06 |   58.127 |     7.71 |\\n|  1792 |    448 |   8960 |   11.450 |   156.51 |   58.697 |     7.63 |\\n|  1792 |    448 |  10752 |   11.627 |   154.12 |   59.421 |     7.54 |\\n|  1792 |    448 |  12544 |   11.809 |   151.75 |   59.686 |     7.51 |\\n|  1792 |    448 |  14336 |   12.007 |   149.24 |   60.075 |     7.46 |\\n|  1792 |    448 |  16128 |   12.251 |   146.27 |   60.624 |     7.39 |\\n|  1792 |    448 |  17920 |   12.639 |   141.79 |   60.977 |     7.35 |\\n|  1792 |    448 |  19712 |   13.113 |   136.66 |   61.481 |     7.29 |\\n|  1792 |    448 |  21504 |   13.639 |   131.39 |   62.117 |     7.21 |\\n|  1792 |    448 |  23296 |   14.184 |   126.34 |   62.393 |     7.18 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;And there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa\\"&gt;https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;As you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.&lt;/p&gt;\\n\\n&lt;h1&gt;Final comparison&lt;/h1&gt;\\n\\n&lt;p&gt;An image comparing 1 of each in one image, looks like this&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6\\"&gt;https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (&lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt; vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, &lt;strong&gt;so these quants are quite good quality.&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;For the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;90-95GB RAM on Q2_K_XL, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;100-110GB RAM on IQ3_XXS, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;115-140GB RAM on Q3_K_XL, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;115-135GB RAM on IQ3_KS, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;161-177GB RAM on IQ4_XS, rest on VRAM.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Someone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it&amp;#39;s because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.&lt;/p&gt;\\n\\n&lt;p&gt;For DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8_0 ctx (I didn&amp;#39;t use it here, but it lets me use 64K at q8 with the same config as 32K at f16).&lt;/p&gt;\\n\\n&lt;p&gt;Hope this post can help someone interested in these results, any question is welcome!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"Llama 405B","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lwnj5x","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"panchovix","discussion_type":null,"num_comments":59,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/","subreddit_subscribers":497504,"created_utc":1752179851,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2flbvy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752180785,"send_replies":true,"parent_id":"t3_1lwnj5x","score":14,"author_fullname":"t2_n321yfw5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Heya u/panchovix thanks for kicking the tires on my ik\\\\_llama.cpp exclusive quants! Great to hear you have them running and getting more speed out of your \\"unique rig\\" with 5 CUDA GPUs across all the great quants available.\\n\\nI'm gonna upload a new recipe IQ3\\\\_KS DeepSeek-R1-0528 today as your testing with the TNG-R1T2-Chimera helped confirm it is pretty good!\\n\\nCheers!\\n\\n\\\\*UPDATE\\\\* Currently uploading my latest recipe [ubergarm/DeepSeek-R1-0528-GGUF](https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF#iq3_ks-281463-gib-3598-bpw) with best in class perplexity for the size of \`Final estimate: PPL = 3.2983 +/- 0.01759\`. Weighs in at 281.463 GiB (3.598 BPW) so perfect for the 256 GB RAM plus a couple GPUs club!","edited":1752186792,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2flbvy","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Heya &lt;a href=\\"/u/panchovix\\"&gt;u/panchovix&lt;/a&gt; thanks for kicking the tires on my ik_llama.cpp exclusive quants! Great to hear you have them running and getting more speed out of your &amp;quot;unique rig&amp;quot; with 5 CUDA GPUs across all the great quants available.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m gonna upload a new recipe IQ3_KS DeepSeek-R1-0528 today as your testing with the TNG-R1T2-Chimera helped confirm it is pretty good!&lt;/p&gt;\\n\\n&lt;p&gt;Cheers!&lt;/p&gt;\\n\\n&lt;p&gt;*UPDATE* Currently uploading my latest recipe &lt;a href=\\"https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF#iq3_ks-281463-gib-3598-bpw\\"&gt;ubergarm/DeepSeek-R1-0528-GGUF&lt;/a&gt; with best in class perplexity for the size of &lt;code&gt;Final estimate: PPL = 3.2983 +/- 0.01759&lt;/code&gt;. Weighs in at 281.463 GiB (3.598 BPW) so perfect for the 256 GB RAM plus a couple GPUs club!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2flbvy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752180785,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fjjv0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ii_social","can_mod_post":false,"created_utc":1752180282,"send_replies":true,"parent_id":"t3_1lwnj5x","score":6,"author_fullname":"t2_tohvxz80x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you very much for the rigor sir, please never stop sharing! &lt;3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fjjv0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you very much for the rigor sir, please never stop sharing! &amp;lt;3&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fjjv0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752180282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g2c4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"waiting_for_zban","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g0ehn","score":1,"author_fullname":"t2_13yxr6ze7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It seems the distributed inference (at least the consumer one) is still inefficient, and lots of room for improvements. Nevertheless, great insights. It is always nice to see concrete benchmarks!","edited":false,"author_flair_css_class":null,"name":"t1_n2g2c4o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It seems the distributed inference (at least the consumer one) is still inefficient, and lots of room for improvements. Nevertheless, great insights. It is always nice to see concrete benchmarks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g2c4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185944,"author_flair_text":null,"collapsed":false,"created_utc":1752185944,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g0ehn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fzrc1","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, take or give maybe 512-2GB per GPU. Some GPUs have 2 GB left sometimes (i.e. a 5090) and sometimes they have 512MB left, or even less in the IQ4\\\\_XS case (like 150MB on the A6000 lol).\\n\\nHonestly not sure how to explain it besides having the values, there is some buffers that are loaded when you actually generate for example and it also depends at which context are you writing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g0ehn","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, take or give maybe 512-2GB per GPU. Some GPUs have 2 GB left sometimes (i.e. a 5090) and sometimes they have 512MB left, or even less in the IQ4_XS case (like 150MB on the A6000 lol).&lt;/p&gt;\\n\\n&lt;p&gt;Honestly not sure how to explain it besides having the values, there is some buffers that are loaded when you actually generate for example and it also depends at which context are you writing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g0ehn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185322,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752185322,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fzrc1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"waiting_for_zban","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fod4d","score":1,"author_fullname":"t2_13yxr6ze7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks! This is really interesting, although a more detailed breakdown is needed to understand the modularity and interplay between VRAM / RAM. \\n\\nI assume when you say the rest is on the VRAM, I assume it is fully taken (208 GB)? So the the total is ~308GB of VRAM + RAM for the Q2_K_XL?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2fzrc1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks! This is really interesting, although a more detailed breakdown is needed to understand the modularity and interplay between VRAM / RAM. &lt;/p&gt;\\n\\n&lt;p&gt;I assume when you say the rest is on the VRAM, I assume it is fully taken (208 GB)? So the the total is ~308GB of VRAM + RAM for the Q2_K_XL?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fzrc1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185115,"author_flair_text":null,"treatment_tags":[],"created_utc":1752185115,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fod4d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1752181655,"send_replies":true,"parent_id":"t1_n2fn547","score":3,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8\\\\_0 ctx (I didn't use it here, but it lets me use 64K at q8 with the same config as 32K at f16).\\n\\nI will add the RAM used on each quant, but for the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):\\n\\n* 90-95GB RAM on Q2\\\\_K\\\\_XL, rest on VRAM.\\n* 100-110GB RAM on IQ3\\\\_XXS, rest on VRAM.\\n* 115-140GB RAM on Q3\\\\_K\\\\_XL, rest on VRAM.\\n* 115-135GB RAM on IQ3\\\\_KS, rest on VRAM.\\n* 161-177GB RAM on IQ4\\\\_XS, rest on VRAM.\\n\\nSomeone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it's because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fod4d","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8_0 ctx (I didn&amp;#39;t use it here, but it lets me use 64K at q8 with the same config as 32K at f16).&lt;/p&gt;\\n\\n&lt;p&gt;I will add the RAM used on each quant, but for the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;90-95GB RAM on Q2_K_XL, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;100-110GB RAM on IQ3_XXS, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;115-140GB RAM on Q3_K_XL, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;115-135GB RAM on IQ3_KS, rest on VRAM.&lt;/li&gt;\\n&lt;li&gt;161-177GB RAM on IQ4_XS, rest on VRAM.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Someone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it&amp;#39;s because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fod4d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752181655,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g7uzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"waiting_for_zban","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g44ph","score":2,"author_fullname":"t2_13yxr6ze7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; It is a linear plot.\\n\\nThe ticks on the x axis looked uneven, so I thought it was logarithmic. You're right it is linear. My bad. This is really interesting btw, I will see when I will have time to give it a try. I am waiting for a new HDD, it's impossible to keep track of all the LLM model sizes, so got a bigger disk to store them.","edited":false,"author_flair_css_class":null,"name":"t1_n2g7uzw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;It is a linear plot.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The ticks on the x axis looked uneven, so I thought it was logarithmic. You&amp;#39;re right it is linear. My bad. This is really interesting btw, I will see when I will have time to give it a try. I am waiting for a new HDD, it&amp;#39;s impossible to keep track of all the LLM model sizes, so got a bigger disk to store them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g7uzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752187725,"author_flair_text":null,"collapsed":false,"created_utc":1752187725,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g44ph","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g0td6","score":1,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't follow? It is a linear plot.  \\\\\`y=mx+b\\\\\` with b being the fixed size of the tensors offloaded onto VRAM and the slope set by the quantization e.g. q8\\\\_0 or fp16.\\n\\nThe x axis is the llama-server context size you choose e.g 8k would be \\\\\`-c 8192\\\\\` and 64k context would be \\\\\`-c 65536\\\\\`. \\n\\nI looked at the total VRAM used in \\\\\`nvidia-smi\\\\\` for the process to collect the few data points.\\n\\nMost of the model runs on system RAM, that is typical and the usual way to run these big MoEs with hybrid inference with ik\\\\_llama.cpp or llama.cpp. Works great for smaller moe's too.\\n\\nThe tl;dr; is that my newer quants which have about \\\\~12GiB VRAM of tensors offloaded \\"fixed\\" can fit 32k context with a single 16GB VRAM GPU. You can run 64k context with a single 24GB VRAM GPU.\\n\\nIt is kinda surprising and great when you first see it.\\n\\nMake sense?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g44ph","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t follow? It is a linear plot.  \`y=mx+b\` with b being the fixed size of the tensors offloaded onto VRAM and the slope set by the quantization e.g. q8_0 or fp16.&lt;/p&gt;\\n\\n&lt;p&gt;The x axis is the llama-server context size you choose e.g 8k would be \`-c 8192\` and 64k context would be \`-c 65536\`. &lt;/p&gt;\\n\\n&lt;p&gt;I looked at the total VRAM used in \`nvidia-smi\` for the process to collect the few data points.&lt;/p&gt;\\n\\n&lt;p&gt;Most of the model runs on system RAM, that is typical and the usual way to run these big MoEs with hybrid inference with ik_llama.cpp or llama.cpp. Works great for smaller moe&amp;#39;s too.&lt;/p&gt;\\n\\n&lt;p&gt;The tl;dr; is that my newer quants which have about ~12GiB VRAM of tensors offloaded &amp;quot;fixed&amp;quot; can fit 32k context with a single 16GB VRAM GPU. You can run 64k context with a single 24GB VRAM GPU.&lt;/p&gt;\\n\\n&lt;p&gt;It is kinda surprising and great when you first see it.&lt;/p&gt;\\n\\n&lt;p&gt;Make sense?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g44ph/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752186518,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752186518,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g0td6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"waiting_for_zban","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fqv60","score":2,"author_fullname":"t2_13yxr6ze7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very interesting, i nearly fell for that linear looking plot. The x axis was confusing. This is only context size (in Vram) or both model + context size (32gb sounds unrealistic, unless a lot is offloaded to ram)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2g0td6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very interesting, i nearly fell for that linear looking plot. The x axis was confusing. This is only context size (in Vram) or both model + context size (32gb sounds unrealistic, unless a lot is offloaded to ram)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g0td6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185456,"author_flair_text":null,"treatment_tags":[],"created_utc":1752185456,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fqv60","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752182406,"send_replies":true,"parent_id":"t1_n2fn547","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/lfu1vpiq54cf1.png?width=2140&amp;format=png&amp;auto=webp&amp;s=ad0177953551db8dd909aae145e2bda7900f181b\\n\\nThis is for one of my older models that used full size Q8\\\\_0 for the GPU offload tensors. My newer smaller quants are much slimmer so they take up less \\"fixed size\\" but the linear relationship is similar. MLA is pretty impressive here compard to MQA or even GQA!\\n\\nI just checked and some of my newer quants only use less than 12GiB \\"fixed size\\" so fit 32k context in under 16GB and 64k context in 24GB VRAM.","edited":1752186261,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fqv60","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/lfu1vpiq54cf1.png?width=2140&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad0177953551db8dd909aae145e2bda7900f181b\\"&gt;https://preview.redd.it/lfu1vpiq54cf1.png?width=2140&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad0177953551db8dd909aae145e2bda7900f181b&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This is for one of my older models that used full size Q8_0 for the GPU offload tensors. My newer smaller quants are much slimmer so they take up less &amp;quot;fixed size&amp;quot; but the linear relationship is similar. MLA is pretty impressive here compard to MQA or even GQA!&lt;/p&gt;\\n\\n&lt;p&gt;I just checked and some of my newer quants only use less than 12GiB &amp;quot;fixed size&amp;quot; so fit 32k context in under 16GB and 64k context in 24GB VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fqv60/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182406,"media_metadata":{"lfu1vpiq54cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":73,"x":108,"u":"https://preview.redd.it/lfu1vpiq54cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbddf2c554e7bc4c25e3fc175eec827ce517c2ae"},{"y":146,"x":216,"u":"https://preview.redd.it/lfu1vpiq54cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c23ba5c5a22cc0e6cdca9d9f154cf3b746c9e933"},{"y":217,"x":320,"u":"https://preview.redd.it/lfu1vpiq54cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6dbee3f0a9a3f13746209957bed917fea381964d"},{"y":435,"x":640,"u":"https://preview.redd.it/lfu1vpiq54cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a10e5a424e16ef6b346b999de4293c23bf3563a"},{"y":653,"x":960,"u":"https://preview.redd.it/lfu1vpiq54cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cf922669c69fda7a83b6df3c0741c0213da7fa20"},{"y":734,"x":1080,"u":"https://preview.redd.it/lfu1vpiq54cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cb56c3a6dd36155e858ee78806d356513e5553a"}],"s":{"y":1456,"x":2140,"u":"https://preview.redd.it/lfu1vpiq54cf1.png?width=2140&amp;format=png&amp;auto=webp&amp;s=ad0177953551db8dd909aae145e2bda7900f181b"},"id":"lfu1vpiq54cf1"}},"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fn547","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"waiting_for_zban","can_mod_post":false,"created_utc":1752181297,"send_replies":true,"parent_id":"t3_1lwnj5x","score":6,"author_fullname":"t2_13yxr6ze7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you have a context_size / ram_size_consumption chart? I am curious how much would it be consuming for useful usage?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fn547","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have a context_size / ram_size_consumption chart? I am curious how much would it be consuming for useful usage?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fn547/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752181297,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fxlfp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KernQ","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fqwnm","score":1,"author_fullname":"t2_1pozn81kn1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nice one, thanks.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2fxlfp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice one, thanks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fxlfp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184440,"author_flair_text":null,"treatment_tags":[],"created_utc":1752184440,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fqwnm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752182418,"send_replies":true,"parent_id":"t1_n2fqbss","score":3,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I find to benchmark way easier on ik llamacpp, with ./llama-sweep-bench.\\n\\nFor the commands on the post for example, I just replaced llama-server with llama-sweep-bench and it just runs out of the box! No file needed.\\n\\nI won't lie, I'm not sure how to use llama-bench from main llamacpp, I think it is bugged with -ot because I could never make it work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fqwnm","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I find to benchmark way easier on ik llamacpp, with ./llama-sweep-bench.&lt;/p&gt;\\n\\n&lt;p&gt;For the commands on the post for example, I just replaced llama-server with llama-sweep-bench and it just runs out of the box! No file needed.&lt;/p&gt;\\n\\n&lt;p&gt;I won&amp;#39;t lie, I&amp;#39;m not sure how to use llama-bench from main llamacpp, I think it is bugged with -ot because I could never make it work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fqwnm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182418,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fxvc5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fryx1","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Man many thanks, I wanted to run some benchmarks on main llamacpp but I just couldn't, this is just perfect.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2fxvc5","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man many thanks, I wanted to run some benchmarks on main llamacpp but I just couldn&amp;#39;t, this is just perfect.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fxvc5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184524,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752184524,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fryx1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752182737,"send_replies":true,"parent_id":"t1_n2fqbss","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"llama-sweep-bench is the easiest way to compare speeds across kv-cache depth. This gives a better view of how fast it would actually be with longer context size.\\n\\nI maintain a branch for mainline llama.cpp as well if you want to compare speeds between ik and mainline: [https://github.com/ubergarm/llama.cpp/tree/ug/port-sweep-bench](https://github.com/ubergarm/llama.cpp/tree/ug/port-sweep-bench)\\n\\nJust rebased and updated to latest tip","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fryx1","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama-sweep-bench is the easiest way to compare speeds across kv-cache depth. This gives a better view of how fast it would actually be with longer context size.&lt;/p&gt;\\n\\n&lt;p&gt;I maintain a branch for mainline llama.cpp as well if you want to compare speeds between ik and mainline: &lt;a href=\\"https://github.com/ubergarm/llama.cpp/tree/ug/port-sweep-bench\\"&gt;https://github.com/ubergarm/llama.cpp/tree/ug/port-sweep-bench&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Just rebased and updated to latest tip&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fryx1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182737,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fqbss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KernQ","can_mod_post":false,"created_utc":1752182244,"send_replies":true,"parent_id":"t3_1lwnj5x","score":3,"author_fullname":"t2_1pozn81kn1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool rig!\\n\\nWhat is your methodology for the benchmarks? I see the llama-server settings, but not the data used to test them. (Eg if I wanted to reproduce or compare my rig).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fqbss","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool rig!&lt;/p&gt;\\n\\n&lt;p&gt;What is your methodology for the benchmarks? I see the llama-server settings, but not the data used to test them. (Eg if I wanted to reproduce or compare my rig).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fqbss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182244,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fr3vc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752182479,"send_replies":true,"parent_id":"t1_n2foigd","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can get big PP gains inceasing batch sizes e.g. -ub 4096 -b 4096 etc... but you might have to offload one less layer which could hurt TG little bit. Its all trade-offs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fr3vc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can get big PP gains inceasing batch sizes e.g. -ub 4096 -b 4096 etc... but you might have to offload one less layer which could hurt TG little bit. Its all trade-offs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fr3vc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182479,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2foigd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1752181699,"send_replies":true,"parent_id":"t3_1lwnj5x","score":3,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I really like TNG-R1T2-Chimera too, I've been using ubergarm's IQ2_KS. I just swapped it out in place from the same size R1-0528. Performance t/s wise with the same config is matching with normal R1-0528 but true to the model card's word, it definitely thinks less so it's a lot faster in action.\\n\\nYour prompt processing is really crazy with that setup, my 3080+4060ti combo doesnt even come close. Like 30 pp 10 tg with the bulk of it on an EPYC 7702.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2foigd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really like TNG-R1T2-Chimera too, I&amp;#39;ve been using ubergarm&amp;#39;s IQ2_KS. I just swapped it out in place from the same size R1-0528. Performance t/s wise with the same config is matching with normal R1-0528 but true to the model card&amp;#39;s word, it definitely thinks less so it&amp;#39;s a lot faster in action.&lt;/p&gt;\\n\\n&lt;p&gt;Your prompt processing is really crazy with that setup, my 3080+4060ti combo doesnt even come close. Like 30 pp 10 tg with the bulk of it on an EPYC 7702.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2foigd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752181699,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g0vpj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fx9ns","score":1,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"body":"TIL! If you can send me the info it would be appreciated, I may take a look! Basically here for used is just local and aliexpress/alibaba. Ebay and similar are most of the time not an option here.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2g0vpj","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TIL! If you can send me the info it would be appreciated, I may take a look! Basically here for used is just local and aliexpress/alibaba. Ebay and similar are most of the time not an option here.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g0vpj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185477,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752185477,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gkzh1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752192060,"send_replies":true,"parent_id":"t1_n2gamdz","score":2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're referring to state sales tax in the US, which is like VAT in Europe. Not all states have it. If you do your homework, you'll find forwarders that have warehouses in states that don't charge a sales tax. I can't stress this enough: do your own research and know service you are/aren't getting and what charges you'll pay beforehand.\\n\\nCan't help you with that server. Again, I'm not affiliated with any such company. Just use one to buy from the US, and another to buy from Japan.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2gkzh1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re referring to state sales tax in the US, which is like VAT in Europe. Not all states have it. If you do your homework, you&amp;#39;ll find forwarders that have warehouses in states that don&amp;#39;t charge a sales tax. I can&amp;#39;t stress this enough: do your own research and know service you are/aren&amp;#39;t getting and what charges you&amp;#39;ll pay beforehand.&lt;/p&gt;\\n\\n&lt;p&gt;Can&amp;#39;t help you with that server. Again, I&amp;#39;m not affiliated with any such company. Just use one to buy from the US, and another to buy from Japan.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gkzh1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192060,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gamdz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752188615,"send_replies":true,"parent_id":"t1_n2g7p4o","score":1,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can't remember if VAT or other taxes but when I order something to be delivered in the US (CA) I get a 10% tax while when I get it delivered to France it's a 20% tax. My fear is that with forwarding, I'd pay 10% tax when it would be delivered in the US to the forwarding company and then 20% when the forwarding company sends the goods to France.\\nHow do you avoid that double taxation?\\nThx!\\n\\n\\nBtw, I want to bring back a server I got delivered to the US (family members) and I'm not sure I'll be able to avoid paying taxes again when bringing it myself with my luggage back to France:.😭","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2gamdz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t remember if VAT or other taxes but when I order something to be delivered in the US (CA) I get a 10% tax while when I get it delivered to France it&amp;#39;s a 20% tax. My fear is that with forwarding, I&amp;#39;d pay 10% tax when it would be delivered in the US to the forwarding company and then 20% when the forwarding company sends the goods to France.\\nHow do you avoid that double taxation?\\nThx!&lt;/p&gt;\\n\\n&lt;p&gt;Btw, I want to bring back a server I got delivered to the US (family members) and I&amp;#39;m not sure I&amp;#39;ll be able to avoid paying taxes again when bringing it myself with my luggage back to France:.😭&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gamdz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188615,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g7p4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g66ot","score":2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What tariff situation? I live in Europe. I only use this service for items located in the US that I want to buy. The tariffs are for imports into the US.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2g7p4o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What tariff situation? I live in Europe. I only use this service for items located in the US that I want to buy. The tariffs are for imports into the US.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g7p4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752187672,"author_flair_text":null,"treatment_tags":[],"created_utc":1752187672,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g66ot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fx9ns","score":1,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting!\\nWhat is the tariff situation with forwarding?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2g66ot","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting!\\nWhat is the tariff situation with forwarding?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g66ot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752187184,"author_flair_text":null,"treatment_tags":[],"created_utc":1752187184,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fx9ns","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fu1vp","score":3,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Pro tip: you don't need a seller to ship to Chile or anywhere. Register with a forwarding company and you can bundle multiple orders in one package and even save on shipping. I've been doing this for over 10 years, ordering from the US, and shipping to Europe. There are several others you can choose from. They all let you store your purchases free of charge for 30 days and let you bundle them in one shipment to save on shipping costs. Some offer repackaging to minimize volume and weight, some just put all boxes into a bigger box. You can always ask the seller to minimize the size of the box if you don't want them to open your order. My experience is most sellers will oblige if you ask nicely.\\n\\nBeen using the same forwarding company all these years (DM if interested, no affiliation whatsoever with anyone). Moved country twice (3 destination countries total) and it's worked beautifully. I average about 8 orders/year and never had an issue in over 10 years. Just do your homework googling the forwarding company and calculating shipping and import charges.\\n\\nFor ES CPUs, get those from China. I've been using ES Xeons for years without issue (though haven't gotten to the 8480). As always, do your homework beforehand about which are good and which are lemons. There are super long threads for ES CPUs at the STH forums where you can learn everything you need and find the codes for ones to get. The sellers are in China anyway, so those you can ship directly to you, whether you buy from ebay or from aliexpress.","edited":false,"author_flair_css_class":null,"name":"t1_n2fx9ns","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pro tip: you don&amp;#39;t need a seller to ship to Chile or anywhere. Register with a forwarding company and you can bundle multiple orders in one package and even save on shipping. I&amp;#39;ve been doing this for over 10 years, ordering from the US, and shipping to Europe. There are several others you can choose from. They all let you store your purchases free of charge for 30 days and let you bundle them in one shipment to save on shipping costs. Some offer repackaging to minimize volume and weight, some just put all boxes into a bigger box. You can always ask the seller to minimize the size of the box if you don&amp;#39;t want them to open your order. My experience is most sellers will oblige if you ask nicely.&lt;/p&gt;\\n\\n&lt;p&gt;Been using the same forwarding company all these years (DM if interested, no affiliation whatsoever with anyone). Moved country twice (3 destination countries total) and it&amp;#39;s worked beautifully. I average about 8 orders/year and never had an issue in over 10 years. Just do your homework googling the forwarding company and calculating shipping and import charges.&lt;/p&gt;\\n\\n&lt;p&gt;For ES CPUs, get those from China. I&amp;#39;ve been using ES Xeons for years without issue (though haven&amp;#39;t gotten to the 8480). As always, do your homework beforehand about which are good and which are lemons. There are super long threads for ES CPUs at the STH forums where you can learn everything you need and find the codes for ones to get. The sellers are in China anyway, so those you can ship directly to you, whether you buy from ebay or from aliexpress.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fx9ns/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184340,"author_flair_text":null,"collapsed":false,"created_utc":1752184340,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gskpe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gmn4g","score":2,"author_fullname":"t2_ql2vu0wz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"let's wait then for the TR Pro 9000 line to fall in price","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2gskpe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;let&amp;#39;s wait then for the TR Pro 9000 line to fall in price&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gskpe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752194730,"author_flair_text":null,"treatment_tags":[],"created_utc":1752194730,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gmn4g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gl4ad","score":1,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm mostly interested because the 128 PCIe 5.0 lanes. 9985WX/9995WX goes way beyond my budget sadly (I have gotten the GPUs in the span of 3-4 years, not all in one go haha)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2gmn4g","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m mostly interested because the 128 PCIe 5.0 lanes. 9985WX/9995WX goes way beyond my budget sadly (I have gotten the GPUs in the span of 3-4 years, not all in one go haha)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gmn4g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192634,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752192634,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gl4ad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fu1vp","score":2,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I was planning for a 9955WX\\n\\ndon't, it's a noob trap. It won't go past 170/180GB/s because of the number of CCDs. Only the 9985WX/9995wx can utilize 8 channels in full","edited":false,"author_flair_css_class":null,"name":"t1_n2gl4ad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I was planning for a 9955WX&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;don&amp;#39;t, it&amp;#39;s a noob trap. It won&amp;#39;t go past 170/180GB/s because of the number of CCDs. Only the 9985WX/9995wx can utilize 8 channels in full&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gl4ad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192107,"author_flair_text":null,"collapsed":false,"created_utc":1752192107,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fu1vp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fsvds","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was planning for a 9955WX, which should be 8 channels, and perform as a 9950X, maybe a bit slower, but with 128 PCIe 5.0 lanes instead of 24 lol. But well also these things take ages to arrive here on Chile, so I know they get released now on July, but prob will be here on September-October and being hopeful.\\n\\nThe but on that buying older Server setups is that I don't have a way to, on Chile at least. Checking some ebay sellers very few of them send here but the shipment is just nuts, more than the price of the CPU/MB/etc. It is still cheaper than a new TRx 9000 but not by much :(.\\n\\nAn option I haven't seen yet is on Aliexpress/alibaba, as I buy some electronic tools from there and it takes just 5-7 days to get here.\\n\\nI kinda want the X16 5.0 slots, as my PP is limited now by the bandwidth, as when doing this part, it saturates at 26-28 GiB/s. With X16 5.0 PP would be quite improved (I did the jump from X8 4.0 to X8 5.0 and literally got 2X PP t/s)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fu1vp","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was planning for a 9955WX, which should be 8 channels, and perform as a 9950X, maybe a bit slower, but with 128 PCIe 5.0 lanes instead of 24 lol. But well also these things take ages to arrive here on Chile, so I know they get released now on July, but prob will be here on September-October and being hopeful.&lt;/p&gt;\\n\\n&lt;p&gt;The but on that buying older Server setups is that I don&amp;#39;t have a way to, on Chile at least. Checking some ebay sellers very few of them send here but the shipment is just nuts, more than the price of the CPU/MB/etc. It is still cheaper than a new TRx 9000 but not by much :(.&lt;/p&gt;\\n\\n&lt;p&gt;An option I haven&amp;#39;t seen yet is on Aliexpress/alibaba, as I buy some electronic tools from there and it takes just 5-7 days to get here.&lt;/p&gt;\\n\\n&lt;p&gt;I kinda want the X16 5.0 slots, as my PP is limited now by the bandwidth, as when doing this part, it saturates at 26-28 GiB/s. With X16 5.0 PP would be quite improved (I did the jump from X8 4.0 to X8 5.0 and literally got 2X PP t/s)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fu1vp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752183363,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752183363,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fsvds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fqlkl","score":3,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"TR is a pretty bad deal, and if you go for 4 DIMMs only it's even worse. You'll starve the CPU for memory bandwidth. Take a look at Epyc and 4th Gen Xeon Scalable. Motherboard costs about the same as TR, but the 8480 ES CPUs are very cheap (well under 200 a piece) and 2k will net you 512GB at 4800. The Xeon having 8 channels means you get way more memory bandwidth even with 4800 sticks vs TR, and you also get AMX which supercharges inference on CPU.\\n\\nTBH, if DDR5 RDIMMs were cheaper I'd sell all my Eoycs and P40s and move to those ES CPUs and just keep the 3090s.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2fsvds","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TR is a pretty bad deal, and if you go for 4 DIMMs only it&amp;#39;s even worse. You&amp;#39;ll starve the CPU for memory bandwidth. Take a look at Epyc and 4th Gen Xeon Scalable. Motherboard costs about the same as TR, but the 8480 ES CPUs are very cheap (well under 200 a piece) and 2k will net you 512GB at 4800. The Xeon having 8 channels means you get way more memory bandwidth even with 4800 sticks vs TR, and you also get AMX which supercharges inference on CPU.&lt;/p&gt;\\n\\n&lt;p&gt;TBH, if DDR5 RDIMMs were cheaper I&amp;#39;d sell all my Eoycs and P40s and move to those ES CPUs and just keep the 3090s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fsvds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752183008,"author_flair_text":null,"treatment_tags":[],"created_utc":1752183008,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fqlkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1752182325,"send_replies":true,"parent_id":"t1_n2fp0vj","score":5,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nice question! The MSI Carbon X670E has 3 PCIe slots (2 from CPU, X8/X8 at PCIe 5.0) and one from chipset (X4 4.0).\\n\\nIt has also 4 M2 ports, in which, 2 are connected to the CPU at PCIe 5.0 X4, and the bottom 2 are connected to the chipset, at PCIe 4.0 X4.\\n\\nSo it is like this:\\n\\n* 5090 (1) on X8 5.0 PCIe CPU slot.\\n* 5090 (2) on X8 5.0 PCIe CPU slot.\\n* RTX A6000 on X4 4.0 PCIe Chipset slot.\\n* 4090 (1) on X4 5.0 on M2 CPU slot, with a M2 to PCIe adapter (ADT Link F43SG), running at X4 4.0, the adapter supports PCIe Gen 5 but the 4090 doesn't.\\n* 4090 (2) on X4 5.0 on M2 CPU slot, with a M2 to PCIe adapter (ADT Link F43SG), running at X4 4.0.\\n* 3090 (1) on X4 4.0 on M2 Chipset slot, with a M2 to PCIe adapter (ADT Link F43SP), the adapter supports PCIe Gen 5 but neither the 3090 or the slot doesn't.\\n* 3090 (1) on X4 4.0 on M2 Chipset slot, with a M2 to PCIe adapter (ADT Link F43SP).\\n\\nI plan to move to Threadripper 9000 on Q3/Q4. By some unexpected events, I have some money issues I have to resolve, so probably will wait until end of the year to do the jump. But I won't sell those GPUs lol, as I got them all at a good price, except maybe one 5090.\\n\\nThe jump is quite expensive, as 256GB RAM at 6000Mhz is about 1800USD with 4 DIMMs, motherboard is another 1000 USD and CPU is another 2000 USD in Chile. I don't pay with credit so I have to save about \\\\~5000USD for this.","edited":1752182898,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fqlkl","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice question! The MSI Carbon X670E has 3 PCIe slots (2 from CPU, X8/X8 at PCIe 5.0) and one from chipset (X4 4.0).&lt;/p&gt;\\n\\n&lt;p&gt;It has also 4 M2 ports, in which, 2 are connected to the CPU at PCIe 5.0 X4, and the bottom 2 are connected to the chipset, at PCIe 4.0 X4.&lt;/p&gt;\\n\\n&lt;p&gt;So it is like this:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;5090 (1) on X8 5.0 PCIe CPU slot.&lt;/li&gt;\\n&lt;li&gt;5090 (2) on X8 5.0 PCIe CPU slot.&lt;/li&gt;\\n&lt;li&gt;RTX A6000 on X4 4.0 PCIe Chipset slot.&lt;/li&gt;\\n&lt;li&gt;4090 (1) on X4 5.0 on M2 CPU slot, with a M2 to PCIe adapter (ADT Link F43SG), running at X4 4.0, the adapter supports PCIe Gen 5 but the 4090 doesn&amp;#39;t.&lt;/li&gt;\\n&lt;li&gt;4090 (2) on X4 5.0 on M2 CPU slot, with a M2 to PCIe adapter (ADT Link F43SG), running at X4 4.0.&lt;/li&gt;\\n&lt;li&gt;3090 (1) on X4 4.0 on M2 Chipset slot, with a M2 to PCIe adapter (ADT Link F43SP), the adapter supports PCIe Gen 5 but neither the 3090 or the slot doesn&amp;#39;t.&lt;/li&gt;\\n&lt;li&gt;3090 (1) on X4 4.0 on M2 Chipset slot, with a M2 to PCIe adapter (ADT Link F43SP).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I plan to move to Threadripper 9000 on Q3/Q4. By some unexpected events, I have some money issues I have to resolve, so probably will wait until end of the year to do the jump. But I won&amp;#39;t sell those GPUs lol, as I got them all at a good price, except maybe one 5090.&lt;/p&gt;\\n\\n&lt;p&gt;The jump is quite expensive, as 256GB RAM at 6000Mhz is about 1800USD with 4 DIMMs, motherboard is another 1000 USD and CPU is another 2000 USD in Chile. I don&amp;#39;t pay with credit so I have to save about ~5000USD for this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fqlkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182325,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hxd9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2h7mug","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[llama.cpp also supports AMX](https://github.com/ggml-org/llama.cpp/pull/8998), but I haven't heard anything about it's performance there.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2hxd9x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/8998\\"&gt;llama.cpp also supports AMX&lt;/a&gt;, but I haven&amp;#39;t heard anything about it&amp;#39;s performance there.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2hxd9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752210453,"author_flair_text":null,"treatment_tags":[],"created_utc":1752210453,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2h7mug","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1752199975,"send_replies":true,"parent_id":"t1_n2fp0vj","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"u/FullstackSensei is AMX only useful under ktransformers? Relying on just one repo to use AMX might be risky in the future. If llama.cpp and ik\\\\_llama supports AMX for MoE models then it is worth considering Xeon 8480.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h7mug","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"/u/FullstackSensei\\"&gt;u/FullstackSensei&lt;/a&gt; is AMX only useful under ktransformers? Relying on just one repo to use AMX might be risky in the future. If llama.cpp and ik_llama supports AMX for MoE models then it is worth considering Xeon 8480.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2h7mug/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752199975,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fp0vj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752181851,"send_replies":true,"parent_id":"t3_1lwnj5x","score":2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing these results. How are all the GPUs connected? I mean where do you get all those 20 PCIe 4.0 on top of the x16 5.0 lanes?\\n\\nAnd have you considered moving your rig to an Epyc? You lose the 5.0 lanes with a Rome or Milan Epyc but gain 128 Gen 4 lanes. And if you don't mind throwing 2k at 512GB DDR5 you can even get a dual Xeon 8480 Es system with AMX that'll further speed up those CPU bound layers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fp0vj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing these results. How are all the GPUs connected? I mean where do you get all those 20 PCIe 4.0 on top of the x16 5.0 lanes?&lt;/p&gt;\\n\\n&lt;p&gt;And have you considered moving your rig to an Epyc? You lose the 5.0 lanes with a Rome or Milan Epyc but gain 128 Gen 4 lanes. And if you don&amp;#39;t mind throwing 2k at 512GB DDR5 you can even get a dual Xeon 8480 Es system with AMX that&amp;#39;ll further speed up those CPU bound layers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fp0vj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752181851,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g0qli","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752185430,"send_replies":true,"parent_id":"t1_n2fzi1v","score":1,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"body":"The thing is that PCIe 4.0 X16 is the same as PCIe 5.0 X8 so that's indeed a bottleneck only for this specific offloading case haha.\\n\\nI went from X8 4.0 to X8 5.0 and doubled my PP t/s, so I can imagine maybe not a similar jump but a noticeable one at X16 5.0.\\n\\nAlso 64GB for 100USD on the server side is quite good. You guys have it lucky on ebay :(","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2g0qli","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The thing is that PCIe 4.0 X16 is the same as PCIe 5.0 X8 so that&amp;#39;s indeed a bottleneck only for this specific offloading case haha.&lt;/p&gt;\\n\\n&lt;p&gt;I went from X8 4.0 to X8 5.0 and doubled my PP t/s, so I can imagine maybe not a similar jump but a noticeable one at X16 5.0.&lt;/p&gt;\\n\\n&lt;p&gt;Also 64GB for 100USD on the server side is quite good. You guys have it lucky on ebay :(&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g0qli/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185430,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fzi1v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fyh41","score":2,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"On an Epyc server, you would (will ;) ) have enough lane to go for ×16 so PCIe 4.0 x16 have the same bandwidth as PCIe 5.0 x8 if I'm not mistaken.\\n\\n  \\nEDIT: the specific server was a package deal, but for my 2T dual socket server, I bought DDR4 3200 64GB stick for $100 each on EBay.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2fzi1v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On an Epyc server, you would (will ;) ) have enough lane to go for ×16 so PCIe 4.0 x16 have the same bandwidth as PCIe 5.0 x8 if I&amp;#39;m not mistaken.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: the specific server was a package deal, but for my 2T dual socket server, I bought DDR4 3200 64GB stick for $100 each on EBay.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fzi1v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185033,"author_flair_text":null,"treatment_tags":[],"created_utc":1752185033,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fyh41","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fxw4q","score":2,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because this started as a gaming PC and well things happened lol.\\n\\nMobo: 350USD, CPU: 350USD, RAM: 700USD, total: 1400USD. All used but the RAM.\\n\\nThat's not counting PSUs etc as when I change to Threadripper I will reuse them.\\n\\nA epyc for sure will have more performance.\\n\\nAlso damn 1TB DDR4 is that cheap? Didn't know that. I want to go for PCIe 5.0 if I go Epyc, as I get limited on PP by the PCIe 5.0 X8 bandwidth (26-28 GiB/s)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2fyh41","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because this started as a gaming PC and well things happened lol.&lt;/p&gt;\\n\\n&lt;p&gt;Mobo: 350USD, CPU: 350USD, RAM: 700USD, total: 1400USD. All used but the RAM.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s not counting PSUs etc as when I change to Threadripper I will reuse them.&lt;/p&gt;\\n\\n&lt;p&gt;A epyc for sure will have more performance.&lt;/p&gt;\\n\\n&lt;p&gt;Also damn 1TB DDR4 is that cheap? Didn&amp;#39;t know that. I want to go for PCIe 5.0 if I go Epyc, as I get limited on PP by the PCIe 5.0 X8 bandwidth (26-28 GiB/s)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fyh41/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184709,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752184709,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fxw4q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fwa46","score":1,"author_fullname":"t2_7rqtc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why did you not get a server board ? I would not be surprised if I could have better perf when putting your GPUs on a server that cost me $2500 for Epyc 7742 and 1024 GB (8x128) ECC DDR4 RAM on a ROMED8-2T mobo. (my actual server is different as I went for dual socket for other purposes than LLM).\\n\\nHow much did you server cost, not counting GPUs ?","edited":false,"author_flair_css_class":null,"name":"t1_n2fxw4q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why did you not get a server board ? I would not be surprised if I could have better perf when putting your GPUs on a server that cost me $2500 for Epyc 7742 and 1024 GB (8x128) ECC DDR4 RAM on a ROMED8-2T mobo. (my actual server is different as I went for dual socket for other purposes than LLM).&lt;/p&gt;\\n\\n&lt;p&gt;How much did you server cost, not counting GPUs ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fxw4q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184531,"author_flair_text":null,"collapsed":false,"created_utc":1752184531,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fwa46","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fvgso","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It is kinda both.\\n\\n7800X3D and lower end CPUs (or 9800X3D and lower) have just 1 CCD, so that means you get limited by that before the actual max theoretical bandwidth.\\n\\n7900X/7950X/9900X/9950X have 2 CCDs, so there you can be near the theoretical 100 GB/s at 6000Mhz.\\n\\nNow, consumer CPUs don't support 4 channels, so your limit there is just that, using 2 or 4 DIMMs.\\n\\nFor example TRx 7960X/7970X/9960X/9970X have 4 CCDs and 4 channels, so these ones can do a max theoretical of 160-190 GB/s.\\n\\nAnd then you have things like a 7995WX/9995WX Pro CPUs with 8 channels and 12 CCDs, and the max theoretical is about 700 GB/s. Also I think Epyc have 12 channels so prob even more.\\n\\nFor Intel sadly I'm not sure how it works, but I think it doesn't support 4 channels either on the consumer side.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fwa46","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is kinda both.&lt;/p&gt;\\n\\n&lt;p&gt;7800X3D and lower end CPUs (or 9800X3D and lower) have just 1 CCD, so that means you get limited by that before the actual max theoretical bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;7900X/7950X/9900X/9950X have 2 CCDs, so there you can be near the theoretical 100 GB/s at 6000Mhz.&lt;/p&gt;\\n\\n&lt;p&gt;Now, consumer CPUs don&amp;#39;t support 4 channels, so your limit there is just that, using 2 or 4 DIMMs.&lt;/p&gt;\\n\\n&lt;p&gt;For example TRx 7960X/7970X/9960X/9970X have 4 CCDs and 4 channels, so these ones can do a max theoretical of 160-190 GB/s.&lt;/p&gt;\\n\\n&lt;p&gt;And then you have things like a 7995WX/9995WX Pro CPUs with 8 channels and 12 CCDs, and the max theoretical is about 700 GB/s. Also I think Epyc have 12 channels so prob even more.&lt;/p&gt;\\n\\n&lt;p&gt;For Intel sadly I&amp;#39;m not sure how it works, but I think it doesn&amp;#39;t support 4 channels either on the consumer side.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fwa46/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184038,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752184038,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fvgso","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makistsa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ft8eu","score":1,"author_fullname":"t2_3l1o090d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is it because of the cpu or because of the 2 dimms per channel? My ddr4 intel system had 54GB/s with 1dpc(2x32gb) and fell to 46GB/s with the same settings with 4x32.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2fvgso","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it because of the cpu or because of the 2 dimms per channel? My ddr4 intel system had 54GB/s with 1dpc(2x32gb) and fell to 46GB/s with the same settings with 4x32.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fvgso/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752183790,"author_flair_text":null,"treatment_tags":[],"created_utc":1752183790,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ft8eu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752183116,"send_replies":true,"parent_id":"t1_n2fst8f","score":1,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It really is, it is the main limitation for my TG t/s sadly. A 7900X/7950X/9900X/9950X would bump that to 100 GB/s and it would quite a nice improvement, but sadly the PCIe lanes on consumer boards is really bad, and that is another bottleneck I have on my system.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ft8eu","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It really is, it is the main limitation for my TG t/s sadly. A 7900X/7950X/9900X/9950X would bump that to 100 GB/s and it would quite a nice improvement, but sadly the PCIe lanes on consumer boards is really bad, and that is another bottleneck I have on my system.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2ft8eu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752183116,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fst8f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1752182990,"send_replies":true,"parent_id":"t3_1lwnj5x","score":2,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"«DDR5 6000Mhz, max bandwidth at about 60-62 GB/s»\\n\\nOuch !\\n\\nEDIT : How much did you server cost ? I really wonder what kind of perf one would get with the same budget but a different allocation (either less GPU power but Epyc Gen 4 with 12 memory channels, or probably similar GPU power but Epyc Gen 2 with 8 memory channels of DDR4 3200, the later being my own choice ).","edited":1752183642,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fst8f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;«DDR5 6000Mhz, max bandwidth at about 60-62 GB/s»&lt;/p&gt;\\n\\n&lt;p&gt;Ouch !&lt;/p&gt;\\n\\n&lt;p&gt;EDIT : How much did you server cost ? I really wonder what kind of perf one would get with the same budget but a different allocation (either less GPU power but Epyc Gen 4 with 12 memory channels, or probably similar GPU power but Epyc Gen 2 with 8 memory channels of DDR4 3200, the later being my own choice ).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fst8f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182990,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ft4or","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"created_utc":1752183085,"send_replies":true,"parent_id":"t3_1lwnj5x","score":2,"author_fullname":"t2_c2f558x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the exquisite level of detail, very much appreciated!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ft4or","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the exquisite level of detail, very much appreciated!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2ft4or/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752183085,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kxzzr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iand4","score":2,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"body":"64GB on each should work, when I got these 48GB ones they weren't supposedly supported either haha.\\n\\nI explained it here [https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2fqlkl/](https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2fqlkl/)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2kxzzr","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;64GB on each should work, when I got these 48GB ones they weren&amp;#39;t supposedly supported either haha.&lt;/p&gt;\\n\\n&lt;p&gt;I explained it here &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2fqlkl/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2fqlkl/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2kxzzr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752253614,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752253614,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iand4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurrdurrmeh","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gqdym","score":1,"author_fullname":"t2_i63k7zcc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Noice. Thanks for confirming. \\n\\nI’ve just realised that even the 9950X3D can only address 192GB RAM, so the new 64GB DDIMMs are not so useful 😞\\n\\nI am new to this sort of hardware. So you mind explaining how you can attach 5 GPUs to your motherboard even though it has only 3 PCIe slots?","edited":false,"author_flair_css_class":null,"name":"t1_n2iand4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Noice. Thanks for confirming. &lt;/p&gt;\\n\\n&lt;p&gt;I’ve just realised that even the 9950X3D can only address 192GB RAM, so the new 64GB DDIMMs are not so useful 😞&lt;/p&gt;\\n\\n&lt;p&gt;I am new to this sort of hardware. So you mind explaining how you can attach 5 GPUs to your motherboard even though it has only 3 PCIe slots?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwnj5x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2iand4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752217235,"author_flair_text":null,"collapsed":false,"created_utc":1752217235,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gqdym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gnvq0","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yep it's those ones, but in white!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gqdym","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep it&amp;#39;s those ones, but in white!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gqdym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752193960,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752193960,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gnvq0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurrdurrmeh","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gmbp5","score":1,"author_fullname":"t2_i63k7zcc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you! Looks like they’re 6400 parts running at 6000. Which is amazing for 4 DIMMs!\\n\\nEdit:\\nLooks like they’re these:\\nhttps://a.co/d/7CmdN6l","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2gnvq0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! Looks like they’re 6400 parts running at 6000. Which is amazing for 4 DIMMs!&lt;/p&gt;\\n\\n&lt;p&gt;Edit:\\nLooks like they’re these:\\n&lt;a href=\\"https://a.co/d/7CmdN6l\\"&gt;https://a.co/d/7CmdN6l&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gnvq0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752193069,"author_flair_text":null,"treatment_tags":[],"created_utc":1752193069,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gmbp5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752192524,"send_replies":true,"parent_id":"t1_n2ghqis","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure about the model itself, but booted to windows and got this image, if it helps\\n\\nhttps://preview.redd.it/04jtyhkxz4cf1.png?width=587&amp;format=png&amp;auto=webp&amp;s=19d8810e4d9d249a933124059185c3d63858a837","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gmbp5","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure about the model itself, but booted to windows and got this image, if it helps&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/04jtyhkxz4cf1.png?width=587&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19d8810e4d9d249a933124059185c3d63858a837\\"&gt;https://preview.redd.it/04jtyhkxz4cf1.png?width=587&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19d8810e4d9d249a933124059185c3d63858a837&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gmbp5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192524,"media_metadata":{"04jtyhkxz4cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":156,"x":108,"u":"https://preview.redd.it/04jtyhkxz4cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f2332bd70232d98865710400fe41689eecf19ea"},{"y":312,"x":216,"u":"https://preview.redd.it/04jtyhkxz4cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd93ba2e62325b2a363df32846e25c1f82e6b441"},{"y":462,"x":320,"u":"https://preview.redd.it/04jtyhkxz4cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c7a2c694c868505588bd1618377e05b012eec76"}],"s":{"y":848,"x":587,"u":"https://preview.redd.it/04jtyhkxz4cf1.png?width=587&amp;format=png&amp;auto=webp&amp;s=19d8810e4d9d249a933124059185c3d63858a837"},"id":"04jtyhkxz4cf1"}},"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ghqis","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurrdurrmeh","can_mod_post":false,"created_utc":1752190957,"send_replies":true,"parent_id":"t3_1lwnj5x","score":2,"author_fullname":"t2_i63k7zcc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"hey, are you running 4x48GB DDR5 at 6000 stable on a consumer board?? that's amazing! what RAM SKUs are you using?\\n\\n  \\nI thought 4 DIMMs would never get to 6000...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ghqis","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;hey, are you running 4x48GB DDR5 at 6000 stable on a consumer board?? that&amp;#39;s amazing! what RAM SKUs are you using?&lt;/p&gt;\\n\\n&lt;p&gt;I thought 4 DIMMs would never get to 6000...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2ghqis/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752190957,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g1k2w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g0d0t","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For these specific cases ram B/W matters a lot. Basically if I had 2x60GB/s bandwidth, I would get almost 2x the performance.\\n\\nSo with 3x on your case or a bit more, that is already about 3x times faster.\\n\\nAnd X16 3.0 is still 2 times as fast as X4 4.0, and those are CPU lanes without chipset latency. All those small things add a lot!\\n\\nIt's just I started this with a gaming PC, a 3080 that I had in the past and things just escalated lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g1k2w","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For these specific cases ram B/W matters a lot. Basically if I had 2x60GB/s bandwidth, I would get almost 2x the performance.&lt;/p&gt;\\n\\n&lt;p&gt;So with 3x on your case or a bit more, that is already about 3x times faster.&lt;/p&gt;\\n\\n&lt;p&gt;And X16 3.0 is still 2 times as fast as X4 4.0, and those are CPU lanes without chipset latency. All those small things add a lot!&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s just I started this with a gaming PC, a 3080 that I had in the past and things just escalated lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g1k2w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185696,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752185696,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g0d0t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fv90g","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have 4x3090 but I do have fuller lanes.. PCIE 3.0 only and with PLX switches.\\n\\nMy ram b/w is closer to 200, but I thought that mainly affects t/g and all that extra vram would help a lot more.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2g0d0t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 4x3090 but I do have fuller lanes.. PCIE 3.0 only and with PLX switches.&lt;/p&gt;\\n\\n&lt;p&gt;My ram b/w is closer to 200, but I thought that mainly affects t/g and all that extra vram would help a lot more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2g0d0t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185308,"author_flair_text":null,"treatment_tags":[],"created_utc":1752185308,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fv90g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752183726,"send_replies":true,"parent_id":"t1_n2ful9t","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends of your PC CPU + amount of GPUs.\\n\\nServer/Prosumer CPU+MB with lanes, good RAM bandwidth and less GPUs -&gt; way faster TG t/s.\\n\\nI'm getting limited by:\\n\\n* Weak CPU\\n* Small amount of PCIe lanes\\n* Weak RAM bandwidth\\n* Too many GPUs\\n\\nSomeday when I get a CPU+MB with more lanes I will try.\\n\\nNow if you run fully on GPU, that will be hugely more faster than my setup, and even better if you have pcie lanes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fv90g","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends of your PC CPU + amount of GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;Server/Prosumer CPU+MB with lanes, good RAM bandwidth and less GPUs -&amp;gt; way faster TG t/s.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m getting limited by:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Weak CPU&lt;/li&gt;\\n&lt;li&gt;Small amount of PCIe lanes&lt;/li&gt;\\n&lt;li&gt;Weak RAM bandwidth&lt;/li&gt;\\n&lt;li&gt;Too many GPUs&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Someday when I get a CPU+MB with more lanes I will try.&lt;/p&gt;\\n\\n&lt;p&gt;Now if you run fully on GPU, that will be hugely more faster than my setup, and even better if you have pcie lanes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2fv90g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752183726,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ful9t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752183526,"send_replies":true,"parent_id":"t3_1lwnj5x","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Man, I thought you'd get more prompt t/s. I was getting 40 when testing IQ2_XXS. Its only a little smaller. Using Q8 cache and ffn_exps though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ful9t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man, I thought you&amp;#39;d get more prompt t/s. I was getting 40 when testing IQ2_XXS. Its only a little smaller. Using Q8 cache and ffn_exps though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2ful9t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752183526,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gmi4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752192587,"send_replies":true,"parent_id":"t1_n2gkkqo","score":1,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeh it is 7, but that is a 234GB model, so it is amazing it is usable when offloading. I'm not even close to running it fully on GPUs except if I get a 6000 PRO or 2xA6000/2x6000 Ada/2x5000 PRO.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gmi4a","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeh it is 7, but that is a 234GB model, so it is amazing it is usable when offloading. I&amp;#39;m not even close to running it fully on GPUs except if I get a 6000 PRO or 2xA6000/2x6000 Ada/2x5000 PRO.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gmi4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192587,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gkkqo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1752191918,"send_replies":true,"parent_id":"t3_1lwnj5x","score":1,"author_fullname":"t2_ql2vu0wz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Q2_K_XL performs really good for a system like this!\\n\\nI mean, with 7(? i lost count) GPUs it ought to be, no surprise there","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gkkqo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Q2_K_XL performs really good for a system like this!&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I mean, with 7(? i lost count) GPUs it ought to be, no surprise there&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gkkqo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191918,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kxoqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaitzu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gt03r","score":1,"author_fullname":"t2_oft0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thank you &lt;3","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2kxoqg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thank you &amp;lt;3&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2kxoqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752253527,"author_flair_text":null,"treatment_tags":[],"created_utc":1752253527,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gt03r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752194880,"send_replies":true,"parent_id":"t1_n2gsoyi","score":3,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Posted the zentimings on another comment here [https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2gmbp5/](https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2gmbp5/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gt03r","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Posted the zentimings on another comment here &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2gmbp5/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/comment/n2gmbp5/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwnj5x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gt03r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752194880,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gsoyi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaitzu","can_mod_post":false,"created_utc":1752194772,"send_replies":true,"parent_id":"t3_1lwnj5x","score":1,"author_fullname":"t2_oft0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how do you run all those ram sticks at 6000mhz ?!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gsoyi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how do you run all those ram sticks at 6000mhz ?!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2gsoyi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752194772,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hpoal","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Design1880","can_mod_post":false,"created_utc":1752207010,"send_replies":true,"parent_id":"t3_1lwnj5x","score":1,"author_fullname":"t2_1l045lxp4v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"cool so what are you doing with it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hpoal","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;cool so what are you doing with it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/n2hpoal/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752207010,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwnj5x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
