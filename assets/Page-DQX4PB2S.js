import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi all,\\n  \\nI'm running OpenWebUI v0.6.15 (though I've reproduced it on older versions), and I'm having a consistent problem where my prompt is seemingly truncated. Whether I use the API or the web UI, the model's response clearly indicates that it's not getting the entire prompt.\\n\\nWhen I paste the list before the instructions \\"print the first and last lines\\" as a sanity check, it consistently prints the last line, but it always picks the 3rd or 4th last line as the \\"first\\" line, implying the beginning of the list is being cut off. When I put the instructions _before_ the list, the model just summarizes the list and asks \\"anything else\\", implying the instructions are being cut off. I've tried pasting the list and attaching it as a CSV file, but I get the same results either way.\\n\\nMy file is 70 lines with ~1300 characters per line. OpenWebUI's statistics say my full prompt is ~60k tokens.\\n\\nI've tested with \`qwen3:30b-a3b-q4_K_M\` and \`gemma3:4b\`, which have 40k and 128k context sizes, respectively. My prompt is too big for \`qwen3\`, though it should be getting about half of the lines (it seems to only be getting the last few based on the response). \`gemma3\` should be able to handle it fine.\\n\\nHas anyone experienced something like this? I've tried manually increasing the context size via the advanced params, but nothing changes. Does OpenWebUI silently or \\"smartly\\" truncate prompts? Is this just an inherent limitation of the models (128k context in theory means far less in practice)?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"OpenWebUI - Truncating Context or Model Limitation?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltdi5y","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_22v9emww","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751839652,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m running OpenWebUI v0.6.15 (though I&amp;#39;ve reproduced it on older versions), and I&amp;#39;m having a consistent problem where my prompt is seemingly truncated. Whether I use the API or the web UI, the model&amp;#39;s response clearly indicates that it&amp;#39;s not getting the entire prompt.&lt;/p&gt;\\n\\n&lt;p&gt;When I paste the list before the instructions &amp;quot;print the first and last lines&amp;quot; as a sanity check, it consistently prints the last line, but it always picks the 3rd or 4th last line as the &amp;quot;first&amp;quot; line, implying the beginning of the list is being cut off. When I put the instructions &lt;em&gt;before&lt;/em&gt; the list, the model just summarizes the list and asks &amp;quot;anything else&amp;quot;, implying the instructions are being cut off. I&amp;#39;ve tried pasting the list and attaching it as a CSV file, but I get the same results either way.&lt;/p&gt;\\n\\n&lt;p&gt;My file is 70 lines with ~1300 characters per line. OpenWebUI&amp;#39;s statistics say my full prompt is ~60k tokens.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve tested with &lt;code&gt;qwen3:30b-a3b-q4_K_M&lt;/code&gt; and &lt;code&gt;gemma3:4b&lt;/code&gt;, which have 40k and 128k context sizes, respectively. My prompt is too big for &lt;code&gt;qwen3&lt;/code&gt;, though it should be getting about half of the lines (it seems to only be getting the last few based on the response). &lt;code&gt;gemma3&lt;/code&gt; should be able to handle it fine.&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone experienced something like this? I&amp;#39;ve tried manually increasing the context size via the advanced params, but nothing changes. Does OpenWebUI silently or &amp;quot;smartly&amp;quot; truncate prompts? Is this just an inherent limitation of the models (128k context in theory means far less in practice)?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltdi5y","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Coronoi","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltdi5y/openwebui_truncating_context_or_model_limitation/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltdi5y/openwebui_truncating_context_or_model_limitation/","subreddit_subscribers":496034,"created_utc":1751839652,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qtqcu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PermanentLiminality","can_mod_post":false,"created_utc":1751856882,"send_replies":true,"parent_id":"t3_1ltdi5y","score":2,"author_fullname":"t2_19zqycaf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The model has a max context it can deal with.  Whatever is running the model usually also has a max context setting and then Open WebUI has its setting.  All three need to support your desired context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qtqcu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The model has a max context it can deal with.  Whatever is running the model usually also has a max context setting and then Open WebUI has its setting.  All three need to support your desired context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltdi5y/openwebui_truncating_context_or_model_limitation/n1qtqcu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751856882,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltdi5y","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pnfqd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751841411,"send_replies":true,"parent_id":"t3_1ltdi5y","score":1,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried Openwebui via Lmstudio the other day and despite identical params, I was getting odd results.\\n\\n\\nMaybe it was the same issue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pnfqd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried Openwebui via Lmstudio the other day and despite identical params, I was getting odd results.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe it was the same issue.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltdi5y/openwebui_truncating_context_or_model_limitation/n1pnfqd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841411,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltdi5y","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t3la6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1751896540,"send_replies":true,"parent_id":"t3_1ltdi5y","score":1,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No the answer you’re looking for but I’d recommend revisiting whatever you’re doing to bypass a 60k prompt.\\n\\nWith the current state of local models, that size prompt, is going to cause all types of accuracy problems, especially with lower-end models which are already lobotomized.  \\n\\nAs others have mentioned, look at your max context settings etc, id recommend not using ollama and instead use something with more customization.  Processing 60k tokens is going to be pretty slow though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t3la6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No the answer you’re looking for but I’d recommend revisiting whatever you’re doing to bypass a 60k prompt.&lt;/p&gt;\\n\\n&lt;p&gt;With the current state of local models, that size prompt, is going to cause all types of accuracy problems, especially with lower-end models which are already lobotomized.  &lt;/p&gt;\\n\\n&lt;p&gt;As others have mentioned, look at your max context settings etc, id recommend not using ollama and instead use something with more customization.  Processing 60k tokens is going to be pretty slow though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltdi5y/openwebui_truncating_context_or_model_limitation/n1t3la6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751896540,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltdi5y","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(t,{data:l});export{i as default};
