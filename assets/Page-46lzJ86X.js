import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I finally got a RTX 3060 12GB to start using AI. Now I wanted to know what's the heaviest it can run and if there are new methods of increasing performance by now. Ideally, I can't read at speed of light so models that might run at 4-6 words per second is enough.\\n\\nI can't upgrade from 12GB to 32GB ram yet, so what is this GPU capable of running asides from Wizard Viccuna 13b?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Heaviest model that can be ran with RTX 3060 12Gb?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyhuuq","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.63,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_eljq22kg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752373998,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I finally got a RTX 3060 12GB to start using AI. Now I wanted to know what&amp;#39;s the heaviest it can run and if there are new methods of increasing performance by now. Ideally, I can&amp;#39;t read at speed of light so models that might run at 4-6 words per second is enough.&lt;/p&gt;\\n\\n&lt;p&gt;I can&amp;#39;t upgrade from 12GB to 32GB ram yet, so what is this GPU capable of running asides from Wizard Viccuna 13b?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lyhuuq","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"WEREWOLF_BX13","discussion_type":null,"num_comments":18,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/","subreddit_subscribers":498344,"created_utc":1752373998,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u4jqh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WEREWOLF_BX13","can_mod_post":false,"created_utc":1752375596,"send_replies":true,"parent_id":"t1_n2u1jyh","score":1,"author_fullname":"t2_eljq22kg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"will give it a try","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u4jqh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;will give it a try&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2u4jqh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752375596,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u1jyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752374394,"send_replies":true,"parent_id":"t3_1lyhuuq","score":9,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Phi 4 is probably best all around. Gemma 3 12b is good too with vision. Qwen 3 14b worth a go too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u1jyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Phi 4 is probably best all around. Gemma 3 12b is good too with vision. Qwen 3 14b worth a go too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2u1jyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374394,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u4hav","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WEREWOLF_BX13","can_mod_post":false,"created_utc":1752375570,"send_replies":true,"parent_id":"t1_n2u2dfy","score":1,"author_fullname":"t2_eljq22kg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"will take a look","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u4hav","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;will take a look&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2u4hav/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752375570,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u2dfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"duyntnet","can_mod_post":false,"created_utc":1752374716,"send_replies":true,"parent_id":"t3_1lyhuuq","score":4,"author_fullname":"t2_4d4pk3c4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run quantized 14B or smaller models with decent speed. Try newest models first because they generally are better. Some models: Qwen 3 14B, Gemma 3 12B, Mistral Nemo.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u2dfy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run quantized 14B or smaller models with decent speed. Try newest models first because they generally are better. Some models: Qwen 3 14B, Gemma 3 12B, Mistral Nemo.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2u2dfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374716,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2uevmp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752379997,"send_replies":true,"parent_id":"t3_1lyhuuq","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Start from Mistral 12B, Gemma 12B, Qwen 14B, Phi, etc, then you can start exploring finetunes\\n(I think you should expect much faster than 4 t/s)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2uevmp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Start from Mistral 12B, Gemma 12B, Qwen 14B, Phi, etc, then you can start exploring finetunes\\n(I think you should expect much faster than 4 t/s)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2uevmp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752379997,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v3guz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Final_Wheel_7486","can_mod_post":false,"created_utc":1752392792,"send_replies":true,"parent_id":"t3_1lyhuuq","score":1,"author_fullname":"t2_cyrs5dhp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can absolutely NOT recommend Phi 4 because Gemma 3 12b and Qwen 3 14b exist. Phi 4 is terrible compared to those.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v3guz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can absolutely NOT recommend Phi 4 because Gemma 3 12b and Qwen 3 14b exist. Phi 4 is terrible compared to those.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2v3guz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752392792,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vsw8u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TCaschy","can_mod_post":false,"created_utc":1752407096,"send_replies":true,"parent_id":"t3_1lyhuuq","score":1,"author_fullname":"t2_3pc4ugc3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"gemma 3 12b is pretty great with my 3060 12gb. for reasoning/thinking model, I've recently been using unsloth-Qwen3-30B-A3B-GGUF:Q2\\\\_K\\\\_XL and its been pretty great as well with 20+ tk/s and good accuracy on more complicated tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vsw8u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gemma 3 12b is pretty great with my 3060 12gb. for reasoning/thinking model, I&amp;#39;ve recently been using unsloth-Qwen3-30B-A3B-GGUF:Q2_K_XL and its been pretty great as well with 20+ tk/s and good accuracy on more complicated tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2vsw8u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752407096,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vwckj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evolution31415","can_mod_post":false,"created_utc":1752408663,"send_replies":true,"parent_id":"t3_1lyhuuq","score":1,"author_fullname":"t2_1oih32c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The heaviest possible is [Qwen3 32B](https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF) with 2-bits quantization.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vwckj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The heaviest possible is &lt;a href=\\"https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF\\"&gt;Qwen3 32B&lt;/a&gt; with 2-bits quantization.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2vwckj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752408663,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2w6n50","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Ellary-","can_mod_post":false,"created_utc":1752412785,"send_replies":true,"parent_id":"t3_1lyhuuq","score":1,"author_fullname":"t2_s4zzntp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run up to 32b, around 3\\\\~ tps.  \\nRunning Gemma 3 27b at 4.5\\\\~ tps, Gemma 3 12b 25\\\\~ tps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2w6n50","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run up to 32b, around 3~ tps.&lt;br/&gt;\\nRunning Gemma 3 27b at 4.5~ tps, Gemma 3 12b 25~ tps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2w6n50/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752412785,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wdv5g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ProposalOrganic1043","can_mod_post":false,"created_utc":1752415352,"send_replies":true,"parent_id":"t3_1lyhuuq","score":1,"author_fullname":"t2_cd4wttkt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Lets make a reverse benchmark, top models that could be run on a particular graphics card with a specific quantisation?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wdv5g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lets make a reverse benchmark, top models that could be run on a particular graphics card with a specific quantisation?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2wdv5g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752415352,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v861s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2v7efu","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah you get tiny context but I think that is fine because using tiny contexts is one of the best ways to squeeze more performance out of local LLMs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2v861s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah you get tiny context but I think that is fine because using tiny contexts is one of the best ways to squeeze more performance out of local LLMs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2v861s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752395523,"author_flair_text":null,"treatment_tags":[],"created_utc":1752395523,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2v7efu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"social_tech_10","can_mod_post":false,"created_utc":1752395065,"send_replies":true,"parent_id":"t1_n2u9e19","score":2,"author_fullname":"t2_2e1p9ppm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This.  Mistral-small is very very good in this size range.  Even if you can only offload 90% to GPU, it won't run that much slower than 100% GPU, if speed is not your primary concern.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v7efu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This.  Mistral-small is very very good in this size range.  Even if you can only offload 90% to GPU, it won&amp;#39;t run that much slower than 100% GPU, if speed is not your primary concern.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2v7efu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752395065,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u9e19","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752377632,"send_replies":true,"parent_id":"t3_1lyhuuq","score":1,"author_fullname":"t2_131eezppgs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run around 22B or so in 4 bit","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u9e19","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run around 22B or so in 4 bit&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2u9e19/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752377632,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v22i0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2u1dfn","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Welcome to January 2024.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2v22i0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Welcome to January 2024.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2v22i0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752391993,"author_flair_text":null,"treatment_tags":[],"created_utc":1752391993,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u1dfn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752374323,"send_replies":true,"parent_id":"t1_n2u16b1","score":6,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama 2 13b ðŸ˜‚ðŸ˜‚","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u1dfn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 2 13b ðŸ˜‚ðŸ˜‚&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2u1dfn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374323,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v3ioc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Final_Wheel_7486","can_mod_post":false,"created_utc":1752392820,"send_replies":true,"parent_id":"t1_n2u16b1","score":1,"author_fullname":"t2_cyrs5dhp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v3ioc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2v3ioc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752392820,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vky5h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cool-Chemical-5629","can_mod_post":false,"created_utc":1752403020,"send_replies":true,"parent_id":"t1_n2u16b1","score":1,"author_fullname":"t2_qz1qjc86","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://i.redd.it/vaiioxqvdmcf1.gif","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vky5h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://i.redd.it/vaiioxqvdmcf1.gif\\"&gt;https://i.redd.it/vaiioxqvdmcf1.gif&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyhuuq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2vky5h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752403020,"media_metadata":{"vaiioxqvdmcf1":{"status":"valid","e":"AnimatedImage","m":"image/gif","p":[{"y":60,"x":108,"u":"https://preview.redd.it/vaiioxqvdmcf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=1a8baa69951353c6152b49c3c2fac06664e37445"},{"y":121,"x":216,"u":"https://preview.redd.it/vaiioxqvdmcf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=9bac9b60632062c7208235980edf5c3975208281"},{"y":180,"x":320,"u":"https://preview.redd.it/vaiioxqvdmcf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=f9863bb02e41ea5c1ad4fb52561ee7d45a3668ef"}],"s":{"y":281,"gif":"https://i.redd.it/vaiioxqvdmcf1.gif","mp4":"https://preview.redd.it/vaiioxqvdmcf1.gif?format=mp4&amp;s=92992b70a8373ed9bd4af4220e00757adb084530","x":498},"id":"vaiioxqvdmcf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u16b1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConZ372","can_mod_post":false,"created_utc":1752374247,"send_replies":true,"parent_id":"t3_1lyhuuq","score":0,"author_fullname":"t2_z6uistk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wizard-Vicuna 13B, Llama 2 13B, Mistral 7B are all good models you can run at a reasonable speed with one 3060, look into exllama they have some pretty good performance gains on NVIDIA hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u16b1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wizard-Vicuna 13B, Llama 2 13B, Mistral 7B are all good models you can run at a reasonable speed with one 3060, look into exllama they have some pretty good performance gains on NVIDIA hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/n2u16b1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374247,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyhuuq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
