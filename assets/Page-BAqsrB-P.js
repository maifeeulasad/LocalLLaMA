import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const o=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:`I'm a beginner developer who just completed my first AI project. In past, I almost dedicated to traditional frontend, backend and toolchain development and know a little knowledges about AI. Recently, I'm working for a toolchain project of myself and compositing its documents. An idea suddenly emerges, I could utilize MCP to told AI project's details and make agent help me coding. After communicating with GPT, I decided to adopt the following technology stacks:

* **Backend**: FastAPI + Python
* **Vector DB**: ChromaDB (with memory fallback)
* **Embeddings**: Sentence Transformers
* **LLM**: Local Qwen2.5-7B via Ollama
* **Architecture**: RAG (Retrieval-Augmented Generation) 

Before vectoring document, I decided to split chunks from every document instead of directly adopting, considering that the model token requirment is limited and documents contains lots markdown and markdown involves lots subtiltle like h2, h3, h4. Approximately spending half hours, I finished this target and successed vectoring documents and chunks. But according to results from test units, outcomes based on similarity pattern looks so bad. Because some keywords don't explicitly present on original text and result in unavaliable information matched. Then I read about multi-round retrieval. The idea: do a broad search first, then refine it. It actually worked better! Not perfect, but definitely an improvement. 

When tasks were above finished, I start to call local LLMs through ollama. The development of later story is better smoth than data preprocess. With the prompts that match the context of the input information, splice in the input problem, and the large model quickly gives me the answer I want. But the practice of MCP is terrible for me. GPT gives me lots dirty codes which include tedious access chain using any type, invalid function signature and incorrect parameters pass. What's worst, it's no support MCP integration for Cursor IDE I often use. Therefore, AI told me calling function by HTTP is fine compared to MCP. Ultimately, I had to give up call the knowledge base by MCP method.`,user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"I built a RAG-powered knowledge base for docs of my project using FastAPI + Ollama. Here's what I learned.",link_flair_richtext:[{e:"text",t:"Discussion"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!1,name:"t3_1lsox8o",quarantine:!1,link_flair_text_color:"light",upvote_ratio:.58,author_flair_background_color:null,subreddit_type:"public",ups:2,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_bg90uypls",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"Discussion",can_mod_post:!1,score:2,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"self",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!0,mod_note:null,created:1751763680,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"self.LocalLLaMA",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I&amp;#39;m a beginner developer who just completed my first AI project. In past, I almost dedicated to traditional frontend, backend and toolchain development and know a little knowledges about AI. Recently, I&amp;#39;m working for a toolchain project of myself and compositing its documents. An idea suddenly emerges, I could utilize MCP to told AI project&amp;#39;s details and make agent help me coding. After communicating with GPT, I decided to adopt the following technology stacks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: FastAPI + Python&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vector DB&lt;/strong&gt;: ChromaDB (with memory fallback)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embeddings&lt;/strong&gt;: Sentence Transformers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: Local Qwen2.5-7B via Ollama&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: RAG (Retrieval-Augmented Generation) &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before vectoring document, I decided to split chunks from every document instead of directly adopting, considering that the model token requirment is limited and documents contains lots markdown and markdown involves lots subtiltle like h2, h3, h4. Approximately spending half hours, I finished this target and successed vectoring documents and chunks. But according to results from test units, outcomes based on similarity pattern looks so bad. Because some keywords don&amp;#39;t explicitly present on original text and result in unavaliable information matched. Then I read about multi-round retrieval. The idea: do a broad search first, then refine it. It actually worked better! Not perfect, but definitely an improvement. &lt;/p&gt;

&lt;p&gt;When tasks were above finished, I start to call local LLMs through ollama. The development of later story is better smoth than data preprocess. With the prompts that match the context of the input information, splice in the input problem, and the large model quickly gives me the answer I want. But the practice of MCP is terrible for me. GPT gives me lots dirty codes which include tedious access chain using any type, invalid function signature and incorrect parameters pass. What&amp;#39;s worst, it&amp;#39;s no support MCP integration for Cursor IDE I often use. Therefore, AI told me calling function by HTTP is fine compared to MCP. Ultimately, I had to give up call the knowledge base by MCP method.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,view_count:null,archived:!1,no_follow:!1,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"5f921ea4-c7bc-11ed-9c23-3a00622979b4",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#646d73",id:"1lsox8o",is_robot_indexable:!0,num_duplicates:0,report_reasons:null,author:"Ansurfen",discussion_type:null,num_comments:0,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1lsox8o/i_built_a_ragpowered_knowledge_base_for_docs_of/",stickied:!1,url:"https://www.reddit.com/r/LocalLLaMA/comments/1lsox8o/i_built_a_ragpowered_knowledge_base_for_docs_of/",subreddit_subscribers:495396,created_utc:1751763680,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[],before:null}}],i=()=>e.jsx(t,{data:o});export{i as default};
