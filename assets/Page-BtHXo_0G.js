import{j as e}from"./index-CNyNkRpk.js";import{R as a}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm getting back into AI/ML and planning to run LLMs and other SOTA models in my free time. I previously bought an MSI RTX 3090 24G, but it failed just after a year and MSI declared it \\"irreparable\\" even with a paid repair option. Can you believe a card that expensive came with just a one-year warranty? Super frustrating and waste of huge money!\\n\\nAnyway, due to the current GPU prices in Japan, NVIDIA seems out of my budget. So I'm considering switching to an AMD GPU instead. I'm currently using:\\n* Intel Core i9-13900K\\n* ASUS Prime Z790-P-CSM motherboard\\n* Corsair RM1000x (1000W PSU)\\n\\nBelow are my main concerns:\\n1. Compatibility: Will an AMD GPU work smoothly with my current setup?\\n2. AI/ML support: Are AMD cards reliable enough for AI/ML? I know NVIDIA has better support with cuDNN (CUDA), but I'm open to alternatives.\\n3. Local/online deals: Any tips on where to get affordable AMD cards in Nagoya (Japan)?\\n\\nWould love to hear from anyone who's using AMD GPUs for ML or has been in a similar situation. Thanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is AMD GPU a viable choice for AI/ML task with Intel processor?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lveslz","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1e9b3jq068","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752054977,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m getting back into AI/ML and planning to run LLMs and other SOTA models in my free time. I previously bought an MSI RTX 3090 24G, but it failed just after a year and MSI declared it &amp;quot;irreparable&amp;quot; even with a paid repair option. Can you believe a card that expensive came with just a one-year warranty? Super frustrating and waste of huge money!&lt;/p&gt;\\n\\n&lt;p&gt;Anyway, due to the current GPU prices in Japan, NVIDIA seems out of my budget. So I&amp;#39;m considering switching to an AMD GPU instead. I&amp;#39;m currently using:\\n* Intel Core i9-13900K\\n* ASUS Prime Z790-P-CSM motherboard\\n* Corsair RM1000x (1000W PSU)&lt;/p&gt;\\n\\n&lt;p&gt;Below are my main concerns:\\n1. Compatibility: Will an AMD GPU work smoothly with my current setup?\\n2. AI/ML support: Are AMD cards reliable enough for AI/ML? I know NVIDIA has better support with cuDNN (CUDA), but I&amp;#39;m open to alternatives.\\n3. Local/online deals: Any tips on where to get affordable AMD cards in Nagoya (Japan)?&lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear from anyone who&amp;#39;s using AMD GPUs for ML or has been in a similar situation. Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lveslz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"exotic_soba","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lveslz/is_amd_gpu_a_viable_choice_for_aiml_task_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lveslz/is_amd_gpu_a_viable_choice_for_aiml_task_with/","subreddit_subscribers":497025,"created_utc":1752054977,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25c6di","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752055459,"send_replies":true,"parent_id":"t3_1lveslz","score":4,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are using anything llama.cpp based, AMD gpus will work fine. Vulkan and ROCm are in a good state. Assuming you can use linux, PyTorch+ROCm support is out of the box if you want to hand-roll anything with that.\\n\\nYou can literally put in any GPU in your setup hardware-wise, i cant remember the last time there is compatability issues in the last 20 years.\\n\\nTechYesCity on youtube makes some videos about japan local deals, idk if thats helpful for your location though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25c6di","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are using anything llama.cpp based, AMD gpus will work fine. Vulkan and ROCm are in a good state. Assuming you can use linux, PyTorch+ROCm support is out of the box if you want to hand-roll anything with that.&lt;/p&gt;\\n\\n&lt;p&gt;You can literally put in any GPU in your setup hardware-wise, i cant remember the last time there is compatability issues in the last 20 years.&lt;/p&gt;\\n\\n&lt;p&gt;TechYesCity on youtube makes some videos about japan local deals, idk if thats helpful for your location though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lveslz/is_amd_gpu_a_viable_choice_for_aiml_task_with/n25c6di/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752055459,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lveslz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),o=()=>e.jsx(a,{data:t});export{o as default};
