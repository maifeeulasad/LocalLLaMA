[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi,\n\nI was wondering since I pay so much for Claude Code, if I can somehow use any llocal LLM model for coding simliar for coding?\n\nI have an 4080 Super and 32GB RAM (which I know is not a lot), is there any model that I can use for coding llocally? Sorry I have not been keeping up every day with new models etc.\n\nAnd if yes, is there any way to use Cursor with it? Im using Claude Code Terminal within Cursor currently.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Want to switch from Claude code (I have a 4080 Super)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mda326",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_9w9hk",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753890377,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I was wondering since I pay so much for Claude Code, if I can somehow use any llocal LLM model for coding simliar for coding?&lt;/p&gt;\n\n&lt;p&gt;I have an 4080 Super and 32GB RAM (which I know is not a lot), is there any model that I can use for coding llocally? Sorry I have not been keeping up every day with new models etc.&lt;/p&gt;\n\n&lt;p&gt;And if yes, is there any way to use Cursor with it? Im using Claude Code Terminal within Cursor currently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mda326",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "nofuture09",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/",
            "subreddit_subscribers": 507275,
            "created_utc": 1753890377,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n60p6wi",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "InsideYork",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n600dm1",
                                "score": 3,
                                "author_fullname": "t2_12s3hn4y0b",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You can use that but I’d use qwen code https://github.com/QwenLM/qwen-code",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n60p6wi",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can use that but I’d use qwen code &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mda326",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n60p6wi/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753898211,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753898211,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n600dm1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "prusswan",
                      "can_mod_post": false,
                      "created_utc": 1753891411,
                      "send_replies": true,
                      "parent_id": "t1_n5zxy7i",
                      "score": 2,
                      "author_fullname": "t2_kegwk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Do you know if Qwen3 Coder (run locally) can be hooked up to work like Windsurf's Cascade? Or I need to use a different IDE/plugin to get a similar effect",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n600dm1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you know if Qwen3 Coder (run locally) can be hooked up to work like Windsurf&amp;#39;s Cascade? Or I need to use a different IDE/plugin to get a similar effect&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mda326",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n600dm1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753891411,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6174wq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "mnt_brain",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n60ficr",
                                "score": 1,
                                "author_fullname": "t2_1mtt9dytfn",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Only for long conversations- for like 3-turns its probably golden",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6174wq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Only for long conversations- for like 3-turns its probably golden&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mda326",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n6174wq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753903269,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753903269,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n60ficr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "COBECT",
                      "can_mod_post": false,
                      "created_utc": 1753895627,
                      "send_replies": true,
                      "parent_id": "t1_n5zxy7i",
                      "score": 3,
                      "author_fullname": "t2_1umam7ln",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "32k context is too low for Clion since it has pretty bit system prompt.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n60ficr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;32k context is too low for Clion since it has pretty bit system prompt.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mda326",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n60ficr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753895627,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5zxy7i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DarkArtsMastery",
            "can_mod_post": false,
            "created_utc": 1753890732,
            "send_replies": true,
            "parent_id": "t3_1mda326",
            "score": 23,
            "author_fullname": "t2_1a0fe1cy0a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Tomorrow there will be Qwen3 Coder 30A3, I will make a post on how to fit in within 16GB GPUs fully with decent 32k context window. I strongly suspect it will be the best coding model by far for local use.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5zxy7i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tomorrow there will be Qwen3 Coder 30A3, I will make a post on how to fit in within 16GB GPUs fully with decent 32k context window. I strongly suspect it will be the best coding model by far for local use.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n5zxy7i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753890732,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mda326",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 23
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n60gqk8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "abnormal_human",
            "can_mod_post": false,
            "created_utc": 1753895952,
            "send_replies": true,
            "parent_id": "t3_1mda326",
            "score": 10,
            "author_fullname": "t2_5y02z",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I mean sure, you can, but it won't be as good as Claude. \n\nAs much as I like local AI, and have 240GB of Ada generation VRAM sitting around here, I tend to use paid services like Claude for coding because small differences in quality make big differences in productivity and speed also matters.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60gqk8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I mean sure, you can, but it won&amp;#39;t be as good as Claude. &lt;/p&gt;\n\n&lt;p&gt;As much as I like local AI, and have 240GB of Ada generation VRAM sitting around here, I tend to use paid services like Claude for coding because small differences in quality make big differences in productivity and speed also matters.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n60gqk8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753895952,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mda326",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n60bg80",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "grabber4321",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n607kh0",
                                "score": 3,
                                "author_fullname": "t2_1180s0gl",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You can use also Docs functionality in Continue extension to point to a website with documentation and it will work as sort of a RAG system to pull from the site's knowledge. \n\nOr you can point to folder you are workin within and have it look through the files.\n\nThis is a bit deeper work that you can do. You do need a good GPU for it. I've worked on 1070 with 8GB and now working on 5070 ti 16GB.\n\nCheck out the docs here:\nhttps://docs.continue.dev/\n\nBest models I've used for coding have been based on Deepseek/Qwen.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n60bg80",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can use also Docs functionality in Continue extension to point to a website with documentation and it will work as sort of a RAG system to pull from the site&amp;#39;s knowledge. &lt;/p&gt;\n\n&lt;p&gt;Or you can point to folder you are workin within and have it look through the files.&lt;/p&gt;\n\n&lt;p&gt;This is a bit deeper work that you can do. You do need a good GPU for it. I&amp;#39;ve worked on 1070 with 8GB and now working on 5070 ti 16GB.&lt;/p&gt;\n\n&lt;p&gt;Check out the docs here:\n&lt;a href=\"https://docs.continue.dev/\"&gt;https://docs.continue.dev/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Best models I&amp;#39;ve used for coding have been based on Deepseek/Qwen.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mda326",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n60bg80/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753894536,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753894536,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n60a381",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "grabber4321",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n607kh0",
                                "score": 2,
                                "author_fullname": "t2_1180s0gl",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Tab completes are done via 3B model. You use 7B or 14B for larger pieces. \n\nYou need to look at Continue and Copilot VS Code extensions: \n\nhttps://www.youtube.com/watch?v=qbE9pKOby3A",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n60a381",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tab completes are done via 3B model. You use 7B or 14B for larger pieces. &lt;/p&gt;\n\n&lt;p&gt;You need to look at Continue and Copilot VS Code extensions: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=qbE9pKOby3A\"&gt;https://www.youtube.com/watch?v=qbE9pKOby3A&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mda326",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n60a381/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753894168,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753894168,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n607kh0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FORLLM",
                      "can_mod_post": false,
                      "created_utc": 1753893458,
                      "send_replies": true,
                      "parent_id": "t1_n602fzi",
                      "score": 2,
                      "author_fullname": "t2_1rav5zv0dv",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "My immediate instinct is to tell the OP no, you flat out can't replace claude code with a small local model, but since it sounds like you have real experience, would you mind elaborating on what you can replace? When you say working on smaller pieces Are you using like code complete or just passing very small amounts of the codebase as context or something else? Any elaboration on workflow that you've found helpful would be awesome if you're willing to share.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n607kh0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My immediate instinct is to tell the OP no, you flat out can&amp;#39;t replace claude code with a small local model, but since it sounds like you have real experience, would you mind elaborating on what you can replace? When you say working on smaller pieces Are you using like code complete or just passing very small amounts of the codebase as context or something else? Any elaboration on workflow that you&amp;#39;ve found helpful would be awesome if you&amp;#39;re willing to share.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mda326",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n607kh0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753893458,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n602fzi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "grabber4321",
            "can_mod_post": false,
            "created_utc": 1753892008,
            "send_replies": true,
            "parent_id": "t3_1mda326",
            "score": 5,
            "author_fullname": "t2_1180s0gl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen2.5-Coder-Instruct-14B and Qwen2.5-Coder-Instruct-7B are still my go-to's.\n\nPeople also like Devstral, but I havent used it a lot. \n\nIts not going to match a full blown online service but if you work on smaller pieces it can do good work (sometimes even better than online services)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n602fzi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen2.5-Coder-Instruct-14B and Qwen2.5-Coder-Instruct-7B are still my go-to&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;People also like Devstral, but I havent used it a lot. &lt;/p&gt;\n\n&lt;p&gt;Its not going to match a full blown online service but if you work on smaller pieces it can do good work (sometimes even better than online services)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n602fzi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753892008,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mda326",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n613rnv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1753902296,
            "send_replies": true,
            "parent_id": "t3_1mda326",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "No, not really. The size of models you could run with that hardware wouldn't figure out how to use agentic tools half the time and will melt down. You at least need to add like, a 3090 also with your 4080 so you can do a 32B model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n613rnv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, not really. The size of models you could run with that hardware wouldn&amp;#39;t figure out how to use agentic tools half the time and will melt down. You at least need to add like, a 3090 also with your 4080 so you can do a 32B model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n613rnv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753902296,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mda326",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n620z29",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Unlucky-Message8866",
            "can_mod_post": false,
            "created_utc": 1753911759,
            "send_replies": true,
            "parent_id": "t3_1mda326",
            "score": 1,
            "author_fullname": "t2_de5gm7w40",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "sorry mate but 128k tokens are barely enough for the system prompt, the relevant files you are working on and 3 turns to accomplish whatever you ask.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n620z29",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;sorry mate but 128k tokens are barely enough for the system prompt, the relevant files you are working on and 3 turns to accomplish whatever you ask.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n620z29/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753911759,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mda326",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62rvti",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sleepy_roger",
            "can_mod_post": false,
            "created_utc": 1753920393,
            "send_replies": true,
            "parent_id": "t3_1mda326",
            "score": 1,
            "author_fullname": "t2_usojvms",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You're going to need a much better machine.\n\n\nLocal AI is amazing but we're not at the point yet where a local model small enough to run on a 4080 can really meaningfully compete.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62rvti",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;re going to need a much better machine.&lt;/p&gt;\n\n&lt;p&gt;Local AI is amazing but we&amp;#39;re not at the point yet where a local model small enough to run on a 4080 can really meaningfully compete.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mda326/want_to_switch_from_claude_code_i_have_a_4080/n62rvti/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753920393,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mda326",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]