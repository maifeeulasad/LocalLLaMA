[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi all,\n\nI wanted to share my recent experience (and save others some hours of troubleshooting!) trying to run the new GPT-OSS-20B F16/MXFP4/MOE GGUF models locally via `llama.cpp` and `llama-cpp-python` — and to confirm that as of August 7, 2025, this is NOT yet supported, regardless of what you try.\n\n# What I did:\n\n1. Built an isolated Python virtual environment Using Windows 11, Python 3.11, latest pip, etc.\n2. Compiled llama-cpp-python from source\n   * Cloned [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python) with `--recursive`\n   * Explicitly updated the `vendor/llama.cpp` submodule:\n      * Switched to upstream origin: `git remote set-url origin` [`https://github.com/ggerganov/llama.cpp.git`](https://github.com/ggerganov/llama.cpp.git)\n      * Checked out latest `master`, did `git pull origin master`\n      * Confirmed commit:yamlCopyEditcommit 5fd160bbd9d70b94b5b11b0001fd7f477005e4a0 (HEAD -&gt; master, tag: b6106, origin/master, origin/HEAD) Date:   Wed Aug 6 15:14:40 2025 -0700 \n   * Compiled with `FORCE_CMAKE=1`, CPU only\n3. Downloaded the official Unsloth GPT-OSS-20B F16 GGUF\n   * 13.4 GB\n   * Downloaded directly from HuggingFace, verified SHA256, file size matches exactly.\n4. Tested file integrity with a custom Python script:\n   * Confirmed GGUF header, no corruption, full SHA256 check.\n5. Tried loading the model with llama\\_cpp.Llama (chat\\_format=\"gpt-oss\")\n   * Also tested with the latest compiled `main.exe` from `llama.cpp` directly.\n   * Tried both with F16 and Q0\\_0 versions.\n\n# The error (every single time):\n\n    pgsqlCopyEditgguf_init_from_file_impl: tensor 'blk.0.ffn_down_exps.weight' has invalid ggml type 39 (NONE)\n    gguf_init_from_file_impl: failed to read tensor info\n    llama_model_load: error loading model: llama_model_loader: failed to load model from xxx.gguf\n    llama_model_load_from_file_impl: failed to load model\n    [ERRO] Failed to load model from file: xxx.gguf\n    \n\n# What this means:\n\n* As of the most recent commit (`b6106`, Aug 6, 2025) on `llama.cpp` and the latest source build of `llama-cpp-python`, there is still NO support for the new MXFP4 tensor type (ggml type 39) required by GPT-OSS F16/MXFP4/MOE models.\n* This is not an issue with your build, Python, environment, or file.\n* The GGUF files themselves are valid and pass header/hash checks.\n* No one can run these models locally via vanilla llama.cpp at this time**.** (I even tried other quantizations; only the latest MXFP4/F16 fail like this.)\n\n# What to do?\n\n* Wait for an official update / PR / patch in llama.cpp that adds MXFP4 and GPT-OSS F16/MOE support.\n* Track issues on [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp/issues) and the HuggingFace repo for progress.\n* When that happens, just update and recompile — no extra hacks should be needed.\n\n# Conclusion:\n\nIf you’re seeing  \n`gguf_init_from_file_impl: tensor 'blk.0.ffn_down_exps.weight' has invalid ggml type 39 (NONE)`  \ntrying to load GPT-OSS-20B F16/MXFP4, **it’s not you — it’s the code!**\n\nWe’re all waiting for upstream support.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "GPT-OSS-20B F16/MXFP4 GGUF Models Not Loading on Latest llama.cpp: \"tensor ... has invalid ggml type 39 (NONE)\"",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjm5vm",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_l2tfh53yn",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754528046,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I wanted to share my recent experience (and save others some hours of troubleshooting!) trying to run the new GPT-OSS-20B F16/MXFP4/MOE GGUF models locally via &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;llama-cpp-python&lt;/code&gt; — and to confirm that as of August 7, 2025, this is NOT yet supported, regardless of what you try.&lt;/p&gt;\n\n&lt;h1&gt;What I did:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Built an isolated Python virtual environment Using Windows 11, Python 3.11, latest pip, etc.&lt;/li&gt;\n&lt;li&gt;Compiled llama-cpp-python from source\n\n&lt;ul&gt;\n&lt;li&gt;Cloned &lt;a href=\"https://github.com/abetlen/llama-cpp-python\"&gt;abetlen/llama-cpp-python&lt;/a&gt; with &lt;code&gt;--recursive&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Explicitly updated the &lt;code&gt;vendor/llama.cpp&lt;/code&gt; submodule:\n\n&lt;ul&gt;\n&lt;li&gt;Switched to upstream origin: &lt;code&gt;git remote set-url origin&lt;/code&gt; &lt;a href=\"https://github.com/ggerganov/llama.cpp.git\"&gt;&lt;code&gt;https://github.com/ggerganov/llama.cpp.git&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Checked out latest &lt;code&gt;master&lt;/code&gt;, did &lt;code&gt;git pull origin master&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Confirmed commit:yamlCopyEditcommit 5fd160bbd9d70b94b5b11b0001fd7f477005e4a0 (HEAD -&amp;gt; master, tag: b6106, origin/master, origin/HEAD) Date:   Wed Aug 6 15:14:40 2025 -0700 &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Compiled with &lt;code&gt;FORCE_CMAKE=1&lt;/code&gt;, CPU only&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Downloaded the official Unsloth GPT-OSS-20B F16 GGUF\n\n&lt;ul&gt;\n&lt;li&gt;13.4 GB&lt;/li&gt;\n&lt;li&gt;Downloaded directly from HuggingFace, verified SHA256, file size matches exactly.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Tested file integrity with a custom Python script:\n\n&lt;ul&gt;\n&lt;li&gt;Confirmed GGUF header, no corruption, full SHA256 check.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Tried loading the model with llama_cpp.Llama (chat_format=&amp;quot;gpt-oss&amp;quot;)\n\n&lt;ul&gt;\n&lt;li&gt;Also tested with the latest compiled &lt;code&gt;main.exe&lt;/code&gt; from &lt;code&gt;llama.cpp&lt;/code&gt; directly.&lt;/li&gt;\n&lt;li&gt;Tried both with F16 and Q0_0 versions.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;The error (every single time):&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;pgsqlCopyEditgguf_init_from_file_impl: tensor &amp;#39;blk.0.ffn_down_exps.weight&amp;#39; has invalid ggml type 39 (NONE)\ngguf_init_from_file_impl: failed to read tensor info\nllama_model_load: error loading model: llama_model_loader: failed to load model from xxx.gguf\nllama_model_load_from_file_impl: failed to load model\n[ERRO] Failed to load model from file: xxx.gguf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;What this means:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;As of the most recent commit (&lt;code&gt;b6106&lt;/code&gt;, Aug 6, 2025) on &lt;code&gt;llama.cpp&lt;/code&gt; and the latest source build of &lt;code&gt;llama-cpp-python&lt;/code&gt;, there is still NO support for the new MXFP4 tensor type (ggml type 39) required by GPT-OSS F16/MXFP4/MOE models.&lt;/li&gt;\n&lt;li&gt;This is not an issue with your build, Python, environment, or file.&lt;/li&gt;\n&lt;li&gt;The GGUF files themselves are valid and pass header/hash checks.&lt;/li&gt;\n&lt;li&gt;No one can run these models locally via vanilla llama.cpp at this time&lt;strong&gt;.&lt;/strong&gt; (I even tried other quantizations; only the latest MXFP4/F16 fail like this.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What to do?&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Wait for an official update / PR / patch in llama.cpp that adds MXFP4 and GPT-OSS F16/MOE support.&lt;/li&gt;\n&lt;li&gt;Track issues on &lt;a href=\"https://github.com/ggerganov/llama.cpp/issues\"&gt;ggerganov/llama.cpp&lt;/a&gt; and the HuggingFace repo for progress.&lt;/li&gt;\n&lt;li&gt;When that happens, just update and recompile — no extra hacks should be needed.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Conclusion:&lt;/h1&gt;\n\n&lt;p&gt;If you’re seeing&lt;br/&gt;\n&lt;code&gt;gguf_init_from_file_impl: tensor &amp;#39;blk.0.ffn_down_exps.weight&amp;#39; has invalid ggml type 39 (NONE)&lt;/code&gt;&lt;br/&gt;\ntrying to load GPT-OSS-20B F16/MXFP4, &lt;strong&gt;it’s not you — it’s the code!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We’re all waiting for upstream support.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?auto=webp&amp;s=15dec2ef279707b2b7293f298adf65c120367689",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f92890af939223e811c78aea793ad74924524124",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7a52293722d602fe6fdebcf196182f6b7c5e573",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0624f431dbc1aca0fda020f274bbe55097bc3029",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf04d8aa804465078ae5a4e47e5c8fb229efaa46",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1c420daf96989a2ffdd13a56551243766e0602e",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=014bf6642a86d58418fa1570ec5de73a70a0c2e9",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mjm5vm",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "PT_OV",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjm5vm/gptoss20b_f16mxfp4_gguf_models_not_loading_on/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjm5vm/gptoss20b_f16mxfp4_gguf_models_not_loading_on/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754528046,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7c6gq0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Entubulated",
            "can_mod_post": false,
            "created_utc": 1754529177,
            "send_replies": true,
            "parent_id": "t3_1mjm5vm",
            "score": 8,
            "author_fullname": "t2_1opxde6hyq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Something went sideways for you.  MXFP4 is properly supported from [build 6096](https://github.com/ggml-org/llama.cpp/releases/tag/b6096) forward. I'd suggest cloning into a new directory and building again, or if you're downloading binaries, check again.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7c6gq0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Something went sideways for you.  MXFP4 is properly supported from &lt;a href=\"https://github.com/ggml-org/llama.cpp/releases/tag/b6096\"&gt;build 6096&lt;/a&gt; forward. I&amp;#39;d suggest cloning into a new directory and building again, or if you&amp;#39;re downloading binaries, check again.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjm5vm/gptoss20b_f16mxfp4_gguf_models_not_loading_on/n7c6gq0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754529177,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjm5vm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7c6sjh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754529289,
            "send_replies": true,
            "parent_id": "t3_1mjm5vm",
            "score": 1,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I just pulled and am on the same version as you (but vanilla llama.cpp and not using python) and am not getting the problem.  I even rebuilt CPU only and it still worked.  I am using my own quant, but it should be identical to the unsloth F16/MXFP4.\n\nDid you ever have MXFP4 working?  Is it possible there's something wrong with your config?  Maybe `-DGGML_NATIVE=ON` for the build if you aren't already in case it requires AVX or something?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7c6sjh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just pulled and am on the same version as you (but vanilla llama.cpp and not using python) and am not getting the problem.  I even rebuilt CPU only and it still worked.  I am using my own quant, but it should be identical to the unsloth F16/MXFP4.&lt;/p&gt;\n\n&lt;p&gt;Did you ever have MXFP4 working?  Is it possible there&amp;#39;s something wrong with your config?  Maybe &lt;code&gt;-DGGML_NATIVE=ON&lt;/code&gt; for the build if you aren&amp;#39;t already in case it requires AVX or something?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjm5vm/gptoss20b_f16mxfp4_gguf_models_not_loading_on/n7c6sjh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754529289,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjm5vm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]