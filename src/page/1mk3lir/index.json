[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "\n\nWhat are some models for general purposes (chatgpt-like) can i run on my rig locally, for free? Gtx 1660 super, xeon e5-2650v2, 16gb ddr3, sata ssd.\nI'm ok with speed of answers as slow as 1 minute when using 4096 context. Any ideas?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Models for general purposes",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mk3lir",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1se0ho1z9o",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754581543,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some models for general purposes (chatgpt-like) can i run on my rig locally, for free? Gtx 1660 super, xeon e5-2650v2, 16gb ddr3, sata ssd.\nI&amp;#39;m ok with speed of answers as slow as 1 minute when using 4096 context. Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mk3lir",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Imaginary_Bread9711",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mk3lir/models_for_general_purposes/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk3lir/models_for_general_purposes/",
            "subreddit_subscribers": 513417,
            "created_utc": 1754581543,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7g0ju9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "PermanentLiminality",
            "can_mod_post": false,
            "created_utc": 1754584512,
            "send_replies": true,
            "parent_id": "t3_1mk3lir",
            "score": 2,
            "author_fullname": "t2_19zqycaf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would try the just released qwen3 4B models.  These will fit easy in your 6GB VRAM GPU and run fast.   About the best you will do on this hardware.  You can try larger models as 8B models in q4 will fit too.   \n\nIf you want better results try the just released Qwen 30B A3B in a Q4 quant or the GPT-OSS 20B.  Much smarter, but will be much slower.   Probably around 5tk/s.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7g0ju9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would try the just released qwen3 4B models.  These will fit easy in your 6GB VRAM GPU and run fast.   About the best you will do on this hardware.  You can try larger models as 8B models in q4 will fit too.   &lt;/p&gt;\n\n&lt;p&gt;If you want better results try the just released Qwen 30B A3B in a Q4 quant or the GPT-OSS 20B.  Much smarter, but will be much slower.   Probably around 5tk/s.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk3lir/models_for_general_purposes/n7g0ju9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754584512,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk3lir",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7fxxc7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754583754,
            "send_replies": true,
            "parent_id": "t3_1mk3lir",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://old.reddit.com/r/LocalLLaMA/comments/1mepueg/how_to_get_started/n6baoq8/\n\nhttps://old.reddit.com/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6ianq5/\n\nDDR3 has like 20 GB/s bandwidth, so even MoE models will be painfully slow. Unfortunately you need a better setup for any barely smart model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7fxxc7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1mepueg/how_to_get_started/n6baoq8/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1mepueg/how_to_get_started/n6baoq8/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6ianq5/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6ianq5/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;DDR3 has like 20 GB/s bandwidth, so even MoE models will be painfully slow. Unfortunately you need a better setup for any barely smart model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk3lir/models_for_general_purposes/n7fxxc7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754583754,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk3lir",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ibdg8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Clipbeam",
            "can_mod_post": false,
            "created_utc": 1754609143,
            "send_replies": true,
            "parent_id": "t3_1mk3lir",
            "score": 1,
            "author_fullname": "t2_1uxihq0vev",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It really depends on what level of complexity you're expecting the model to help you with. For my app I settled for qwen3:4b as the median default model, and it's performing pretty decent for summarization, minimal tool calling and RAG (but I limit my context window to 4K to run on lower end hardware)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ibdg8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It really depends on what level of complexity you&amp;#39;re expecting the model to help you with. For my app I settled for qwen3:4b as the median default model, and it&amp;#39;s performing pretty decent for summarization, minimal tool calling and RAG (but I limit my context window to 4K to run on lower end hardware)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk3lir/models_for_general_purposes/n7ibdg8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754609143,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk3lir",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]