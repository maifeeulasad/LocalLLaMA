[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "1. Why is llama.cpp needed ? Like what does it actually do ? If a model weights are available then loading the architecture and model weights will be enough right ? Does that the work it does ?  \n2. How does llama.cpp make inference faster ? Also could it have been written in something else than C++ (Like C or any other language) ?  \n3. If llama.cpp exists then why use ollama or LM Studio ?\n\nPlease if you come across this post and know anyone of these answers please answer. Also I am a newbie so maybe these questions could seem silly from your POV but still don't be mean",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Some Questions (Curiosity) Regarding ollama , llama.cpp and LM Studio for a complete beginner",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1melcsm",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_7m88zu40",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754017129,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Why is llama.cpp needed ? Like what does it actually do ? If a model weights are available then loading the architecture and model weights will be enough right ? Does that the work it does ?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;How does llama.cpp make inference faster ? Also could it have been written in something else than C++ (Like C or any other language) ?&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;If llama.cpp exists then why use ollama or LM Studio ?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please if you come across this post and know anyone of these answers please answer. Also I am a newbie so maybe these questions could seem silly from your POV but still don&amp;#39;t be mean&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1melcsm",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Rukelele_Dixit21",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/",
            "subreddit_subscribers": 508191,
            "created_utc": 1754017129,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ak0wo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GL-AI",
            "can_mod_post": false,
            "created_utc": 1754021600,
            "send_replies": true,
            "parent_id": "t3_1melcsm",
            "score": 2,
            "author_fullname": "t2_1sr5yw3yg0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "a lot of llama.cpp's usefulness comes from its quantizations. you can shrink the 16 bit weights that most models use down to 8 bit, 6bit, etc, all the way down to 1 bit. So it makes it easy to select a model size + precision that is good for your system.\n\nllama.cpp also supports a lot of different backends, and it allows you to use CPU and GPU at the same time for inference.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ak0wo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;a lot of llama.cpp&amp;#39;s usefulness comes from its quantizations. you can shrink the 16 bit weights that most models use down to 8 bit, 6bit, etc, all the way down to 1 bit. So it makes it easy to select a model size + precision that is good for your system.&lt;/p&gt;\n\n&lt;p&gt;llama.cpp also supports a lot of different backends, and it allows you to use CPU and GPU at the same time for inference.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6ak0wo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754021600,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1melcsm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6atqgd",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Betadoggo_",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ar0ru",
                                "score": 2,
                                "author_fullname": "t2_a177eog2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If you're just testing models and don't care about open vs closed source, LM Studio is probably the easiest. For a webapp that's available all the time with more than one user you probably want to use [https://openrouter.ai/](https://openrouter.ai/) providers since it will be a lot cheaper than self hosting the models in most cases. They have instructions for converting your existing openai api calls to openrouter api calls in their [quickstart guide](https://openrouter.ai/docs/quickstart). LM studio also has an [openai compatible api](https://lmstudio.ai/docs/app/api/endpoints/openai) that you could use for testing.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6atqgd",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re just testing models and don&amp;#39;t care about open vs closed source, LM Studio is probably the easiest. For a webapp that&amp;#39;s available all the time with more than one user you probably want to use &lt;a href=\"https://openrouter.ai/\"&gt;https://openrouter.ai/&lt;/a&gt; providers since it will be a lot cheaper than self hosting the models in most cases. They have instructions for converting your existing openai api calls to openrouter api calls in their &lt;a href=\"https://openrouter.ai/docs/quickstart\"&gt;quickstart guide&lt;/a&gt;. LM studio also has an &lt;a href=\"https://lmstudio.ai/docs/app/api/endpoints/openai\"&gt;openai compatible api&lt;/a&gt; that you could use for testing.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1melcsm",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6atqgd/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754026180,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754026180,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ar0ru",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Rukelele_Dixit21",
                      "can_mod_post": false,
                      "created_utc": 1754024840,
                      "send_replies": true,
                      "parent_id": "t1_n6aprvr",
                      "score": 1,
                      "author_fullname": "t2_7m88zu40",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks a lot. I am starting now which should I use ollama or LM Studio ? Also if I was using making a web app but it was making a call to chatgpt (openAI) for doing a task. How can I do this with an open source LLM ?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ar0ru",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks a lot. I am starting now which should I use ollama or LM Studio ? Also if I was using making a web app but it was making a call to chatgpt (openAI) for doing a task. How can I do this with an open source LLM ?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1melcsm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6ar0ru/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754024840,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6auynn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "agntdrake",
                      "can_mod_post": false,
                      "created_utc": 1754026798,
                      "send_replies": true,
                      "parent_id": "t1_n6aprvr",
                      "score": 1,
                      "author_fullname": "t2_4emw8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is mostly right, but Ollama has its own implementations of many models which don't use llama.cpp and it also has a UI which was just released.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6auynn",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is mostly right, but Ollama has its own implementations of many models which don&amp;#39;t use llama.cpp and it also has a UI which was just released.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1melcsm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6auynn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754026798,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6aprvr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Betadoggo_",
            "can_mod_post": false,
            "created_utc": 1754024246,
            "send_replies": true,
            "parent_id": "t3_1melcsm",
            "score": 2,
            "author_fullname": "t2_a177eog2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "1. llamacpp is a program developed for running models. llamacpp implements the model architectures from scratch with a focus on improving performance on a wide range of hardware. llamacpp is popular because it's the fastest option for users who don't have access to large Nvidia gpus.\n\n  \n2. There's too much technical info to talk about here, so simply: llamacpp is fast because it's developers have spent a lot of time optimizing it. It could have been written in other languages, but I assume c++ was chosen because that's what ggerganov preferred. Mistral.rs is a similar project but written in rust.  \n  \n3. Ollama and LM Studio are wrappers for llamacpp. Some people use them because they add features that aren't available or easily accessible in regular llamacpp. Ollama makes setup faster (with many drawbacks) and LM studio adds a UI (while being closed source).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6aprvr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;llamacpp is a program developed for running models. llamacpp implements the model architectures from scratch with a focus on improving performance on a wide range of hardware. llamacpp is popular because it&amp;#39;s the fastest option for users who don&amp;#39;t have access to large Nvidia gpus.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There&amp;#39;s too much technical info to talk about here, so simply: llamacpp is fast because it&amp;#39;s developers have spent a lot of time optimizing it. It could have been written in other languages, but I assume c++ was chosen because that&amp;#39;s what ggerganov preferred. Mistral.rs is a similar project but written in rust.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ollama and LM Studio are wrappers for llamacpp. Some people use them because they add features that aren&amp;#39;t available or easily accessible in regular llamacpp. Ollama makes setup faster (with many drawbacks) and LM studio adds a UI (while being closed source).&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6aprvr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754024246,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1melcsm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6b339l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Ok_Cow1976",
                      "can_mod_post": false,
                      "created_utc": 1754031118,
                      "send_replies": true,
                      "parent_id": "t1_n6alfvy",
                      "score": 2,
                      "author_fullname": "t2_3pwbsmdr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It's insanely crazy for me to hear that people use a gui because it's easier to download gguf file. The ones who do  that are really from stone age. Yet I do see that lms and ollama advertise their products with this included. I am insanely blown away",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6b339l",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s insanely crazy for me to hear that people use a gui because it&amp;#39;s easier to download gguf file. The ones who do  that are really from stone age. Yet I do see that lms and ollama advertise their products with this included. I am insanely blown away&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1melcsm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6b339l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754031118,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6b723u",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "boringcynicism",
                      "can_mod_post": false,
                      "created_utc": 1754033302,
                      "send_replies": true,
                      "parent_id": "t1_n6alfvy",
                      "score": 1,
                      "author_fullname": "t2_1hz0lz0k5i",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Why was C++ chosen?\n\nBecause of performance. C would be the same but a bit more annoying to write.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6b723u",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt; Why was C++ chosen?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Because of performance. C would be the same but a bit more annoying to write.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1melcsm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6b723u/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754033302,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6alfvy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GreenGreasyGreasels",
            "can_mod_post": false,
            "created_utc": 1754022231,
            "send_replies": true,
            "parent_id": "t3_1melcsm",
            "score": 1,
            "author_fullname": "t2_14purh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The weights are just the data files. You need a program to run it. Llama.cpp is that program. It is popular for two reasons - it can use both your graphics card and processor to speed up running, and it can use compressed model files called gguf that can fit into your computer.\n\nLM Studio and Ollama give llama.cpp a nice gui and hide the command line complexities for you and in addition give you a way to easily download models and also chat with them as well. A download, run and chat 3-in-1, and provide a lot of quality of life improvements.\n\nRewriting it in C or Go is likely not going to provide a lot of improvements. Why was C++ chosen? Provably because the author who wrote it thought it best.\n\nPerhaps in the future a rewrite might be useful if Mojo pans out, but that's speculating.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6alfvy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The weights are just the data files. You need a program to run it. Llama.cpp is that program. It is popular for two reasons - it can use both your graphics card and processor to speed up running, and it can use compressed model files called gguf that can fit into your computer.&lt;/p&gt;\n\n&lt;p&gt;LM Studio and Ollama give llama.cpp a nice gui and hide the command line complexities for you and in addition give you a way to easily download models and also chat with them as well. A download, run and chat 3-in-1, and provide a lot of quality of life improvements.&lt;/p&gt;\n\n&lt;p&gt;Rewriting it in C or Go is likely not going to provide a lot of improvements. Why was C++ chosen? Provably because the author who wrote it thought it best.&lt;/p&gt;\n\n&lt;p&gt;Perhaps in the future a rewrite might be useful if Mojo pans out, but that&amp;#39;s speculating.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1melcsm/some_questions_curiosity_regarding_ollama/n6alfvy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754022231,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1melcsm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]