[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey Folks \n\nNeed GPU selection suggestion before i make the purchase\n\nWhere i live, i am getting GeForce RTX 5060 Ti 16GB GDDR7 at USD 500 , buying 4 of these devices would be a good choice (yes i will also be buying  new RIG / CPU / MB/ PS), hence not worrying about backward compatibility.\n\nMy use case : (Is not gaming) i want to use these devices for LLM inferencing (say Llama / DeepSeek etc) as well as fine-tuning (for my fun projects/side gigs). Hence i would need a large VRAM , getting a 64GB vRAM device is super expensive. So i am considering if i can today start with 2 x GeForce RTX 5060 Ti 16GB , this gets me to 32GB of VRAM and then later add 2 more of these and get 64GB VRAM.\n\nNeed your suggestions on if this approach suffice my use case, should i consider any other device type etc.\n\nWould there be hard challenges in combining GPU memory from 4 cards and use the combined memory for large model inferencing ? also for Fine-tuning. Wondering if someone has achieved this setup ?\n\nüôè",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "GeForce RTX 5060 Ti 16GB good for LLama LLM inferencing/Fintuning ?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mam6of",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_b9h602vjf",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753622591,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks &lt;/p&gt;\n\n&lt;p&gt;Need GPU selection suggestion before i make the purchase&lt;/p&gt;\n\n&lt;p&gt;Where i live, i am getting GeForce RTX 5060 Ti 16GB GDDR7 at USD 500 , buying 4 of these devices would be a good choice (yes i will also be buying  new RIG / CPU / MB/ PS), hence not worrying about backward compatibility.&lt;/p&gt;\n\n&lt;p&gt;My use case : (Is not gaming) i want to use these devices for LLM inferencing (say Llama / DeepSeek etc) as well as fine-tuning (for my fun projects/side gigs). Hence i would need a large VRAM , getting a 64GB vRAM device is super expensive. So i am considering if i can today start with 2 x GeForce RTX 5060 Ti 16GB , this gets me to 32GB of VRAM and then later add 2 more of these and get 64GB VRAM.&lt;/p&gt;\n\n&lt;p&gt;Need your suggestions on if this approach suffice my use case, should i consider any other device type etc.&lt;/p&gt;\n\n&lt;p&gt;Would there be hard challenges in combining GPU memory from 4 cards and use the combined memory for large model inferencing ? also for Fine-tuning. Wondering if someone has achieved this setup ?&lt;/p&gt;\n\n&lt;p&gt;üôè&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mam6of",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "kingksingh",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/",
            "subreddit_subscribers": 505881,
            "created_utc": 1753622591,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fims0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Jatilq",
            "can_mod_post": false,
            "created_utc": 1753623398,
            "send_replies": true,
            "parent_id": "t3_1mam6of",
            "score": 3,
            "author_fullname": "t2_3txs3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I don't know this card that well, but I'm looking at two things. Cheap price for 16gb in a modern card. Others will chime in with better advice. I just picked up x2 3060 cards for a value of 400-500 for my second machine. Thats 24vram. Do what you can afford. Average used price I see for a 3090 is 700-900 r/hardwareswap. My cards arent as fast, but I traded 1 card I couldn't use and paid 210 for the other. Comes down to value for the Vram you're getting.\n\nBottom line. Get what YOU can afford.",
            "edited": 1753623628,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fims0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know this card that well, but I&amp;#39;m looking at two things. Cheap price for 16gb in a modern card. Others will chime in with better advice. I just picked up x2 3060 cards for a value of 400-500 for my second machine. Thats 24vram. Do what you can afford. Average used price I see for a 3090 is 700-900 &lt;a href=\"/r/hardwareswap\"&gt;r/hardwareswap&lt;/a&gt;. My cards arent as fast, but I traded 1 card I couldn&amp;#39;t use and paid 210 for the other. Comes down to value for the Vram you&amp;#39;re getting.&lt;/p&gt;\n\n&lt;p&gt;Bottom line. Get what YOU can afford.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/n5fims0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753623398,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mam6of",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gh799",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Cane_P",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5fl7cv",
                                "score": 2,
                                "author_fullname": "t2_3ogvvuuj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Relatively low memory bandwidth and compute. Someone else might be able to tell you how bad.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gh799",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Relatively low memory bandwidth and compute. Someone else might be able to tell you how bad.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mam6of",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/n5gh799/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753634131,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753634131,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5fl7cv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "kingksingh",
                      "can_mod_post": false,
                      "created_utc": 1753624280,
                      "send_replies": true,
                      "parent_id": "t1_n5fh2r0",
                      "score": 1,
                      "author_fullname": "t2_b9h602vjf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes it Could be.\nAny pointers on what is bad and how bad is bad",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5fl7cv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes it Could be.\nAny pointers on what is bad and how bad is bad&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mam6of",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/n5fl7cv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753624280,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5fh2r0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "prusswan",
            "can_mod_post": false,
            "created_utc": 1753622845,
            "send_replies": true,
            "parent_id": "t3_1mam6of",
            "score": 5,
            "author_fullname": "t2_kegwk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "5060 TI is the worst card you can get in the 16GB range, there is a reason why it is so cheap",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fh2r0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;5060 TI is the worst card you can get in the 16GB range, there is a reason why it is so cheap&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/n5fh2r0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753622845,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mam6of",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fr5f2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "QFGTrialByFire",
            "can_mod_post": false,
            "created_utc": 1753626246,
            "send_replies": true,
            "parent_id": "t3_1mam6of",
            "score": 2,
            "author_fullname": "t2_1h4o7f23eh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Before buying .. do you already have a pytorch cuda friendly GPU? Try running a smaller/distilled version of a model on it eg qwen 3 8B. In 4bit quant mode it only needs 8GB of vram to load/run. Try training it using lora that'll give you a better idea of what you might want as a local setup or if you even need more. I can run that model on a 3080ti with only 12Gb vram and also train it using lora. So if you have anything grater than 8Gb or more of vram and compatible cuda with pytorch it'll run right now. Use it before spending any money. Usually you're better off sorting out your training data set on something small to see if data is good, learning rates etc. Then when you're ready try renting something for an hour to train a better/larger model on that data. It is usually cheaper than buying. eg you can rent that 5060Ti for $0.088/hr on vastai right now. A 5060Ti is \\~$430 on amazon. For that price you could almost run that rented on vastai for 200days continuously. If after running on your current GPU you need more local run stuff at least you have a much better idea of what you need. For most small training/side projects you dont need a massive system as the model already has been trained on so much.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fr5f2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Before buying .. do you already have a pytorch cuda friendly GPU? Try running a smaller/distilled version of a model on it eg qwen 3 8B. In 4bit quant mode it only needs 8GB of vram to load/run. Try training it using lora that&amp;#39;ll give you a better idea of what you might want as a local setup or if you even need more. I can run that model on a 3080ti with only 12Gb vram and also train it using lora. So if you have anything grater than 8Gb or more of vram and compatible cuda with pytorch it&amp;#39;ll run right now. Use it before spending any money. Usually you&amp;#39;re better off sorting out your training data set on something small to see if data is good, learning rates etc. Then when you&amp;#39;re ready try renting something for an hour to train a better/larger model on that data. It is usually cheaper than buying. eg you can rent that 5060Ti for $0.088/hr on vastai right now. A 5060Ti is ~$430 on amazon. For that price you could almost run that rented on vastai for 200days continuously. If after running on your current GPU you need more local run stuff at least you have a much better idea of what you need. For most small training/side projects you dont need a massive system as the model already has been trained on so much.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mam6of/geforce_rtx_5060_ti_16gb_good_for_llama_llm/n5fr5f2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753626246,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mam6of",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]