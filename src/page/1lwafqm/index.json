[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "hey guys!\n\nbecause of privacy conerns and censorship i;ve decided to give local LLM a try.\n\ndownloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future. \n\ncouple of questions:\n\ncan the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn't it fully oflline? \n\ndo you have any other models that you recommended?\n\nis there a way to \"stream\" the model to my network so I will be able to acsses and ask things from othe computers? \n\nis there something else i need to know about local LLMs?\n\n  \nThank you!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "running local LLM for the first time",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1lwafqm",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_e6v0plyv",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752147328,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey guys!&lt;/p&gt;\n\n&lt;p&gt;because of privacy conerns and censorship i;ve decided to give local LLM a try.&lt;/p&gt;\n\n&lt;p&gt;downloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future. &lt;/p&gt;\n\n&lt;p&gt;couple of questions:&lt;/p&gt;\n\n&lt;p&gt;can the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn&amp;#39;t it fully oflline? &lt;/p&gt;\n\n&lt;p&gt;do you have any other models that you recommended?&lt;/p&gt;\n\n&lt;p&gt;is there a way to &amp;quot;stream&amp;quot; the model to my network so I will be able to acsses and ask things from othe computers? &lt;/p&gt;\n\n&lt;p&gt;is there something else i need to know about local LLMs?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lwafqm",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Routine_Author961",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/",
            "subreddit_subscribers": 497021,
            "created_utc": 1752147328,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2cig59",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MaxKruse96",
            "can_mod_post": false,
            "created_utc": 1752148051,
            "send_replies": true,
            "parent_id": "t3_1lwafqm",
            "score": 3,
            "author_fullname": "t2_pfi81",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "just as a headsup: if you manage to work with lmstudio, ollama is a step down for 99% of use cases. dont bother.\n\nModels only know what you tell them.   \nModels cannot access the internet, they arent programs. You can, however, give the Model some tools (anything from web search, to access to your google mail etc)\n\nModels to use depend entirely on your VRAM. If you have a GPU with 12gb, Models of a size of 14b is the soft-limit of what makes sense.\n\nFor STEM: Qwen3 (8b or 14b)  \nFor \"World Knowledge\" or Rephrasing of texts you want to write: Gemma3 12b\n\nThose are the 2 big ones, that come in smaller variants too if needed.\n\nIn terms of \"streaming\" your model, LMStudio offers an \"OpenAI Compatible Server\", its in the developer tab on the left. Once enabled, you can load a model and its available through [http://localhost:1234/v1/](http://localhost:1234/v1/)\n\nUI's to access \"remote\" (e.g. on another machine usually) LLMs include OpenwebUI and AnythingLLM. The setup for those is good to search in their docs.\n\nIn general, its good to know \\*how\\* to ask questions. Models \\*will\\* answer, but if you ask it something it cant possibly know, you get nonsense. See it as a very simple yet complicated \"If this information would be accessible by an intern in a company, the LLM probably knows it\".",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2cig59",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;just as a headsup: if you manage to work with lmstudio, ollama is a step down for 99% of use cases. dont bother.&lt;/p&gt;\n\n&lt;p&gt;Models only know what you tell them.&lt;br/&gt;\nModels cannot access the internet, they arent programs. You can, however, give the Model some tools (anything from web search, to access to your google mail etc)&lt;/p&gt;\n\n&lt;p&gt;Models to use depend entirely on your VRAM. If you have a GPU with 12gb, Models of a size of 14b is the soft-limit of what makes sense.&lt;/p&gt;\n\n&lt;p&gt;For STEM: Qwen3 (8b or 14b)&lt;br/&gt;\nFor &amp;quot;World Knowledge&amp;quot; or Rephrasing of texts you want to write: Gemma3 12b&lt;/p&gt;\n\n&lt;p&gt;Those are the 2 big ones, that come in smaller variants too if needed.&lt;/p&gt;\n\n&lt;p&gt;In terms of &amp;quot;streaming&amp;quot; your model, LMStudio offers an &amp;quot;OpenAI Compatible Server&amp;quot;, its in the developer tab on the left. Once enabled, you can load a model and its available through &lt;a href=\"http://localhost:1234/v1/\"&gt;http://localhost:1234/v1/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;UI&amp;#39;s to access &amp;quot;remote&amp;quot; (e.g. on another machine usually) LLMs include OpenwebUI and AnythingLLM. The setup for those is good to search in their docs.&lt;/p&gt;\n\n&lt;p&gt;In general, its good to know *how* to ask questions. Models *will* answer, but if you ask it something it cant possibly know, you get nonsense. See it as a very simple yet complicated &amp;quot;If this information would be accessible by an intern in a company, the LLM probably knows it&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2cig59/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752148051,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lwafqm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]