[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Thanks to the recent price surge on crypto I have rougly 10k I can spend on equipments. I have always wanted to run sota models like deepseek R1 or GLM 4.5 locally, and also fine tuning them. So far the mac studio 256gb model looks good, but I wanted to ask if there are any better alternatives.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best way to spend 7k on local model",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdgr6n",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.63,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ekrnmt5z",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753905468,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the recent price surge on crypto I have rougly 10k I can spend on equipments. I have always wanted to run sota models like deepseek R1 or GLM 4.5 locally, and also fine tuning them. So far the mac studio 256gb model looks good, but I wanted to ask if there are any better alternatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdgr6n",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "monoidconcat",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/",
            "subreddit_subscribers": 507275,
            "created_utc": 1753905468,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62guy4",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Willing_Landscape_61",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62eyz1",
                                "score": 1,
                                "author_fullname": "t2_8lvrytgw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For MoE , you can get a good speed up by specifying what you offload to the GPU.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62guy4",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For MoE , you can get a good speed up by specifying what you offload to the GPU.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgr6n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62guy4/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753916768,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753916768,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62j0mz",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62eyz1",
                                "score": 2,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "These big models use the MoE architecture where only a fraction of the model is active for any given token, e.g. Kimi K2 is 1000B parameters but only 32B will be active for a given token generation.  Most of those 32B are effectively random, but some aren't.  So really you could view it as a ~10B model with a random ~2% of a 990B model... Kind of.  But basically if you offload the 10B part you only need 10B of VRAM and you can run that 1/3 of the model very quickly.  The CPU then runs the remaining large part but now only needs to process 22B parameters instead of 32B for a ~50% speedup.\n\nAlso, it would just be silly to not have a GPU if you're building an AI rig :).  A 5090 will run a 32B model super fast if you need speed for some tasks.  If you didn't have it a 32B model would be as slow as Kimi K2.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62j0mz",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These big models use the MoE architecture where only a fraction of the model is active for any given token, e.g. Kimi K2 is 1000B parameters but only 32B will be active for a given token generation.  Most of those 32B are effectively random, but some aren&amp;#39;t.  So really you could view it as a ~10B model with a random ~2% of a 990B model... Kind of.  But basically if you offload the 10B part you only need 10B of VRAM and you can run that 1/3 of the model very quickly.  The CPU then runs the remaining large part but now only needs to process 22B parameters instead of 32B for a ~50% speedup.&lt;/p&gt;\n\n&lt;p&gt;Also, it would just be silly to not have a GPU if you&amp;#39;re building an AI rig :).  A 5090 will run a 32B model super fast if you need speed for some tasks.  If you didn&amp;#39;t have it a 32B model would be as slow as Kimi K2.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgr6n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62j0mz/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753917460,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753917460,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62eyz1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Agitated_Camel1886",
                      "can_mod_post": false,
                      "created_utc": 1753916171,
                      "send_replies": true,
                      "parent_id": "t1_n61qvrg",
                      "score": 1,
                      "author_fullname": "t2_1jf10fah7i",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "(A newbie here)\nWhat's the reason behind a GPU plus many RAM? Won't the inference speed be limited by RAM speed anyway?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62eyz1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;(A newbie here)\nWhat&amp;#39;s the reason behind a GPU plus many RAM? Won&amp;#39;t the inference speed be limited by RAM speed anyway?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgr6n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62eyz1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753916171,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61qvrg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753908826,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 4,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would advise against the 256GB Studio as that's too small for large models at decent quants.  At Q4: GLM-4.5-358B is ~202GB, Qwen3-Coder-480B is 271GB, Deepseek is 379GB, Kimi-K2 is 578GB.\n\nIf you aren't comfortable building something, the Mac Studio 512GB is a decent option (since the body of your post says $10k).  Enough memory for most models and good speed \n\nIf you're comfortable building something:\n\n- 5090: $2500\n- 8x64GB DDR5-5600: $2400\n- Epyc 9004: Price depends on part and market, but say $1500?\n- Motherboard: $700 (begrudgingly recc H13SSL)\n\nThat hits your $7k and you have room to expand with more memory down the line.  You could spend a bit more to get DDR5-6400 to be ready to upgrade to a Epyc 9005 when those drop in price (the cheap ones are mostly bad but the 9255 is ~okay).  The 5090 is a little overkill and you could get a 3090 without losing a lot of capabilities.  For the Epyc 9004, the 9B14 is a good deal right now IMHO.  Watch out for QS/ES chips since compatibility is spotty with those.\n\nNote you will _not_ be fine tuning them for less than, say, $100k?  Probably more :).  You'll want to rent hardware for that.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61qvrg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would advise against the 256GB Studio as that&amp;#39;s too small for large models at decent quants.  At Q4: GLM-4.5-358B is ~202GB, Qwen3-Coder-480B is 271GB, Deepseek is 379GB, Kimi-K2 is 578GB.&lt;/p&gt;\n\n&lt;p&gt;If you aren&amp;#39;t comfortable building something, the Mac Studio 512GB is a decent option (since the body of your post says $10k).  Enough memory for most models and good speed &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re comfortable building something:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5090: $2500&lt;/li&gt;\n&lt;li&gt;8x64GB DDR5-5600: $2400&lt;/li&gt;\n&lt;li&gt;Epyc 9004: Price depends on part and market, but say $1500?&lt;/li&gt;\n&lt;li&gt;Motherboard: $700 (begrudgingly recc H13SSL)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That hits your $7k and you have room to expand with more memory down the line.  You could spend a bit more to get DDR5-6400 to be ready to upgrade to a Epyc 9005 when those drop in price (the cheap ones are mostly bad but the 9255 is ~okay).  The 5090 is a little overkill and you could get a 3090 without losing a lot of capabilities.  For the Epyc 9004, the 9B14 is a good deal right now IMHO.  Watch out for QS/ES chips since compatibility is spotty with those.&lt;/p&gt;\n\n&lt;p&gt;Note you will &lt;em&gt;not&lt;/em&gt; be fine tuning them for less than, say, $100k?  Probably more :).  You&amp;#39;ll want to rent hardware for that.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n61qvrg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753908826,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61j54y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1753906702,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 2,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm not an expert on such configs (for such big models I prefer to for cloud providers, since my use case do not require total privacy... stil, there are providers with quite honest ToS) but I don't think you can not expect to fine tune a 600+B MoE locally with this budget (until the next magic trick from unsloth).\nabout inference it... yeh, probably someone here can suggest some quite effective configurations (still, I assume you will have to offload at least portion of the weights to ram)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61j54y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not an expert on such configs (for such big models I prefer to for cloud providers, since my use case do not require total privacy... stil, there are providers with quite honest ToS) but I don&amp;#39;t think you can not expect to fine tune a 600+B MoE locally with this budget (until the next magic trick from unsloth).\nabout inference it... yeh, probably someone here can suggest some quite effective configurations (still, I assume you will have to offload at least portion of the weights to ram)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n61j54y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753906702,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61s91b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "perelmanych",
            "can_mod_post": false,
            "created_utc": 1753909206,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 2,
            "author_fullname": "t2_63q8kong",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It is either Mac Studio or EPYC server. No other alternatives in your budget to run big models like DeepSeek-R1 at a somehow acceptable speeds. Personally I would go with EPYC server + RTX 3090, since if there soon will be 2T models you still will be able to fit their low quants into RAM or alternatively you can use higher quants than Mac Studio of the same models. As a downside Mac Studio probably will be a bit faster, of course a lot depends on the specific model of EPYC server that you choose.\n\nFinetuning of such big models is out of reach even for moderate labs, unless you have spare several years.",
            "edited": 1753909390,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61s91b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is either Mac Studio or EPYC server. No other alternatives in your budget to run big models like DeepSeek-R1 at a somehow acceptable speeds. Personally I would go with EPYC server + RTX 3090, since if there soon will be 2T models you still will be able to fit their low quants into RAM or alternatively you can use higher quants than Mac Studio of the same models. As a downside Mac Studio probably will be a bit faster, of course a lot depends on the specific model of EPYC server that you choose.&lt;/p&gt;\n\n&lt;p&gt;Finetuning of such big models is out of reach even for moderate labs, unless you have spare several years.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n61s91b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753909206,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6204b8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753911506,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 1,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I also bought a Mac Studio 512GB after selling some crypto - but if that's your whole bag rather than like 10% of it or whatever, I'd say hold off for now and wait. Hardware prices are going to come down, and models are going to keep getting better for the same size of RAM. Right now you only need a Mac with 128GB of RAM to run GLM 4.5 Air, which is a f\\*\\*\\*ing amazing model. I've been testing it in chat and agent apps, and it feels as smart and useful as Claude Sonnet. It only takes up 70-80GB of VRAM with 128k context",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6204b8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I also bought a Mac Studio 512GB after selling some crypto - but if that&amp;#39;s your whole bag rather than like 10% of it or whatever, I&amp;#39;d say hold off for now and wait. Hardware prices are going to come down, and models are going to keep getting better for the same size of RAM. Right now you only need a Mac with 128GB of RAM to run GLM 4.5 Air, which is a f***ing amazing model. I&amp;#39;ve been testing it in chat and agent apps, and it feels as smart and useful as Claude Sonnet. It only takes up 70-80GB of VRAM with 128k context&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n6204b8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753911506,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62nos6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753918982,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Probably not your best option but worth mentioning for your research: dual RTX 8000. These are coming down in price and are about 2k each. This will give you 96 GB of fast vram.\n\nI have one rtx 8000 it works great! I can run 70b with 6k context just under 10 tokens a second. Two would be great for 100b models like CommandA, Glm 4.5 air, mistral large.\n\n\nI have not fine tuned across two gpus before so I’m not sure if you will be able to fine tune the larger models. \n\nIf your budget is 10,000, you might be able full send a RTX pro 6000. There is a 300 watt version called Max Q which might be more readily available at a lower price than the non-Max Q version.\n\n\nAnd as others have said, it will be a challenge to run the largest open source models like deepseek and kimi with good tps.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62nos6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Probably not your best option but worth mentioning for your research: dual RTX 8000. These are coming down in price and are about 2k each. This will give you 96 GB of fast vram.&lt;/p&gt;\n\n&lt;p&gt;I have one rtx 8000 it works great! I can run 70b with 6k context just under 10 tokens a second. Two would be great for 100b models like CommandA, Glm 4.5 air, mistral large.&lt;/p&gt;\n\n&lt;p&gt;I have not fine tuned across two gpus before so I’m not sure if you will be able to fine tune the larger models. &lt;/p&gt;\n\n&lt;p&gt;If your budget is 10,000, you might be able full send a RTX pro 6000. There is a 300 watt version called Max Q which might be more readily available at a lower price than the non-Max Q version.&lt;/p&gt;\n\n&lt;p&gt;And as others have said, it will be a challenge to run the largest open source models like deepseek and kimi with good tps.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62nos6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753918982,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]