[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm thinking about buying a GMKTEK Evo-2.\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&gt; 10tk/s)? I'm undecided between the 64 GB and 128 GB RAM versions, but I'm leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.\n\nEDIT: Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What modes can expect I run on an AMD Ryzen AI Max+ 395?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lvh87a",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 16,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_bk6b6yhm",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 16,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1752066643,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752063277,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking about buying a GMKTEK Evo-2.\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&amp;gt; 10tk/s)? I&amp;#39;m undecided between the 64 GB and 128 GB RAM versions, but I&amp;#39;m leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thank you all so much for the great answers! I&amp;#39;m new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lvh87a",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "electrickangaroo31",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/",
            "subreddit_subscribers": 496591,
            "created_utc": 1752063277,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n25z57o",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "simracerman",
                      "can_mod_post": false,
                      "created_utc": 1752064947,
                      "send_replies": true,
                      "parent_id": "t1_n25xx8u",
                      "score": 12,
                      "author_fullname": "t2_vbzgnic",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That’s the answer. MoE is the future (at least from emerging trends), and so far it’s keeping up with dense models just fine. Helps that MoE or 22B active is slightly slower than a dense 22B active. This makes VRAM size your only constraint.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n25z57o",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s the answer. MoE is the future (at least from emerging trends), and so far it’s keeping up with dense models just fine. Helps that MoE or 22B active is slightly slower than a dense 22B active. This makes VRAM size your only constraint.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lvh87a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25z57o/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752064947,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 12
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n25xx8u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Krowken",
            "can_mod_post": false,
            "created_utc": 1752064520,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 17,
            "author_fullname": "t2_1k5i09iuq0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think the biggest advantage of getting the 128GB version  would be that you could run some of the larger mixture of expert models. Any dense model taking up more than 64GB of vram would likely be limited by the low memory bandwidth anyways. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n25xx8u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think the biggest advantage of getting the 128GB version  would be that you could run some of the larger mixture of expert models. Any dense model taking up more than 64GB of vram would likely be limited by the low memory bandwidth anyways. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25xx8u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752064520,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 17
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n25xskt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "rorowhat",
            "can_mod_post": false,
            "created_utc": 1752064474,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 7,
            "author_fullname": "t2_yq51a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Of course the 128gb will run larger models, but if you want good speeds the 64gb might be the sweet spot.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n25xskt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Of course the 128gb will run larger models, but if you want good speeds the 64gb might be the sweet spot.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25xskt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752064474,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n25zjvj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "MaxKruse96",
            "can_mod_post": false,
            "created_utc": 1752065087,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 7,
            "author_fullname": "t2_pfi81",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Get the 128gb one, you want to be future-proofed for new MoE models that run fast and have good knowledge/capabilities. If you want to buy it for inference now and then never touch it again, might as well build a cluster of 3060Ti",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n25zjvj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Get the 128gb one, you want to be future-proofed for new MoE models that run fast and have good knowledge/capabilities. If you want to buy it for inference now and then never touch it again, might as well build a cluster of 3060Ti&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25zjvj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752065087,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n26q304",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Secure_Reflection409",
                      "can_mod_post": false,
                      "created_utc": 1752073136,
                      "send_replies": true,
                      "parent_id": "t1_n263c3i",
                      "score": 1,
                      "author_fullname": "t2_by77ogdhr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm surprised it's as high as 14, very nice.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n26q304",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m surprised it&amp;#39;s as high as 14, very nice.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lvh87a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n26q304/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752073136,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n263c3i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Mushoz",
            "can_mod_post": false,
            "created_utc": 1752066360,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 4,
            "author_fullname": "t2_gwpq7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen3-235B-Q3\\_K\\_XL UD quants fit perfectly, and give me a very acceptable 14 tokens/s. Dots.lm1 in Q4\\_K\\_XL UD quants easily fits as well, and is a really fun model. Around 22 tokens/s. I would go with 128GB. I am loving this laptop :)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n263c3i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3-235B-Q3_K_XL UD quants fit perfectly, and give me a very acceptable 14 tokens/s. Dots.lm1 in Q4_K_XL UD quants easily fits as well, and is a really fun model. Around 22 tokens/s. I would go with 128GB. I am loving this laptop :)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n263c3i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752066360,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n266bf6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "uti24",
            "can_mod_post": false,
            "created_utc": 1752067321,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 4,
            "author_fullname": "t2_13hbro",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It depends on the speed you can tolerate.\n\nMemory bandwidth of AMD Ryzen AI Max+ 395 is 250GB/s, but in practice it is about 200GB/s\n\nSo if you load model that uses all 96GB of VRAM you will have about 2t/s.\n\nAnd anything less will run faster proportionally.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n266bf6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends on the speed you can tolerate.&lt;/p&gt;\n\n&lt;p&gt;Memory bandwidth of AMD Ryzen AI Max+ 395 is 250GB/s, but in practice it is about 200GB/s&lt;/p&gt;\n\n&lt;p&gt;So if you load model that uses all 96GB of VRAM you will have about 2t/s.&lt;/p&gt;\n\n&lt;p&gt;And anything less will run faster proportionally.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n266bf6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752067321,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n26fpys",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "pkmxtw",
            "can_mod_post": false,
            "created_utc": 1752070186,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 3,
            "author_fullname": "t2_a2gtk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The new Hunyuan-80B-A13B would be almost the perfect size for AI Max+ 395 128GB.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n26fpys",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The new Hunyuan-80B-A13B would be almost the perfect size for AI Max+ 395 128GB.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n26fpys/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752070186,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n263qad",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "electrickangaroo31",
            "can_mod_post": false,
            "created_utc": 1752066491,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 1,
            "author_fullname": "t2_bk6b6yhm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy.\nI plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n263qad",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you all so much for the great answers! I&amp;#39;m new to this, and, to be honest, my main concern is privacy.\nI plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n263qad/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752066491,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n26uo0y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "randomfoo2",
            "can_mod_post": false,
            "created_utc": 1752074407,
            "send_replies": true,
            "parent_id": "t3_1lvh87a",
            "score": 1,
            "author_fullname": "t2_eztox",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "In my testing:\n\n* Qwen3-235B-A22B-UD-Q3\\_K\\_XL (96.59 GiB) can run at 12.3 tok/s\n* Llama-4-Scout-17B-16E-Instruct-UD-Q4\\_K\\_XL (57.73 GiB) can run at 19.95 tok/s\n\nThere are other MoEs to test, at least that should give you a ballpark of what's possible.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n26uo0y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In my testing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3-235B-A22B-UD-Q3_K_XL (96.59 GiB) can run at 12.3 tok/s&lt;/li&gt;\n&lt;li&gt;Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL (57.73 GiB) can run at 19.95 tok/s&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;There are other MoEs to test, at least that should give you a ballpark of what&amp;#39;s possible.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n26uo0y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752074407,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvh87a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]