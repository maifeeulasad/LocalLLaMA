[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I saw a fellow post this question on a forum, and I decided to ask the same question to ChatGPT5 ( I suggest you ask to your models also ) hahaha. Look: Unbeliable. Even Grok 3 answered correctly.\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "ChatGPT5 says that there are 3 letters \"B\" in the word \"Blueberry\". Test by yourself !",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 77,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mko9fw",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.21,
            "author_flair_background_color": null,
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_8c7clfk1",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/oUx-vVREEJM0-O2EnLCwBQZmNOQ6ccK1mtrCPjGuhWw.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754636099,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw a fellow post this question on a forum, and I decided to ask the same question to ChatGPT5 ( I suggest you ask to your models also ) hahaha. Look: Unbeliable. Even Grok 3 answered correctly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/hcv9za9ztqhf1.jpeg",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/hcv9za9ztqhf1.jpeg?auto=webp&amp;s=f9645bc6ac25468c1632c959d7b0f3cafe4b41a8",
                    "width": 1354,
                    "height": 745
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/hcv9za9ztqhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=51a6564370efcd4e8a671b1e00c603fd115a189a",
                      "width": 108,
                      "height": 59
                    },
                    {
                      "url": "https://preview.redd.it/hcv9za9ztqhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=150d288a28d1e31216c94bb0e523ee1f9e9ca737",
                      "width": 216,
                      "height": 118
                    },
                    {
                      "url": "https://preview.redd.it/hcv9za9ztqhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ecba930509fe4a3513797203576c2fde989baa60",
                      "width": 320,
                      "height": 176
                    },
                    {
                      "url": "https://preview.redd.it/hcv9za9ztqhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd36324b4d3c54cc9452a627d3b833eb79db5ddc",
                      "width": 640,
                      "height": 352
                    },
                    {
                      "url": "https://preview.redd.it/hcv9za9ztqhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=27f980dfa906606a59165782813cff67831ef268",
                      "width": 960,
                      "height": 528
                    },
                    {
                      "url": "https://preview.redd.it/hcv9za9ztqhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cf4d659c1d1c231be0c45623a7197c9607dfd066",
                      "width": 1080,
                      "height": 594
                    }
                  ],
                  "variants": {},
                  "id": "WQF4iqw1FzNVfkaiixAhJIqoVdBud1MGD_91DllhfD4"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mko9fw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Current-Stop7806",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/",
            "stickied": false,
            "url": "https://i.redd.it/hcv9za9ztqhf1.jpeg",
            "subreddit_subscribers": 513814,
            "created_utc": 1754636099,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7k67ma",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "random-tomato",
            "can_mod_post": false,
            "created_utc": 1754636756,
            "send_replies": true,
            "parent_id": "t3_1mko9fw",
            "score": 9,
            "author_fullname": "t2_fmd6oq5v6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Please post in r/OpenAI or r/ChatGPT instead!! This is not local by any means.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k67ma",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Please post in &lt;a href=\"/r/OpenAI\"&gt;r/OpenAI&lt;/a&gt; or &lt;a href=\"/r/ChatGPT\"&gt;r/ChatGPT&lt;/a&gt; instead!! This is not local by any means.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7k67ma/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754636756,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mko9fw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7k5hk5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Varterove_muke",
            "can_mod_post": false,
            "created_utc": 1754636364,
            "send_replies": true,
            "parent_id": "t3_1mko9fw",
            "score": 5,
            "author_fullname": "t2_11hokrn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "AGI achived 🥳",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k5hk5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;AGI achived 🥳&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7k5hk5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754636364,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1mko9fw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7l30h8",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "HiddenoO",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7kzigp",
                                "score": 1,
                                "author_fullname": "t2_8127x",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Nobody is claiming that these failure states are the same for each model, and they might be something very specific, such as a specific combination of tokens in your prompt or how you format the character in this scenario (B vs. 'B' vs. \"B\" etc.). In my benchmarks, I've had SotA models switch between providing a summary (correct behavior) and acting as one of the people involved (wrong behavior) in like 0.1% of of input conversations, and just replacing a single word with a synonym could change it back to the correct behavior. Most people just don't test these models enough to come across these occurrences, or assume it was something else.\n\nThis task in particular is kind of pointless because it's usually less about the model's capabilities and more about how the tokenizer handles single letters and whether the developers specifically added training data for letter counting which isn't something you'd naturally have much training data of.",
                                "edited": 1754654384,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7l30h8",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nobody is claiming that these failure states are the same for each model, and they might be something very specific, such as a specific combination of tokens in your prompt or how you format the character in this scenario (B vs. &amp;#39;B&amp;#39; vs. &amp;quot;B&amp;quot; etc.). In my benchmarks, I&amp;#39;ve had SotA models switch between providing a summary (correct behavior) and acting as one of the people involved (wrong behavior) in like 0.1% of of input conversations, and just replacing a single word with a synonym could change it back to the correct behavior. Most people just don&amp;#39;t test these models enough to come across these occurrences, or assume it was something else.&lt;/p&gt;\n\n&lt;p&gt;This task in particular is kind of pointless because it&amp;#39;s usually less about the model&amp;#39;s capabilities and more about how the tokenizer handles single letters and whether the developers specifically added training data for letter counting which isn&amp;#39;t something you&amp;#39;d naturally have much training data of.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mko9fw",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7l30h8/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754653990,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754653990,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7kzigp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Creative-Size2658",
                      "can_mod_post": false,
                      "created_utc": 1754652519,
                      "send_replies": true,
                      "parent_id": "t1_n7k8xy2",
                      "score": 1,
                      "author_fullname": "t2_1f3xb4r4ae",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Doesn't seem to be a problem for Qwen3 30B though. Even in different languages. Didn't prevent Altman from saying that Chinese stole his work.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7kzigp",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Doesn&amp;#39;t seem to be a problem for Qwen3 30B though. Even in different languages. Didn&amp;#39;t prevent Altman from saying that Chinese stole his work.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mko9fw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7kzigp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754652519,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k8xy2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "HiddenoO",
            "can_mod_post": false,
            "created_utc": 1754638276,
            "send_replies": true,
            "parent_id": "t3_1mko9fw",
            "score": 3,
            "author_fullname": "t2_8127x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Every single model out there still has failure states, including from questions that might look trivial to humans. If you think this says anything about how well the model performs, in general, you frankly haven't understood how these models work at all.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k8xy2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Every single model out there still has failure states, including from questions that might look trivial to humans. If you think this says anything about how well the model performs, in general, you frankly haven&amp;#39;t understood how these models work at all.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7k8xy2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754638276,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mko9fw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7k8rq3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AccomplishedAir769",
            "can_mod_post": false,
            "created_utc": 1754638177,
            "send_replies": true,
            "parent_id": "t3_1mko9fw",
            "score": 2,
            "author_fullname": "t2_4z3cs6yj4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "dang we went from strawberry to blueberry. really hope cranberry is next.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k8rq3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;dang we went from strawberry to blueberry. really hope cranberry is next.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7k8rq3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754638177,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mko9fw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kc3t7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ZinTheNurse",
            "can_mod_post": false,
            "created_utc": 1754640090,
            "send_replies": true,
            "parent_id": "t3_1mko9fw",
            "score": 1,
            "author_fullname": "t2_utri23fpd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here’s what typically happens inside models like me with a word like “blueberry,” and why “3” pops out so often.\n\n# How I “see” blueberry\n\n1. **Tokenization first, not characters.** I don’t ingest b-l-u-e-b-e-r-r-y as 9 characters. A tokenizer splits it into a few subword tokens, often something like “blue” + “berry” or similar splits. Letters inside a token aren’t explicitly represented as countable items. \n2. **Then prediction over chunks.** When asked “how many b’s,” the model is reasoning over those chunks, not iterating a pointer over individual characters. Pure character counting is a weak spot for token-based LLMs and is well documented in studies on counting tasks. \n\n# Why “3” becomes a repeated, automatic error\n\n1. **Morpheme heuristic leakage.** “blueberry” naturally splits into “blue” and “berry.” A bad internal shortcut is to treat “b at the start of ‘blue’” + “b at the start of ‘berry’,” then unconsciously double-count the word-initial b again. That flawed morpheme-start heuristic yields 3. You can see the same pattern in community reports where models justify “2 r’s” in “strawberry” by saying “one in straw, one in berry.” It’s the same chunk-first bias, just with r’s.\n2. **Training noise and memetic prior.** This exact failure became a meme for “strawberry,” with many screenshots and blog posts repeating the wrong answer. Noisy exposure to that pattern can bias the model toward confidently repeating it, especially under short, chatty prompts. Similar blueberry threads are appearing now too. \n3. **Counting is genuinely hard for LLMs.** Recent analyses argue the core issue is symbolic counting itself, not just token frequency. Even with different tokenizations, letter-count questions show elevated error rates unless you force a stepwise, character-level procedure.\n\n# The net effect\n\n* I parse “blueberry” as a couple of subword pieces.\n* I try to answer a numeric question using patterns over those pieces, not a literal per-character pass.\n* Heuristics and noisy priors nudge me to “3,” and without explicit instruction to index letters, I don’t correct the shortcut. This mirrors the well-known “strawberry” failure mode across LLMs. \n\n# How to prevent it\n\n* Tell the model to **spell the word with indices and tally** the target letter, or to **run code** to count. Accuracy jumps when you force character-level reasoning or use a tool.\n\nIf you want, give me a small list of words and a letter to count, and I’ll do the strict index-by-index method so you can see it behave correctly",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kc3t7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here’s what typically happens inside models like me with a word like “blueberry,” and why “3” pops out so often.&lt;/p&gt;\n\n&lt;h1&gt;How I “see” blueberry&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Tokenization first, not characters.&lt;/strong&gt; I don’t ingest b-l-u-e-b-e-r-r-y as 9 characters. A tokenizer splits it into a few subword tokens, often something like “blue” + “berry” or similar splits. Letters inside a token aren’t explicitly represented as countable items. &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Then prediction over chunks.&lt;/strong&gt; When asked “how many b’s,” the model is reasoning over those chunks, not iterating a pointer over individual characters. Pure character counting is a weak spot for token-based LLMs and is well documented in studies on counting tasks. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Why “3” becomes a repeated, automatic error&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Morpheme heuristic leakage.&lt;/strong&gt; “blueberry” naturally splits into “blue” and “berry.” A bad internal shortcut is to treat “b at the start of ‘blue’” + “b at the start of ‘berry’,” then unconsciously double-count the word-initial b again. That flawed morpheme-start heuristic yields 3. You can see the same pattern in community reports where models justify “2 r’s” in “strawberry” by saying “one in straw, one in berry.” It’s the same chunk-first bias, just with r’s.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Training noise and memetic prior.&lt;/strong&gt; This exact failure became a meme for “strawberry,” with many screenshots and blog posts repeating the wrong answer. Noisy exposure to that pattern can bias the model toward confidently repeating it, especially under short, chatty prompts. Similar blueberry threads are appearing now too. &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Counting is genuinely hard for LLMs.&lt;/strong&gt; Recent analyses argue the core issue is symbolic counting itself, not just token frequency. Even with different tokenizations, letter-count questions show elevated error rates unless you force a stepwise, character-level procedure.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;The net effect&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I parse “blueberry” as a couple of subword pieces.&lt;/li&gt;\n&lt;li&gt;I try to answer a numeric question using patterns over those pieces, not a literal per-character pass.&lt;/li&gt;\n&lt;li&gt;Heuristics and noisy priors nudge me to “3,” and without explicit instruction to index letters, I don’t correct the shortcut. This mirrors the well-known “strawberry” failure mode across LLMs. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;How to prevent it&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tell the model to &lt;strong&gt;spell the word with indices and tally&lt;/strong&gt; the target letter, or to &lt;strong&gt;run code&lt;/strong&gt; to count. Accuracy jumps when you force character-level reasoning or use a tool.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you want, give me a small list of words and a letter to count, and I’ll do the strict index-by-index method so you can see it behave correctly&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7kc3t7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754640090,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mko9fw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ks352",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1754649047,
            "send_replies": true,
            "parent_id": "t3_1mko9fw",
            "score": 1,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "local models work better than that",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ks352",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;local models work better than that&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mko9fw/chatgpt5_says_that_there_are_3_letters_b_in_the/n7ks352/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754649047,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mko9fw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]