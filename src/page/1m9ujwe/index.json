[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I run a LLM for home use, like sorting big text files. Nothing fancy, just more or less boring administrative stuff. I use Qwen3-30B-A3B-128K-UD-Q6\\_K\\_XL for this (by Unsloth) on a CPU only environment (Mini PC with Ryzen and 64GB RAM). I can load and use about 55GB of RAM, so eg. a 45GB LLM + 8GB for data aka context, but big models are very slow (below 1token/s). The A3B model is refreshingly fast (several token/s) but takes ages before it really works - aka reasoning. Although it's sometimes helpful, it also often eats all the given 32k/64k or 128k context before it outputs any results. And it also repeats and repeats and repeats the same train of thoughts. No\\_think is faster (if it works at all) but also worse in the results, so - any alternatives? This A3B model is really usable fast, but takes soooo long to think...",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Anything as fast as Qwen A3B?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9ujwe",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_3q3msk1c",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753539717,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I run a LLM for home use, like sorting big text files. Nothing fancy, just more or less boring administrative stuff. I use Qwen3-30B-A3B-128K-UD-Q6_K_XL for this (by Unsloth) on a CPU only environment (Mini PC with Ryzen and 64GB RAM). I can load and use about 55GB of RAM, so eg. a 45GB LLM + 8GB for data aka context, but big models are very slow (below 1token/s). The A3B model is refreshingly fast (several token/s) but takes ages before it really works - aka reasoning. Although it&amp;#39;s sometimes helpful, it also often eats all the given 32k/64k or 128k context before it outputs any results. And it also repeats and repeats and repeats the same train of thoughts. No_think is faster (if it works at all) but also worse in the results, so - any alternatives? This A3B model is really usable fast, but takes soooo long to think...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m9ujwe",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Pogo4Fufu",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/",
            "subreddit_subscribers": 504973,
            "created_utc": 1753539717,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5brxm5",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5b30mr",
                                "score": 1,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Try adding temperature.  These are literally designed from the start with the expectation that there is some randomness and running temp=0 is proven to degrade them.  Indeed the reasoning process relies on it.  You may well be having problems due this.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5brxm5",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try adding temperature.  These are literally designed from the start with the expectation that there is some randomness and running temp=0 is proven to degrade them.  Indeed the reasoning process relies on it.  You may well be having problems due this.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9ujwe",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5brxm5/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753563473,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753563473,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5b30mr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pogo4Fufu",
                      "can_mod_post": false,
                      "created_utc": 1753555328,
                      "send_replies": true,
                      "parent_id": "t1_n5a0v14",
                      "score": 2,
                      "author_fullname": "t2_3q3msk1c",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "ERNIE is on my list but not compatible to my backend - yet.  \nQwen often simply ignores the no\\_think command, not sure why and when that happens. Perhaps a bug with my backend or whatever, no idea.  \nI tend to use heat 0 for my stuff as I don't need the LMM to be \"smart\" or \"fanciful\" or \"creative\". And as expected, the LLM just finished the job (see my other answer) - after \\~13k reasoning + output, so 'only' about 1:8 ratio today. I mean, it works, but just takes soooo long to finally give an answer.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5b30mr",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ERNIE is on my list but not compatible to my backend - yet.&lt;br/&gt;\nQwen often simply ignores the no_think command, not sure why and when that happens. Perhaps a bug with my backend or whatever, no idea.&lt;br/&gt;\nI tend to use heat 0 for my stuff as I don&amp;#39;t need the LMM to be &amp;quot;smart&amp;quot; or &amp;quot;fanciful&amp;quot; or &amp;quot;creative&amp;quot;. And as expected, the LLM just finished the job (see my other answer) - after ~13k reasoning + output, so &amp;#39;only&amp;#39; about 1:8 ratio today. I mean, it works, but just takes soooo long to finally give an answer.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9ujwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5b30mr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753555328,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5a0v14",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753543434,
            "send_replies": true,
            "parent_id": "t3_1m9ujwe",
            "score": 4,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Since you have 8GB of VRAM you'll (almost certainly) get more speed out of a 8B model loaded entirely on GPU than the 30B-A3B that needs GPU+CPU.  If you want a similar but different model, ERNIE-4.5-21B-A3B-PT was recently released and should be equally fast.\n\nFor the Qwen3 series (including 30B-A3B) you can also append `/no_think` at the end of the prompt to avoid thinking.  It's not clear if you're using it, but sounds like you aren't and don't want it?  Also FWIW, it sounds like Qwen will be publishing a `Instruct-30B-A3B` next week that won't have reasoning at all.\n\nFinally, check your sampler settings.  IIRC Qwen recommends low temperatures and some other to avoid excessive reasoning.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5a0v14",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Since you have 8GB of VRAM you&amp;#39;ll (almost certainly) get more speed out of a 8B model loaded entirely on GPU than the 30B-A3B that needs GPU+CPU.  If you want a similar but different model, ERNIE-4.5-21B-A3B-PT was recently released and should be equally fast.&lt;/p&gt;\n\n&lt;p&gt;For the Qwen3 series (including 30B-A3B) you can also append &lt;code&gt;/no_think&lt;/code&gt; at the end of the prompt to avoid thinking.  It&amp;#39;s not clear if you&amp;#39;re using it, but sounds like you aren&amp;#39;t and don&amp;#39;t want it?  Also FWIW, it sounds like Qwen will be publishing a &lt;code&gt;Instruct-30B-A3B&lt;/code&gt; next week that won&amp;#39;t have reasoning at all.&lt;/p&gt;\n\n&lt;p&gt;Finally, check your sampler settings.  IIRC Qwen recommends low temperatures and some other to avoid excessive reasoning.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5a0v14/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753543434,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9ujwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5azdl5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pogo4Fufu",
                      "can_mod_post": false,
                      "created_utc": 1753554153,
                      "send_replies": true,
                      "parent_id": "t1_n5amlwb",
                      "score": 1,
                      "author_fullname": "t2_3q3msk1c",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Well, the difference in speed is neglectable - at least in my case or my tests. As VRAM is not the problem (I use RAM anyway), the performance loss is more or less directly proportional to the size in GB - or 1:0.65. A theoretical improve of 30% is not bad, but also not that much, so I decided to better improve or keep the accuracy. If you really have to take care of every MB of VRAM - yes.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5azdl5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, the difference in speed is neglectable - at least in my case or my tests. As VRAM is not the problem (I use RAM anyway), the performance loss is more or less directly proportional to the size in GB - or 1:0.65. A theoretical improve of 30% is not bad, but also not that much, so I decided to better improve or keep the accuracy. If you really have to take care of every MB of VRAM - yes.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9ujwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5azdl5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753554153,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5amlwb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GreenTreeAndBlueSky",
            "can_mod_post": false,
            "created_utc": 1753550212,
            "send_replies": true,
            "parent_id": "t3_1m9ujwe",
            "score": 1,
            "author_fullname": "t2_1p50pl73j2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You are using too big of a quant. \nUse q4_k_m and you'll have almost no degradation in performance but you'll have much higher speeds on cpu.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5amlwb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You are using too big of a quant. \nUse q4_k_m and you&amp;#39;ll have almost no degradation in performance but you&amp;#39;ll have much higher speeds on cpu.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5amlwb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753550212,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9ujwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5cp8bb",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "DeProgrammer99",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5a4cxl",
                                                    "score": 1,
                                                    "author_fullname": "t2_w4j8t",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Sounds like you might want to adjust your sampling parameters, like increasing repetition penalty. Also, if you're sorting files in bulk, is your process at least using batched inference? That can be several times faster.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5cp8bb",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sounds like you might want to adjust your sampling parameters, like increasing repetition penalty. Also, if you&amp;#39;re sorting files in bulk, is your process at least using batched inference? That can be several times faster.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m9ujwe",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5cp8bb/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753575261,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753575261,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5a4cxl",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "Pogo4Fufu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5a0ypk",
                                          "score": 3,
                                          "author_fullname": "t2_3q3msk1c",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "No\\_think showed several times significant worse results, so I avoid it. The main problem is really the endless repetitions of Qwen's reasoning. It still works on the file above - 9k reasoning so far or 1:6 ratio input:reasoning. And not over yet - omfg. I'm quite sure it will give a 'good' answer, after estimated 15k reasoning. It's not the first time I see a ratio of about 1:10. And the job is not really that hard: extract information from more-or-less structured text, like 'information.is.hidden.0003.in.here.BREAK.FGH&amp;/FVGH'. The texts are to hard for grep, awk or sed, so a LLM is the way to go, but the information is private and must not go online to Gemini &amp; Co.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5a4cxl",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No_think showed several times significant worse results, so I avoid it. The main problem is really the endless repetitions of Qwen&amp;#39;s reasoning. It still works on the file above - 9k reasoning so far or 1:6 ratio input:reasoning. And not over yet - omfg. I&amp;#39;m quite sure it will give a &amp;#39;good&amp;#39; answer, after estimated 15k reasoning. It&amp;#39;s not the first time I see a ratio of about 1:10. And the job is not really that hard: extract information from more-or-less structured text, like &amp;#39;information.is.hidden.0003.in.here.BREAK.FGH&amp;amp;/FVGH&amp;#39;. The texts are to hard for grep, awk or sed, so a LLM is the way to go, but the information is private and must not go online to Gemini &amp;amp; Co.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m9ujwe",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5a4cxl/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753544522,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753544522,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5a0ypk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LogicalAnimation",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n59x9k7",
                                "score": 1,
                                "author_fullname": "t2_625i4zoy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "have you tried adding /no\\_think to your prompt？ I don't think you can get much better than a3b on cpu+ram combo, maybe you can try the iq3/iq4 quants of [DeepSeek-R1-0528-Qwen3-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B)? The hunyuan 80b a13b is slower and sometimes performs badly.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5a0ypk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;have you tried adding /no_think to your prompt？ I don&amp;#39;t think you can get much better than a3b on cpu+ram combo, maybe you can try the iq3/iq4 quants of &lt;a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"&gt;DeepSeek-R1-0528-Qwen3-8B&lt;/a&gt;? The hunyuan 80b a13b is slower and sometimes performs badly.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9ujwe",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5a0ypk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753543465,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753543465,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n59x9k7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pogo4Fufu",
                      "can_mod_post": false,
                      "created_utc": 1753542300,
                      "send_replies": true,
                      "parent_id": "t1_n59qzd4",
                      "score": 2,
                      "author_fullname": "t2_3q3msk1c",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I guess smaller Qwen models will have the same problem - endless reasoning. I just tried again to work on a text file - 1,3k (words) incl. the job description and examples how and what to do. Qwen's answer so far: 5k reasoning... it's just hilariously broken sometimes.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n59x9k7",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I guess smaller Qwen models will have the same problem - endless reasoning. I just tried again to work on a text file - 1,3k (words) incl. the job description and examples how and what to do. Qwen&amp;#39;s answer so far: 5k reasoning... it&amp;#39;s just hilariously broken sometimes.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9ujwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n59x9k7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753542300,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n59qzd4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "nerdlord420",
            "can_mod_post": false,
            "created_utc": 1753540280,
            "send_replies": true,
            "parent_id": "t3_1m9ujwe",
            "score": 1,
            "author_fullname": "t2_a6s61",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try Qwen3-1.7B or Qwen3-4B? Also if you're using a mini pc with a Ryzen iGPU, you might get faster speeds if you offload at least one layer to the iGPU via vulkan",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n59qzd4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try Qwen3-1.7B or Qwen3-4B? Also if you&amp;#39;re using a mini pc with a Ryzen iGPU, you might get faster speeds if you offload at least one layer to the iGPU via vulkan&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n59qzd4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753540280,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9ujwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5a0vy0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "chregu",
            "can_mod_post": false,
            "created_utc": 1753543442,
            "send_replies": true,
            "parent_id": "t3_1m9ujwe",
            "score": 1,
            "author_fullname": "t2_2rzgu",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just add /no\\_think to your prompt? Then it doesn't reason (usually).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5a0vy0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just add /no_think to your prompt? Then it doesn&amp;#39;t reason (usually).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9ujwe/anything_as_fast_as_qwen_a3b/n5a0vy0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753543442,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9ujwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]