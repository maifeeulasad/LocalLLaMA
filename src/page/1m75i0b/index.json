[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi,  \nI have 2x 7900 XTX and not getting any model run with them in a docker.\n\n    docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\n    \n    docker run -it \\\n      --dns=1.1.1.1 \\\n      --dns=8.8.8.8 \\\n      --network=host \\\n      --group-add=video \\\n      --ipc=host \\\n      --cap-add=SYS_PTRACE \\\n      --security-opt seccomp=unconfined \\\n      --privileged \\\n      --device /dev/kfd \\\n      --device /dev/dri \\\n      -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\n      -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\n      -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n      -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\n      -e PYTORCH_TUNABLEOP_ENABLED=1 \\\n      -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\n      -e PYTORCH_ROCM_ARCH=\"gfx1100\" \\\n      -e GPU_MAX_HW_QUEUES=1 \\\n      -v /home/ubuntu/vllm_models:/workspace/models \\\n      rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\n\napt update &amp;&amp; apt install -y git build-essential\n\npip install ninja\n\npip3 install -U xformers --index-url [https://download.pytorch.org/whl/rocm6.3](https://download.pytorch.org/whl/rocm6.3)\n\n    vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\n      --dtype float16 \\\n      --kv-cache-dtype auto \\\n      --tensor-parallel-size 2 \\\n      --trust-remote-code \\\n      --tokenizer_mode auto \\\n      --port 8000 \\\n      --host 0.0.0.0 \n\nHave tried different models, some may go to a point where vllm says \"started and waiting\" but then when trying to chat with it all crashes.\n\nHow this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.\n\nEDIT: I trusted you AMD, I really did....",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Getting a model run with vLLM and 7900 XTX",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m75i0b",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1jk2ep8a52",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753272524,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753266053,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI have 2x 7900 XTX and not getting any model run with them in a docker.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\n\ndocker run -it \\\n  --dns=1.1.1.1 \\\n  --dns=8.8.8.8 \\\n  --network=host \\\n  --group-add=video \\\n  --ipc=host \\\n  --cap-add=SYS_PTRACE \\\n  --security-opt seccomp=unconfined \\\n  --privileged \\\n  --device /dev/kfd \\\n  --device /dev/dri \\\n  -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\n  -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\n  -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\n  -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\n  -e PYTORCH_TUNABLEOP_ENABLED=1 \\\n  -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\n  -e PYTORCH_ROCM_ARCH=&amp;quot;gfx1100&amp;quot; \\\n  -e GPU_MAX_HW_QUEUES=1 \\\n  -v /home/ubuntu/vllm_models:/workspace/models \\\n  rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;apt update &amp;amp;&amp;amp; apt install -y git build-essential&lt;/p&gt;\n\n&lt;p&gt;pip install ninja&lt;/p&gt;\n\n&lt;p&gt;pip3 install -U xformers --index-url &lt;a href=\"https://download.pytorch.org/whl/rocm6.3\"&gt;https://download.pytorch.org/whl/rocm6.3&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\n  --dtype float16 \\\n  --kv-cache-dtype auto \\\n  --tensor-parallel-size 2 \\\n  --trust-remote-code \\\n  --tokenizer_mode auto \\\n  --port 8000 \\\n  --host 0.0.0.0 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Have tried different models, some may go to a point where vllm says &amp;quot;started and waiting&amp;quot; but then when trying to chat with it all crashes.&lt;/p&gt;\n\n&lt;p&gt;How this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I trusted you AMD, I really did....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m75i0b",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Rich_Artist_8327",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/",
            "subreddit_subscribers": 503257,
            "created_utc": 1753266053,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]