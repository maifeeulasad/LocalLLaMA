[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm planning to run a local LLM for code analysis and modification. Specifically, I want to:  \n\\- Analyze and potentially modify a Python script with around 1000 lines of code  \n\\- Use a GPU with 24GB VRAM  \n  \nCan anyone share experience with:  \n\\- Approximate token/second generation speed  \n\\- Which models work best for code tasks (e.g., CodeLlama, WizardCoder)  \n\\- Recommended hardware configurations\n\n  \nThanks",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Performance Expectations for Local LLM with 24GB GPU - Code Analysis &amp; Modification",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mbghx5",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_8tetfmez5",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753710099,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m planning to run a local LLM for code analysis and modification. Specifically, I want to:&lt;br/&gt;\n- Analyze and potentially modify a Python script with around 1000 lines of code&lt;br/&gt;\n- Use a GPU with 24GB VRAM  &lt;/p&gt;\n\n&lt;p&gt;Can anyone share experience with:&lt;br/&gt;\n- Approximate token/second generation speed&lt;br/&gt;\n- Which models work best for code tasks (e.g., CodeLlama, WizardCoder)&lt;br/&gt;\n- Recommended hardware configurations&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mbghx5",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "BarberPlane3020",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/",
            "subreddit_subscribers": 506191,
            "created_utc": 1753710099,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5lzgju",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CBW1255",
            "can_mod_post": false,
            "created_utc": 1753711408,
            "send_replies": true,
            "parent_id": "t3_1mbghx5",
            "score": 3,
            "author_fullname": "t2_uprpjkzls",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you are used to running proprietary models + MCP you are not going to be pleasantly surprised.\n\nCurrently, with the setup you specified, you can get the inference speed but that's it. \n\nAsking in this forum might get you a few different answers since, after all, you are in LocalLLaMA, but... No. There's no comparison to Claude, chatGPT, Grok etc.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5lzgju",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you are used to running proprietary models + MCP you are not going to be pleasantly surprised.&lt;/p&gt;\n\n&lt;p&gt;Currently, with the setup you specified, you can get the inference speed but that&amp;#39;s it. &lt;/p&gt;\n\n&lt;p&gt;Asking in this forum might get you a few different answers since, after all, you are in LocalLLaMA, but... No. There&amp;#39;s no comparison to Claude, chatGPT, Grok etc.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5lzgju/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753711408,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbghx5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5m3tli",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MaxKruse96",
            "can_mod_post": false,
            "created_utc": 1753712729,
            "send_replies": true,
            "parent_id": "t3_1mbghx5",
            "score": 2,
            "author_fullname": "t2_pfi81",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "1k lines of python can be estimated to be around 15k tokens (assuming \\~15 tokens per line which is average i'd assume, heavily depends on the code though)\n\nthat means at least 15k context input + 15k context output + 2k tokens prompting etc. so 32k+++ (with \"diff only\", output context obv smaller, but lets assume worst case)\n\nRight now the goto local model would be devstral at that size, because its code you'd wnat to use q6 quant, so 16gb vram usage from that alone, add the context which would be (assumption, i cant even test that) at least 10gb by itself, we are in offloading territory, meaning low tokens/s in such a big dense model. You can expect roughly 10-15 token/s if you have a powerful gpu, e.g. a 4090. a 3090 will be on the lower end of this.\n\nas for hardware configuration, 4090 are insanely hard to get, so a 3090 second hand, or a 5090 (but then you have better speeds too). CPU isnt too important, but obv dont want some 4core. Def would go for 64gb ram at least, loading stuff in and out can take a bit of time and capacity.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5m3tli",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;1k lines of python can be estimated to be around 15k tokens (assuming ~15 tokens per line which is average i&amp;#39;d assume, heavily depends on the code though)&lt;/p&gt;\n\n&lt;p&gt;that means at least 15k context input + 15k context output + 2k tokens prompting etc. so 32k+++ (with &amp;quot;diff only&amp;quot;, output context obv smaller, but lets assume worst case)&lt;/p&gt;\n\n&lt;p&gt;Right now the goto local model would be devstral at that size, because its code you&amp;#39;d wnat to use q6 quant, so 16gb vram usage from that alone, add the context which would be (assumption, i cant even test that) at least 10gb by itself, we are in offloading territory, meaning low tokens/s in such a big dense model. You can expect roughly 10-15 token/s if you have a powerful gpu, e.g. a 4090. a 3090 will be on the lower end of this.&lt;/p&gt;\n\n&lt;p&gt;as for hardware configuration, 4090 are insanely hard to get, so a 3090 second hand, or a 5090 (but then you have better speeds too). CPU isnt too important, but obv dont want some 4core. Def would go for 64gb ram at least, loading stuff in and out can take a bit of time and capacity.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5m3tli/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753712729,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbghx5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5mc612",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Secure_Reflection409",
            "can_mod_post": false,
            "created_utc": 1753715165,
            "send_replies": true,
            "parent_id": "t3_1mbghx5",
            "score": 2,
            "author_fullname": "t2_by77ogdhr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We're in a really weird space right now where all the killer enthusiast models are in the 200 - 400b range (96 to 300GB VRAM) but the absolute best enthusiast hardware is 32GB and it costs the fucking earth.\n\n\n24GB isn't enough for even the current best 32b models, either.\n\n\nWe really need prosumer cards to go straight to 96GB (and not for 8 grand, ffs), even if the compute is heavily limited or we're going to be priced right out of all the fun stuff.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5mc612",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re in a really weird space right now where all the killer enthusiast models are in the 200 - 400b range (96 to 300GB VRAM) but the absolute best enthusiast hardware is 32GB and it costs the fucking earth.&lt;/p&gt;\n\n&lt;p&gt;24GB isn&amp;#39;t enough for even the current best 32b models, either.&lt;/p&gt;\n\n&lt;p&gt;We really need prosumer cards to go straight to 96GB (and not for 8 grand, ffs), even if the compute is heavily limited or we&amp;#39;re going to be priced right out of all the fun stuff.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5mc612/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753715165,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbghx5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5m2pcb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "BarberPlane3020",
                      "can_mod_post": false,
                      "created_utc": 1753712397,
                      "send_replies": true,
                      "parent_id": "t1_n5m14tx",
                      "score": 1,
                      "author_fullname": "t2_8tetfmez5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I plan use RTX 3090 24GB or maybe something better (RTX 4090 or RTX 5090)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5m2pcb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I plan use RTX 3090 24GB or maybe something better (RTX 4090 or RTX 5090)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mbghx5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5m2pcb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753712397,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5m14tx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Winter-Reveal5295",
            "can_mod_post": false,
            "created_utc": 1753711920,
            "send_replies": true,
            "parent_id": "t3_1mbghx5",
            "score": 1,
            "author_fullname": "t2_1ig1zyrqs2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What 24GB GPU are you planning on using?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5m14tx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What 24GB GPU are you planning on using?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5m14tx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753711920,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbghx5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5o2zz6",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "BarberPlane3020",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5o12ja",
                                          "score": 1,
                                          "author_fullname": "t2_8tetfmez5",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5o2zz6",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mbghx5",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5o2zz6/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753732845,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753732845,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5o12ja",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Physical-Citron5153",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5nzoiy",
                                "score": 1,
                                "author_fullname": "t2_clhgguip",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Close to 30 TPS \nAlthough i am not using Tensor Parallelism",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5o12ja",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Close to 30 TPS \nAlthough i am not using Tensor Parallelism&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mbghx5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5o12ja/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753732296,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753732296,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5nzoiy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "BarberPlane3020",
                      "can_mod_post": false,
                      "created_utc": 1753731898,
                      "send_replies": true,
                      "parent_id": "t1_n5nw20u",
                      "score": 1,
                      "author_fullname": "t2_8tetfmez5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hi, can you let me know how many tokens/sec you got with 2x RTX 3090?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5nzoiy",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi, can you let me know how many tokens/sec you got with 2x RTX 3090?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mbghx5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5nzoiy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753731898,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ocoj6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "serige",
                      "can_mod_post": false,
                      "created_utc": 1753735570,
                      "send_replies": true,
                      "parent_id": "t1_n5nw20u",
                      "score": 1,
                      "author_fullname": "t2_kxwnu",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Upgraded to how much ram and at what quant please?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ocoj6",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Upgraded to how much ram and at what quant please?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mbghx5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5ocoj6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753735570,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5nw20u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Physical-Citron5153",
            "can_mod_post": false,
            "created_utc": 1753730857,
            "send_replies": true,
            "parent_id": "t3_1mbghx5",
            "score": 1,
            "author_fullname": "t2_clhgguip",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "As i said to many people the only model in that range which is usable is Devstral 24B which i ran on my 2X RTX 3090 and that model is kinda OK but not anything serious. Its just for fun\n\nI recently upgraded my setup RAM and now i am using the new Qwen 235B 2507 Instruct and it is the first time i am seeing hope in local models in my test it performed pretty well. \n \nplan your setup so it can run these new MoE models which are at least some decent models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5nw20u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As i said to many people the only model in that range which is usable is Devstral 24B which i ran on my 2X RTX 3090 and that model is kinda OK but not anything serious. Its just for fun&lt;/p&gt;\n\n&lt;p&gt;I recently upgraded my setup RAM and now i am using the new Qwen 235B 2507 Instruct and it is the first time i am seeing hope in local models in my test it performed pretty well. &lt;/p&gt;\n\n&lt;p&gt;plan your setup so it can run these new MoE models which are at least some decent models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5nw20u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753730857,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbghx5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5pmxqd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Bus9917",
            "can_mod_post": false,
            "created_utc": 1753750386,
            "send_replies": true,
            "parent_id": "t3_1mbghx5",
            "score": 1,
            "author_fullname": "t2_1u3ele924k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Testing the new GLM 4.5 Air: it's amazing.  \nCurious how much performance someone could get out of it partially offloaded or CPU only.\n\nI've used most coder and general models up to Qwen3 235B at Q3, including Kimi-Dev-72B, and a bunch of 32B and smaller models:\n\n24B models are the lower end of the elbow in the ability curve - can help with basic coding tasks well, but struggle with advanced analysis and changes.  \n  \n32B is a significant step up in terms of ability, but still quite far off online flagship models.\n\nQwen3 235B A22B starts to get closer, GLM 4.5 seems similar in ability.\n\nWhat's the rest of your system look like?  \nAre you upgrading an existing build with a GPU or starting a fresh build?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5pmxqd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Testing the new GLM 4.5 Air: it&amp;#39;s amazing.&lt;br/&gt;\nCurious how much performance someone could get out of it partially offloaded or CPU only.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used most coder and general models up to Qwen3 235B at Q3, including Kimi-Dev-72B, and a bunch of 32B and smaller models:&lt;/p&gt;\n\n&lt;p&gt;24B models are the lower end of the elbow in the ability curve - can help with basic coding tasks well, but struggle with advanced analysis and changes.  &lt;/p&gt;\n\n&lt;p&gt;32B is a significant step up in terms of ability, but still quite far off online flagship models.&lt;/p&gt;\n\n&lt;p&gt;Qwen3 235B A22B starts to get closer, GLM 4.5 seems similar in ability.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the rest of your system look like?&lt;br/&gt;\nAre you upgrading an existing build with a GPU or starting a fresh build?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbghx5/performance_expectations_for_local_llm_with_24gb/n5pmxqd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753750386,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbghx5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]