[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I ran the [vLLM provided benchmarks](https://github.com/vllm-project/vllm/tree/main/benchmarks) `serve` (online serving throughput) and `throughput` (offline serving throughput) for `gpt-oss-120b` on my H100 96GB with the ShareGPT benchmark data.\n\nCan confirm it fits snugly in 96GB. Numbers below.\n\n# Throughput Benchmark (offline serving throughput)\n\nCommand: `vllm bench serve --model \"openai/gpt-oss-120b\"`\n\n    ============ Serving Benchmark Result ============\n    Successful requests:                     1000\n    Benchmark duration (s):                  47.81\n    Total input tokens:                      1022745\n    Total generated tokens:                  48223\n    Request throughput (req/s):              20.92\n    Output token throughput (tok/s):         1008.61\n    Total Token throughput (tok/s):          22399.88\n    ---------------Time to First Token----------------\n    Mean TTFT (ms):                          18806.63\n    Median TTFT (ms):                        18631.45\n    P99 TTFT (ms):                           36522.62\n    -----Time per Output Token (excl. 1st token)------\n    Mean TPOT (ms):                          283.85\n    Median TPOT (ms):                        271.48\n    P99 TPOT (ms):                           801.98\n    ---------------Inter-token Latency----------------\n    Mean ITL (ms):                           231.50\n    Median ITL (ms):                         267.02\n    P99 ITL (ms):                            678.42\n    ==================================================\n\n# Serve Benchmark (online serving throughput)\n\nCommand: `vllm bench latency --model \"openai/gpt-oss-120b\"`\n\n    Avg latency: 1.3391752537339925 seconds\n    10% percentile latency: 1.277150624152273 seconds\n    25% percentile latency: 1.30161597346887 seconds\n    50% percentile latency: 1.3404422830790281 seconds\n    75% percentile latency: 1.3767581032589078 seconds\n    90% percentile latency: 1.393262314144522 seconds\n    99% percentile latency: 1.4468831585347652 seconds",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "vLLM latency/throughput benchmarks for gpt-oss-120b",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 81,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mijza6",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.95,
            "author_flair_background_color": "transparent",
            "ups": 51,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "is_original_content": false,
            "author_fullname": "t2_1a48h7vf",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 51,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/CkiPy_94iF46m6sozte4q0xipLvoNhob8Av1r371Asc.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754424752,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I ran the &lt;a href=\"https://github.com/vllm-project/vllm/tree/main/benchmarks\"&gt;vLLM provided benchmarks&lt;/a&gt; &lt;code&gt;serve&lt;/code&gt; (online serving throughput) and &lt;code&gt;throughput&lt;/code&gt; (offline serving throughput) for &lt;code&gt;gpt-oss-120b&lt;/code&gt; on my H100 96GB with the ShareGPT benchmark data.&lt;/p&gt;\n\n&lt;p&gt;Can confirm it fits snugly in 96GB. Numbers below.&lt;/p&gt;\n\n&lt;h1&gt;Throughput Benchmark (offline serving throughput)&lt;/h1&gt;\n\n&lt;p&gt;Command: &lt;code&gt;vllm bench serve --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  47.81\nTotal input tokens:                      1022745\nTotal generated tokens:                  48223\nRequest throughput (req/s):              20.92\nOutput token throughput (tok/s):         1008.61\nTotal Token throughput (tok/s):          22399.88\n---------------Time to First Token----------------\nMean TTFT (ms):                          18806.63\nMedian TTFT (ms):                        18631.45\nP99 TTFT (ms):                           36522.62\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          283.85\nMedian TPOT (ms):                        271.48\nP99 TPOT (ms):                           801.98\n---------------Inter-token Latency----------------\nMean ITL (ms):                           231.50\nMedian ITL (ms):                         267.02\nP99 ITL (ms):                            678.42\n==================================================\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Serve Benchmark (online serving throughput)&lt;/h1&gt;\n\n&lt;p&gt;Command: &lt;code&gt;vllm bench latency --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Avg latency: 1.3391752537339925 seconds\n10% percentile latency: 1.277150624152273 seconds\n25% percentile latency: 1.30161597346887 seconds\n50% percentile latency: 1.3404422830790281 seconds\n75% percentile latency: 1.3767581032589078 seconds\n90% percentile latency: 1.393262314144522 seconds\n99% percentile latency: 1.4468831585347652 seconds\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/bz9j2b92d9hf1.png",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/bz9j2b92d9hf1.png?auto=webp&amp;s=f4537f0f422810ae514de68f6a07b87764fd88d3",
                    "width": 2695,
                    "height": 1574
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/bz9j2b92d9hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c11c3140455b7faeb590a3bf0df168bd37f153f1",
                      "width": 108,
                      "height": 63
                    },
                    {
                      "url": "https://preview.redd.it/bz9j2b92d9hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f0d32efb2dbb850088837decea852e2c7f216980",
                      "width": 216,
                      "height": 126
                    },
                    {
                      "url": "https://preview.redd.it/bz9j2b92d9hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=07654141634decbf9b896303af7caddb1575997f",
                      "width": 320,
                      "height": 186
                    },
                    {
                      "url": "https://preview.redd.it/bz9j2b92d9hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c7db73fb89602975462166e965bca3fd42eb685",
                      "width": 640,
                      "height": 373
                    },
                    {
                      "url": "https://preview.redd.it/bz9j2b92d9hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c7305c5c1ea1f409820387a5147feddc8c865c16",
                      "width": 960,
                      "height": 560
                    },
                    {
                      "url": "https://preview.redd.it/bz9j2b92d9hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a59413d800f13ae883ef3683a06ed1962ec705c2",
                      "width": 1080,
                      "height": 630
                    }
                  ],
                  "variants": {},
                  "id": "QSXGsN0xEL2pMJPlSHxzuAzjIjcwxz798p-YwZh29U4"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mijza6",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "entsnack",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "dark",
            "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/",
            "stickied": false,
            "url": "https://i.redd.it/bz9j2b92d9hf1.png",
            "subreddit_subscribers": 511364,
            "created_utc": 1754424752,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n743ajv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Felladrin",
            "can_mod_post": false,
            "created_utc": 1754426286,
            "send_replies": true,
            "parent_id": "t3_1mijza6",
            "score": 5,
            "author_fullname": "t2_wp9mv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Good info! Thanks for sharing!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n743ajv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Good info! Thanks for sharing!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/n743ajv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754426286,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mijza6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n74ywqm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754436596,
                      "send_replies": true,
                      "parent_id": "t1_n74paad",
                      "score": 2,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I used the default parameters. My serving command is \\`vllm serve openai/gpt-oss-120b\\`.\n\nYou could try \\`--enforce-eager\\`. Also make sure you don't see any error messages about \"unquantizing\", and that your libraries are up to date. I'm on pytorch 2.8, Cuda 12.8, latest transformers and triton 3.4.0, latest triton\\_kernels.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74ywqm",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I used the default parameters. My serving command is `vllm serve openai/gpt-oss-120b`.&lt;/p&gt;\n\n&lt;p&gt;You could try `--enforce-eager`. Also make sure you don&amp;#39;t see any error messages about &amp;quot;unquantizing&amp;quot;, and that your libraries are up to date. I&amp;#39;m on pytorch 2.8, Cuda 12.8, latest transformers and triton 3.4.0, latest triton_kernels.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mijza6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/n74ywqm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754436596,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n74paad",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Zbogus",
            "can_mod_post": false,
            "created_utc": 1754433407,
            "send_replies": true,
            "parent_id": "t3_1mijza6",
            "score": 3,
            "author_fullname": "t2_13ne7s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Do you know what parameters are used to vllm for this? I am seeing considerably slower on the same hardware",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74paad",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you know what parameters are used to vllm for this? I am seeing considerably slower on the same hardware&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/n74paad/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754433407,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mijza6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n74ykny",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754436485,
                      "send_replies": true,
                      "parent_id": "t1_n74xyu7",
                      "score": 2,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It's complicated. I should post a tutorial. This is the vLLM installation command:\n\n    uv pip install --pre vllm==0.10.1+gptoss \\\n       --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n       --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n       --index-strategy unsafe-best-match\n\nYou also need pytorch 2.8:\n\n```\npip install torch==2.8.0 --index-url https://download.pytorch.org/whl/test/cu128\n```\n\nYou also need triton and triton_kernels to use mxfp4:\n\n```\npip install triton==3.4.0\npip install git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n```",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74ykny",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s complicated. I should post a tutorial. This is the vLLM installation command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;uv pip install --pre vllm==0.10.1+gptoss \\\n   --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n   --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n   --index-strategy unsafe-best-match\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You also need pytorch 2.8:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\npip install torch==2.8.0 --index-url https://download.pytorch.org/whl/test/cu128\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;You also need triton and triton_kernels to use mxfp4:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\npip install triton==3.4.0\npip install git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mijza6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/n74ykny/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754436485,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n74xyu7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "itsmebcc",
            "can_mod_post": false,
            "created_utc": 1754436283,
            "send_replies": true,
            "parent_id": "t3_1mijza6",
            "score": 2,
            "author_fullname": "t2_43j7l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I cannot seem to be able to build the vllm to run this. Do you have the command you used to build this?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74xyu7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I cannot seem to be able to build the vllm to run this. Do you have the command you used to build this?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/n74xyu7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754436283,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mijza6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n75ex7g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754442029,
                      "send_replies": true,
                      "parent_id": "t1_n75ck3j",
                      "score": 1,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "gpt-oss models use the MXFP4 format natively, which means they use 4.25 bits per parameter. bf16/fp16 is about 3.75x larger. Hopper and Blackwell GPUs support MXFP4 (Blackwell supports it in hardware). The model I'm running is in its native format from the OpenAI Huggingface repo.\n\nEdit: Also 120B is an MoE with 5.1B active parameters per forward pass.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n75ex7g",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;gpt-oss models use the MXFP4 format natively, which means they use 4.25 bits per parameter. bf16/fp16 is about 3.75x larger. Hopper and Blackwell GPUs support MXFP4 (Blackwell supports it in hardware). The model I&amp;#39;m running is in its native format from the OpenAI Huggingface repo.&lt;/p&gt;\n\n&lt;p&gt;Edit: Also 120B is an MoE with 5.1B active parameters per forward pass.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mijza6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/n75ex7g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754442029,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n75ck3j",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "greying_panda",
            "can_mod_post": false,
            "created_utc": 1754441224,
            "send_replies": true,
            "parent_id": "t3_1mijza6",
            "score": 1,
            "author_fullname": "t2_52h19pjc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How is this deployed? 96GB VRAM for a 120B model seems incongruent without heavy quantization or offloading (naively 120B should be 240GB in 16bit just for parameters, no?)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75ck3j",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How is this deployed? 96GB VRAM for a 120B model seems incongruent without heavy quantization or offloading (naively 120B should be 240GB in 16bit just for parameters, no?)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/n75ck3j/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754441224,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mijza6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]