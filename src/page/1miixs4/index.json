[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey //locallama,\n\nI‚Äôm using the desktop‚ÄØStudio app for local inference and love having everything in one place.  \n\nWhat I‚Äôm missing is any way to \\*\\*point LM‚ÄØStudio at an external OpenAI‚Äëcompatible endpoint\\*\\* (Ollama, a hosted OpenAI‚Äëstyle server, etc.) from within the UI.\n\nI know alternatives like \\*\\*Open‚ÄØWebUI\\*\\* can be extended to proxy external services, but I‚Äôd rather keep a single, native desktop tool for inference, prompt‚Äëengineering, and any API calls.\n\n\n\n\n\n1. Is the lack of external‚ÄëAPI support a deliberate limitation right now?\n\n2. Is there a roadmap for adding OpenAI‚Äëstyle function‚Äëcalling or a plug‚Äëin system that can forward requests to another compatible server?  \n\n3. Are there any known work‚Äëarounds (local proxy scripts, community extensions etc.) that let LM‚ÄØStudio act as a client for an external OpenAI‚Äëcompatible API?\n\n\n\nAny official statements, issue/PR links, or community hacks would be hugely helpful. Thanks! üôè",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why doesn‚Äôt LM‚ÄØStudio let me call external OpenAI‚Äëcompatible APIs (e.g., Ollama)?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1miixs4",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_41wov",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754422417,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey //locallama,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm using the desktop‚ÄØStudio app for local inference and love having everything in one place.  &lt;/p&gt;\n\n&lt;p&gt;What I‚Äôm missing is any way to **point LM‚ÄØStudio at an external OpenAI‚Äëcompatible endpoint** (Ollama, a hosted OpenAI‚Äëstyle server, etc.) from within the UI.&lt;/p&gt;\n\n&lt;p&gt;I know alternatives like **Open‚ÄØWebUI** can be extended to proxy external services, but I‚Äôd rather keep a single, native desktop tool for inference, prompt‚Äëengineering, and any API calls.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Is the lack of external‚ÄëAPI support a deliberate limitation right now?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is there a roadmap for adding OpenAI‚Äëstyle function‚Äëcalling or a plug‚Äëin system that can forward requests to another compatible server?  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any known work‚Äëarounds (local proxy scripts, community extensions etc.) that let LM‚ÄØStudio act as a client for an external OpenAI‚Äëcompatible API?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any official statements, issue/PR links, or community hacks would be hugely helpful. Thanks! üôè&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1miixs4",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "myusuf3",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1miixs4/why_doesnt_lm_studio_let_me_call_external/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1miixs4/why_doesnt_lm_studio_let_me_call_external/",
            "subreddit_subscribers": 511364,
            "created_utc": 1754422417,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n73u06v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lesser-than",
            "can_mod_post": false,
            "created_utc": 1754423269,
            "send_replies": true,
            "parent_id": "t3_1miixs4",
            "score": 2,
            "author_fullname": "t2_98d256k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think its rather intentional to be honest , its much easier to support when you also provide the inference engine with the app and do not have to keep up with for testing and what not. I think, LM studio also provides an optional server mode, its been awhile but thats essentially how you use it in vscode and other external tools.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n73u06v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think its rather intentional to be honest , its much easier to support when you also provide the inference engine with the app and do not have to keep up with for testing and what not. I think, LM studio also provides an optional server mode, its been awhile but thats essentially how you use it in vscode and other external tools.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miixs4/why_doesnt_lm_studio_let_me_call_external/n73u06v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754423269,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miixs4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n73tvho",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Savantskie1",
            "can_mod_post": false,
            "created_utc": 1754423225,
            "send_replies": true,
            "parent_id": "t3_1miixs4",
            "score": 1,
            "author_fullname": "t2_7qb4luzhc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "To do this, would be dumb. LM Studio, ollama, and llama.cpp, are all for hosting your own models. that's the whole point... use something like sillytavern, or as you said openwebui. They are meant to utilize EVERYTHING. Not just local LLMs. LM Studio is FOR HOSTING YOUR OWN LLM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n73tvho",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To do this, would be dumb. LM Studio, ollama, and llama.cpp, are all for hosting your own models. that&amp;#39;s the whole point... use something like sillytavern, or as you said openwebui. They are meant to utilize EVERYTHING. Not just local LLMs. LM Studio is FOR HOSTING YOUR OWN LLM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miixs4/why_doesnt_lm_studio_let_me_call_external/n73tvho/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754423225,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miixs4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]