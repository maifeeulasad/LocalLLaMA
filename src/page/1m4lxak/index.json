[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.\n\nWhen I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.\n\nSince the process is rather token-intensive and slow (reproducing the entire text in chunked form), I'm considering a fine-tune of a smallish LLM. I'd be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Semantic chunking using LLMs",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1m4lxak",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_19icy5o1t9",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753008337,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use LLMs for semantic text chunking. Models in the range of 24 to 32B, quantized between Q4 and Q6, give me the most robust results. Mistral-Small-3.2, Gemma-27B and Qwen3-32B all work well, Mistral and Gemma seem to be a bit better with certain non-English languages.&lt;/p&gt;\n\n&lt;p&gt;When I go lower, results are still ok with Qwen3-14B, but below that reconstruction errors go up quickly.&lt;/p&gt;\n\n&lt;p&gt;Since the process is rather token-intensive and slow (reproducing the entire text in chunked form), I&amp;#39;m considering a fine-tune of a smallish LLM. I&amp;#39;d be happy to hear some tips from people who are doing similar stuff, like other models to consider or tweaks to make the output more robust.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m4lxak",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "mnze_brngo_7325",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/",
            "subreddit_subscribers": 501752,
            "created_utc": 1753008337,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n45ketu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "mnze_brngo_7325",
                      "can_mod_post": false,
                      "created_utc": 1753013849,
                      "send_replies": true,
                      "parent_id": "t1_n45i8pe",
                      "score": 1,
                      "author_fullname": "t2_19icy5o1t9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, that matches my experience. The regex idea is indeed interesting. I tried to output the lines at which to split verbatim and this didn't work well at all. I'd expect regex to suffer from a similar problem.\n\nI start by doing a static split based on markdown headers and I keep the input at 1 - 2k tokens. That tends to work well.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n45ketu",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, that matches my experience. The regex idea is indeed interesting. I tried to output the lines at which to split verbatim and this didn&amp;#39;t work well at all. I&amp;#39;d expect regex to suffer from a similar problem.&lt;/p&gt;\n\n&lt;p&gt;I start by doing a static split based on markdown headers and I keep the input at 1 - 2k tokens. That tends to work well.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m4lxak",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/n45ketu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753013849,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n45i8pe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lly0571",
            "can_mod_post": false,
            "created_utc": 1753012911,
            "send_replies": true,
            "parent_id": "t3_1m4lxak",
            "score": 3,
            "author_fullname": "t2_70vzcleel",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here are my previous chunking test results on some **Chinese** news texts:\n\n- Qwen3-14B-AWQ (✅)\n- Qwen2.5-14B-AWQ (☑️, Successful most of the time)\n- Qwen3-32B-AWQ (✅)\n- Qwen3-30B-A3B-q6 (☑️, Acceptable but noticeably worse than 14B)\n- Qwen3-8B-q4 (☑️, Fails occasionally)\n- Qwen3-4B (❌)\n- GLM4-0414-9B-q8 (☑️, Successful most of the time, better than Minicpm)\n- Minicpm4-8B-marlin-vllm (❌)\n- Gemma3-4b-it-qat-q4 (❌)\n- Gemma3-12b-it-qat-q4 (❌)\n- Gemma3-27b-it-qat-q4 (☑️)\n- mistral-small-3.2-22b-q4 (☑️, Worse than Gemma3-27b)\n- Llama4-Scout (API, ✅)\n(✅ = Reliable performance, ☑️ = Functional but not stable, ❌ = Unusable)\n\nI think Qwen3 performs relatively well among open-source models (though models like 30B-A3B and smaller are unreliable, and 14B being barely usable). GLM4 shows a similar pattern to Qwen3. Gemma3 underperforms Qwen in Chinese tasks, but may perform better in English?\n\nFor this text chunking task with SFT, I think generating a regex to represent chunking positions would be more effective. Directly outputting offset positions is unreliable, and full-text chunking is slow. But you need to organize your data first.\n\nAdditionally, current state-of-the-art text embeddings appear less sensitive to chunking requirements. For example, LLM-based embeddings like Qwen3-Embedding support 32K context length, making them usable for 2-4K text segments at least. Maybe chunking is not that necessary nowadays.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n45i8pe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here are my previous chunking test results on some &lt;strong&gt;Chinese&lt;/strong&gt; news texts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3-14B-AWQ (✅)&lt;/li&gt;\n&lt;li&gt;Qwen2.5-14B-AWQ (☑️, Successful most of the time)&lt;/li&gt;\n&lt;li&gt;Qwen3-32B-AWQ (✅)&lt;/li&gt;\n&lt;li&gt;Qwen3-30B-A3B-q6 (☑️, Acceptable but noticeably worse than 14B)&lt;/li&gt;\n&lt;li&gt;Qwen3-8B-q4 (☑️, Fails occasionally)&lt;/li&gt;\n&lt;li&gt;Qwen3-4B (❌)&lt;/li&gt;\n&lt;li&gt;GLM4-0414-9B-q8 (☑️, Successful most of the time, better than Minicpm)&lt;/li&gt;\n&lt;li&gt;Minicpm4-8B-marlin-vllm (❌)&lt;/li&gt;\n&lt;li&gt;Gemma3-4b-it-qat-q4 (❌)&lt;/li&gt;\n&lt;li&gt;Gemma3-12b-it-qat-q4 (❌)&lt;/li&gt;\n&lt;li&gt;Gemma3-27b-it-qat-q4 (☑️)&lt;/li&gt;\n&lt;li&gt;mistral-small-3.2-22b-q4 (☑️, Worse than Gemma3-27b)&lt;/li&gt;\n&lt;li&gt;Llama4-Scout (API, ✅)\n(✅ = Reliable performance, ☑️ = Functional but not stable, ❌ = Unusable)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I think Qwen3 performs relatively well among open-source models (though models like 30B-A3B and smaller are unreliable, and 14B being barely usable). GLM4 shows a similar pattern to Qwen3. Gemma3 underperforms Qwen in Chinese tasks, but may perform better in English?&lt;/p&gt;\n\n&lt;p&gt;For this text chunking task with SFT, I think generating a regex to represent chunking positions would be more effective. Directly outputting offset positions is unreliable, and full-text chunking is slow. But you need to organize your data first.&lt;/p&gt;\n\n&lt;p&gt;Additionally, current state-of-the-art text embeddings appear less sensitive to chunking requirements. For example, LLM-based embeddings like Qwen3-Embedding support 32K context length, making them usable for 2-4K text segments at least. Maybe chunking is not that necessary nowadays.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/n45i8pe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753012911,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m4lxak",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n45jhu4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "mnze_brngo_7325",
                      "can_mod_post": false,
                      "created_utc": 1753013459,
                      "send_replies": true,
                      "parent_id": "t1_n45g0ii",
                      "score": 1,
                      "author_fullname": "t2_19icy5o1t9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I tried this, without much luck. All models tend to hallucinate line numbers or the choice of split points is much worse, resulting in poor chunks. Also tried to have it output the content of the line at which to split. Similarly poor results, and obviously can lead to ambiguous results depending on the text.\n\nWhich model did you use for that and how big was the input text?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n45jhu4",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I tried this, without much luck. All models tend to hallucinate line numbers or the choice of split points is much worse, resulting in poor chunks. Also tried to have it output the content of the line at which to split. Similarly poor results, and obviously can lead to ambiguous results depending on the text.&lt;/p&gt;\n\n&lt;p&gt;Which model did you use for that and how big was the input text?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m4lxak",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/n45jhu4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753013459,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n45g0ii",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Chromix_",
            "can_mod_post": false,
            "created_utc": 1753011892,
            "send_replies": true,
            "parent_id": "t3_1m4lxak",
            "score": 1,
            "author_fullname": "t2_k7w2h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Prefix each paragraph in the input data with consecutive numbers. Let the LLM only output the numbers of the chunks that belong together. Results in super-fast generation and no hallucination errors in the text.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n45g0ii",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Prefix each paragraph in the input data with consecutive numbers. Let the LLM only output the numbers of the chunks that belong together. Results in super-fast generation and no hallucination errors in the text.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m4lxak/semantic_chunking_using_llms/n45g0ii/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753011892,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m4lxak",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]