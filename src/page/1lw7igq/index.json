[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. \n\nI want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. \n\nThe goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  \n\nWhat would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Survivalist Edge AI?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lw7igq",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_4efmo",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752136260,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. &lt;/p&gt;\n\n&lt;p&gt;I want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. &lt;/p&gt;\n\n&lt;p&gt;The goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp;amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  &lt;/p&gt;\n\n&lt;p&gt;What would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lw7igq",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "xibbie",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/",
            "subreddit_subscribers": 497021,
            "created_utc": 1752136260,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2bzink",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Chromix_",
            "can_mod_post": false,
            "created_utc": 1752138674,
            "send_replies": true,
            "parent_id": "t3_1lw7igq",
            "score": 8,
            "author_fullname": "t2_k7w2h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This has been discussed here in different ways before, it's an interesting exercise. A smartphone specialized for AI would be the most power-efficient and practical. A laptop will also do just fine, or even better - but less practical and more power-hungry. Here are the bits and pieces:\n\n* Which model for [running on a smartphone](https://www.reddit.com/r/LocalLLaMA/comments/1l3z2m3/best_world_knowledge_model_that_can_run_on_your/) \\+ RAG\n* [Which smart model](https://www.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/) to pick\n* Practical, user-contributed [benchmarks on smartphones](https://www.reddit.com/r/LocalLLaMA/comments/1glx6a5/phone_llms_benchmarks/)\n* Smartphone [SoC benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/) and discussion\n* Some \"older\" [SoC benchmarks by year](https://www.reddit.com/r/LocalLLaMA/comments/1gdwm4o/ai_scores_of_mobile_socs_by_brand_year_and_segment/)\n\nNo matter the solution you pick, it'll be outdated in 3 years.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2bzink",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This has been discussed here in different ways before, it&amp;#39;s an interesting exercise. A smartphone specialized for AI would be the most power-efficient and practical. A laptop will also do just fine, or even better - but less practical and more power-hungry. Here are the bits and pieces:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Which model for &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1l3z2m3/best_world_knowledge_model_that_can_run_on_your/\"&gt;running on a smartphone&lt;/a&gt; + RAG&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/\"&gt;Which smart model&lt;/a&gt; to pick&lt;/li&gt;\n&lt;li&gt;Practical, user-contributed &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1glx6a5/phone_llms_benchmarks/\"&gt;benchmarks on smartphones&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Smartphone &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/\"&gt;SoC benchmarks&lt;/a&gt; and discussion&lt;/li&gt;\n&lt;li&gt;Some &amp;quot;older&amp;quot; &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1gdwm4o/ai_scores_of_mobile_socs_by_brand_year_and_segment/\"&gt;SoC benchmarks by year&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No matter the solution you pick, it&amp;#39;ll be outdated in 3 years.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bzink/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752138674,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw7igq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2bwiom",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dramatic-Zebra-7213",
            "can_mod_post": false,
            "created_utc": 1752136934,
            "send_replies": true,
            "parent_id": "t3_1lw7igq",
            "score": 3,
            "author_fullname": "t2_4tpi1g8j8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The best device is probably just a regular laptop.\n\nAndroid devices have some limitations in comparison to a full-blown pc that makes the laptop a better choice.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2bwiom",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The best device is probably just a regular laptop.&lt;/p&gt;\n\n&lt;p&gt;Android devices have some limitations in comparison to a full-blown pc that makes the laptop a better choice.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bwiom/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752136934,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw7igq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2c1nr6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lothariusdark",
            "can_mod_post": false,
            "created_utc": 1752139909,
            "send_replies": true,
            "parent_id": "t3_1lw7igq",
            "score": 3,
            "author_fullname": "t2_idhb522c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A searchable copy of Wikipedia will be far more usable in a survival situation than any AI model.\n\nCombine that with an e ink device and you have a rather durable and extremely energy efficient solution.\n\nAs long as small models are as stupid and hallucinatory as they are currently, they are at best useless and at worst a detriment to your choices.\n\nOnce small models approach below 1% hallucination rates this might become an interesting topic, but until then this is a fruitless discussion.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2c1nr6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A searchable copy of Wikipedia will be far more usable in a survival situation than any AI model.&lt;/p&gt;\n\n&lt;p&gt;Combine that with an e ink device and you have a rather durable and extremely energy efficient solution.&lt;/p&gt;\n\n&lt;p&gt;As long as small models are as stupid and hallucinatory as they are currently, they are at best useless and at worst a detriment to your choices.&lt;/p&gt;\n\n&lt;p&gt;Once small models approach below 1% hallucination rates this might become an interesting topic, but until then this is a fruitless discussion.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2c1nr6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752139909,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw7igq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "fe89e94a-13f2-11f0-a9de-6262c74956cf",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2bydlf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Asleep-Ratio7535",
            "can_mod_post": false,
            "created_utc": 1752138007,
            "send_replies": true,
            "parent_id": "t3_1lw7igq",
            "score": 2,
            "author_fullname": "t2_1lfyddwf0c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ask your AI how to build a nuclear power plant, SMR is going to be your best choice.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2bydlf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 4"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ask your AI how to build a nuclear power plant, SMR is going to be your best choice.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bydlf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752138007,
            "author_flair_text": "Llama 4",
            "treatment_tags": [],
            "link_id": "t3_1lw7igq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#b0ae9b",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2bz7sg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1752138495,
            "send_replies": true,
            "parent_id": "t3_1lw7igq",
            "score": 3,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I keep printed books for this kind of contingency.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2bz7sg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I keep printed books for this kind of contingency.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bz7sg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752138495,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1lw7igq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]