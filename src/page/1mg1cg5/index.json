[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).\n\nAiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?\n\n\n16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).\n\n24 GB (go for size - Radeon 7900 - can get new for 800-1000)\n\n24 GB (3090 - refurbished around 1000, new closet to 1500)\n\nPrimary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?\n\nWhat would you do, and why? Or - where can I read up to better understand my options? Threads like this (https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/) make me think offloading to CPU/RAM doesn't need to absolutely throttle throughput when running locally...",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Guide for GPU Purchase for Local LLM?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mg1cg5",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2orcezep",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754168901,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).&lt;/p&gt;\n\n&lt;p&gt;Aiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?&lt;/p&gt;\n\n&lt;p&gt;16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).&lt;/p&gt;\n\n&lt;p&gt;24 GB (go for size - Radeon 7900 - can get new for 800-1000)&lt;/p&gt;\n\n&lt;p&gt;24 GB (3090 - refurbished around 1000, new closet to 1500)&lt;/p&gt;\n\n&lt;p&gt;Primary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?&lt;/p&gt;\n\n&lt;p&gt;What would you do, and why? Or - where can I read up to better understand my options? Threads like this (&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/&lt;/a&gt;) make me think offloading to CPU/RAM doesn&amp;#39;t need to absolutely throttle throughput when running locally...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mg1cg5",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "DanManPanther",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
            "subreddit_subscribers": 509053,
            "created_utc": 1754168901,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6lankq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754169885,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 8,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "No matter what you get you will want more VRAM inside a month, I promise. So get the most VRAM you can conceivably afford and make sure you’ve got room in your computer for the next time you’ve got GPU money.\n\nPinkie swear this is the truth.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lankq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No matter what you get you will want more VRAM inside a month, I promise. So get the most VRAM you can conceivably afford and make sure you’ve got room in your computer for the next time you’ve got GPU money.&lt;/p&gt;\n\n&lt;p&gt;Pinkie swear this is the truth.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lankq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754169885,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6lu4j1",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Tyme4Trouble",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6lgtat",
                                "score": 2,
                                "author_fullname": "t2_973amyap",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'm running vLLM with LMCache.\n\n    export LMCACHE_USE_EXPERIMENTAL=True\n    export LMCACHE_CHUNK_SIZE=256\n    export LMCACHE_LOCAL_CPU=True\n    export LMCACHE_MAX_LOCAL_CPU_SIZE=5.0\n    vllm serve ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W8A8 \\\n      --host 0.0.0.0 \\\n      --port 8000 \\\n      --tensor-parallel-size 2 \\\n      --gpu-memory-utilization 0.9 \\\n      --max-model-len 131072 \\\n      --max-num-seqs 8 \\\n      --trust-remote-code \\\n      --disable-log-requests \\\n      --enable-chunked-prefill \\\n      --max-num-batched-tokens 2048 \\\n      --cuda-graph-sizes 8 \\\n      --enable-prefix-caching \\\n      --enable-auto-tool-choice \\\n      --tool-call-parser hermes \\\n      --kv-transfer-config '{\"kv_connector\":\"LMCacheConnectorV1\", \"kv_role\":\"kv_both\"}' \\\n      --enable-expert-parallel\n\nNote that in my testing an NVLink bridge was necessary to get past about 110 tok/s. Something to do with the number of active parameters. NVLink only benefited Batch &gt; 1 on Qwen3-32B\n\nEdit: Fix the codeblock so it looks right.",
                                "edited": 1754176869,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6lu4j1",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running vLLM with LMCache.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;export LMCACHE_USE_EXPERIMENTAL=True\nexport LMCACHE_CHUNK_SIZE=256\nexport LMCACHE_LOCAL_CPU=True\nexport LMCACHE_MAX_LOCAL_CPU_SIZE=5.0\nvllm serve ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W8A8 \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --tensor-parallel-size 2 \\\n  --gpu-memory-utilization 0.9 \\\n  --max-model-len 131072 \\\n  --max-num-seqs 8 \\\n  --trust-remote-code \\\n  --disable-log-requests \\\n  --enable-chunked-prefill \\\n  --max-num-batched-tokens 2048 \\\n  --cuda-graph-sizes 8 \\\n  --enable-prefix-caching \\\n  --enable-auto-tool-choice \\\n  --tool-call-parser hermes \\\n  --kv-transfer-config &amp;#39;{&amp;quot;kv_connector&amp;quot;:&amp;quot;LMCacheConnectorV1&amp;quot;, &amp;quot;kv_role&amp;quot;:&amp;quot;kv_both&amp;quot;}&amp;#39; \\\n  --enable-expert-parallel\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note that in my testing an NVLink bridge was necessary to get past about 110 tok/s. Something to do with the number of active parameters. NVLink only benefited Batch &amp;gt; 1 on Qwen3-32B&lt;/p&gt;\n\n&lt;p&gt;Edit: Fix the codeblock so it looks right.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mg1cg5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lu4j1/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754176671,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754176671,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6lgtat",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Filo0104",
                      "can_mod_post": false,
                      "created_utc": 1754171972,
                      "send_replies": true,
                      "parent_id": "t1_n6lc2dv",
                      "score": 1,
                      "author_fullname": "t2_1qq10p5wjo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "How do you run qwen30b with 128k ctx? i have the same dual 3090s with vllm i Can only fit 107k ctx with ~60tk/s",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6lgtat",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do you run qwen30b with 128k ctx? i have the same dual 3090s with vllm i Can only fit 107k ctx with ~60tk/s&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg1cg5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lgtat/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754171972,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6lc2dv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Tyme4Trouble",
            "can_mod_post": false,
            "created_utc": 1754170365,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 2,
            "author_fullname": "t2_973amyap",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sorry to say, but for a 30-32B parameter model you’re going to need ~20GB of vRAM just to hold the weights at and 10-20 GB of addition vRAM for a large enough context window for it to be useful for coding. \n\nFor reference I’m able to run Qwen3-Coder-30B-A3B at Q8 at 150tok/s with a context window of 128k (FP16 KV) on dual 3090s. \n\nMight want to wait and see what the Radeon AI Pro R9700 ends up retailing for. It’ll have 32GB of vRAM and proper FP16 and FP8 support. \n\nEven then, Qwen3-Coder-30B is great for its size, but cannot compete with Claude.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lc2dv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry to say, but for a 30-32B parameter model you’re going to need ~20GB of vRAM just to hold the weights at and 10-20 GB of addition vRAM for a large enough context window for it to be useful for coding. &lt;/p&gt;\n\n&lt;p&gt;For reference I’m able to run Qwen3-Coder-30B-A3B at Q8 at 150tok/s with a context window of 128k (FP16 KV) on dual 3090s. &lt;/p&gt;\n\n&lt;p&gt;Might want to wait and see what the Radeon AI Pro R9700 ends up retailing for. It’ll have 32GB of vRAM and proper FP16 and FP8 support. &lt;/p&gt;\n\n&lt;p&gt;Even then, Qwen3-Coder-30B is great for its size, but cannot compete with Claude.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lc2dv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754170365,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6lgr17",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1754171951,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Isn't the 32GB version of MI50 the best bang for the buck for inference on Linux?\nDepending on how many you can fit, it's hard to beat if you can go with the recommended Ubuntu version.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lgr17",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t the 32GB version of MI50 the best bang for the buck for inference on Linux?\nDepending on how many you can fit, it&amp;#39;s hard to beat if you can go with the recommended Ubuntu version.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lgr17/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754171951,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6liyyt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1754172724,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Unless you are not allergic to cloud due to nonexistent privacy use openrouter. I cannot tolerate privacy loss so I run local.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6liyyt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unless you are not allergic to cloud due to nonexistent privacy use openrouter. I cannot tolerate privacy loss so I run local.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6liyyt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754172724,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6limvo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1754172608,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 0,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "3090 where I live is 650 used.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6limvo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;3090 where I live is 650 used.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6limvo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754172608,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]