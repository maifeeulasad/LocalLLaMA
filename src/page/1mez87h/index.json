[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "i've been trying to get qwen 2.5 14b gguf cause i hear vllm can use 2 gpu's (i have a 2060 6gb vram and 4060 16 gb vram) and i can't use the other model types cause of memory, i have windows 10, and using wsl doesn't make sense to use , cause it would make thing slower , so i've been trying to get vllm-windows to work, but i keep getting this error\n\n    Traceback (most recent call last):\n    File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n    File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Scripts\\vllm.exe\\__main__.py\", line 6, in &lt;module&gt;\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\main.py\", line 54, in main\n    args.dispatch_function(args)\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\serve.py\", line 61, in cmd\n    uvloop_impl.run(run_server(args))\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py\", line 118, in run\n    return __asyncio.run(\n    ^^^^^^^^^^^^^^\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py\", line 194, in run\n    return runner.run(main)\n    ^^^^^^^^^^^^^^^^\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"winloop/loop.pyx\", line 1539, in winloop.loop.Loop.run_until_complete\n    return future.result()\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py\", line 70, in wrapper\n    return await main\n    ^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 1801, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 1821, in run_server_worker\n    async with build_async_engine_client(args, client_config) as engine_client:\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n    ^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 167, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n    File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n    ^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py\", line 203, in build_async_engine_client_from_engine_args\n    async_llm = AsyncLLM.from_vllm_config(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py\", line 163, in from_vllm_config\n    return cls(\n    ^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py\", line 100, in __init__\n    self.tokenizer = init_tokenizer_from_configs(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py\", line 111, in init_tokenizer_from_configs\n    return TokenizerGroup(\n    ^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py\", line 24, in __init__\n    self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer.py\", line 263, in get_tokenizer\n    encoder_config = get_sentence_transformer_tokenizer_config(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\config.py\", line 623, in get_sentence_transformer_tokenizer_config\n    if not encoder_dict and not model.startswith(\"/\"):\n    ^^^^^^^^^^^^^^^^\n    AttributeError: 'WindowsPath' object has no attribute 'startswith'",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "anyone managed to run vllm windows with gguf?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mez87h",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_jr02j",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754061447,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been trying to get qwen 2.5 14b gguf cause i hear vllm can use 2 gpu&amp;#39;s (i have a 2060 6gb vram and 4060 16 gb vram) and i can&amp;#39;t use the other model types cause of memory, i have windows 10, and using wsl doesn&amp;#39;t make sense to use , cause it would make thing slower , so i&amp;#39;ve been trying to get vllm-windows to work, but i keep getting this error&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):\nFile &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 198, in _run_module_as_main\nFile &amp;quot;&amp;lt;frozen runpy&amp;gt;&amp;quot;, line 88, in _run_code\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Scripts\\vllm.exe\\__main__.py&amp;quot;, line 6, in &amp;lt;module&amp;gt;\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\main.py&amp;quot;, line 54, in main\nargs.dispatch_function(args)\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\cli\\serve.py&amp;quot;, line 61, in cmd\nuvloop_impl.run(run_server(args))\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py&amp;quot;, line 118, in run\nreturn __asyncio.run(\n^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py&amp;quot;, line 194, in run\nreturn runner.run(main)\n^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\runners.py&amp;quot;, line 118, in run\nreturn self._loop.run_until_complete(task)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;winloop/loop.pyx&amp;quot;, line 1539, in winloop.loop.Loop.run_until_complete\nreturn future.result()\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\winloop\\__init__.py&amp;quot;, line 70, in wrapper\nreturn await main\n^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 1801, in run_server\nawait run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 1821, in run_server_worker\nasync with build_async_engine_client(args, client_config) as engine_client:\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py&amp;quot;, line 210, in __aenter__\nreturn await anext(self.gen)\n^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 167, in build_async_engine_client\nasync with build_async_engine_client_from_engine_args(\nFile &amp;quot;C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py&amp;quot;, line 210, in __aenter__\nreturn await anext(self.gen)\n^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\entrypoints\\openai\\api_server.py&amp;quot;, line 203, in build_async_engine_client_from_engine_args\nasync_llm = AsyncLLM.from_vllm_config(\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py&amp;quot;, line 163, in from_vllm_config\nreturn cls(\n^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\v1\\engine\\async_llm.py&amp;quot;, line 100, in __init__\nself.tokenizer = init_tokenizer_from_configs(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py&amp;quot;, line 111, in init_tokenizer_from_configs\nreturn TokenizerGroup(\n^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer_group.py&amp;quot;, line 24, in __init__\nself.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\tokenizer.py&amp;quot;, line 263, in get_tokenizer\nencoder_config = get_sentence_transformer_tokenizer_config(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile &amp;quot;C:\\Dev\\tools\\vllm\\vllm-env\\Lib\\site-packages\\vllm\\transformers_utils\\config.py&amp;quot;, line 623, in get_sentence_transformer_tokenizer_config\nif not encoder_dict and not model.startswith(&amp;quot;/&amp;quot;):\n^^^^^^^^^^^^^^^^\nAttributeError: &amp;#39;WindowsPath&amp;#39; object has no attribute &amp;#39;startswith&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mez87h",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "emaayan",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/",
            "subreddit_subscribers": 508771,
            "created_utc": 1754061447,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6d9x3t",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "__JockY__",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6d9iw8",
                                "score": 1,
                                "author_fullname": "t2_qf8h7ka8",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Gotcha. Just stay away from GGUF with vLLM and you’ll be fine.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6d9x3t",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gotcha. Just stay away from GGUF with vLLM and you’ll be fine.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mez87h",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/n6d9x3t/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754063093,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754063093,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6d9iw8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "emaayan",
                      "can_mod_post": false,
                      "created_utc": 1754062981,
                      "send_replies": true,
                      "parent_id": "t1_n6d8mw8",
                      "score": 1,
                      "author_fullname": "t2_jr02j",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "thanks i think i tried ik\\_llama but i don't seem to find a windows variant, my main target is  to try and have qwen3 with tooling to work on it. and i've been trying to find out the most performant runtime there is.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6d9iw8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;thanks i think i tried ik_llama but i don&amp;#39;t seem to find a windows variant, my main target is  to try and have qwen3 with tooling to work on it. and i&amp;#39;ve been trying to find out the most performant runtime there is.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mez87h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/n6d9iw8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754062981,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6d8mw8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754062728,
            "send_replies": true,
            "parent_id": "t3_1mez87h",
            "score": 1,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GGUF is poorly supported in vLLM for Linux, let alone Windows.\n\nUse llama.cpp or ik_llama for GGUF quants. It’ll just work. If you’re set on using vLLM then use GPTQ or AWQ quants. They’ll work great.\n\nJust don’t use GGUF with vLLM. That way is just pain, crashes, and pointless frustration.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6d8mw8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GGUF is poorly supported in vLLM for Linux, let alone Windows.&lt;/p&gt;\n\n&lt;p&gt;Use llama.cpp or ik_llama for GGUF quants. It’ll just work. If you’re set on using vLLM then use GPTQ or AWQ quants. They’ll work great.&lt;/p&gt;\n\n&lt;p&gt;Just don’t use GGUF with vLLM. That way is just pain, crashes, and pointless frustration.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/n6d8mw8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754062728,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mez87h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6db5yp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "13henday",
            "can_mod_post": false,
            "created_utc": 1754063449,
            "send_replies": true,
            "parent_id": "t3_1mez87h",
            "score": 1,
            "author_fullname": "t2_81577uk2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Vllm doesn’t support windows. Use docker or just move this code over to wsl.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6db5yp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Vllm doesn’t support windows. Use docker or just move this code over to wsl.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/n6db5yp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754063449,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mez87h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6eernt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Zangwuz",
                      "can_mod_post": false,
                      "created_utc": 1754074826,
                      "send_replies": true,
                      "parent_id": "t1_n6e84gs",
                      "score": 1,
                      "author_fullname": "t2_2d3q22zf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not officially but you for sure can use it on windows without wsl if you are adventurous and it's probably what he is talking about and since he talked about wsl he probably knows about it too.  \n[https://github.com/SystemPanic/vllm-windows](https://github.com/SystemPanic/vllm-windows)  \nI've tried just one week ago for the curiosity and it worked but i would personally not bother with VLLM just for two gpus with different vram quantity.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6eernt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not officially but you for sure can use it on windows without wsl if you are adventurous and it&amp;#39;s probably what he is talking about and since he talked about wsl he probably knows about it too.&lt;br/&gt;\n&lt;a href=\"https://github.com/SystemPanic/vllm-windows\"&gt;https://github.com/SystemPanic/vllm-windows&lt;/a&gt;&lt;br/&gt;\nI&amp;#39;ve tried just one week ago for the curiosity and it worked but i would personally not bother with VLLM just for two gpus with different vram quantity.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mez87h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/n6eernt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754074826,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6e84gs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pro-editor-1105",
            "can_mod_post": false,
            "created_utc": 1754072875,
            "send_replies": true,
            "parent_id": "t3_1mez87h",
            "score": 1,
            "author_fullname": "t2_uptissiz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "vLLM is basically an error whack-a-mole lol\n\n  \nAlso you need to install WSL for VLLM to work, it does not work on windows at all.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6e84gs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vLLM is basically an error whack-a-mole lol&lt;/p&gt;\n\n&lt;p&gt;Also you need to install WSL for VLLM to work, it does not work on windows at all.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/n6e84gs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754072875,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mez87h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6fs03u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1754090246,
            "send_replies": true,
            "parent_id": "t3_1mez87h",
            "score": 1,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is... Not how I'd run this.\n\nObligatory: Why are you using Windows?\n\nWith that out of the way, though...\n\nWhy are you using vLLM? vLLM is less of a \"I'm going to spin up a model\", and more of a \"I need to serve 50+ people per GPU\" kind of framework. It's kind of overkill for personal use.\n\nSecondly: Why are you using GGUF with vLLM? GGUF comes out of the LlamaCPP ecosystem, and is more optimized for being fairly easy to produce and run on a variety of hardware compared to other quantizations, rather than for performance, necessarily.\n\nFor vLLM, I'd suggest using AWQ or GPTQ.\n\nNext: Why are you trying to use two different GPUs with vLLM? vLLM supports tensor parallelism (and I guess maybe pipeline...?) but my understanding is that it's a lot better when both GPUs are the same (and in particular have the same amount of memory).\n\nMy personal recommendation:\n\nSwap to LlamaCPP. It's ubiquitous, natively supports GGUF, and can use GPUs of different VRAM capacities fairly well if needed (though I would recommend picking up a quant appropriately sized to your primary GPU if possible).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6fs03u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is... Not how I&amp;#39;d run this.&lt;/p&gt;\n\n&lt;p&gt;Obligatory: Why are you using Windows?&lt;/p&gt;\n\n&lt;p&gt;With that out of the way, though...&lt;/p&gt;\n\n&lt;p&gt;Why are you using vLLM? vLLM is less of a &amp;quot;I&amp;#39;m going to spin up a model&amp;quot;, and more of a &amp;quot;I need to serve 50+ people per GPU&amp;quot; kind of framework. It&amp;#39;s kind of overkill for personal use.&lt;/p&gt;\n\n&lt;p&gt;Secondly: Why are you using GGUF with vLLM? GGUF comes out of the LlamaCPP ecosystem, and is more optimized for being fairly easy to produce and run on a variety of hardware compared to other quantizations, rather than for performance, necessarily.&lt;/p&gt;\n\n&lt;p&gt;For vLLM, I&amp;#39;d suggest using AWQ or GPTQ.&lt;/p&gt;\n\n&lt;p&gt;Next: Why are you trying to use two different GPUs with vLLM? vLLM supports tensor parallelism (and I guess maybe pipeline...?) but my understanding is that it&amp;#39;s a lot better when both GPUs are the same (and in particular have the same amount of memory).&lt;/p&gt;\n\n&lt;p&gt;My personal recommendation:&lt;/p&gt;\n\n&lt;p&gt;Swap to LlamaCPP. It&amp;#39;s ubiquitous, natively supports GGUF, and can use GPUs of different VRAM capacities fairly well if needed (though I would recommend picking up a quant appropriately sized to your primary GPU if possible).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mez87h/anyone_managed_to_run_vllm_windows_with_gguf/n6fs03u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754090246,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mez87h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]