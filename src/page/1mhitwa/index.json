[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I am just getting started with serious research, I wanted to work on MOE models. Here are my assumptions and thinking of buying hardware based on that. \n\nCurrent hardware: i7(13th gen 8 cores) + 64 RAM + RTX 4060. Current GPU hardware is pretty limited 8GB VRAM - not suited for any serious work. Also I do not reside in US, and most of the high end GPUs are 1.5x-2x price if I could find one in first place. Luckily most of my friend circle travel from US to my country, so I can get it from there - used 3090 with 24 GB is a good option but I will fall into serious risk if it stops working after a while, so I want to invest on 5090 at 2.4k possible upgrade if my work goes well.\n\nAssumptions: With MOE architecture system RAM + VRAM can work hand in hand enabling users work on best models locally.   \nVRAM contains active experts + gating network.   \nSystem RAM contains whole MOE model. Based on input tokens -  active parameters are selected. - if everything is in VRAM inference is no brainer.   \n  \nBut my question is how realistic is to expect Higher possibly 128 GB ram + 5090 can I expect to run models like GLM-Air 106B - 12B active parameters. \n\nAlso I was open to M3-Ultra but based on my research - due to lack of Cuda like architecture even 512 GB is not suitable for fine tuning - can someone correct me on this. \n\nPS: I'm actually planning to work full-time on this, so any help is appreciated.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Suggestion for upgrading hardware for MOE inference and fine-tuning.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhitwa",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_u5scsvlj",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754326350,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am just getting started with serious research, I wanted to work on MOE models. Here are my assumptions and thinking of buying hardware based on that. &lt;/p&gt;\n\n&lt;p&gt;Current hardware: i7(13th gen 8 cores) + 64 RAM + RTX 4060. Current GPU hardware is pretty limited 8GB VRAM - not suited for any serious work. Also I do not reside in US, and most of the high end GPUs are 1.5x-2x price if I could find one in first place. Luckily most of my friend circle travel from US to my country, so I can get it from there - used 3090 with 24 GB is a good option but I will fall into serious risk if it stops working after a while, so I want to invest on 5090 at 2.4k possible upgrade if my work goes well.&lt;/p&gt;\n\n&lt;p&gt;Assumptions: With MOE architecture system RAM + VRAM can work hand in hand enabling users work on best models locally.&lt;br/&gt;\nVRAM contains active experts + gating network.&lt;br/&gt;\nSystem RAM contains whole MOE model. Based on input tokens -  active parameters are selected. - if everything is in VRAM inference is no brainer.   &lt;/p&gt;\n\n&lt;p&gt;But my question is how realistic is to expect Higher possibly 128 GB ram + 5090 can I expect to run models like GLM-Air 106B - 12B active parameters. &lt;/p&gt;\n\n&lt;p&gt;Also I was open to M3-Ultra but based on my research - due to lack of Cuda like architecture even 512 GB is not suitable for fine tuning - can someone correct me on this. &lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;m actually planning to work full-time on this, so any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhitwa",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Icy_Gas8807",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754326350,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z2hcx",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Icy_Gas8807",
                      "can_mod_post": false,
                      "created_utc": 1754355776,
                      "send_replies": true,
                      "parent_id": "t1_n6xif4a",
                      "score": 1,
                      "author_fullname": "t2_u5scsvlj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for sharing!!!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z2hcx",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for sharing!!!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhitwa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6z2hcx/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754355776,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xif4a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "perelmanych",
            "can_mod_post": false,
            "created_utc": 1754337789,
            "send_replies": true,
            "parent_id": "t3_1mhitwa",
            "score": 3,
            "author_fullname": "t2_63q8kong",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You will not be able to fine tune GLM-4.5 air unless you go with M3 Ultra 512Gb variant and will be fine with Lora not full finetuning. Full fine tuning of GLM-4.5 air requires almost 2Tb of VRAM. Here is calculator that you can use [https://apxml.com/tools/vram-calculator](https://apxml.com/tools/vram-calculator)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xif4a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You will not be able to fine tune GLM-4.5 air unless you go with M3 Ultra 512Gb variant and will be fine with Lora not full finetuning. Full fine tuning of GLM-4.5 air requires almost 2Tb of VRAM. Here is calculator that you can use &lt;a href=\"https://apxml.com/tools/vram-calculator\"&gt;https://apxml.com/tools/vram-calculator&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6xif4a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754337789,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhitwa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6wmt5u",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LagOps91",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6wir54",
                                "score": 2,
                                "author_fullname": "t2_3wi6j7vwh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "i meant that you can run it with your 4060 and 64gb ram. if you want to upgrade? sure, feel free to do it! but if you want to run GLM-4.5 Air then i don't think an upgrade is needed.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6wmt5u",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i meant that you can run it with your 4060 and 64gb ram. if you want to upgrade? sure, feel free to do it! but if you want to run GLM-4.5 Air then i don&amp;#39;t think an upgrade is needed.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhitwa",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6wmt5u/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754328727,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754328727,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6wir54",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Icy_Gas8807",
                      "can_mod_post": false,
                      "created_utc": 1754327626,
                      "send_replies": true,
                      "parent_id": "t1_n6wgck4",
                      "score": 1,
                      "author_fullname": "t2_u5scsvlj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Can I go for 5090? As it is highly difficult to find a 4090 at good price.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6wir54",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can I go for 5090? As it is highly difficult to find a 4090 at good price.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhitwa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6wir54/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754327626,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6wgck4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1754326970,
            "send_replies": true,
            "parent_id": "t3_1mhitwa",
            "score": 2,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You should be able to run GLM-4.5 air with your current setup. just use the vram for context and non-expert tensors and load the rest to ram. you should be fine with this!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6wgck4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You should be able to run GLM-4.5 air with your current setup. just use the vram for context and non-expert tensors and load the rest to ram. you should be fine with this!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6wgck4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754326970,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhitwa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6xbdm2",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Hamza9575",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6xa4kj",
                                "score": 1,
                                "author_fullname": "t2_2z5hm75",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Why mac studio ? if running ai without others interference is the point for locally running ai then why would you run it on apple device which is completely controlled by apple. Normal desktops can run linux an open source operating system which means you eliminate the issue of microsoft or apple controlling the device ie the exact same principle behind running the ai locally that is ai companies are not controlling your local ai model.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6xbdm2",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why mac studio ? if running ai without others interference is the point for locally running ai then why would you run it on apple device which is completely controlled by apple. Normal desktops can run linux an open source operating system which means you eliminate the issue of microsoft or apple controlling the device ie the exact same principle behind running the ai locally that is ai companies are not controlling your local ai model.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhitwa",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6xbdm2/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754335700,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754335700,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6xa4kj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Icy_Gas8807",
                      "can_mod_post": false,
                      "created_utc": 1754335340,
                      "send_replies": true,
                      "parent_id": "t1_n6x8vse",
                      "score": 1,
                      "author_fullname": "t2_u5scsvlj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Interesting, but I’ve seen people claiming sweet spot at 4 bit quantisation. Also, inference is not only my priority, if that’s the case you will be better of trying Mac Studio. I want to fine tune a model, I know cloud would be a better way, and also the number of parameters will be coming down going forward. Still very confused whether to go for 5090 or not!!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6xa4kj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting, but I’ve seen people claiming sweet spot at 4 bit quantisation. Also, inference is not only my priority, if that’s the case you will be better of trying Mac Studio. I want to fine tune a model, I know cloud would be a better way, and also the number of parameters will be coming down going forward. Still very confused whether to go for 5090 or not!!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhitwa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6xa4kj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754335340,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6x8vse",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Hamza9575",
            "can_mod_post": false,
            "created_utc": 1754334982,
            "send_replies": true,
            "parent_id": "t3_1mhitwa",
            "score": 1,
            "author_fullname": "t2_2z5hm75",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Personally i would go just go for something like a 16gb nvidia 5060ti. And combine with like 192gb or 256gb desktop cpu and motherboard combo, ie 4 slot motherboard and 9800x3d cpu. This should allow you to run massive models directly on the cpu via lllama.cpp, probably much better than paying the insane markup of nvidia high vram cards. 256gb ram for example allows you to run the 2 bit quantized version of kimi k2 model, currentlt the most advanced ai model. While it is impossible to get 256gb vram on nvidia cards without paying millions of dollars.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6x8vse",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Personally i would go just go for something like a 16gb nvidia 5060ti. And combine with like 192gb or 256gb desktop cpu and motherboard combo, ie 4 slot motherboard and 9800x3d cpu. This should allow you to run massive models directly on the cpu via lllama.cpp, probably much better than paying the insane markup of nvidia high vram cards. 256gb ram for example allows you to run the 2 bit quantized version of kimi k2 model, currentlt the most advanced ai model. While it is impossible to get 256gb vram on nvidia cards without paying millions of dollars.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhitwa/suggestion_for_upgrading_hardware_for_moe/n6x8vse/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754334982,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhitwa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]