[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Ambiguous or restrictive license\n\nEven if they say \"OSS\", the license usually has limitations (non-commercial use, prohibition in certain areas, etc.), which takes it out of the spirit of free software.\n\nIncomplete or closed code\n\nMany times they do not release training, datasets or original weights. What they publish is a model already quantized or cut, without the pipeline that allows it to be reproduced.\n\nCut version\n\nThe published version is usually inferior to the one they use internally, with less capacity or precision.\n\nExample: a reduced or quantized 12B that loses performance compared to other real open models.\n\nCorporate posture\n\nThey sell it as \"great contribution to the community\", but rather it is a gesture not to be left behind in front of Meta, Mistral or Hugging Face, who have made complete releases.\n\nDisconnection with the ecosystem\n\nThe community of r/LocalLLaMA and other forums already has more powerful and really open models, so the OpenAI thing sounds almost like a joke.\n\nIn short, people see it as \"open-washing\": using the term open source for something that, in practice, does not meet the values or real advantages of open source.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "In the case you are looking at - that OSS model of OpenAI - there are several points that explain why the community takes it as a poor contribution to open source:",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mkoc97",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_p7nqw2dg",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754636385,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ambiguous or restrictive license&lt;/p&gt;\n\n&lt;p&gt;Even if they say &amp;quot;OSS&amp;quot;, the license usually has limitations (non-commercial use, prohibition in certain areas, etc.), which takes it out of the spirit of free software.&lt;/p&gt;\n\n&lt;p&gt;Incomplete or closed code&lt;/p&gt;\n\n&lt;p&gt;Many times they do not release training, datasets or original weights. What they publish is a model already quantized or cut, without the pipeline that allows it to be reproduced.&lt;/p&gt;\n\n&lt;p&gt;Cut version&lt;/p&gt;\n\n&lt;p&gt;The published version is usually inferior to the one they use internally, with less capacity or precision.&lt;/p&gt;\n\n&lt;p&gt;Example: a reduced or quantized 12B that loses performance compared to other real open models.&lt;/p&gt;\n\n&lt;p&gt;Corporate posture&lt;/p&gt;\n\n&lt;p&gt;They sell it as &amp;quot;great contribution to the community&amp;quot;, but rather it is a gesture not to be left behind in front of Meta, Mistral or Hugging Face, who have made complete releases.&lt;/p&gt;\n\n&lt;p&gt;Disconnection with the ecosystem&lt;/p&gt;\n\n&lt;p&gt;The community of &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; and other forums already has more powerful and really open models, so the OpenAI thing sounds almost like a joke.&lt;/p&gt;\n\n&lt;p&gt;In short, people see it as &amp;quot;open-washing&amp;quot;: using the term open source for something that, in practice, does not meet the values or real advantages of open source.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mkoc97",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Ok_Exchange_8504",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/",
            "subreddit_subscribers": 513814,
            "created_utc": 1754636385,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7k91vn",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Ok_Exchange_8504",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7k85qd",
                                                    "score": 0,
                                                    "author_fullname": "t2_p7nqw2dg",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Fair enough — my critique isn’t exclusive to OpenAI, it’s about the recurring pattern in “OSS” LLM releases.  \nEven with original weights, without a reproducible training pipeline and with aggressive content filters, it’s hard to call it a *full* open-source contribution.  \nMXFP4 is indeed interesting — I just think the gap between the PR headline and what’s actually usable is still too wide.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7k91vn",
                                                    "is_submitter": true,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fair enough — my critique isn’t exclusive to OpenAI, it’s about the recurring pattern in “OSS” LLM releases.&lt;br/&gt;\nEven with original weights, without a reproducible training pipeline and with aggressive content filters, it’s hard to call it a &lt;em&gt;full&lt;/em&gt; open-source contribution.&lt;br/&gt;\nMXFP4 is indeed interesting — I just think the gap between the PR headline and what’s actually usable is still too wide.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mkoc97",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/n7k91vn/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754638338,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754638338,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 0
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7k85qd",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Mushoz",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7k7ud0",
                                          "score": 2,
                                          "author_fullname": "t2_gwpq7",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Qwen, Kimi and Deepseek also haven't published a reproducible training pipeline either. So why hold that against OpenAI, but not others? Furthermore, they DID publish the original weights. The model was trained in MXFP4.\n\nThe only thing that I agree with is the unfortunate inclusion of heavy content filters, but that's a point you are raising now and wasn't even in your original post.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7k85qd",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen, Kimi and Deepseek also haven&amp;#39;t published a reproducible training pipeline either. So why hold that against OpenAI, but not others? Furthermore, they DID publish the original weights. The model was trained in MXFP4.&lt;/p&gt;\n\n&lt;p&gt;The only thing that I agree with is the unfortunate inclusion of heavy content filters, but that&amp;#39;s a point you are raising now and wasn&amp;#39;t even in your original post.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mkoc97",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/n7k85qd/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754637828,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754637828,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 1,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7kr7yk",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Ok_Exchange_8504",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7k7ud0",
                                          "score": 1,
                                          "author_fullname": "t2_p7nqw2dg",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "From a research perspective, this is interesting. From an open-source *ecosystem*perspective, it’s still a half-step.  \nIt’s like building a beautiful house but leaving out the roof and the walls — technically a house, but not a practical one. \n**Respuesta:**\n\"Hay mucho que analizar en este caso, y puedo ver por qué algunos miembros de la comunidad están molestos. La licencia Apache 2.0 es una buena elección, y probar la viabilidad de entrenamiento a 4 bits es valioso para el campo de IA. Sin embargo, no estoy de acuerdo en que la publicación sea un ejemplo de cómo contribuir a un ecosistema de código abierto en su totalidad.\nLa falta de un pipeline reproducible de entrenamiento, el no proporcionar los pesos originales y la presencia de content filters pesados hacen que la contribución no sea lo suficientemente transparente y accesible.\nEs como construir una hermosa casa pero dejando sin paredes y techos. Puede ser una casa en teoría, pero no es práctica. Estoy de acuerdo en que esta contribución tiene potencial, pero creo que podría haberse hecho de una manera más abierta y compartida, lo cual habría generado más confianza y respeto en la comunidad.\" [end of text]",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7kr7yk",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;From a research perspective, this is interesting. From an open-source &lt;em&gt;ecosystem&lt;/em&gt;perspective, it’s still a half-step.&lt;br/&gt;\nIt’s like building a beautiful house but leaving out the roof and the walls — technically a house, but not a practical one. \n&lt;strong&gt;Respuesta:&lt;/strong&gt;\n&amp;quot;Hay mucho que analizar en este caso, y puedo ver por qué algunos miembros de la comunidad están molestos. La licencia Apache 2.0 es una buena elección, y probar la viabilidad de entrenamiento a 4 bits es valioso para el campo de IA. Sin embargo, no estoy de acuerdo en que la publicación sea un ejemplo de cómo contribuir a un ecosistema de código abierto en su totalidad.\nLa falta de un pipeline reproducible de entrenamiento, el no proporcionar los pesos originales y la presencia de content filters pesados hacen que la contribución no sea lo suficientemente transparente y accesible.\nEs como construir una hermosa casa pero dejando sin paredes y techos. Puede ser una casa en teoría, pero no es práctica. Estoy de acuerdo en que esta contribución tiene potencial, pero creo que podría haberse hecho de una manera más abierta y compartida, lo cual habría generado más confianza y respeto en la comunidad.&amp;quot; [end of text]&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mkoc97",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/n7kr7yk/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754648604,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754648604,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7k7ud0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Ok_Exchange_8504",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7k7hxv",
                                "score": -1,
                                "author_fullname": "t2_p7nqw2dg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Fair point on the Apache 2.0 license and MXFP4 — I get that proving 4-bit training viability is valuable for the LLM community.  \nMy point wasn’t that quantization is bad, but that the release feels incomplete compared to what “OSS” usually implies: no reproducible training pipeline, no original weights, and heavy content filters.  \nFrom a research perspective, this is interesting. From an open-source *ecosystem*perspective, it’s still a half-step.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7k7ud0",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fair point on the Apache 2.0 license and MXFP4 — I get that proving 4-bit training viability is valuable for the LLM community.&lt;br/&gt;\nMy point wasn’t that quantization is bad, but that the release feels incomplete compared to what “OSS” usually implies: no reproducible training pipeline, no original weights, and heavy content filters.&lt;br/&gt;\nFrom a research perspective, this is interesting. From an open-source &lt;em&gt;ecosystem&lt;/em&gt;perspective, it’s still a half-step.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mkoc97",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/n7k7ud0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754637650,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754637650,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": -1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7k7hxv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Mushoz",
                      "can_mod_post": false,
                      "created_utc": 1754637461,
                      "send_replies": true,
                      "parent_id": "t1_n7k7d2y",
                      "score": 1,
                      "author_fullname": "t2_gwpq7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Don't get me wrong, the models definitely have issues: It is censored beyond belief, which will definitely have made the models dumber and less useful. But the critique you have provided completely misses the mark.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7k7hxv",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t get me wrong, the models definitely have issues: It is censored beyond belief, which will definitely have made the models dumber and less useful. But the critique you have provided completely misses the mark.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkoc97",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/n7k7hxv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754637461,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k7d2y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mushoz",
            "can_mod_post": false,
            "created_utc": 1754637387,
            "send_replies": true,
            "parent_id": "t3_1mkoc97",
            "score": 7,
            "author_fullname": "t2_gwpq7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It has the most permissive license possible: Apache 2.0. Training code of datasets are almost never shared, that holds true for all other AI companies as well. Furthermore, your quantization critique is actually one of the most positive contributions possible:\n\n`Native MXFP4 quantization: The models are trained with native MXFP4 precision`\n\nSource: [https://github.com/openai/gpt-oss](https://github.com/openai/gpt-oss)\n\nEg, it isn't nerfed. It's actually trained in MXFP4 precision. Showing that this is possible and viable can lead to a 4x reduction in compute and memory for training LLMs, which is a huge advantage for anyone training LLMs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k7d2y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It has the most permissive license possible: Apache 2.0. Training code of datasets are almost never shared, that holds true for all other AI companies as well. Furthermore, your quantization critique is actually one of the most positive contributions possible:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Native MXFP4 quantization: The models are trained with native MXFP4 precision&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Source: &lt;a href=\"https://github.com/openai/gpt-oss\"&gt;https://github.com/openai/gpt-oss&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Eg, it isn&amp;#39;t nerfed. It&amp;#39;s actually trained in MXFP4 precision. Showing that this is possible and viable can lead to a 4x reduction in compute and memory for training LLMs, which is a huge advantage for anyone training LLMs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/n7k7d2y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754637387,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkoc97",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7k6xu6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "custodiam99",
            "can_mod_post": false,
            "created_utc": 1754637157,
            "send_replies": true,
            "parent_id": "t3_1mkoc97",
            "score": 0,
            "author_fullname": "t2_nqnhgqqf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Well I think it is good to have them just to compare.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k6xu6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well I think it is good to have them just to compare.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkoc97/in_the_case_you_are_looking_at_that_oss_model_of/n7k6xu6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754637157,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkoc97",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]