[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey folks, I’m getting serious AI fever.\n\nI know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.\n\nIs it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?\n\nI know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.\n\nIs it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?\n\nShould I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "AI fever D:",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1lyu7bf",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.29,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_qq6spcu23",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752417048,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I’m getting serious AI fever.&lt;/p&gt;\n\n&lt;p&gt;I know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.&lt;/p&gt;\n\n&lt;p&gt;Is it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?&lt;/p&gt;\n\n&lt;p&gt;I know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.&lt;/p&gt;\n\n&lt;p&gt;Is it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?&lt;/p&gt;\n\n&lt;p&gt;Should I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lyu7bf",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Czydera",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
            "subreddit_subscribers": 498343,
            "created_utc": 1752417048,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2wo3mw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Czydera",
                      "can_mod_post": false,
                      "created_utc": 1752418585,
                      "send_replies": true,
                      "parent_id": "t1_n2wli3f",
                      "score": 1,
                      "author_fullname": "t2_qq6spcu23",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yup, I heard that. Unified memory isn't everything :/",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2wo3mw",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup, I heard that. Unified memory isn&amp;#39;t everything :/&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lyu7bf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wo3mw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752418585,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2wli3f",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "And-Bee",
            "can_mod_post": false,
            "created_utc": 1752417793,
            "send_replies": true,
            "parent_id": "t3_1lyu7bf",
            "score": 4,
            "author_fullname": "t2_a81fjhk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Terrible prompt processing speed on Mac. If your prompt is large then you will be waiting a few minutes for the full response. Factor that into your decision.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wli3f",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Terrible prompt processing speed on Mac. If your prompt is large then you will be waiting a few minutes for the full response. Factor that into your decision.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wli3f/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752417793,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lyu7bf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2wrm78",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SuddenOutlandishness",
            "can_mod_post": false,
            "created_utc": 1752419644,
            "send_replies": true,
            "parent_id": "t3_1lyu7bf",
            "score": 2,
            "author_fullname": "t2_wko7p7u",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I got the M4 Max MBP w/ 128GB. You can run quantized 70b models and also have a few smaller models loaded in memory as well for fast switching (think agent workflows).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wrm78",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I got the M4 Max MBP w/ 128GB. You can run quantized 70b models and also have a few smaller models loaded in memory as well for fast switching (think agent workflows).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wrm78/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752419644,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lyu7bf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2wpt6h",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AI_Tonic",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2wn94i",
                                "score": 1,
                                "author_fullname": "t2_100b8v9zrg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "i share my model weights on remote providers like huggingface , currently serving \"localmodels\" on 9 h200s there , and more on other clouds ... local is cool but the reason it even exists is that open source is discoverable , and it's hosted on cloud. since i cant deal with the threat model using my 2GB sound card i do what i can another way , and everything i host is just a docker deploy away for local llama folks ;-) here's an example : https://huggingface.co/spaces/Tonic/Math?docker=true am i a bad guy for it ? (hope not ;-) )",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2wpt6h",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 3.1"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i share my model weights on remote providers like huggingface , currently serving &amp;quot;localmodels&amp;quot; on 9 h200s there , and more on other clouds ... local is cool but the reason it even exists is that open source is discoverable , and it&amp;#39;s hosted on cloud. since i cant deal with the threat model using my 2GB sound card i do what i can another way , and everything i host is just a docker deploy away for local llama folks ;-) here&amp;#39;s an example : &lt;a href=\"https://huggingface.co/spaces/Tonic/Math?docker=true\"&gt;https://huggingface.co/spaces/Tonic/Math?docker=true&lt;/a&gt; am i a bad guy for it ? (hope not ;-) )&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lyu7bf",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wpt6h/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752419104,
                                "author_flair_text": "Llama 3.1",
                                "treatment_tags": [],
                                "created_utc": 1752419104,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#93b1ba",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2wn94i",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "simracerman",
                      "can_mod_post": false,
                      "created_utc": 1752418330,
                      "send_replies": true,
                      "parent_id": "t1_n2wk6dg",
                      "score": 4,
                      "author_fullname": "t2_vbzgnic",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I fully agree that retail hardware is always more expensive to procure and maintain than any cloud provider. That said, cloud providers are NEVER local.\n\nThe word local means you have physical access to the hardware and software. In your case, you have access to the software, but not hardware per se. it’s like asking your friend Bob to give you remote access to his powerful home PC to install LLMs. Well, Bob is a great friend, but if  one day he decides to look your my data, or accidentally leaks it because it’s local to him and crap happens, your data is no longer safe.\n\nAll that said, every threat model calls for different defense measures. If your threat model is basic, then no worries at all. To know what’s your threat model, ask a Large LLM to rate it for you, and make sure to provide it with all necessary context like the data you feed AI, and the data it generates out for you.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2wn94i",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I fully agree that retail hardware is always more expensive to procure and maintain than any cloud provider. That said, cloud providers are NEVER local.&lt;/p&gt;\n\n&lt;p&gt;The word local means you have physical access to the hardware and software. In your case, you have access to the software, but not hardware per se. it’s like asking your friend Bob to give you remote access to his powerful home PC to install LLMs. Well, Bob is a great friend, but if  one day he decides to look your my data, or accidentally leaks it because it’s local to him and crap happens, your data is no longer safe.&lt;/p&gt;\n\n&lt;p&gt;All that said, every threat model calls for different defense measures. If your threat model is basic, then no worries at all. To know what’s your threat model, ask a Large LLM to rate it for you, and make sure to provide it with all necessary context like the data you feed AI, and the data it generates out for you.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lyu7bf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wn94i/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752418330,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n2wvnrv",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Ok_Appearance3584",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n2wqhcl",
                                          "score": 1,
                                          "author_fullname": "t2_oyxj85n1",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Well, I'm not doing it professionally but as part of my work and as a curiosity.\n\n\nFor my use case, you could even use DGX Spark and get good results, you'd break even in half a year or so.\n\n\nThe problem I have is that APIs are too expensive for the amount of input/output tokens I consume for relatively trivial stuff and rental is too expensive for some of the 24/7 agents I run locally.\n\n\nBig batch processing is OK for rental but too inflexible if you want more complex dynamics and APIs are OK for more lightweight stuff but cannot handle high token counts on a budget.\n\n\nAlso, I am experimenting with continuous finetuning agents that train themselves after every response. They're the next big thing IMO. But 10x compute costs. Not gonna get them from cloud APIs any time soon, doesn't scale well for them.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n2wvnrv",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, I&amp;#39;m not doing it professionally but as part of my work and as a curiosity.&lt;/p&gt;\n\n&lt;p&gt;For my use case, you could even use DGX Spark and get good results, you&amp;#39;d break even in half a year or so.&lt;/p&gt;\n\n&lt;p&gt;The problem I have is that APIs are too expensive for the amount of input/output tokens I consume for relatively trivial stuff and rental is too expensive for some of the 24/7 agents I run locally.&lt;/p&gt;\n\n&lt;p&gt;Big batch processing is OK for rental but too inflexible if you want more complex dynamics and APIs are OK for more lightweight stuff but cannot handle high token counts on a budget.&lt;/p&gt;\n\n&lt;p&gt;Also, I am experimenting with continuous finetuning agents that train themselves after every response. They&amp;#39;re the next big thing IMO. But 10x compute costs. Not gonna get them from cloud APIs any time soon, doesn&amp;#39;t scale well for them.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1lyu7bf",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wvnrv/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1752420864,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1752420864,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n2wqhcl",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AI_Tonic",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2wokim",
                                "score": 1,
                                "author_fullname": "t2_100b8v9zrg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "at a million tokens a day you would pay off your 8 node H100s in several years (are you actually selling datasets??) compared to using the deepseek api for just a few dollars a day ... i'd love to learn more about which models you're using though , i make quite a lot of datasets too and cloud/renting is way cheaper than buying in my experience, but i'm not selling datasets , just open sourcing them...",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2wqhcl",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 3.1"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;at a million tokens a day you would pay off your 8 node H100s in several years (are you actually selling datasets??) compared to using the deepseek api for just a few dollars a day ... i&amp;#39;d love to learn more about which models you&amp;#39;re using though , i make quite a lot of datasets too and cloud/renting is way cheaper than buying in my experience, but i&amp;#39;m not selling datasets , just open sourcing them...&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lyu7bf",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wqhcl/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752419303,
                                "author_flair_text": "Llama 3.1",
                                "treatment_tags": [],
                                "created_utc": 1752419303,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#93b1ba",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2wokim",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Ok_Appearance3584",
                      "can_mod_post": false,
                      "created_utc": 1752418726,
                      "send_replies": true,
                      "parent_id": "t1_n2wk6dg",
                      "score": 2,
                      "author_fullname": "t2_oyxj85n1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not true if you're running serious AI workloads (such as processing data with LLMs), local hardware will pay off eventually or sometimes pretty soon (in a matter of months).\n\n\nI've calculated my use cases process a minimum of about a million output tokens per day, sometimes multiples of this. Input tokens would be a similar amount, if not more so.\n\n\nAn example of my use case is generating a synthetic instruct dataset for a low resource language. Other use cases include more traditional data processing and problem solving with expanded, agentic thinking. One prompt with recursive agent decomposition might produce a hundred thousand outout tokens or more.\n\n\nBut I agree, if all you do is chat and code, probably cheaper to not go local. If you do high token quantity data processing, or don't want to give away your private data, maybe cheaper to go local. ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2wokim",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not true if you&amp;#39;re running serious AI workloads (such as processing data with LLMs), local hardware will pay off eventually or sometimes pretty soon (in a matter of months).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve calculated my use cases process a minimum of about a million output tokens per day, sometimes multiples of this. Input tokens would be a similar amount, if not more so.&lt;/p&gt;\n\n&lt;p&gt;An example of my use case is generating a synthetic instruct dataset for a low resource language. Other use cases include more traditional data processing and problem solving with expanded, agentic thinking. One prompt with recursive agent decomposition might produce a hundred thousand outout tokens or more.&lt;/p&gt;\n\n&lt;p&gt;But I agree, if all you do is chat and code, probably cheaper to not go local. If you do high token quantity data processing, or don&amp;#39;t want to give away your private data, maybe cheaper to go local. &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lyu7bf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wokim/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752418726,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2wqj37",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "SlowFail2433",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2wpjmc",
                                "score": 3,
                                "author_fullname": "t2_131eezppgs",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The API models don’t ever change. Only the clients like ChatGPT and Gemini app change over time.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2wqj37",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The API models don’t ever change. Only the clients like ChatGPT and Gemini app change over time.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lyu7bf",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wqj37/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752419318,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752419318,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2wqr36",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AI_Tonic",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2wpjmc",
                                "score": 1,
                                "author_fullname": "t2_100b8v9zrg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yeah i have to agree with slowfail below , deploying models on cloud doesnt have this problem at all (my personal experience, i think i might not understand what you mean)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2wqr36",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 3.1"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah i have to agree with slowfail below , deploying models on cloud doesnt have this problem at all (my personal experience, i think i might not understand what you mean)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lyu7bf",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wqr36/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752419385,
                                "author_flair_text": "Llama 3.1",
                                "treatment_tags": [],
                                "created_utc": 1752419385,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#93b1ba",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2wpjmc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "InvertedVantage",
                      "can_mod_post": false,
                      "created_utc": 1752419024,
                      "send_replies": true,
                      "parent_id": "t1_n2wk6dg",
                      "score": 2,
                      "author_fullname": "t2_4me51",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "The one other reason I would say local is worth it is because retail models can degrade or change over time and having a local model means you have a known quantity that will not change.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2wpjmc",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The one other reason I would say local is worth it is because retail models can degrade or change over time and having a local model means you have a known quantity that will not change.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lyu7bf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wpjmc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752419024,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2wk6dg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AI_Tonic",
            "can_mod_post": false,
            "created_utc": 1752417386,
            "send_replies": true,
            "parent_id": "t3_1lyu7bf",
            "score": 2,
            "author_fullname": "t2_100b8v9zrg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "retail computing is never worth it if you do the math , until we get a lot of small models (which i do hope might happen soon) there's literally no financial sense in buying something to \"run locally\" because it will never perform well enough and you're always better off using providers or cloud solutions (just my two cents) . \n\nbtw i host and run a lot of models at any given time on cloud , but i still consider it local , because it's ... local to me . so please dont give me hate , i just have a very old and lame computer :-) \n\nbut to answer your question , check the ONNX models and see what works for you , that's what you'll be running on a mac (128GB can run basically 7-16B models with fairly sized context windows)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wk6dg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3.1"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;retail computing is never worth it if you do the math , until we get a lot of small models (which i do hope might happen soon) there&amp;#39;s literally no financial sense in buying something to &amp;quot;run locally&amp;quot; because it will never perform well enough and you&amp;#39;re always better off using providers or cloud solutions (just my two cents) . &lt;/p&gt;\n\n&lt;p&gt;btw i host and run a lot of models at any given time on cloud , but i still consider it local , because it&amp;#39;s ... local to me . so please dont give me hate , i just have a very old and lame computer :-) &lt;/p&gt;\n\n&lt;p&gt;but to answer your question , check the ONNX models and see what works for you , that&amp;#39;s what you&amp;#39;ll be running on a mac (128GB can run basically 7-16B models with fairly sized context windows)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wk6dg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752417386,
            "author_flair_text": "Llama 3.1",
            "treatment_tags": [],
            "link_id": "t3_1lyu7bf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#93b1ba",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2wnysn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Toooooool",
            "can_mod_post": false,
            "created_utc": 1752418544,
            "send_replies": true,
            "parent_id": "t3_1lyu7bf",
            "score": 1,
            "author_fullname": "t2_8llornh4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can do like this one guy did and invest in MI50's for cheap\n\n[https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ](https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wnysn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can do like this one guy did and invest in MI50&amp;#39;s for cheap&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ\"&gt;https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wnysn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752418544,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lyu7bf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]