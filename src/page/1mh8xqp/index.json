[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.\n\nAnyone have something like a llama model run on pi5 with 8GB RAM?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "lcoal llm on raspberry pi",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mh8xqp",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_l7ryo",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754300230,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is it feasable to have a local model run on a pi 5 ? The response time is actually not that important.&lt;/p&gt;\n\n&lt;p&gt;Anyone have something like a llama model run on pi5 with 8GB RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mh8xqp",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "overlydelicioustea",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/",
            "subreddit_subscribers": 509911,
            "created_utc": 1754300230,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6ukhdw",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "overlydelicioustea",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6uikvj",
                                          "score": 1,
                                          "author_fullname": "t2_l7ryo",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "the bot itself is running allready. currently it querys the model i have running on my main pc, but i was just wondering weather i could offload it to a pi. i allready have one running as my pi-hole and other stuff server, (the discord bot also runs on that pi) so i might just upgrade it to a 5 and try it out.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6ukhdw",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the bot itself is running allready. currently it querys the model i have running on my main pc, but i was just wondering weather i could offload it to a pi. i allready have one running as my pi-hole and other stuff server, (the discord bot also runs on that pi) so i might just upgrade it to a 5 and try it out.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mh8xqp",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6ukhdw/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754304552,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754304552,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6uikvj",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Jan49_",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ugi12",
                                "score": 1,
                                "author_fullname": "t2_u79v3xvn",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If it really doesn't matter, then you can maybe take a model larger than your RAM for better quality. But then it will be veeeery slow, because it needs too offload and then load parts of it into RAM. - I don't know if Ollama supports offloading.\n\nIf you want a simple implementation then I would run the model with Ollama (A Pi 5 is basically a low powered PC with Linux, so no difference to your main PC/Laptop).\nOllama opens a local API that you can really easily access with a python script or any other programming language.\n\nFor your purpose just write a small python script (use a discord bot library) and then use the Ollama API. Should be easily programmable in no time.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6uikvj",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If it really doesn&amp;#39;t matter, then you can maybe take a model larger than your RAM for better quality. But then it will be veeeery slow, because it needs too offload and then load parts of it into RAM. - I don&amp;#39;t know if Ollama supports offloading.&lt;/p&gt;\n\n&lt;p&gt;If you want a simple implementation then I would run the model with Ollama (A Pi 5 is basically a low powered PC with Linux, so no difference to your main PC/Laptop).\nOllama opens a local API that you can really easily access with a python script or any other programming language.&lt;/p&gt;\n\n&lt;p&gt;For your purpose just write a small python script (use a discord bot library) and then use the Ollama API. Should be easily programmable in no time.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mh8xqp",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6uikvj/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754303628,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754303628,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ujf3x",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "yami_no_ko",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ugi12",
                                "score": 1,
                                "author_fullname": "t2_30y9lbr0",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Ollama may be possible but seems quite much of an overhead for a RPI and it has an habit of obstructing what's going on under the hood, which is not what you want on a device where you need to keep an eye on RAM use and need to avoid swapping at all costs.\n\nI'd compile llama.cpp from source and use that with gguf models instead.\n\n[https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)",
                                "edited": 1754304928,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ujf3x",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama may be possible but seems quite much of an overhead for a RPI and it has an habit of obstructing what&amp;#39;s going on under the hood, which is not what you want on a device where you need to keep an eye on RAM use and need to avoid swapping at all costs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d compile llama.cpp from source and use that with gguf models instead.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp\"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mh8xqp",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6ujf3x/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754304043,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754304043,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ugi12",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "overlydelicioustea",
                      "can_mod_post": false,
                      "created_utc": 1754302567,
                      "send_replies": true,
                      "parent_id": "t1_n6uetf1",
                      "score": 1,
                      "author_fullname": "t2_l7ryo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "it really doesnt matter since i have a specific thing with it in mind (basically a discord bot that replies semi-randomly to people). When it runs, noone will even notice that it takes a few minute to infer a post.\n\nI just wanted to gather a rough expectation since i currently dont have a pi5 with enough ram.\n\nHow would i run it on a pi? Simply with ollama too?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ugi12",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;it really doesnt matter since i have a specific thing with it in mind (basically a discord bot that replies semi-randomly to people). When it runs, noone will even notice that it takes a few minute to infer a post.&lt;/p&gt;\n\n&lt;p&gt;I just wanted to gather a rough expectation since i currently dont have a pi5 with enough ram.&lt;/p&gt;\n\n&lt;p&gt;How would i run it on a pi? Simply with ollama too?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh8xqp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6ugi12/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754302567,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6uetf1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Jan49_",
            "can_mod_post": false,
            "created_utc": 1754301671,
            "send_replies": true,
            "parent_id": "t3_1mh8xqp",
            "score": 1,
            "author_fullname": "t2_u79v3xvn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would suggest some models especially designed for \"edge devices\" like Gemma3n E4B or E2B, Qwen3-4b, SmolLM3-4b and SmallThinker-4BA0.6B.\n\nAll of them even include a benchmark and speed comparison with the Pi 5 on their technical report.\n\nAnd trust me, \"response time is not that important\" is not true. You will play with it once and then never touch it. No one wants to wait 5+ minutes for an answer.\n\nSo choose a model that fits your RAM. At best smaller than 6GB (because to run the model you'll need an overhead of 1-2GB)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uetf1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would suggest some models especially designed for &amp;quot;edge devices&amp;quot; like Gemma3n E4B or E2B, Qwen3-4b, SmolLM3-4b and SmallThinker-4BA0.6B.&lt;/p&gt;\n\n&lt;p&gt;All of them even include a benchmark and speed comparison with the Pi 5 on their technical report.&lt;/p&gt;\n\n&lt;p&gt;And trust me, &amp;quot;response time is not that important&amp;quot; is not true. You will play with it once and then never touch it. No one wants to wait 5+ minutes for an answer.&lt;/p&gt;\n\n&lt;p&gt;So choose a model that fits your RAM. At best smaller than 6GB (because to run the model you&amp;#39;ll need an overhead of 1-2GB)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6uetf1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754301671,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh8xqp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uisoe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "yami_no_ko",
            "can_mod_post": false,
            "created_utc": 1754303739,
            "send_replies": true,
            "parent_id": "t3_1mh8xqp",
            "score": 2,
            "author_fullname": "t2_30y9lbr0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've been running gemma 2b with llama.cpp on a RPI400 (Which just has 4 gb of RAM) like one year ago: [https://huggingface.co/unsloth/gemma-2-it-GGUF](https://huggingface.co/unsloth/gemma-2-it-GGUF) \n\nSo I would say running a llm on a 8gb rpi5 should definitely be possible. Given that we have more capable models now than we had one year ago, it should even work better than it did for me.\n\nYou need to take into account that:\n\n\\- The model needs to fit into RAM and still leave enough space for context and OS overhead\n\n\\- You need to avoid running into swapping. (Best to deactivate swap alltogether for the time you're running an LLM on your PI). \n\n`sudo swapoff -a`deactivates it until the next reboot. Keep in mind that without swap it will freeze as soon as you're overflowing the maximum amount of RAM you're working with.\n\nLeaving swap on especially on the PI may severely degrade your SD card when it starts swapping the model or context during interference.\n\nOther than that it is feasable to run a small language model on the RPI and still reach acceptable speeds depending on the model size and quantization.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uisoe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running gemma 2b with llama.cpp on a RPI400 (Which just has 4 gb of RAM) like one year ago: &lt;a href=\"https://huggingface.co/unsloth/gemma-2-it-GGUF\"&gt;https://huggingface.co/unsloth/gemma-2-it-GGUF&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;So I would say running a llm on a 8gb rpi5 should definitely be possible. Given that we have more capable models now than we had one year ago, it should even work better than it did for me.&lt;/p&gt;\n\n&lt;p&gt;You need to take into account that:&lt;/p&gt;\n\n&lt;p&gt;- The model needs to fit into RAM and still leave enough space for context and OS overhead&lt;/p&gt;\n\n&lt;p&gt;- You need to avoid running into swapping. (Best to deactivate swap alltogether for the time you&amp;#39;re running an LLM on your PI). &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sudo swapoff -a&lt;/code&gt;deactivates it until the next reboot. Keep in mind that without swap it will freeze as soon as you&amp;#39;re overflowing the maximum amount of RAM you&amp;#39;re working with.&lt;/p&gt;\n\n&lt;p&gt;Leaving swap on especially on the PI may severely degrade your SD card when it starts swapping the model or context during interference.&lt;/p&gt;\n\n&lt;p&gt;Other than that it is feasable to run a small language model on the RPI and still reach acceptable speeds depending on the model size and quantization.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6uisoe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754303739,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh8xqp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6unujb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Sicarius_The_First",
            "can_mod_post": false,
            "created_utc": 1754306075,
            "send_replies": true,
            "parent_id": "t3_1mh8xqp",
            "score": 2,
            "author_fullname": "t2_ik8czvp65",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes:\n\n[https://huggingface.co/SicariusSicariiStuff/Impish\\_LLAMA\\_4B](https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B)\n\nThis is one of the main reasons for its existence.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6unujb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B\"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_4B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is one of the main reasons for its existence.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6unujb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754306075,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh8xqp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ugjpp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "overlydelicioustea",
                      "can_mod_post": false,
                      "created_utc": 1754302591,
                      "send_replies": true,
                      "parent_id": "t1_n6uetl3",
                      "score": 2,
                      "author_fullname": "t2_l7ryo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "what are you on about?\n\nI just wanted to gather a rough expectation since i currently dont have a pi5 with enough ram. If I had one i would just try it out and not post here.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ugjpp",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;what are you on about?&lt;/p&gt;\n\n&lt;p&gt;I just wanted to gather a rough expectation since i currently dont have a pi5 with enough ram. If I had one i would just try it out and not post here.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh8xqp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6ugjpp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754302591,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6uihrr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Final_Wheel_7486",
                      "can_mod_post": false,
                      "created_utc": 1754303584,
                      "send_replies": true,
                      "parent_id": "t1_n6uetl3",
                      "score": 3,
                      "author_fullname": "t2_cyrs5dhp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Huh??? The question was fine.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6uihrr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Huh??? The question was fine.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh8xqp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6uihrr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754303584,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6uetl3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "LOW_SCORE",
            "no_follow": true,
            "author": "Xamanthas",
            "can_mod_post": false,
            "created_utc": 1754301673,
            "send_replies": true,
            "parent_id": "t3_1mh8xqp",
            "score": -8,
            "author_fullname": "t2_e6bnx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": true,
            "body": "No. You are missing so much foundational/basic knowledge that I dont even know where to start. You invoke llama tier model in the same sentence as a raspberry pi before later saying you dont even have the hardware. Just rent or buy something actually usable",
            "edited": 1754308988,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uetl3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No. You are missing so much foundational/basic knowledge that I dont even know where to start. You invoke llama tier model in the same sentence as a raspberry pi before later saying you dont even have the hardware. Just rent or buy something actually usable&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": "comment score below threshold",
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh8xqp/lcoal_llm_on_raspberry_pi/n6uetl3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754301673,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh8xqp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -8
          }
        }
      ],
      "before": null
    }
  }
]