[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi! So I've been playing around with everyone's baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.\n\nShoutout to u/Skatardude10 and u/farkinga\n\nSo everyone knows it's a great idea to offload some/all tensors to RAM with these models if you can't fit them all. But from what I gathered, if you offload them using \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\", the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.\n\nWhat is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).\n\nThis is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).\n\nThe test is simple \"write me a sophisticated web scraper\". I run it once, then regenerate it to compare (I don't quite understand draft model context, noob, again).\n\n|~~With Qwen3 4b draft model\\*~~|No draft model|\n|:-|:-|\n|~~Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s~~|Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s|\n|~~Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s~~|Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s|\n\n*edit: tried* u/AliNT77\\*'s tip: set draft model's cache to Q8 Q8 and you'll have a higher acceptance rate with the smaller mode, allowing you to go up with main model's context and gain some speed.\\*\n\n\\* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):\n\n* XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)\n* DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s\n* Unsloth Qwen3 0.6B  - 33%, 19t/s\n* **Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s**\n* Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.\n\nWhat was the acceptance rate for 4B you're gonna ask... 67%.\n\nWhy do this instead of trying to offload some layers and try to gain performance this way? I don't know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)\n\nNow this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I'm unemployed, but I'm not THAT unemployed.\n\nQuestions:\n\n1. 1.7b vs 4b draft model. This obvs needs more testing and longer context, but I'm assuming that 4b will perform better than 1.7b with more complex code.\n2. What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?\n3. Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.\n4. I've got a Ryzen CPU. It's massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I'm assuming this is normal and there are no optimisations for AMD cpus?\n5. Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.\n6. Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don't these exist out in the wild?\n7. How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it's localllama so I dare not ask about bigger applications)\n\nWell, if you read all of this, here's your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp; caches, but I am not done yet. This doesn't crash the system, any other combo does. So if you've got more than 12gb vram, you might get away with more context.\n\nStart with: LLAMA\\_SET\\_ROWS=1  \n\\--model \"(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4\\_K\\_XL.gguf\"  \n\\--model-draft \"(full path)/Qwen3-4B-Q8\\_0.gguf\"  \n\\--override-tensor \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\" (yet to test this, but it can now be replaced with --cpu-moe)  \n\\--flash-attn  \n~~--ctx-size 192000~~  \n\\--ctx-size 262144 --cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--threads -1  \n\\--n-gpu-layers 99  \n\\--n-gpu-layers-draft 99  \n~~--ctx-size-draft 1024 --cache-type-k-draft q4\\_0 --cache-type-v-draft q4\\_0~~  \n\\--ctx-size-draft 24567 --cache-type-v-draft q8\\_0 --cache-type-v-draft q8\\_0\n\nor you can do for more speed (30t/s)/accuracy, but less context.  \n\\--ctx-size 131072 --cache-type-k q8\\_0 --cache-type-v q8\\_0  \n\\--ctx-size-draft 24576 --cache-type-k-draft q8\\_0 --cache-type-v-draft q8\\_0  \n\\--batch-size 1024 --ubatch-size 1024\n\nThese settings get you to 11197MiB /  12227MiB vram on the gpu.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Qwen3 30B A3b --override-tensor + Qwen3 4b draft = &lt;3 (22 vs 14 t/s)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfpgae",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.77,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 14,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_wxxnd",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 14,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754147599,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754138032,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! So I&amp;#39;ve been playing around with everyone&amp;#39;s baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.&lt;/p&gt;\n\n&lt;p&gt;Shoutout to &lt;a href=\"/u/Skatardude10\"&gt;u/Skatardude10&lt;/a&gt; and &lt;a href=\"/u/farkinga\"&gt;u/farkinga&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So everyone knows it&amp;#39;s a great idea to offload some/all tensors to RAM with these models if you can&amp;#39;t fit them all. But from what I gathered, if you offload them using &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot;, the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.&lt;/p&gt;\n\n&lt;p&gt;What is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).&lt;/p&gt;\n\n&lt;p&gt;This is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).&lt;/p&gt;\n\n&lt;p&gt;The test is simple &amp;quot;write me a sophisticated web scraper&amp;quot;. I run it once, then regenerate it to compare (I don&amp;#39;t quite understand draft model context, noob, again).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;del&gt;With Qwen3 4b draft model*&lt;/del&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;No draft model&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;em&gt;edit: tried&lt;/em&gt; &lt;a href=\"/u/AliNT77\"&gt;u/AliNT77&lt;/a&gt;*&amp;#39;s tip: set draft model&amp;#39;s cache to Q8 Q8 and you&amp;#39;ll have a higher acceptance rate with the smaller mode, allowing you to go up with main model&amp;#39;s context and gain some speed.*&lt;/p&gt;\n\n&lt;p&gt;* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)&lt;/li&gt;\n&lt;li&gt;DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 0.6B  - 33%, 19t/s&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What was the acceptance rate for 4B you&amp;#39;re gonna ask... 67%.&lt;/p&gt;\n\n&lt;p&gt;Why do this instead of trying to offload some layers and try to gain performance this way? I don&amp;#39;t know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)&lt;/p&gt;\n\n&lt;p&gt;Now this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I&amp;#39;m unemployed, but I&amp;#39;m not THAT unemployed.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;1.7b vs 4b draft model. This obvs needs more testing and longer context, but I&amp;#39;m assuming that 4b will perform better than 1.7b with more complex code.&lt;/li&gt;\n&lt;li&gt;What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?&lt;/li&gt;\n&lt;li&gt;Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve got a Ryzen CPU. It&amp;#39;s massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I&amp;#39;m assuming this is normal and there are no optimisations for AMD cpus?&lt;/li&gt;\n&lt;li&gt;Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.&lt;/li&gt;\n&lt;li&gt;Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don&amp;#39;t these exist out in the wild?&lt;/li&gt;\n&lt;li&gt;How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it&amp;#39;s localllama so I dare not ask about bigger applications)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Well, if you read all of this, here&amp;#39;s your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp;amp; caches, but I am not done yet. This doesn&amp;#39;t crash the system, any other combo does. So if you&amp;#39;ve got more than 12gb vram, you might get away with more context.&lt;/p&gt;\n\n&lt;p&gt;Start with: LLAMA_SET_ROWS=1&lt;br/&gt;\n--model &amp;quot;(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4_K_XL.gguf&amp;quot;&lt;br/&gt;\n--model-draft &amp;quot;(full path)/Qwen3-4B-Q8_0.gguf&amp;quot;&lt;br/&gt;\n--override-tensor &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot; (yet to test this, but it can now be replaced with --cpu-moe)&lt;br/&gt;\n--flash-attn&lt;br/&gt;\n&lt;del&gt;--ctx-size 192000&lt;/del&gt;&lt;br/&gt;\n--ctx-size 262144 --cache-type-k q4_0 --cache-type-v q4_0&lt;br/&gt;\n--threads -1&lt;br/&gt;\n--n-gpu-layers 99&lt;br/&gt;\n--n-gpu-layers-draft 99&lt;br/&gt;\n&lt;del&gt;--ctx-size-draft 1024 --cache-type-k-draft q4_0 --cache-type-v-draft q4_0&lt;/del&gt;&lt;br/&gt;\n--ctx-size-draft 24567 --cache-type-v-draft q8_0 --cache-type-v-draft q8_0&lt;/p&gt;\n\n&lt;p&gt;or you can do for more speed (30t/s)/accuracy, but less context.&lt;br/&gt;\n--ctx-size 131072 --cache-type-k q8_0 --cache-type-v q8_0&lt;br/&gt;\n--ctx-size-draft 24576 --cache-type-k-draft q8_0 --cache-type-v-draft q8_0&lt;br/&gt;\n--batch-size 1024 --ubatch-size 1024&lt;/p&gt;\n\n&lt;p&gt;These settings get you to 11197MiB /  12227MiB vram on the gpu.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1mfpgae",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "igorwarzocha",
            "discussion_type": null,
            "num_comments": 26,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
            "subreddit_subscribers": 509054,
            "created_utc": 1754138032,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6k7mfl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Aaaaaaaaaeeeee",
                      "can_mod_post": false,
                      "created_utc": 1754156986,
                      "send_replies": true,
                      "parent_id": "t1_n6ip1cz",
                      "score": 1,
                      "author_fullname": "t2_el5pibmej",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "On huggingface/accelerate it let you run with a CPU and GPU in 4bit? I'm sure it might be possible with GPTQ AWQ, but I never really read anybody who tried the CPU side and 4-bit. That would be great since we may not need llama.cpp on many models unless we wanted to refine the quantization approach to target certain tensors/sublayers more aggressively.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6k7mfl",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;On huggingface/accelerate it let you run with a CPU and GPU in 4bit? I&amp;#39;m sure it might be possible with GPTQ AWQ, but I never really read anybody who tried the CPU side and 4-bit. That would be great since we may not need llama.cpp on many models unless we wanted to refine the quantization approach to target certain tensors/sublayers more aggressively.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpgae",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6k7mfl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754156986,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6jjm14",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kmouratidis",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6itb3f",
                                "score": 1,
                                "author_fullname": "t2_k6u7rfxb",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "&gt; it's VRAM is barely loaded, maybe 4gb\n\nSee, that's the core issue, if most of the model is on RAM, of course you're going to see lower performance.\n\n&gt; like loading every even or odd\n\nTo me this even/odd thing is probably the worst idea of them all. You force all computation to rely on PCIe **and** on RAM for every layer. It's probably better to simply put the first 30 layers on GPU and then the last 10 on CPU, or first 25 layers on GPU and then only the experts of the last 15 layers on CPU. I don't know exactly how that is done, maybe something like this `-ot \"blk\\.(?:[3-4][0-9])\\.ffn.*=CPU\"`?\n\nIn either case, you're comparing unequal things. Might as try offloading the 0.6B draft model 100% to CPU and then as much of the 30B model on the GPU, and compare again. You might even see better results this way.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6jjm14",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;it&amp;#39;s VRAM is barely loaded, maybe 4gb&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;See, that&amp;#39;s the core issue, if most of the model is on RAM, of course you&amp;#39;re going to see lower performance.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;like loading every even or odd&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;To me this even/odd thing is probably the worst idea of them all. You force all computation to rely on PCIe &lt;strong&gt;and&lt;/strong&gt; on RAM for every layer. It&amp;#39;s probably better to simply put the first 30 layers on GPU and then the last 10 on CPU, or first 25 layers on GPU and then only the experts of the last 15 layers on CPU. I don&amp;#39;t know exactly how that is done, maybe something like this &lt;code&gt;-ot &amp;quot;blk\\.(?:[3-4][0-9])\\.ffn.*=CPU&amp;quot;&lt;/code&gt;?&lt;/p&gt;\n\n&lt;p&gt;In either case, you&amp;#39;re comparing unequal things. Might as try offloading the 0.6B draft model 100% to CPU and then as much of the 30B model on the GPU, and compare again. You might even see better results this way.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfpgae",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6jjm14/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754149382,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754149382,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6itb3f",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "igorwarzocha",
                      "can_mod_post": false,
                      "created_utc": 1754140565,
                      "send_replies": true,
                      "parent_id": "t1_n6ip1cz",
                      "score": 1,
                      "author_fullname": "t2_wxxnd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah, as non-scientific as it gets. See my point is, if it makes it faster, are we sure there is no point doing it?\n\n\"enough room to fit extra layers...\" - yeah, but the performance was lower than this, probably dragged down by the CPU. IIRC fitting in bigger context was the issue, once you start loading more layers, you cannot fit the cache on the GPU. The way I set it up basically resulted in a better \"context to speed\" ratio.\n\nre edit, oh yeah I know, I'm just pleased that it works with my preliminary benchmarks. I've got all the options and \"non-crashing commands\" saved, so I can now start using/benching it (afaik there are issues with tools with that model anyway right now, so no rush)\n\nre edit2, I didn't go all the way in with it, but I've tried the simple stuff, like loading every even or odd. I get that you can probably figure out which specific ones should go where, but... you do this, and then they release an update (to engine or to the model), and all this work goes out of the window. It is probably also \"per quants provider and per quants\" kind of a situation as well. At that point my time is probably worth more than this and I should get myself a 2nd GPU. I've got rx6600xt 8gb doing nothing, but it's got no AI cores and I believe it becomes yet another bottleneck, yes I did manage to run both of them in Vulkan.\n\nNow annoyingly I cannot seem to make llama ignore it completely when plugged in so I had to physically pull the power plug (hence why I didn't test it like that properly, there might be some performance to gain, but I'd still be offloading some stuff to the CPU anyway)\n\nedit - ooooooooooof SMI - I might do it later. Basically when you offload the tensors the way I did it with no draft model, the gpu is doing nothing and it's VRAM is barely loaded, maybe 4gb. With the big draft model, it's sitting at 11/12gb and computing at 80%. Before you ask, If I keep more layers on the GPU, it basically doesnt compute anything.",
                      "edited": 1754140884,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6itb3f",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, as non-scientific as it gets. See my point is, if it makes it faster, are we sure there is no point doing it?&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;enough room to fit extra layers...&amp;quot; - yeah, but the performance was lower than this, probably dragged down by the CPU. IIRC fitting in bigger context was the issue, once you start loading more layers, you cannot fit the cache on the GPU. The way I set it up basically resulted in a better &amp;quot;context to speed&amp;quot; ratio.&lt;/p&gt;\n\n&lt;p&gt;re edit, oh yeah I know, I&amp;#39;m just pleased that it works with my preliminary benchmarks. I&amp;#39;ve got all the options and &amp;quot;non-crashing commands&amp;quot; saved, so I can now start using/benching it (afaik there are issues with tools with that model anyway right now, so no rush)&lt;/p&gt;\n\n&lt;p&gt;re edit2, I didn&amp;#39;t go all the way in with it, but I&amp;#39;ve tried the simple stuff, like loading every even or odd. I get that you can probably figure out which specific ones should go where, but... you do this, and then they release an update (to engine or to the model), and all this work goes out of the window. It is probably also &amp;quot;per quants provider and per quants&amp;quot; kind of a situation as well. At that point my time is probably worth more than this and I should get myself a 2nd GPU. I&amp;#39;ve got rx6600xt 8gb doing nothing, but it&amp;#39;s got no AI cores and I believe it becomes yet another bottleneck, yes I did manage to run both of them in Vulkan.&lt;/p&gt;\n\n&lt;p&gt;Now annoyingly I cannot seem to make llama ignore it completely when plugged in so I had to physically pull the power plug (hence why I didn&amp;#39;t test it like that properly, there might be some performance to gain, but I&amp;#39;d still be offloading some stuff to the CPU anyway)&lt;/p&gt;\n\n&lt;p&gt;edit - ooooooooooof SMI - I might do it later. Basically when you offload the tensors the way I did it with no draft model, the gpu is doing nothing and it&amp;#39;s VRAM is barely loaded, maybe 4gb. With the big draft model, it&amp;#39;s sitting at 11/12gb and computing at 80%. Before you ask, If I keep more layers on the GPU, it basically doesnt compute anything.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpgae",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6itb3f/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754140565,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ip1cz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "kmouratidis",
            "can_mod_post": false,
            "created_utc": 1754138915,
            "send_replies": true,
            "parent_id": "t3_1mfpgae",
            "score": 18,
            "author_fullname": "t2_k6u7rfxb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I don't think it makes much sense to use a 4B draft model for a 3B-active model, and neither does \"regenerating\" since it likely hits the KV cache. \n\n&gt; Why do this instead of trying to offload some layers and try to gain performance this way\n\nIf you have enough room in your GPU to fit a whole 4B model (at Q8 no less), you definitely have enough room to fit enough extra layers (the equivalent of 8B since you're running Q4) to make inference faster. Why not share your `nvidia-smi` when you run the 30B-A3B standalone?\n\nEdit: btw, never benchmark with a single generation. Use a proper script like for example one of [these](https://github.com/vllm-project/vllm/tree/main/benchmarks).\n\nEdit 2: Not a big user of llamacpp, but what I would try would be to offload the last parts of each expert instead of whole experts, or last few layers. You don't want some experts fully on CPU while others are fully on GPU (unless you've benchmarked the routing and know some are used way more often and this makes sense). In huggingface/accelerate you can configure a manual `device_map` for where to put any tensor, and can relatively easily do all this. You can experiment quite a bit until you find the best combo.",
            "edited": 1754139358,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ip1cz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think it makes much sense to use a 4B draft model for a 3B-active model, and neither does &amp;quot;regenerating&amp;quot; since it likely hits the KV cache. &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Why do this instead of trying to offload some layers and try to gain performance this way&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If you have enough room in your GPU to fit a whole 4B model (at Q8 no less), you definitely have enough room to fit enough extra layers (the equivalent of 8B since you&amp;#39;re running Q4) to make inference faster. Why not share your &lt;code&gt;nvidia-smi&lt;/code&gt; when you run the 30B-A3B standalone?&lt;/p&gt;\n\n&lt;p&gt;Edit: btw, never benchmark with a single generation. Use a proper script like for example one of &lt;a href=\"https://github.com/vllm-project/vllm/tree/main/benchmarks\"&gt;these&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Edit 2: Not a big user of llamacpp, but what I would try would be to offload the last parts of each expert instead of whole experts, or last few layers. You don&amp;#39;t want some experts fully on CPU while others are fully on GPU (unless you&amp;#39;ve benchmarked the routing and know some are used way more often and this makes sense). In huggingface/accelerate you can configure a manual &lt;code&gt;device_map&lt;/code&gt; for where to put any tensor, and can relatively easily do all this. You can experiment quite a bit until you find the best combo.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6ip1cz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754138915,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfpgae",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 18
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "richtext",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n6iymb0",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "igorwarzocha",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n6iv5f9",
                                                              "score": 2,
                                                              "author_fullname": "t2_wxxnd",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Yeah that's what I meant by \"offloading less OR layers\". I've tried these options -  they performed worse and resulted in a lot less space for context, so either way it's a trade-off. There might be a world where someone figures out \"the\" set of tensors to keep on GPU, but that's not me.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n6iymb0",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah that&amp;#39;s what I meant by &amp;quot;offloading less OR layers&amp;quot;. I&amp;#39;ve tried these options -  they performed worse and resulted in a lot less space for context, so either way it&amp;#39;s a trade-off. There might be a world where someone figures out &amp;quot;the&amp;quot; set of tensors to keep on GPU, but that&amp;#39;s not me.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mfpgae",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iymb0/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754142505,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754142505,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 2
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6iv5f9",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": false,
                                                    "author": "dark-light92",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6iutfp",
                                                    "score": 4,
                                                    "author_fullname": "t2_3lvoq8zw",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "You don't offload layers. You offload tensors. For example, try this: -ot \"ffn\\_(up|down).\\*=CPU\"",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6iv5f9",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [
                                                      {
                                                        "e": "text",
                                                        "t": "llama.cpp"
                                                      }
                                                    ],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You don&amp;#39;t offload layers. You offload tensors. For example, try this: -ot &amp;quot;ffn_(up|down).*=CPU&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfpgae",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": "light",
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iv5f9/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754141250,
                                                    "author_flair_text": "llama.cpp",
                                                    "collapsed": false,
                                                    "created_utc": 1754141250,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": "#bbbdbf",
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 4
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6iutfp",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "igorwarzocha",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6ir514",
                                          "score": 2,
                                          "author_fullname": "t2_wxxnd",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "see my other reply, I didnt go too deep, but offloading less or offloading layers basically resulted in a CPU bottleneck anyway",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6iutfp",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;see my other reply, I didnt go too deep, but offloading less or offloading layers basically resulted in a CPU bottleneck anyway&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfpgae",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iutfp/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754141127,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754141127,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ir514",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "dark-light92",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6iqjyn",
                                "score": 6,
                                "author_fullname": "t2_3lvoq8zw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What else did you try? In the command you provided, everything except the attention tensors will go to the CPU. Which will leave most of the VRAM unutilized.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ir514",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What else did you try? In the command you provided, everything except the attention tensors will go to the CPU. Which will leave most of the VRAM unutilized.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfpgae",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6ir514/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754139732,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754139732,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iqjyn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "igorwarzocha",
                      "can_mod_post": false,
                      "created_utc": 1754139505,
                      "send_replies": true,
                      "parent_id": "t1_n6ipgdf",
                      "score": -4,
                      "author_fullname": "t2_wxxnd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Nope, this didn't result in better performance. Again, from what I've experienced, anything you offload to the CPU will be the bottleneck and will drag down the performance no matter what you do. YMMV, obviously. And I could've probably tested it differently, but I'll leave this to other people.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iqjyn",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nope, this didn&amp;#39;t result in better performance. Again, from what I&amp;#39;ve experienced, anything you offload to the CPU will be the bottleneck and will drag down the performance no matter what you do. YMMV, obviously. And I could&amp;#39;ve probably tested it differently, but I&amp;#39;ll leave this to other people.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpgae",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iqjyn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754139505,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ipgdf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "dark-light92",
            "can_mod_post": false,
            "created_utc": 1754139079,
            "send_replies": true,
            "parent_id": "t3_1mfpgae",
            "score": 8,
            "author_fullname": "t2_3lvoq8zw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You will get better performance by putting more tensors on the GPU using the --override-tensor parameter.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ipgdf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You will get better performance by putting more tensors on the GPU using the --override-tensor parameter.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6ipgdf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754139079,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mfpgae",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6jeeyv",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Secure_Reflection409",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6iylku",
                                "score": 1,
                                "author_fullname": "t2_by77ogdhr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "lol, didn't even see that.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6jeeyv",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;lol, didn&amp;#39;t even see that.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfpgae",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6jeeyv/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754147736,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754147736,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iylku",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754142498,
                      "send_replies": true,
                      "parent_id": "t1_n6itiqm",
                      "score": 5,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It’s not about the quant of the model, it’s about its kv cache quant. Q4_0 kills acceptance rate",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iylku",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s not about the quant of the model, it’s about its kv cache quant. Q4_0 kills acceptance rate&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpgae",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iylku/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754142498,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6j3m8n",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "colin_colout",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6iuzxn",
                                "score": 7,
                                "author_fullname": "t2_14l4ya",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Careful with even 8 bit kv cache.  When you quantize the cache even a little, quality will start to degrade for longer context. Effect is quite minimal when there's only a few tokens in context, but perplexity compounds quickly as you load up context. \n\nFor even a medium context size, you'll generally get better results just using an aggressively quantized model with full cache... Especially in cases with long context. kv quants should be a desperate last resort after all else is exhausted. \n\nI'd take a 2bit unsloth gguf with full cache over a 4bit (or even 8bit) model with 8_0 cache unless I'm using fewer than 1k or so tokens (so almost never)\n\nQuantizing cache to get larger context is like cutting off your arm to lose weight.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6j3m8n",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Careful with even 8 bit kv cache.  When you quantize the cache even a little, quality will start to degrade for longer context. Effect is quite minimal when there&amp;#39;s only a few tokens in context, but perplexity compounds quickly as you load up context. &lt;/p&gt;\n\n&lt;p&gt;For even a medium context size, you&amp;#39;ll generally get better results just using an aggressively quantized model with full cache... Especially in cases with long context. kv quants should be a desperate last resort after all else is exhausted. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d take a 2bit unsloth gguf with full cache over a 4bit (or even 8bit) model with 8_0 cache unless I&amp;#39;m using fewer than 1k or so tokens (so almost never)&lt;/p&gt;\n\n&lt;p&gt;Quantizing cache to get larger context is like cutting off your arm to lose weight.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfpgae",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j3m8n/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754144247,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754144247,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 7
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iuzxn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "igorwarzocha",
                      "can_mod_post": false,
                      "created_utc": 1754141193,
                      "send_replies": true,
                      "parent_id": "t1_n6itiqm",
                      "score": 1,
                      "author_fullname": "t2_wxxnd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Interesting, this could leave more room for context. Again, it probably depends on the task.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iuzxn",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting, this could leave more room for context. Again, it probably depends on the task.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpgae",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iuzxn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754141193,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6itiqm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Secure_Reflection409",
            "can_mod_post": false,
            "created_utc": 1754140643,
            "send_replies": true,
            "parent_id": "t3_1mfpgae",
            "score": 4,
            "author_fullname": "t2_by77ogdhr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I was getting up to 80% acceptance using 0.6b bf16 draft.\n\n\nMaybe give it a shot.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6itiqm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was getting up to 80% acceptance using 0.6b bf16 draft.&lt;/p&gt;\n\n&lt;p&gt;Maybe give it a shot.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6itiqm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754140643,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfpgae",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6j8fkv",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "igorwarzocha",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6j4kuj",
                                                    "score": 3,
                                                    "author_fullname": "t2_wxxnd",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "yup, I'm one of these idiots who spent 2 weeks overclocking their ram afterhours, so it's running as fast as the lottery permits.\n\n\"blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\"=&gt; yeah, but then the context tanks - this works for LLMs with smaller contexts, but I'd rather get ctx over speed, just a personal preference.\n\nRows enabled, a small bump as expected! \n\nCheers",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6j8fkv",
                                                    "is_submitter": true,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yup, I&amp;#39;m one of these idiots who spent 2 weeks overclocking their ram afterhours, so it&amp;#39;s running as fast as the lottery permits.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&amp;quot;=&amp;gt; yeah, but then the context tanks - this works for LLMs with smaller contexts, but I&amp;#39;d rather get ctx over speed, just a personal preference.&lt;/p&gt;\n\n&lt;p&gt;Rows enabled, a small bump as expected! &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfpgae",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j8fkv/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754145842,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754145842,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6j4kuj",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "AliNT77",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6j33my",
                                          "score": 1,
                                          "author_fullname": "t2_66tlmx2l",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Also you're bottlenecked in TG by your system ram's bandwidth. so make sure you're running at least 3600Mhz memory and run at 1800 FCLK to get the most out of your CPU. also experiment with offloading fewer layers to the cpu. Ideally you should offload as few exps to the cpu as possible. \n\nblk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n\nthis example offloads only layer 16 to 49. experiment with this till you fill up your vram. also higher ubatch means higher pp performance. \n\nalso enable this environment var for 5-10% extra performance: \n\nLLAMA\\_SET\\_ROWS=1\n\nso run the command like this: \n\nLLAMA\\_SET\\_ROWS=1 ./llama-server -md etc.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6j4kuj",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also you&amp;#39;re bottlenecked in TG by your system ram&amp;#39;s bandwidth. so make sure you&amp;#39;re running at least 3600Mhz memory and run at 1800 FCLK to get the most out of your CPU. also experiment with offloading fewer layers to the cpu. Ideally you should offload as few exps to the cpu as possible. &lt;/p&gt;\n\n&lt;p&gt;blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/p&gt;\n\n&lt;p&gt;this example offloads only layer 16 to 49. experiment with this till you fill up your vram. also higher ubatch means higher pp performance. &lt;/p&gt;\n\n&lt;p&gt;also enable this environment var for 5-10% extra performance: &lt;/p&gt;\n\n&lt;p&gt;LLAMA_SET_ROWS=1&lt;/p&gt;\n\n&lt;p&gt;so run the command like this: &lt;/p&gt;\n\n&lt;p&gt;LLAMA_SET_ROWS=1 ./llama-server -md etc.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfpgae",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j4kuj/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754144567,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754144567,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6j33my",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AliNT77",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6iysot",
                                "score": 2,
                                "author_fullname": "t2_66tlmx2l",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "here's another one: \n\nuse the IQ4\\_NL quant instead of UD XL. it performs better in ppl tests and is smaller.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6j33my",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;here&amp;#39;s another one: &lt;/p&gt;\n\n&lt;p&gt;use the IQ4_NL quant instead of UD XL. it performs better in ppl tests and is smaller.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfpgae",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j33my/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754144075,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754144075,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iysot",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "igorwarzocha",
                      "can_mod_post": false,
                      "created_utc": 1754142569,
                      "send_replies": true,
                      "parent_id": "t1_n6iygqz",
                      "score": 3,
                      "author_fullname": "t2_wxxnd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "now this is a useful tip! will do!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iysot",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;now this is a useful tip! will do!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpgae",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iysot/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754142569,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6j20qn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AliNT77",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6j0scw",
                                "score": 2,
                                "author_fullname": "t2_66tlmx2l",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Oh and  there’s also another bug. After you fill up the draft ctx, the performance tanks so it’s not good to use smaller context on the draft model.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6j20qn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh and  there’s also another bug. After you fill up the draft ctx, the performance tanks so it’s not good to use smaller context on the draft model.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfpgae",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j20qn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754143707,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754143707,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6j1s1j",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AliNT77",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6j0scw",
                                "score": 1,
                                "author_fullname": "t2_66tlmx2l",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Ive submitted an issue on llamacpp repo",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6j1s1j",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ive submitted an issue on llamacpp repo&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfpgae",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j1s1j/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754143624,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754143624,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6j0scw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "igorwarzocha",
                      "can_mod_post": false,
                      "created_utc": 1754143279,
                      "send_replies": true,
                      "parent_id": "t1_n6iygqz",
                      "score": 2,
                      "author_fullname": "t2_wxxnd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "You are absolutely right!\n\n4b with Q8 Q8 made it quite a lot slower with similar acceptance, but...\n\n0.6b with Q\\_8\\_Q8 reached 68% with 27t/s and I'm back to 262k context on the main model, I''ll edit the op",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6j0scw",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You are absolutely right!&lt;/p&gt;\n\n&lt;p&gt;4b with Q8 Q8 made it quite a lot slower with similar acceptance, but...&lt;/p&gt;\n\n&lt;p&gt;0.6b with Q_8_Q8 reached 68% with 27t/s and I&amp;#39;m back to 262k context on the main model, I&amp;#39;&amp;#39;ll edit the op&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpgae",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j0scw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754143279,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6iygqz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AliNT77",
            "can_mod_post": false,
            "created_utc": 1754142450,
            "send_replies": true,
            "parent_id": "t3_1mfpgae",
            "score": 4,
            "author_fullname": "t2_66tlmx2l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Retest every draft model with draft kv cache at q8_0. In my experience q4_0 kv cache quant absolutely kills acceptance rate. Try q5_1 and q8_0 and you’ll see much higher acceptance rate",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6iygqz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Retest every draft model with draft kv cache at q8_0. In my experience q4_0 kv cache quant absolutely kills acceptance rate. Try q5_1 and q8_0 and you’ll see much higher acceptance rate&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6iygqz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754142450,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfpgae",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "total_awards_received": 0,
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "ups": 1,
            "removal_reason": null,
            "link_id": "t3_1mfpgae",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6j2ozu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "DELETED",
            "no_follow": true,
            "author": "[deleted]",
            "can_mod_post": false,
            "send_replies": true,
            "parent_id": "t3_1mfpgae",
            "score": 1,
            "approved_by": null,
            "report_reasons": null,
            "all_awardings": [],
            "subreddit_id": "t5_81eyvm",
            "body": "[removed]",
            "edited": false,
            "downs": 0,
            "author_flair_css_class": null,
            "collapsed": true,
            "is_submitter": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;",
            "gildings": {},
            "collapsed_reason": null,
            "associated_award": null,
            "stickied": false,
            "subreddit_type": "public",
            "can_gild": false,
            "top_awarded_type": null,
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6j2ozu/",
            "num_reports": null,
            "locked": false,
            "name": "t1_n6j2ozu",
            "created": 1754143936,
            "subreddit": "LocalLLaMA",
            "author_flair_text": null,
            "treatment_tags": [],
            "created_utc": 1754143936,
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "mod_note": null,
            "distinguished": null
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6kz5lh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kironlau",
            "can_mod_post": false,
            "created_utc": 1754166147,
            "send_replies": true,
            "parent_id": "t3_1mfpgae",
            "score": 1,
            "author_fullname": "t2_tb0dz2ds",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "after some code test, changing \"minimum speculative decoding probability\" (greedy) (default: 0.8), to 0.9 could better result\n\n    --draft-p-min 0.90 --draft-min 2 --draft-max 16\n\nhttps://preview.redd.it/hrjwrcuvzngf1.png?width=2539&amp;format=png&amp;auto=webp&amp;s=6c919457daf6bb9a15d97a227b458aa676257769\n\nWhen I was setting draft-p-min as 0.8 (default) , even 0.85. The result of code failed.  \nBut when I change draft-p-min to 0.9, everything works fine.\n\nDraft model= unsloth\\\\Qwen3-0.6B-GGUF\\\\Qwen3-0.6B-Q8\\_0.gguf  \nModel = unsloth\\\\Qwen3-Coder-30B-A3B-Instruct-1M-GGUF\\\\Qwen3-Coder-30B-A3B-Instruct-1M-IQ4\\_XS.gguf",
            "edited": 1754166435,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kz5lh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;after some code test, changing &amp;quot;minimum speculative decoding probability&amp;quot; (greedy) (default: 0.8), to 0.9 could better result&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;--draft-p-min 0.90 --draft-min 2 --draft-max 16\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hrjwrcuvzngf1.png?width=2539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c919457daf6bb9a15d97a227b458aa676257769\"&gt;https://preview.redd.it/hrjwrcuvzngf1.png?width=2539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6c919457daf6bb9a15d97a227b458aa676257769&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;When I was setting draft-p-min as 0.8 (default) , even 0.85. The result of code failed.&lt;br/&gt;\nBut when I change draft-p-min to 0.9, everything works fine.&lt;/p&gt;\n\n&lt;p&gt;Draft model= unsloth\\Qwen3-0.6B-GGUF\\Qwen3-0.6B-Q8_0.gguf&lt;br/&gt;\nModel = unsloth\\Qwen3-Coder-30B-A3B-Instruct-1M-GGUF\\Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_XS.gguf&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/n6kz5lh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754166147,
            "media_metadata": {
              "hrjwrcuvzngf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 57,
                    "x": 108,
                    "u": "https://preview.redd.it/hrjwrcuvzngf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d7249d918e277290c69d90f9a9f27e9bd3ca3ee"
                  },
                  {
                    "y": 115,
                    "x": 216,
                    "u": "https://preview.redd.it/hrjwrcuvzngf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa59e96dfb1439c6a136132f82814c200017a17c"
                  },
                  {
                    "y": 170,
                    "x": 320,
                    "u": "https://preview.redd.it/hrjwrcuvzngf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc31a14f2ee655d2203cb8f243889a6631e1a1e6"
                  },
                  {
                    "y": 341,
                    "x": 640,
                    "u": "https://preview.redd.it/hrjwrcuvzngf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b6865e8f3715efb1278c7d538105c10a47fc6e0"
                  },
                  {
                    "y": 511,
                    "x": 960,
                    "u": "https://preview.redd.it/hrjwrcuvzngf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ac11f0af6613ae979da836fc614ac140bcace52"
                  },
                  {
                    "y": 575,
                    "x": 1080,
                    "u": "https://preview.redd.it/hrjwrcuvzngf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2404956ae1002025a613a78f68ec4a53aa0d75e5"
                  }
                ],
                "s": {
                  "y": 1353,
                  "x": 2539,
                  "u": "https://preview.redd.it/hrjwrcuvzngf1.png?width=2539&amp;format=png&amp;auto=webp&amp;s=6c919457daf6bb9a15d97a227b458aa676257769"
                },
                "id": "hrjwrcuvzngf1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfpgae",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]