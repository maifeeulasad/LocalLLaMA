[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. \n\nI've pulled the project back to make sure there are no differences between what's on the server and what's on my laptop, and my laptop works as expected. \n\nThe server doesn't have access to the outside world, but once everything is up and running, it shouldn't need it, right? \n\nAnyone have any ideas? I've never seen a model behave like this.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "embedding model giving same embeddings regardless of input text?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m6oqxw",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_243il8gu",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753215074,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve pulled the project back to make sure there are no differences between what&amp;#39;s on the server and what&amp;#39;s on my laptop, and my laptop works as expected. &lt;/p&gt;\n\n&lt;p&gt;The server doesn&amp;#39;t have access to the outside world, but once everything is up and running, it shouldn&amp;#39;t need it, right? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas? I&amp;#39;ve never seen a model behave like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m6oqxw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "User1539",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
            "subreddit_subscribers": 503253,
            "created_utc": 1753215074,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n4lnmcj",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "User1539",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n4lleyn",
                                          "score": 2,
                                          "author_fullname": "t2_243il8gu",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "It's hitting the localAI container, and I don't think it's looping back through the proxy. I can see the call hitting the API in the logs, and I'm logging my request and the return, I'm definitely sending different prompts and getting the same embedding.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n4lnmcj",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s hitting the localAI container, and I don&amp;#39;t think it&amp;#39;s looping back through the proxy. I can see the call hitting the API in the logs, and I&amp;#39;m logging my request and the return, I&amp;#39;m definitely sending different prompts and getting the same embedding.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m6oqxw",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4lnmcj/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753219821,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753219821,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "richtext",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n4m59da",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "User1539",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n4lw4k9",
                                                              "score": 1,
                                                              "author_fullname": "t2_243il8gu",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "I wondered if it could have something to do with localai, I'm running CPU only from a docker container. I check the logs, and it says it's up and running, loading the model, and processing the requests. \n\nI haven't seen any error messages at all, which is honestly why I've reached out. Usually, if there's any hint, I can figure it out. But, this says everything is peachy. \n\nThe code is all but a 'hello world' script. It serves an app over a debug server through 8080 in a separate docker container. It uses the OpenAI library to construct a call, and returns the result. I'm outputting both the input text, and output embedding, to stdout, because I'd assumed that somehow I was sending the same data over and over, but coudln't see how in all of 10 lines of code that runs on another system. \n\nNetworking has been completely replaced and the behavior is the same. Now the call never leaves the Docker backbone. The container running the script is talking to the container running localAI without ever going through the host. \n\nI think that leaves configuration of localAI, which I didn't touch. I just grabbed the dockerfile for localai:\n\nFROM localai/localai:latest-cpu \n\nI copy the models directory from the host to the container on build, so I won't have to keep downloading the model, and it shows up and seems to load fine. I'm not seeing any errors in the logs, and I can see a line in the logs saying it processed each api call. \n\nSince I don't really know what the hardware is in the background of this server, I'm counting on localai to configure llama, but I can see where it tells me it has done so successfully and fires up llama.cpp. \n\nI suspected cache too, but like I said, I've all but removed the host's network configuration from the equation, and I'm getting the same results. \n\nI was hoping there was some kind of 'if you do X, embedding models will just return the same embedding' kind of hint or known issue. I've never seen anything like it.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n4m59da",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wondered if it could have something to do with localai, I&amp;#39;m running CPU only from a docker container. I check the logs, and it says it&amp;#39;s up and running, loading the model, and processing the requests. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t seen any error messages at all, which is honestly why I&amp;#39;ve reached out. Usually, if there&amp;#39;s any hint, I can figure it out. But, this says everything is peachy. &lt;/p&gt;\n\n&lt;p&gt;The code is all but a &amp;#39;hello world&amp;#39; script. It serves an app over a debug server through 8080 in a separate docker container. It uses the OpenAI library to construct a call, and returns the result. I&amp;#39;m outputting both the input text, and output embedding, to stdout, because I&amp;#39;d assumed that somehow I was sending the same data over and over, but coudln&amp;#39;t see how in all of 10 lines of code that runs on another system. &lt;/p&gt;\n\n&lt;p&gt;Networking has been completely replaced and the behavior is the same. Now the call never leaves the Docker backbone. The container running the script is talking to the container running localAI without ever going through the host. &lt;/p&gt;\n\n&lt;p&gt;I think that leaves configuration of localAI, which I didn&amp;#39;t touch. I just grabbed the dockerfile for localai:&lt;/p&gt;\n\n&lt;p&gt;FROM localai/localai:latest-cpu &lt;/p&gt;\n\n&lt;p&gt;I copy the models directory from the host to the container on build, so I won&amp;#39;t have to keep downloading the model, and it shows up and seems to load fine. I&amp;#39;m not seeing any errors in the logs, and I can see a line in the logs saying it processed each api call. &lt;/p&gt;\n\n&lt;p&gt;Since I don&amp;#39;t really know what the hardware is in the background of this server, I&amp;#39;m counting on localai to configure llama, but I can see where it tells me it has done so successfully and fires up llama.cpp. &lt;/p&gt;\n\n&lt;p&gt;I suspected cache too, but like I said, I&amp;#39;ve all but removed the host&amp;#39;s network configuration from the equation, and I&amp;#39;m getting the same results. &lt;/p&gt;\n\n&lt;p&gt;I was hoping there was some kind of &amp;#39;if you do X, embedding models will just return the same embedding&amp;#39; kind of hint or known issue. I&amp;#39;ve never seen anything like it.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1m6oqxw",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4m59da/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753225344,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753225344,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n4lw4k9",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Former-Ad-5757",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n4lsh1p",
                                                    "score": 1,
                                                    "author_fullname": "t2_ihsdiwk6k",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "So basically it is either a problem with code, or config or network. I assume that the generation of actual embedding is standard code so that should run equal on your laptop or vmserver. It could be your code, for example maybe you run cuda or amd or mlx code which works on your laptop but not on the server, by default most vms or dockers don’t put a gpu through without separate settings, so is your code only using cpu embedding creating or is it using gpu embedding?\n\nOr else it is config or network.\n\nTry a wireshark or Netscape to look at input and output, is the backend creating the same output.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n4lw4k9",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [
                                                      {
                                                        "e": "text",
                                                        "t": "Llama 3"
                                                      }
                                                    ],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So basically it is either a problem with code, or config or network. I assume that the generation of actual embedding is standard code so that should run equal on your laptop or vmserver. It could be your code, for example maybe you run cuda or amd or mlx code which works on your laptop but not on the server, by default most vms or dockers don’t put a gpu through without separate settings, so is your code only using cpu embedding creating or is it using gpu embedding?&lt;/p&gt;\n\n&lt;p&gt;Or else it is config or network.&lt;/p&gt;\n\n&lt;p&gt;Try a wireshark or Netscape to look at input and output, is the backend creating the same output.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m6oqxw",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": "light",
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4lw4k9/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753222401,
                                                    "author_flair_text": "Llama 3",
                                                    "collapsed": false,
                                                    "created_utc": 1753222401,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": "#c7b594",
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n4lsh1p",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "User1539",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n4lleyn",
                                          "score": 1,
                                          "author_fullname": "t2_243il8gu",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "So, working on your theory that there's a cache in there somewhere, I defined a network in Docker. So, now my Compose.yaml creates a separate network, and I gave the backend and localAI containers their own static IP address on the same subnet. \n\nWithout the request ever leaving the docker containers, it does the same thing. \n\nMy python script is outputting the text that it puts in the API request (through OpenAI's library), and then the embeddings returned. \n\nStill, I see different words going out, and the same embeddings returned. \n\nI repeated this experiment on my laptop, and the result is the same. My laptop works as expected.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n4lsh1p",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So, working on your theory that there&amp;#39;s a cache in there somewhere, I defined a network in Docker. So, now my Compose.yaml creates a separate network, and I gave the backend and localAI containers their own static IP address on the same subnet. &lt;/p&gt;\n\n&lt;p&gt;Without the request ever leaving the docker containers, it does the same thing. &lt;/p&gt;\n\n&lt;p&gt;My python script is outputting the text that it puts in the API request (through OpenAI&amp;#39;s library), and then the embeddings returned. &lt;/p&gt;\n\n&lt;p&gt;Still, I see different words going out, and the same embeddings returned. &lt;/p&gt;\n\n&lt;p&gt;I repeated this experiment on my laptop, and the result is the same. My laptop works as expected.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m6oqxw",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4lsh1p/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753221257,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753221257,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n4lleyn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Former-Ad-5757",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4l8loc",
                                "score": 1,
                                "author_fullname": "t2_ihsdiwk6k",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I would suggest contacting networkguys, for a vm setup you usually want an aggressive caching server to cache all your os updates and regular downloads etc. And as you also say there is a proxy server in play, it certainly sounds like it is going through the proxy server while it shouldn’t. It could be a wrong setting on your end, or a default setting pushed to every vm, or even an over eager transparant proxy.\n\nYou could try connecting to 127.0.0.1 or your internal ip instead of localhost so it skips dns resolving, or perhaps you are trying to access it by its official name and you assume that it is the same as localhost while it is not.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4lleyn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 3"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would suggest contacting networkguys, for a vm setup you usually want an aggressive caching server to cache all your os updates and regular downloads etc. And as you also say there is a proxy server in play, it certainly sounds like it is going through the proxy server while it shouldn’t. It could be a wrong setting on your end, or a default setting pushed to every vm, or even an over eager transparant proxy.&lt;/p&gt;\n\n&lt;p&gt;You could try connecting to 127.0.0.1 or your internal ip instead of localhost so it skips dns resolving, or perhaps you are trying to access it by its official name and you assume that it is the same as localhost while it is not.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m6oqxw",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4lleyn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753219165,
                                "author_flair_text": "Llama 3",
                                "treatment_tags": [],
                                "created_utc": 1753219165,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#c7b594",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4l8loc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "User1539",
                      "can_mod_post": false,
                      "created_utc": 1753215578,
                      "send_replies": true,
                      "parent_id": "t1_n4l7zr2",
                      "score": 1,
                      "author_fullname": "t2_243il8gu",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have a VM hosted at work, behind a proxy server. I can access our git server from it, so I just uploaded to gitlab, then cloned the repository back down. \n\nThe service I'm hosting is just accessible through localhost.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4l8loc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a VM hosted at work, behind a proxy server. I can access our git server from it, so I just uploaded to gitlab, then cloned the repository back down. &lt;/p&gt;\n\n&lt;p&gt;The service I&amp;#39;m hosting is just accessible through localhost.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m6oqxw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4l8loc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753215578,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4l7zr2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Former-Ad-5757",
            "can_mod_post": false,
            "created_utc": 1753215408,
            "send_replies": true,
            "parent_id": "t3_1m6oqxw",
            "score": 1,
            "author_fullname": "t2_ihsdiwk6k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Define external server, because I would guess it is running some kind of caching reverse proxy (transparant or not)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4l7zr2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Define external server, because I would guess it is running some kind of caching reverse proxy (transparant or not)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4l7zr2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753215408,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1m6oqxw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4n6j81",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "No-Mountain3817",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4mf8qu",
                                "score": 1,
                                "author_fullname": "t2_hylfch6q5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "post output of:  \n  \nprint(client)  \nprint(response.model\\_dump\\_json())  \n  \n\\------------------------\n\nOr try these steps to see if you can identify the issue with setup\"\n\nprint(client.base\\_url)\n\nprint(client)\n\n  \n\\# check on both laptop and VM -- whatever client library you're using  \npip show ibm-generative-ai  \n\nprint(response.model\\_dump\\_json(indent=2))  \n\\#look for:  err, status, message  \n\n\n\\#check full embeddings  \nimport numpy as np  \nprint(np.allclose(embedding1, embedding2))",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4n6j81",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;post output of:  &lt;/p&gt;\n\n&lt;p&gt;print(client)&lt;br/&gt;\nprint(response.model_dump_json())  &lt;/p&gt;\n\n&lt;p&gt;------------------------&lt;/p&gt;\n\n&lt;p&gt;Or try these steps to see if you can identify the issue with setup&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;print(client.base_url)&lt;/p&gt;\n\n&lt;p&gt;print(client)&lt;/p&gt;\n\n&lt;p&gt;# check on both laptop and VM -- whatever client library you&amp;#39;re using&lt;br/&gt;\npip show ibm-generative-ai  &lt;/p&gt;\n\n&lt;p&gt;print(response.model_dump_json(indent=2))&lt;br/&gt;\n#look for:  err, status, message  &lt;/p&gt;\n\n&lt;p&gt;#check full embeddings&lt;br/&gt;\nimport numpy as np&lt;br/&gt;\nprint(np.allclose(embedding1, embedding2))&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m6oqxw",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4n6j81/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753237987,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753237987,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4mf8qu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "User1539",
                      "can_mod_post": false,
                      "created_utc": 1753228561,
                      "send_replies": true,
                      "parent_id": "t1_n4mbi1t",
                      "score": 1,
                      "author_fullname": "t2_243il8gu",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What I'm doing:\n\n    def get_embeddings(text):\n        response = client.embeddings.create(\n        model=\"granite-embedding-125m-english\",\n        input=text,\n        encoding_format=\"float\"\n        )\n        query_embedding = response.data[0].embedding\n        print(\"text:\"+text)\n        print(query_embedding,flush=True)\n    return query_embedding\n\nWhat I'm seeing in the log:\n\ntext:apple\n[-0.009278349, -0.037862793, -0.02328104, ...\n\ntext:king\n[-0.009278349, -0.037862793, -0.02328104...\n\nIt works on my laptop, but not on the VM host.",
                      "edited": 1753235996,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4mf8qu",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What I&amp;#39;m doing:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def get_embeddings(text):\n    response = client.embeddings.create(\n    model=&amp;quot;granite-embedding-125m-english&amp;quot;,\n    input=text,\n    encoding_format=&amp;quot;float&amp;quot;\n    )\n    query_embedding = response.data[0].embedding\n    print(&amp;quot;text:&amp;quot;+text)\n    print(query_embedding,flush=True)\nreturn query_embedding\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;What I&amp;#39;m seeing in the log:&lt;/p&gt;\n\n&lt;p&gt;text:apple\n[-0.009278349, -0.037862793, -0.02328104, ...&lt;/p&gt;\n\n&lt;p&gt;text:king\n[-0.009278349, -0.037862793, -0.02328104...&lt;/p&gt;\n\n&lt;p&gt;It works on my laptop, but not on the VM host.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m6oqxw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4mf8qu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753228561,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4mbi1t",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No-Mountain3817",
            "can_mod_post": false,
            "created_utc": 1753227351,
            "send_replies": true,
            "parent_id": "t3_1m6oqxw",
            "score": 1,
            "author_fullname": "t2_hylfch6q5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "  \ntexts = \\[\"hello world\", \"how are you?\", \"machine learning\"\\]\n\nfor text in texts:\n\nprint(\"Input:\", text)\n\nprint(\"Embedding:\", model.encode(text))\n\n  \n  \n\\#hopefully nothing is hardcoded\n\n\\#BAD# embedding = model.encode(\"how are you?\")\n\n\\#GOOD#  \nembeddings = \\[model.encode(t) for t in texts\\]",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4mbi1t",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;texts = [&amp;quot;hello world&amp;quot;, &amp;quot;how are you?&amp;quot;, &amp;quot;machine learning&amp;quot;]&lt;/p&gt;\n\n&lt;p&gt;for text in texts:&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Input:&amp;quot;, text)&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;Embedding:&amp;quot;, model.encode(text))&lt;/p&gt;\n\n&lt;p&gt;#hopefully nothing is hardcoded&lt;/p&gt;\n\n&lt;p&gt;#BAD# embedding = model.encode(&amp;quot;how are you?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;#GOOD#&lt;br/&gt;\nembeddings = [model.encode(t) for t in texts]&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/n4mbi1t/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753227351,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6oqxw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]