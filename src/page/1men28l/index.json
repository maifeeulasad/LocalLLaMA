[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hello r/LocalLLaMA, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by u/send_me_a_ticket (thanks!), just further simplified.\n\n* I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.\n\n# Core Components\n\n* **Engine:** **LM Studio.** Used for downloading models, serving them via a local API, and running the tool server.\n* **Tool Server (RAG):** [**docs-mcp-server**](https://github.com/arabold/docs-mcp-server)**.** Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.\n* **Frontend:** **VS Code +** [**Roo Code**](https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code)**.** The editor extension that connects to the local model server.\n\n# Advantages of this Approach\n\n* **Straightforward Setup:** Uses the LM Studio GUI for most of the configuration.\n* **100% Local &amp; Private:** Code and prompts are not sent to external services.\n* **VRAM-Friendly:** Optimized for running quantized GGUF models on consumer hardware.\n\n# Part 1: Configuring LM Studio\n\n**1. Install LM Studio** Download and install the latest version from the [LM Studio website](https://lmstudio.ai/).\n\n**2. Download Your Models** In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:\n\n* **A Coder LLM:** Example: `qwen/qwen3-coder-30b`\n* **An Embedding Model:** Example: `Qwen/Qwen3-Embedding-0.6B-GGUF`\n\n**3. Tune Model Settings** Navigate to the \"My Models\" tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.\n\nQwen3 doesn't seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.\n\n**4. Configure the** `docs-mcp-server` **Plugin**\n\n* Click the \"Chat\" tab (yellow chat bubble icon on top left).\n* Click on Program on the right.\n* Click on Install, select \\`Edit mcp.json', and replace its entire contents with this:\n\n&amp;#8203;\n\n        {\n          \"mcpServers\": {\n            \"docs-mcp-server\": {\n              \"command\": \"npx\",\n              \"args\": [\n                \"@arabold/docs-mcp-server@latest\"\n              ],\n              \"env\": {\n                \"OPENAI_API_KEY\": \"lmstudio\",\n                \"OPENAI_API_BASE\": \"http://localhost:1234/v1\",\n                \"DOCS_MCP_EMBEDDING_MODEL\": \"text-embedding-qwen3-embedding-0.6b\"\n              }\n            }\n          }\n        }\n\n*Note: Your* `DOCS_MCP_EMBEDDING_MODEL` *value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you'll need to update it here.*\n\nIf it's correct, `the mcp/docs-mcp-server` tab will show things like `Tools`, `scrape_docs`, `search_docs`, ... etc.\n\n**5. Start the Server**\n\n* Navigate to the Local Server tab (`&gt;_` icon on the left).\n* In the top slot, load your coder LLM (e.g., Qwen3-Coder).\n* In the second slot, load your embedding model (e.g., Qwen3-Embeddings).\n* Click **Start Server**.\n* Check the server logs at the bottom to verify that the server is running and the `docs-mcp-server` plugin has loaded correctly.\n\n# Part 2: Configuring VS Code &amp; Roo Code\n\n**1. Install VS Code and Roo Code** Install [Visual Studio Code](https://code.visualstudio.com/). Then, inside VS Code, go to the Extensions tab and search for and install **Roo Code**.\n\n**2. Connect Roo Code to LM Studio**\n\n* In VS Code, click the Roo Code icon in the sidebar.\n* At the bottom, click the gear icon next to your profile name to open the settings.\n* Click **Add Profile**, give it a name (e.g., \"LM Studio\"), and configure it:\n* **LM Provider:** Select `LM Studio`\n* **Base URL:** [`http://127.0.0.1:1234`](http://127.0.0.1:1234) (or your server address)\n* **Model:** Select your coder model's ID (e.g., `qwen/qwen3-coder-30b`, it should appear automatically) .\n* While in the settings, you can go through the other tabs (like \"Auto-Approve\") and toggle preferences to fit your workflow.\n\n**3. Connect Roo Code to the Tool Server** Finally, we have to expose the mcp server to Roo.\n\n* In the Roo Code settings panel, click the 3 horizontal dots (top right), select \"MCP Servers\" from the drop-down menu.\n* Ensure the **\"Enable MCP Servers\"** checkbox is **ENABLED**.\n* Scroll down and click \"Edit Global MCP\", and replace the contents (if any) with this:\n\n&amp;#8203;\n\n    {\n      \"mcpServers\": {\n        \"docs-mcp-server\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"@arabold/docs-mcp-server@latest\"\n          ],\n          \"env\": {\n            \"OPENAI_API_KEY\": \"lmstudio\",\n            \"OPENAI_API_BASE\": \"http://localhost:1234/v1\",\n            \"DOCS_MCP_EMBEDDING_MODEL\": \"text-embedding-qwen3-embedding-0.6b\"\n          },\n          \"alwaysAllow\": [\n            \"fetch_url\",\n            \"remove_docs\",\n            \"scrape_docs\",\n            \"search_docs\",\n            \"list_libraries\",\n            \"find_version\",\n            \"list_jobs\",\n            \"get_job_info\",\n            \"cancel_job\"\n          ],\n          \"disabled\": false\n        }\n      }\n    }\n\n*Note: I'm not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.*\n\nThen you can toggle it on and see a green circle if there's no issues.\n\nYour setup is now complete. You have a local coding assistant that can use the `docs-mcp-server` to perform RAG against documentation you provide.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Guide] The *SIMPLE* Self-Hosted AI Coding That Just Works feat. Qwen3-Coder-Flash",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1men28l",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 85,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_kggm5",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 85,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754031114,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754022492,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by &lt;a href=\"/u/send_me_a_ticket\"&gt;u/send_me_a_ticket&lt;/a&gt; (thanks!), just further simplified.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Core Components&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Engine:&lt;/strong&gt; &lt;strong&gt;LM Studio.&lt;/strong&gt; Used for downloading models, serving them via a local API, and running the tool server.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Tool Server (RAG):&lt;/strong&gt; &lt;a href=\"https://github.com/arabold/docs-mcp-server\"&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;strong&gt;VS Code +&lt;/strong&gt; &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code\"&gt;&lt;strong&gt;Roo Code&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; The editor extension that connects to the local model server.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Advantages of this Approach&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Straightforward Setup:&lt;/strong&gt; Uses the LM Studio GUI for most of the configuration.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; Code and prompts are not sent to external services.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;VRAM-Friendly:&lt;/strong&gt; Optimized for running quantized GGUF models on consumer hardware.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Part 1: Configuring LM Studio&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Install LM Studio&lt;/strong&gt; Download and install the latest version from the &lt;a href=\"https://lmstudio.ai/\"&gt;LM Studio website&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Download Your Models&lt;/strong&gt; In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;A Coder LLM:&lt;/strong&gt; Example: &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;An Embedding Model:&lt;/strong&gt; Example: &lt;code&gt;Qwen/Qwen3-Embedding-0.6B-GGUF&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Tune Model Settings&lt;/strong&gt; Navigate to the &amp;quot;My Models&amp;quot; tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.&lt;/p&gt;\n\n&lt;p&gt;Qwen3 doesn&amp;#39;t seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4. Configure the&lt;/strong&gt; &lt;code&gt;docs-mcp-server&lt;/code&gt; &lt;strong&gt;Plugin&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Click the &amp;quot;Chat&amp;quot; tab (yellow chat bubble icon on top left).&lt;/li&gt;\n&lt;li&gt;Click on Program on the right.&lt;/li&gt;\n&lt;li&gt;Click on Install, select `Edit mcp.json&amp;#39;, and replace its entire contents with this:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    {\n      &amp;quot;mcpServers&amp;quot;: {\n        &amp;quot;docs-mcp-server&amp;quot;: {\n          &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n          &amp;quot;args&amp;quot;: [\n            &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\n          ],\n          &amp;quot;env&amp;quot;: {\n            &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;,\n            &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;,\n            &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot;\n          }\n        }\n      }\n    }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Note: Your&lt;/em&gt; &lt;code&gt;DOCS_MCP_EMBEDDING_MODEL&lt;/code&gt; &lt;em&gt;value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you&amp;#39;ll need to update it here.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;If it&amp;#39;s correct, &lt;code&gt;the mcp/docs-mcp-server&lt;/code&gt; tab will show things like &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;scrape_docs&lt;/code&gt;, &lt;code&gt;search_docs&lt;/code&gt;, ... etc.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5. Start the Server&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Navigate to the Local Server tab (&lt;code&gt;&amp;gt;_&lt;/code&gt; icon on the left).&lt;/li&gt;\n&lt;li&gt;In the top slot, load your coder LLM (e.g., Qwen3-Coder).&lt;/li&gt;\n&lt;li&gt;In the second slot, load your embedding model (e.g., Qwen3-Embeddings).&lt;/li&gt;\n&lt;li&gt;Click &lt;strong&gt;Start Server&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Check the server logs at the bottom to verify that the server is running and the &lt;code&gt;docs-mcp-server&lt;/code&gt; plugin has loaded correctly.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Part 2: Configuring VS Code &amp;amp; Roo Code&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Install VS Code and Roo Code&lt;/strong&gt; Install &lt;a href=\"https://code.visualstudio.com/\"&gt;Visual Studio Code&lt;/a&gt;. Then, inside VS Code, go to the Extensions tab and search for and install &lt;strong&gt;Roo Code&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Connect Roo Code to LM Studio&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In VS Code, click the Roo Code icon in the sidebar.&lt;/li&gt;\n&lt;li&gt;At the bottom, click the gear icon next to your profile name to open the settings.&lt;/li&gt;\n&lt;li&gt;Click &lt;strong&gt;Add Profile&lt;/strong&gt;, give it a name (e.g., &amp;quot;LM Studio&amp;quot;), and configure it:&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LM Provider:&lt;/strong&gt; Select &lt;code&gt;LM Studio&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Base URL:&lt;/strong&gt; &lt;a href=\"http://127.0.0.1:1234\"&gt;&lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;&lt;/a&gt; (or your server address)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Select your coder model&amp;#39;s ID (e.g., &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;, it should appear automatically) .&lt;/li&gt;\n&lt;li&gt;While in the settings, you can go through the other tabs (like &amp;quot;Auto-Approve&amp;quot;) and toggle preferences to fit your workflow.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Connect Roo Code to the Tool Server&lt;/strong&gt; Finally, we have to expose the mcp server to Roo.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In the Roo Code settings panel, click the 3 horizontal dots (top right), select &amp;quot;MCP Servers&amp;quot; from the drop-down menu.&lt;/li&gt;\n&lt;li&gt;Ensure the &lt;strong&gt;&amp;quot;Enable MCP Servers&amp;quot;&lt;/strong&gt; checkbox is &lt;strong&gt;ENABLED&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;Scroll down and click &amp;quot;Edit Global MCP&amp;quot;, and replace the contents (if any) with this:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;docs-mcp-server&amp;quot;: {\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\n      &amp;quot;args&amp;quot;: [\n        &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\n      ],\n      &amp;quot;env&amp;quot;: {\n        &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;,\n        &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;,\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot;\n      },\n      &amp;quot;alwaysAllow&amp;quot;: [\n        &amp;quot;fetch_url&amp;quot;,\n        &amp;quot;remove_docs&amp;quot;,\n        &amp;quot;scrape_docs&amp;quot;,\n        &amp;quot;search_docs&amp;quot;,\n        &amp;quot;list_libraries&amp;quot;,\n        &amp;quot;find_version&amp;quot;,\n        &amp;quot;list_jobs&amp;quot;,\n        &amp;quot;get_job_info&amp;quot;,\n        &amp;quot;cancel_job&amp;quot;\n      ],\n      &amp;quot;disabled&amp;quot;: false\n    }\n  }\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;Note: I&amp;#39;m not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Then you can toggle it on and see a green circle if there&amp;#39;s no issues.&lt;/p&gt;\n\n&lt;p&gt;Your setup is now complete. You have a local coding assistant that can use the &lt;code&gt;docs-mcp-server&lt;/code&gt; to perform RAG against documentation you provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?auto=webp&amp;s=c9b66d5932995b559501dedad243d4991822c62b",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5dca5318ae2d95c180426ac49239f78614273fd3",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d21c6913689d6baa653b4499775bf99e5e67b83",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce1aca8605a116b21a8c31ef08a357697c4e5ce6",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10ea5ed6a277615b50b6e389b4686099afd9f5a9",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=37054aedb1b74092c1a5c30ab58106206841963e",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a8a940e26cd7cbb500200fee501049b651a48b4d",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "5qNLoYTlU6g2KP0U9SNcDZSX-5r69IwrGD3EnHxY9pk"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1men28l",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "xrailgun",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/",
            "subreddit_subscribers": 508773,
            "created_utc": 1754022492,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6b2cs5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "xrailgun",
                      "can_mod_post": false,
                      "created_utc": 1754030717,
                      "send_replies": true,
                      "parent_id": "t1_n6b18jk",
                      "score": 6,
                      "author_fullname": "t2_kggm5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oops... you are right, I've just been running without functionally using it. Turns out it can still work by setting up Roo Code's MCP Servers settings page. Thanks for pointing this out. I'll update the main post.",
                      "edited": 1754031142,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6b2cs5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oops... you are right, I&amp;#39;ve just been running without functionally using it. Turns out it can still work by setting up Roo Code&amp;#39;s MCP Servers settings page. Thanks for pointing this out. I&amp;#39;ll update the main post.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1men28l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6b2cs5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754030717,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6b18jk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Dry-Assistance-367",
            "can_mod_post": false,
            "created_utc": 1754030101,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 15,
            "author_fullname": "t2_f0cffydkx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MCP servers setup in LM Studio only work in the chat window in LM Studio, they do not work in the api server.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6b18jk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MCP servers setup in LM Studio only work in the chat window in LM Studio, they do not work in the api server.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6b18jk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754030101,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 15
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6azbky",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "xrailgun",
                      "can_mod_post": false,
                      "created_utc": 1754029067,
                      "send_replies": true,
                      "parent_id": "t1_n6anprd",
                      "score": 1,
                      "author_fullname": "t2_kggm5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "sorry I've never heard of Windsurf. it looks great, will play around when I have time.\n\nEDIT: Looks like it's accessing the same VSX marketplace as VS Code, so it *probably* has identical behaviour to the extension in VS Code.",
                      "edited": 1754033861,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6azbky",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;sorry I&amp;#39;ve never heard of Windsurf. it looks great, will play around when I have time.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Looks like it&amp;#39;s accessing the same VSX marketplace as VS Code, so it &lt;em&gt;probably&lt;/em&gt; has identical behaviour to the extension in VS Code.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1men28l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6azbky/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754029067,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6anprd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "prusswan",
            "can_mod_post": false,
            "created_utc": 1754023270,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 2,
            "author_fullname": "t2_kegwk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "does it matter if you use Roo Code under VS Code or Windsurf?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6anprd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;does it matter if you use Roo Code under VS Code or Windsurf?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6anprd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754023270,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6bqoop",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Danmoreng",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6bpj44",
                                "score": 4,
                                "author_fullname": "t2_7z26p",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "IQ4_XS from here: https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF\n\nCurrently polishing my install script for windows: https://github.com/Danmoreng/local-qwen3-coder-env",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6bqoop",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;IQ4_XS from here: &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently polishing my install script for windows: &lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1men28l",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6bqoop/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754044237,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754044237,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6bpj44",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "jasonstathame900",
                      "can_mod_post": false,
                      "created_utc": 1754043647,
                      "send_replies": true,
                      "parent_id": "t1_n6bk5se",
                      "score": 2,
                      "author_fullname": "t2_8r2crxlm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Which variant of qwen30b coder are you running. I have a similar spec. Vram is 16gb(ti super), 7800x3d. Running 3bit on lmstudio",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6bpj44",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Which variant of qwen30b coder are you running. I have a similar spec. Vram is 16gb(ti super), 7800x3d. Running 3bit on lmstudio&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1men28l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6bpj44/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754043647,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6bk5se",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Danmoreng",
            "can_mod_post": false,
            "created_utc": 1754040732,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 2,
            "author_fullname": "t2_7z26p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I get ~20 T/s in LMStudio vs ~35 T/s with ik_llama.cpp on my setup.\n\n&gt; Ryzen 5 7600\n\n&gt; 32 GB RAM 5600\n\n&gt; RTx 4070 Ti 12GB",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bk5se",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I get ~20 T/s in LMStudio vs ~35 T/s with ik_llama.cpp on my setup.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Ryzen 5 7600&lt;/p&gt;\n\n&lt;p&gt;32 GB RAM 5600&lt;/p&gt;\n\n&lt;p&gt;RTx 4070 Ti 12GB&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6bk5se/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754040732,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6fqkre",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "xrailgun",
                      "can_mod_post": false,
                      "created_utc": 1754089759,
                      "send_replies": true,
                      "parent_id": "t1_n6bs118",
                      "score": 2,
                      "author_fullname": "t2_kggm5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is it adding the same line or trying to read the same file repeatedly?\n\nIf the former it's probably just model settings like temperature, repeat penalty, etc.\n\nIf the latter, something probably went wrong with the mcp server setup, or it wasn't enabled.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6fqkre",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is it adding the same line or trying to read the same file repeatedly?&lt;/p&gt;\n\n&lt;p&gt;If the former it&amp;#39;s probably just model settings like temperature, repeat penalty, etc.&lt;/p&gt;\n\n&lt;p&gt;If the latter, something probably went wrong with the mcp server setup, or it wasn&amp;#39;t enabled.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1men28l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6fqkre/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754089759,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6bs118",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "prusswan",
            "can_mod_post": false,
            "created_utc": 1754044904,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 1,
            "author_fullname": "t2_kegwk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is there some timeout setting that can be used in case the agent gets stuck? I have seen qwen3 coder keeps adding the same line to a file and needs to be interrupted",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bs118",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is there some timeout setting that can be used in case the agent gets stuck? I have seen qwen3 coder keeps adding the same line to a file and needs to be interrupted&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6bs118/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754044904,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6c4gnb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "1Neokortex1",
            "can_mod_post": false,
            "created_utc": 1754050230,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 1,
            "author_fullname": "t2_ov31f295",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": " Thank you!\nI have been researching Cline, and never heard of Roo. I see its a fork version of cline.\nDoes it matter if you use roo or cline with this setup?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c4gnb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt; Thank you!\nI have been researching Cline, and never heard of Roo. I see its a fork version of cline.\nDoes it matter if you use roo or cline with this setup?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6c4gnb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754050230,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6fst8g",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "false79",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6fp9ss",
                                "score": 2,
                                "author_fullname": "t2_wn888",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "ahh thx for steering me into the right direction. I thought I had had to set up the doc-mcp-server as a stand alone service.\n\nBut now that I look at what the mcp.json is doing in LM Studio, it's using npx command locally to set it up with the env json as arguments.\n\nEdit: Woohoo. Uninstall and re-install mcp plugin within LM Studio worked\n\nhttps://preview.redd.it/ne2in9gswhgf1.png?width=999&amp;format=png&amp;auto=webp&amp;s=cb817cff1ab354a080ca876a841dd11c47ce744a",
                                "edited": 1754092257,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6fst8g",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ahh thx for steering me into the right direction. I thought I had had to set up the doc-mcp-server as a stand alone service.&lt;/p&gt;\n\n&lt;p&gt;But now that I look at what the mcp.json is doing in LM Studio, it&amp;#39;s using npx command locally to set it up with the env json as arguments.&lt;/p&gt;\n\n&lt;p&gt;Edit: Woohoo. Uninstall and re-install mcp plugin within LM Studio worked&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ne2in9gswhgf1.png?width=999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb817cff1ab354a080ca876a841dd11c47ce744a\"&gt;https://preview.redd.it/ne2in9gswhgf1.png?width=999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb817cff1ab354a080ca876a841dd11c47ce744a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1men28l",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6fst8g/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754090524,
                                "media_metadata": {
                                  "ne2in9gswhgf1": {
                                    "status": "valid",
                                    "e": "Image",
                                    "m": "image/png",
                                    "p": [
                                      {
                                        "y": 60,
                                        "x": 108,
                                        "u": "https://preview.redd.it/ne2in9gswhgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0ee6a4d9ff30cebc183a9102032099cd4c0e76f"
                                      },
                                      {
                                        "y": 121,
                                        "x": 216,
                                        "u": "https://preview.redd.it/ne2in9gswhgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbe0ad69eafcbd601d6d46636c0b5f96302ce0b9"
                                      },
                                      {
                                        "y": 179,
                                        "x": 320,
                                        "u": "https://preview.redd.it/ne2in9gswhgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1a785930af6df6cc79676a6500b67bc316f10f8"
                                      },
                                      {
                                        "y": 358,
                                        "x": 640,
                                        "u": "https://preview.redd.it/ne2in9gswhgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56651b60f6fdeb46f7e0a01b08a116fab6fecbda"
                                      },
                                      {
                                        "y": 538,
                                        "x": 960,
                                        "u": "https://preview.redd.it/ne2in9gswhgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf16250735b274a5d1a74473b87f71a732625a98"
                                      }
                                    ],
                                    "s": {
                                      "y": 560,
                                      "x": 999,
                                      "u": "https://preview.redd.it/ne2in9gswhgf1.png?width=999&amp;format=png&amp;auto=webp&amp;s=cb817cff1ab354a080ca876a841dd11c47ce744a"
                                    },
                                    "id": "ne2in9gswhgf1"
                                  }
                                },
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754090524,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6fp9ss",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "xrailgun",
                      "can_mod_post": false,
                      "created_utc": 1754089313,
                      "send_replies": true,
                      "parent_id": "t1_n6dy4ph",
                      "score": 1,
                      "author_fullname": "t2_kggm5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This tutorial doesn't even touch docker at all. Both mcp jsons are within the LM Studio and VS Code/Roo apps entirely.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6fp9ss",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This tutorial doesn&amp;#39;t even touch docker at all. Both mcp jsons are within the LM Studio and VS Code/Roo apps entirely.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1men28l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6fp9ss/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754089313,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6dy4ph",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "false79",
            "can_mod_post": false,
            "created_utc": 1754070008,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 1,
            "author_fullname": "t2_wn888",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I am struggling to get the docs-mcp-server running in Docker Desktop Windows\n\nI defined:\n\nDOCS\\_MCP\\_EMBEDDING\\_MODEL=\"text-embedding-qwen3-embedding-0.6b\"  \nOPENAI\\_API\\_KEY=\"lmstudio\"  \nOPENAI\\_API\\_BASE=\"http://192.168.50.147:1234\" # Location of where LM Studio is running qwen3-coder\n\nin both the .env file as well as passing it through Docker's Run Container's Environmental Variables.\n\n Error in main: ConnectionError: Failed to initialize database connection caused by TypeError: Cannot read properties of undefined (reading '0')\n\nIs there a certain version of \n\n    @arabold/docs-mcp-server\n\nwhere this tutorial works?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dy4ph",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am struggling to get the docs-mcp-server running in Docker Desktop Windows&lt;/p&gt;\n\n&lt;p&gt;I defined:&lt;/p&gt;\n\n&lt;p&gt;DOCS_MCP_EMBEDDING_MODEL=&amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot;&lt;br/&gt;\nOPENAI_API_KEY=&amp;quot;lmstudio&amp;quot;&lt;br/&gt;\nOPENAI_API_BASE=&amp;quot;&lt;a href=\"http://192.168.50.147:1234\"&gt;http://192.168.50.147:1234&lt;/a&gt;&amp;quot; # Location of where LM Studio is running qwen3-coder&lt;/p&gt;\n\n&lt;p&gt;in both the .env file as well as passing it through Docker&amp;#39;s Run Container&amp;#39;s Environmental Variables.&lt;/p&gt;\n\n&lt;p&gt; Error in main: ConnectionError: Failed to initialize database connection caused by TypeError: Cannot read properties of undefined (reading &amp;#39;0&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;Is there a certain version of &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@arabold/docs-mcp-server\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;where this tutorial works?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6dy4ph/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754070008,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6fq8qd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "xrailgun",
                      "can_mod_post": false,
                      "created_utc": 1754089644,
                      "send_replies": true,
                      "parent_id": "t1_n6ekfrs",
                      "score": 1,
                      "author_fullname": "t2_kggm5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It depends on the model size and speed you want, and system RAM. \n\nYou can run models at slowish speeds entirely on CPU. Anything that runs on LM Studio (llama.cpp backend) should work.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6fq8qd",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends on the model size and speed you want, and system RAM. &lt;/p&gt;\n\n&lt;p&gt;You can run models at slowish speeds entirely on CPU. Anything that runs on LM Studio (llama.cpp backend) should work.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1men28l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6fq8qd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754089644,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ekfrs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "National-Ad-1314",
            "can_mod_post": false,
            "created_utc": 1754076505,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 1,
            "author_fullname": "t2_g0jvn5au",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sorry does this require the usual heavy GPU set up?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ekfrs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry does this require the usual heavy GPU set up?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6ekfrs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754076505,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bfpdy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cantgetthistowork",
            "can_mod_post": false,
            "created_utc": 1754038188,
            "send_replies": true,
            "parent_id": "t3_1men28l",
            "score": 0,
            "author_fullname": "t2_j1i0o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "docs-mcp-server equivalent for ik_llama.cpp?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bfpdy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;docs-mcp-server equivalent for ik_llama.cpp?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/n6bfpdy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754038188,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1men28l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]