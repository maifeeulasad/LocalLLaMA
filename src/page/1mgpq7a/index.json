[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi LocalLLaMA,\n\nI’m a bit confused on two levels and need help: \n\n1) What are the best settings to get ollama to utilize all (6) 3090’s so I can use parallel processing. \n\n2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? \n\nAny recommendations on models? \n\nMy use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.\n\nSomeone previously referenced using CommandR+ 104b but I couldn’t get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). \n\nI think another person previously referenced Gemma 27b. I haven’t tried that yet. \n\nI’m a bit lost on configs. \n\nAlso someone suggested vllm instead but I couldn’t seem to get it to work, even with a small model. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Need help- unsure of right ollama configs with 6x 3090’s, also model choice for RAG?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgpq7a",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.44,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rkb6qbej1",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754243683,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LocalLLaMA,&lt;/p&gt;\n\n&lt;p&gt;I’m a bit confused on two levels and need help: &lt;/p&gt;\n\n&lt;p&gt;1) What are the best settings to get ollama to utilize all (6) 3090’s so I can use parallel processing. &lt;/p&gt;\n\n&lt;p&gt;2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? &lt;/p&gt;\n\n&lt;p&gt;Any recommendations on models? &lt;/p&gt;\n\n&lt;p&gt;My use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.&lt;/p&gt;\n\n&lt;p&gt;Someone previously referenced using CommandR+ 104b but I couldn’t get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). &lt;/p&gt;\n\n&lt;p&gt;I think another person previously referenced Gemma 27b. I haven’t tried that yet. &lt;/p&gt;\n\n&lt;p&gt;I’m a bit lost on configs. &lt;/p&gt;\n\n&lt;p&gt;Also someone suggested vllm instead but I couldn’t seem to get it to work, even with a small model. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgpq7a",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "Business-Weekend-537",
            "discussion_type": null,
            "num_comments": 15,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
            "subreddit_subscribers": 509914,
            "created_utc": 1754243683,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6qeyu4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Business-Weekend-537",
                      "can_mod_post": false,
                      "created_utc": 1754244542,
                      "send_replies": true,
                      "parent_id": "t1_n6qdegu",
                      "score": 1,
                      "author_fullname": "t2_rkb6qbej1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Open frame, AsRock ROMED8-2T (7x pcie 4.0. x16) \n\n6x pcie 4.0 x16 for 3090’s\n1x pcie 4.0 x4x4x4x4 for asus m.2 HyperCard (4) nvme adapter \n\nMotherboard only supports m.2 and not nvme natively.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6qeyu4",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Open frame, AsRock ROMED8-2T (7x pcie 4.0. x16) &lt;/p&gt;\n\n&lt;p&gt;6x pcie 4.0 x16 for 3090’s\n1x pcie 4.0 x4x4x4x4 for asus m.2 HyperCard (4) nvme adapter &lt;/p&gt;\n\n&lt;p&gt;Motherboard only supports m.2 and not nvme natively.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgpq7a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qeyu4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754244542,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6qdegu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Expensive_Mirror5247",
            "can_mod_post": false,
            "created_utc": 1754244066,
            "send_replies": true,
            "parent_id": "t3_1mgpq7a",
            "score": 3,
            "author_fullname": "t2_8alx42ew",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "6 3090's? holy fuck bro thats a BEEEEEEEEEEEAST are they in an open frame case or?  got pics? what kind of board are you running them off of?  are you using an expansion bus or were you able to find a decent board with 6 slots?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qdegu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;6 3090&amp;#39;s? holy fuck bro thats a BEEEEEEEEEEEAST are they in an open frame case or?  got pics? what kind of board are you running them off of?  are you using an expansion bus or were you able to find a decent board with 6 slots?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qdegu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754244066,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpq7a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n6qov4v",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "Business-Weekend-537",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n6qji43",
                                                              "score": 1,
                                                              "author_fullname": "t2_rkb6qbej1",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Fair enough. Thanks for the links",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n6qov4v",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fair enough. Thanks for the links&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mgpq7a",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qov4v/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754247594,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754247594,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6qji43",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "ubrtnk",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6qgpy5",
                                                    "score": 2,
                                                    "author_fullname": "t2_7b5i2",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "[https://docs.vllm.ai/en/latest/](https://docs.vllm.ai/en/latest/) \\- Obviously this is the most accurate starting point. The OpenAI Compatible Server section is how you get OWUI to talk to vLLM. It'll be a Many to 1 configuration of models end points in OWUI's configuration, not like with Ollama where Ollama plays the router. \n\n[https://ploomber.io/blog/vllm-deploy/](https://ploomber.io/blog/vllm-deploy/) \\- I used this guide + the help of ChatGPT free to deploy my system a few months ago after I got back from Red Hat's conference in Boston. (I ended up going back to Ollama because I'm lazy and only have 2x 3090s lol)",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6qji43",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://docs.vllm.ai/en/latest/\"&gt;https://docs.vllm.ai/en/latest/&lt;/a&gt; - Obviously this is the most accurate starting point. The OpenAI Compatible Server section is how you get OWUI to talk to vLLM. It&amp;#39;ll be a Many to 1 configuration of models end points in OWUI&amp;#39;s configuration, not like with Ollama where Ollama plays the router. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ploomber.io/blog/vllm-deploy/\"&gt;https://ploomber.io/blog/vllm-deploy/&lt;/a&gt; - I used this guide + the help of ChatGPT free to deploy my system a few months ago after I got back from Red Hat&amp;#39;s conference in Boston. (I ended up going back to Ollama because I&amp;#39;m lazy and only have 2x 3090s lol)&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mgpq7a",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qji43/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754245931,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754245931,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6qgpy5",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Business-Weekend-537",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6qgj9d",
                                          "score": 1,
                                          "author_fullname": "t2_rkb6qbej1",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Got it can you point me to any good vllm tutorials or instructions? I’ve read a couple and couldn’t get it to work yet.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6qgpy5",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Got it can you point me to any good vllm tutorials or instructions? I’ve read a couple and couldn’t get it to work yet.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mgpq7a",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qgpy5/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754245077,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754245077,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6qgj9d",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ubrtnk",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qf5dy",
                                "score": 2,
                                "author_fullname": "t2_7b5i2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "vLLM definitely does - OWUI only needs an OpenAI compatible API endpoint - however, the ease of Ollama's WYSIWYG is gone with the more advanced capabilities that vLLM (and the others). Tensor Parallelism is nice but if you have a desire to have multiple models available ad-hoc or simultaneously, you'll need to add in something like LlamaSwap or configure the vLLM instances (its a 1:1 ratio of service to model) to only use a subset of your total available vRAM, otherwise vLLM will see 144GB of vRAM and say thank you, may I have some more.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6qgj9d",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vLLM definitely does - OWUI only needs an OpenAI compatible API endpoint - however, the ease of Ollama&amp;#39;s WYSIWYG is gone with the more advanced capabilities that vLLM (and the others). Tensor Parallelism is nice but if you have a desire to have multiple models available ad-hoc or simultaneously, you&amp;#39;ll need to add in something like LlamaSwap or configure the vLLM instances (its a 1:1 ratio of service to model) to only use a subset of your total available vRAM, otherwise vLLM will see 144GB of vRAM and say thank you, may I have some more.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgpq7a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qgj9d/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754245019,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754245019,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6t72h3",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "TyraVex",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6snljv",
                                                    "score": 1,
                                                    "author_fullname": "t2_5u6lp8ar",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=1399&amp;format=png&amp;auto=webp&amp;s=4000e53b50a53b85b4d391b2cf88c1687189b081\n\nTabby works for Exllama, so EXL2 and EXL3 formats\n\nThere is an quivalent for GGUF but I haven't tested: [https://github.com/theroyallab/YALS](https://github.com/theroyallab/YALS)",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6t72h3",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/2gnt8z2eaxgf1.png?width=1399&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4000e53b50a53b85b4d391b2cf88c1687189b081\"&gt;https://preview.redd.it/2gnt8z2eaxgf1.png?width=1399&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4000e53b50a53b85b4d391b2cf88c1687189b081&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tabby works for Exllama, so EXL2 and EXL3 formats&lt;/p&gt;\n\n&lt;p&gt;There is an quivalent for GGUF but I haven&amp;#39;t tested: &lt;a href=\"https://github.com/theroyallab/YALS\"&gt;https://github.com/theroyallab/YALS&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mgpq7a",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6t72h3/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754278443,
                                                    "media_metadata": {
                                                      "2gnt8z2eaxgf1": {
                                                        "status": "valid",
                                                        "e": "Image",
                                                        "m": "image/png",
                                                        "p": [
                                                          {
                                                            "y": 84,
                                                            "x": 108,
                                                            "u": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e86478d362a5cc8f840e3fb12fe918d1c6894a07"
                                                          },
                                                          {
                                                            "y": 169,
                                                            "x": 216,
                                                            "u": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e868e982e07ad712cc7501bb3ff1c36b77d20ab2"
                                                          },
                                                          {
                                                            "y": 251,
                                                            "x": 320,
                                                            "u": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee562802f0824ce89f11022e0ea56472499d81da"
                                                          },
                                                          {
                                                            "y": 502,
                                                            "x": 640,
                                                            "u": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b89b6c736b3786ce0caf21fe0e2c5aad632b0ce9"
                                                          },
                                                          {
                                                            "y": 754,
                                                            "x": 960,
                                                            "u": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1cda284a41d5146e718789de7dde9324132fa1e"
                                                          },
                                                          {
                                                            "y": 848,
                                                            "x": 1080,
                                                            "u": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ddc09143ff5ab0eb42ee37ee41581dd63fd62bc"
                                                          }
                                                        ],
                                                        "s": {
                                                          "y": 1099,
                                                          "x": 1399,
                                                          "u": "https://preview.redd.it/2gnt8z2eaxgf1.png?width=1399&amp;format=png&amp;auto=webp&amp;s=4000e53b50a53b85b4d391b2cf88c1687189b081"
                                                        },
                                                        "id": "2gnt8z2eaxgf1"
                                                      }
                                                    },
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754278443,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6snljv",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Business-Weekend-537",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6s5xrj",
                                          "score": 1,
                                          "author_fullname": "t2_rkb6qbej1",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Does Tabby work with Ggufs? Or is it only special formats?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6snljv",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does Tabby work with Ggufs? Or is it only special formats?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mgpq7a",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6snljv/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754271010,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754271010,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6s5xrj",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "TyraVex",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qf5dy",
                                "score": 1,
                                "author_fullname": "t2_5u6lp8ar",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yes, Tabby works perfectly on my end. I find it simpler than vLLM and more efficient VRAM wise. There’s only one config file with around 40 options, each documented within the file itself: [config_sample.yml](https://github.com/theroyallab/tabbyAPI/blob/main/config_sample.yml).  \n\nFor automatic individual model configurations (like `llama-swap`), you can simply create additional config files inside each LLM folder to apply different settings.  \n\nThe only downside is that some obscure quantized models aren’t available on Hugging Face.",
                                "edited": 1754264927,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6s5xrj",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, Tabby works perfectly on my end. I find it simpler than vLLM and more efficient VRAM wise. There’s only one config file with around 40 options, each documented within the file itself: &lt;a href=\"https://github.com/theroyallab/tabbyAPI/blob/main/config_sample.yml\"&gt;config_sample.yml&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;For automatic individual model configurations (like &lt;code&gt;llama-swap&lt;/code&gt;), you can simply create additional config files inside each LLM folder to apply different settings.  &lt;/p&gt;\n\n&lt;p&gt;The only downside is that some obscure quantized models aren’t available on Hugging Face.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgpq7a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6s5xrj/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754264708,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754264708,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6tdaeg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pale_Increase9204",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qf5dy",
                                "score": 1,
                                "author_fullname": "t2_11r1qsoz6o",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Go with vLLM, it will distribute the model across GPUs, and it's a lot faster. See if V1 is supported on RTX 3090; if so, it would be so much faster than OLLAMA could ever dream of.\n\nMy recomandation:\n\nTry to go with MoE arch instead of dense, less VRAM, faster,...\n\nIf you wanna use an embedding model for your RAG, try to use it on CPUs cuz embedding models aren't that huge.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6tdaeg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Go with vLLM, it will distribute the model across GPUs, and it&amp;#39;s a lot faster. See if V1 is supported on RTX 3090; if so, it would be so much faster than OLLAMA could ever dream of.&lt;/p&gt;\n\n&lt;p&gt;My recomandation:&lt;/p&gt;\n\n&lt;p&gt;Try to go with MoE arch instead of dense, less VRAM, faster,...&lt;/p&gt;\n\n&lt;p&gt;If you wanna use an embedding model for your RAG, try to use it on CPUs cuz embedding models aren&amp;#39;t that huge.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgpq7a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6tdaeg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754281136,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754281136,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6qf5dy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Business-Weekend-537",
                      "can_mod_post": false,
                      "created_utc": 1754244597,
                      "send_replies": true,
                      "parent_id": "t1_n6qed5p",
                      "score": 1,
                      "author_fullname": "t2_rkb6qbej1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Will the other options you referenced play nicely with openwebui?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6qf5dy",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Will the other options you referenced play nicely with openwebui?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgpq7a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qf5dy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754244597,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6qed5p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TyraVex",
            "can_mod_post": false,
            "created_utc": 1754244357,
            "send_replies": true,
            "parent_id": "t3_1mgpq7a",
            "score": 2,
            "author_fullname": "t2_5u6lp8ar",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "llama.cpp or ollama is not efficient with multiple GPUs\n\nEXL2, vLLM, and Sglang support tensor parallelism to use all GPUs at the same time, the most friendly and VRAM-efficient being tabbyAPI, which uses EXL2 or EXL3 as its backend. EXL3 tensor parallelism is coming soon (dev branch), but I don't think we can use it yet.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qed5p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llama.cpp or ollama is not efficient with multiple GPUs&lt;/p&gt;\n\n&lt;p&gt;EXL2, vLLM, and Sglang support tensor parallelism to use all GPUs at the same time, the most friendly and VRAM-efficient being tabbyAPI, which uses EXL2 or EXL3 as its backend. EXL3 tensor parallelism is coming soon (dev branch), but I don&amp;#39;t think we can use it yet.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qed5p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754244357,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpq7a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6tpbly",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Business-Weekend-537",
                      "can_mod_post": false,
                      "created_utc": 1754287094,
                      "send_replies": true,
                      "parent_id": "t1_n6sxsdt",
                      "score": 1,
                      "author_fullname": "t2_rkb6qbej1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That was definitely part of the issue. Got it working but am currently in ocr hell.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6tpbly",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That was definitely part of the issue. Got it working but am currently in ocr hell.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgpq7a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6tpbly/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754287094,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6sxsdt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wfgy_engine",
            "can_mod_post": false,
            "created_utc": 1754274716,
            "send_replies": true,
            "parent_id": "t3_1mgpq7a",
            "score": 2,
            "author_fullname": "t2_1tgp8l87vk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "hey  this smells a lot like something we cataloged as a common failure in multi-layer RAG setups:  \nmodel fails silently on first call, no error message, just stalls or gives empty output.\n\nwe saw it a lot when a vector index wasn't fully ready, or if secrets (API / auth) were misaligned across components like OpenWebUI or other LLM wrappers. the worst part: no logs, no crash, just… nothing.\n\nhappy to explain more if you’re curious. it’s more common than people think.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6sxsdt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;hey  this smells a lot like something we cataloged as a common failure in multi-layer RAG setups:&lt;br/&gt;\nmodel fails silently on first call, no error message, just stalls or gives empty output.&lt;/p&gt;\n\n&lt;p&gt;we saw it a lot when a vector index wasn&amp;#39;t fully ready, or if secrets (API / auth) were misaligned across components like OpenWebUI or other LLM wrappers. the worst part: no logs, no crash, just… nothing.&lt;/p&gt;\n\n&lt;p&gt;happy to explain more if you’re curious. it’s more common than people think.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6sxsdt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754274716,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpq7a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]