[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi LocalLLaMA,\n\nI’m a bit confused on two levels and need help: \n\n1) What are the best settings to get ollama to utilize all (6) 3090’s so I can use parallel processing. \n\n2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? \n\nAny recommendations on models? \n\nMy use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.\n\nSomeone previously referenced using CommandR+ 104b but I couldn’t get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). \n\nI think another person previously referenced Gemma 27b. I haven’t tried that yet. \n\nI’m a bit lost on configs. \n\nAlso someone suggested vllm instead but I couldn’t seem to get it to work, even with a small model. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Need help- unsure of right ollama configs with 6x 3090’s, also model choice for RAG?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgpq7a",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.44,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rkb6qbej1",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754243683,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LocalLLaMA,&lt;/p&gt;\n\n&lt;p&gt;I’m a bit confused on two levels and need help: &lt;/p&gt;\n\n&lt;p&gt;1) What are the best settings to get ollama to utilize all (6) 3090’s so I can use parallel processing. &lt;/p&gt;\n\n&lt;p&gt;2) Do I go with an LLM model that can fit on one 3090 or is it ok to go with a bigger model? &lt;/p&gt;\n\n&lt;p&gt;Any recommendations on models? &lt;/p&gt;\n\n&lt;p&gt;My use case is for inference on a RAG dataset using OpenWebUI or Kotaemon.&lt;/p&gt;\n\n&lt;p&gt;Someone previously referenced using CommandR+ 104b but I couldn’t get it to do inference- it just seemed to tie up/lock up the system and provide no answer (no error message though). &lt;/p&gt;\n\n&lt;p&gt;I think another person previously referenced Gemma 27b. I haven’t tried that yet. &lt;/p&gt;\n\n&lt;p&gt;I’m a bit lost on configs. &lt;/p&gt;\n\n&lt;p&gt;Also someone suggested vllm instead but I couldn’t seem to get it to work, even with a small model. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgpq7a",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "Business-Weekend-537",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/",
            "subreddit_subscribers": 509625,
            "created_utc": 1754243683,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6qeyu4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Business-Weekend-537",
                      "can_mod_post": false,
                      "created_utc": 1754244542,
                      "send_replies": true,
                      "parent_id": "t1_n6qdegu",
                      "score": 1,
                      "author_fullname": "t2_rkb6qbej1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Open frame, AsRock ROMED8-2T (7x pcie 4.0. x16) \n\n6x pcie 4.0 x16 for 3090’s\n1x pcie 4.0 x4x4x4x4 for asus m.2 HyperCard (4) nvme adapter \n\nMotherboard only supports m.2 and not nvme natively.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6qeyu4",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Open frame, AsRock ROMED8-2T (7x pcie 4.0. x16) &lt;/p&gt;\n\n&lt;p&gt;6x pcie 4.0 x16 for 3090’s\n1x pcie 4.0 x4x4x4x4 for asus m.2 HyperCard (4) nvme adapter &lt;/p&gt;\n\n&lt;p&gt;Motherboard only supports m.2 and not nvme natively.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgpq7a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qeyu4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754244542,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6qdegu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Expensive_Mirror5247",
            "can_mod_post": false,
            "created_utc": 1754244066,
            "send_replies": true,
            "parent_id": "t3_1mgpq7a",
            "score": 3,
            "author_fullname": "t2_8alx42ew",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "6 3090's? holy fuck bro thats a BEEEEEEEEEEEAST are they in an open frame case or?  got pics? what kind of board are you running them off of?  are you using an expansion bus or were you able to find a decent board with 6 slots?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qdegu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;6 3090&amp;#39;s? holy fuck bro thats a BEEEEEEEEEEEAST are they in an open frame case or?  got pics? what kind of board are you running them off of?  are you using an expansion bus or were you able to find a decent board with 6 slots?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qdegu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754244066,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpq7a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n6qov4v",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "Business-Weekend-537",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n6qji43",
                                                              "score": 1,
                                                              "author_fullname": "t2_rkb6qbej1",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Fair enough. Thanks for the links",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n6qov4v",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fair enough. Thanks for the links&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mgpq7a",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qov4v/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754247594,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754247594,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6qji43",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "ubrtnk",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6qgpy5",
                                                    "score": 1,
                                                    "author_fullname": "t2_7b5i2",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "[https://docs.vllm.ai/en/latest/](https://docs.vllm.ai/en/latest/) \\- Obviously this is the most accurate starting point. The OpenAI Compatible Server section is how you get OWUI to talk to vLLM. It'll be a Many to 1 configuration of models end points in OWUI's configuration, not like with Ollama where Ollama plays the router. \n\n[https://ploomber.io/blog/vllm-deploy/](https://ploomber.io/blog/vllm-deploy/) \\- I used this guide + the help of ChatGPT free to deploy my system a few months ago after I got back from Red Hat's conference in Boston. (I ended up going back to Ollama because I'm lazy and only have 2x 3090s lol)",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6qji43",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://docs.vllm.ai/en/latest/\"&gt;https://docs.vllm.ai/en/latest/&lt;/a&gt; - Obviously this is the most accurate starting point. The OpenAI Compatible Server section is how you get OWUI to talk to vLLM. It&amp;#39;ll be a Many to 1 configuration of models end points in OWUI&amp;#39;s configuration, not like with Ollama where Ollama plays the router. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ploomber.io/blog/vllm-deploy/\"&gt;https://ploomber.io/blog/vllm-deploy/&lt;/a&gt; - I used this guide + the help of ChatGPT free to deploy my system a few months ago after I got back from Red Hat&amp;#39;s conference in Boston. (I ended up going back to Ollama because I&amp;#39;m lazy and only have 2x 3090s lol)&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mgpq7a",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qji43/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754245931,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754245931,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6qgpy5",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Business-Weekend-537",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6qgj9d",
                                          "score": 1,
                                          "author_fullname": "t2_rkb6qbej1",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Got it can you point me to any good vllm tutorials or instructions? I’ve read a couple and couldn’t get it to work yet.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6qgpy5",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Got it can you point me to any good vllm tutorials or instructions? I’ve read a couple and couldn’t get it to work yet.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mgpq7a",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qgpy5/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754245077,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754245077,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6qgj9d",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ubrtnk",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qf5dy",
                                "score": 1,
                                "author_fullname": "t2_7b5i2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "vLLM definitely does - OWUI only needs an OpenAI compatible API endpoint - however, the ease of Ollama's WYSIWYG is gone with the more advanced capabilities that vLLM (and the others). Tensor Parallelism is nice but if you have a desire to have multiple models available ad-hoc or simultaneously, you'll need to add in something like LlamaSwap or configure the vLLM instances (its a 1:1 ratio of service to model) to only use a subset of your total available vRAM, otherwise vLLM will see 144GB of vRAM and say thank you, may I have some more.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6qgj9d",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vLLM definitely does - OWUI only needs an OpenAI compatible API endpoint - however, the ease of Ollama&amp;#39;s WYSIWYG is gone with the more advanced capabilities that vLLM (and the others). Tensor Parallelism is nice but if you have a desire to have multiple models available ad-hoc or simultaneously, you&amp;#39;ll need to add in something like LlamaSwap or configure the vLLM instances (its a 1:1 ratio of service to model) to only use a subset of your total available vRAM, otherwise vLLM will see 144GB of vRAM and say thank you, may I have some more.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgpq7a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qgj9d/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754245019,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754245019,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6s5xrj",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "TyraVex",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qf5dy",
                                "score": 1,
                                "author_fullname": "t2_5u6lp8ar",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yes, Tabby works perfectly on my end. I find it simpler than vLLM and more efficient VRAM wise. There’s only one config file with around 40 options, each documented within the file itself: [config_sample.yml](https://github.com/theroyallab/tabbyAPI/blob/main/config_sample.yml).  \n\nFor automatic individual model configurations (like `llama-swap`), you can simply create additional config files inside each LLM folder to apply different settings.  \n\nThe only downside is that some obscure quantized models aren’t available on Hugging Face.",
                                "edited": 1754264927,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6s5xrj",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, Tabby works perfectly on my end. I find it simpler than vLLM and more efficient VRAM wise. There’s only one config file with around 40 options, each documented within the file itself: &lt;a href=\"https://github.com/theroyallab/tabbyAPI/blob/main/config_sample.yml\"&gt;config_sample.yml&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;For automatic individual model configurations (like &lt;code&gt;llama-swap&lt;/code&gt;), you can simply create additional config files inside each LLM folder to apply different settings.  &lt;/p&gt;\n\n&lt;p&gt;The only downside is that some obscure quantized models aren’t available on Hugging Face.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgpq7a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6s5xrj/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754264708,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754264708,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6qf5dy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Business-Weekend-537",
                      "can_mod_post": false,
                      "created_utc": 1754244597,
                      "send_replies": true,
                      "parent_id": "t1_n6qed5p",
                      "score": 1,
                      "author_fullname": "t2_rkb6qbej1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Will the other options you referenced play nicely with openwebui?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6qf5dy",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Will the other options you referenced play nicely with openwebui?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgpq7a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qf5dy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754244597,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6qed5p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TyraVex",
            "can_mod_post": false,
            "created_utc": 1754244357,
            "send_replies": true,
            "parent_id": "t3_1mgpq7a",
            "score": 1,
            "author_fullname": "t2_5u6lp8ar",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "llama.cpp or ollama is not efficient with multiple GPUs\n\nEXL2, vLLM, and Sglang support tensor parallelism to use all GPUs at the same time, the most friendly and VRAM-efficient being tabbyAPI, which uses EXL2 or EXL3 as its backend. EXL3 tensor parallelism is coming soon (dev branch), but I don't think we can use it yet.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qed5p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llama.cpp or ollama is not efficient with multiple GPUs&lt;/p&gt;\n\n&lt;p&gt;EXL2, vLLM, and Sglang support tensor parallelism to use all GPUs at the same time, the most friendly and VRAM-efficient being tabbyAPI, which uses EXL2 or EXL3 as its backend. EXL3 tensor parallelism is coming soon (dev branch), but I don&amp;#39;t think we can use it yet.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpq7a/need_help_unsure_of_right_ollama_configs_with_6x/n6qed5p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754244357,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpq7a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]