[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "The numbers I get on my M4 Max MacBook Pro are too low. I also believe the numbers I have seen other people report for Nvidia etc are too low.\n\nI am getting about 30 tokens per second on the GGUF from Unsloth. But with 5.1b active parameters, the expected number could be as high as 526/5.1\\*2 = 206 tokens per second based on memory bandwidth divided by model size. This means we are very far from being memory bandwidth constrained. We appear to be heavily compute constrained. That is not typical for inference.\n\nI just downloaded a q4 MLX version. That one gives me about 70 tokens per second. However I am not sure they managed to preserve MXFP4 - it is probably the old kind of q4. Which means although it is the same size, it will be significantly worse performance.\n\nIs this just a question of poor support for mxfp4? Or is the hardware not capable and we will suffer from poor speed until the next generation of chips?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "GPT OSS 120b is not as fast as it should be",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mj7io0",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.61,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_bvqb8ng0",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754493876,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754493598,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The numbers I get on my M4 Max MacBook Pro are too low. I also believe the numbers I have seen other people report for Nvidia etc are too low.&lt;/p&gt;\n\n&lt;p&gt;I am getting about 30 tokens per second on the GGUF from Unsloth. But with 5.1b active parameters, the expected number could be as high as 526/5.1*2 = 206 tokens per second based on memory bandwidth divided by model size. This means we are very far from being memory bandwidth constrained. We appear to be heavily compute constrained. That is not typical for inference.&lt;/p&gt;\n\n&lt;p&gt;I just downloaded a q4 MLX version. That one gives me about 70 tokens per second. However I am not sure they managed to preserve MXFP4 - it is probably the old kind of q4. Which means although it is the same size, it will be significantly worse performance.&lt;/p&gt;\n\n&lt;p&gt;Is this just a question of poor support for mxfp4? Or is the hardware not capable and we will suffer from poor speed until the next generation of chips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mj7io0",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Baldur-Norddahl",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/",
            "subreddit_subscribers": 512427,
            "created_utc": 1754493598,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7908e0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "lakySK",
                      "can_mod_post": false,
                      "created_utc": 1754495041,
                      "send_replies": true,
                      "parent_id": "t1_n78xlmy",
                      "score": 4,
                      "author_fullname": "t2_y9y2q",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hopefully mlx-lm will add support as well ü§û",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7908e0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully mlx-lm will add support as well ü§û&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj7io0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/n7908e0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754495041,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n78xlmy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Accomplished_Ad9530",
            "can_mod_post": false,
            "created_utc": 1754494305,
            "send_replies": true,
            "parent_id": "t3_1mj7io0",
            "score": 7,
            "author_fullname": "t2_88fma001",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "‚ÄúIs this just a question of poor support for mxfp4?‚Äù\n\nYep, that‚Äôs basically the issue currently. I suspect mxfp4 quant support will be added soon, which should result in full quality at full speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78xlmy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;‚ÄúIs this just a question of poor support for mxfp4?‚Äù&lt;/p&gt;\n\n&lt;p&gt;Yep, that‚Äôs basically the issue currently. I suspect mxfp4 quant support will be added soon, which should result in full quality at full speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/n78xlmy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754494305,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj7io0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n793rnj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754496016,
            "send_replies": true,
            "parent_id": "t3_1mj7io0",
            "score": 4,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;  based on memory bandwidth divided by model size.\n\nThis is an inaccurate calculation due to the mix of mxfp4 and bf16 weights.  Of the 5.1B active parameters about 1.6B are bf16 and 3.5B are mxfp4.  Therefore the GB/token is `(2*1.6 + 3.5/2)*1000^3/1024^3` = 4.86GiB.  So theoretical is more like 108tok/s and given the usual 50% efficiency I see from non-dedicated GPU I would expect somewhere around 50-60.\n\nNote that the Q4 you downloaded would change that dramatically because the 1.6B common weights go from being 16b to 4b meaning the active parameters shrink from 4.8GB to 2.4GB.\n\nIncidentally on my machine I actually find it to be more bandwidth efficient that Qwen3-30B, but more compute bound:\n\n| model                          |       size |     params | backend    | threads | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | --------------: | -------------------: |\n| gpt-oss ?B MXFP4+Q4_0          |  58.04 GiB |   116.83 B | CPU        |      16 |  1 |           tg512 |         38.26 ¬± 0.00 |\n| gpt-oss ?B MXFP4+Q4_0          |  58.04 GiB |   116.83 B | CPU        |      32 |  1 |           tg512 |         52.55 ¬± 0.00 |\n| qwen3moe 30B.A3B Q4_K_M        |  17.28 GiB |    30.53 B | CPU        |      16 |  1 |           tg512 |         49.99 ¬± 0.00 |\n| qwen3moe 30B.A3B Q4_K_M        |  17.28 GiB |    30.53 B | CPU        |      32 |  1 |           tg512 |         52.41 ¬± 0.00 |",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n793rnj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;based on memory bandwidth divided by model size.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is an inaccurate calculation due to the mix of mxfp4 and bf16 weights.  Of the 5.1B active parameters about 1.6B are bf16 and 3.5B are mxfp4.  Therefore the GB/token is &lt;code&gt;(2*1.6 + 3.5/2)*1000^3/1024^3&lt;/code&gt; = 4.86GiB.  So theoretical is more like 108tok/s and given the usual 50% efficiency I see from non-dedicated GPU I would expect somewhere around 50-60.&lt;/p&gt;\n\n&lt;p&gt;Note that the Q4 you downloaded would change that dramatically because the 1.6B common weights go from being 16b to 4b meaning the active parameters shrink from 4.8GB to 2.4GB.&lt;/p&gt;\n\n&lt;p&gt;Incidentally on my machine I actually find it to be more bandwidth efficient that Qwen3-30B, but more compute bound:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;model&lt;/th&gt;\n&lt;th align=\"right\"&gt;size&lt;/th&gt;\n&lt;th align=\"right\"&gt;params&lt;/th&gt;\n&lt;th&gt;backend&lt;/th&gt;\n&lt;th align=\"right\"&gt;threads&lt;/th&gt;\n&lt;th align=\"right\"&gt;fa&lt;/th&gt;\n&lt;th align=\"right\"&gt;test&lt;/th&gt;\n&lt;th align=\"right\"&gt;t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;gpt-oss ?B MXFP4+Q4_0&lt;/td&gt;\n&lt;td align=\"right\"&gt;58.04 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;116.83 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;16&lt;/td&gt;\n&lt;td align=\"right\"&gt;1&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg512&lt;/td&gt;\n&lt;td align=\"right\"&gt;38.26 ¬± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gpt-oss ?B MXFP4+Q4_0&lt;/td&gt;\n&lt;td align=\"right\"&gt;58.04 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;116.83 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;32&lt;/td&gt;\n&lt;td align=\"right\"&gt;1&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg512&lt;/td&gt;\n&lt;td align=\"right\"&gt;52.55 ¬± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3moe 30B.A3B Q4_K_M&lt;/td&gt;\n&lt;td align=\"right\"&gt;17.28 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;30.53 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;16&lt;/td&gt;\n&lt;td align=\"right\"&gt;1&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg512&lt;/td&gt;\n&lt;td align=\"right\"&gt;49.99 ¬± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3moe 30B.A3B Q4_K_M&lt;/td&gt;\n&lt;td align=\"right\"&gt;17.28 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;30.53 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;32&lt;/td&gt;\n&lt;td align=\"right\"&gt;1&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg512&lt;/td&gt;\n&lt;td align=\"right\"&gt;52.41 ¬± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/n793rnj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754496016,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj7io0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78z8co",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "SandboChang",
                      "can_mod_post": false,
                      "created_utc": 1754494766,
                      "send_replies": true,
                      "parent_id": "t1_n78w7tn",
                      "score": 3,
                      "author_fullname": "t2_10icmj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I think it is known with Apple Silicon that the TPS drops a lot, more than 50%, when context blows up.\n\nIn my test with my M4 Max 128 GB today, I got from 50 TPS@0 ctx to 10TPS@12k ctx. Not sure if this is really normal but not unheard of.",
                      "edited": 1754495717,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78z8co",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think it is known with Apple Silicon that the TPS drops a lot, more than 50%, when context blows up.&lt;/p&gt;\n\n&lt;p&gt;In my test with my M4 Max 128 GB today, I got from 50 TPS@0 ctx to 10TPS@12k ctx. Not sure if this is really normal but not unheard of.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj7io0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/n78z8co/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754494766,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n78zwer",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "lakySK",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n78yo6t",
                                "score": 2,
                                "author_fullname": "t2_y9y2q",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'm running the beta release if that helps: LM Studio 0.3.22 (Build 1)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n78zwer",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running the beta release if that helps: LM Studio 0.3.22 (Build 1)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj7io0",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/n78zwer/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754494950,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754494950,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n78yo6t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Baldur-Norddahl",
                      "can_mod_post": false,
                      "created_utc": 1754494609,
                      "send_replies": true,
                      "parent_id": "t1_n78w7tn",
                      "score": 2,
                      "author_fullname": "t2_bvqb8ng0",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "My LM Studio refuses to load that one. Will try to nuke all folders related and try a redownload. Anyway 50 tps is also way less than expected.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78yo6t",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My LM Studio refuses to load that one. Will try to nuke all folders related and try a redownload. Anyway 50 tps is also way less than expected.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj7io0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/n78yo6t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754494609,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n78w7tn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lakySK",
            "can_mod_post": false,
            "created_utc": 1754493913,
            "send_replies": true,
            "parent_id": "t3_1mj7io0",
            "score": 2,
            "author_fullname": "t2_y9y2q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "M4 Max 128GB as well. I used LM Studio and the OpenAI MXFP4 GGUF (from https://lmstudio.ai/models/openai/gpt-oss-120b). With no context I'm getting 50+ t/s. Seems to drop &lt;10 t/s with 25k context though...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78w7tn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;M4 Max 128GB as well. I used LM Studio and the OpenAI MXFP4 GGUF (from &lt;a href=\"https://lmstudio.ai/models/openai/gpt-oss-120b\"&gt;https://lmstudio.ai/models/openai/gpt-oss-120b&lt;/a&gt;). With no context I&amp;#39;m getting 50+ t/s. Seems to drop &amp;lt;10 t/s with 25k context though...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/n78w7tn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754493913,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj7io0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]