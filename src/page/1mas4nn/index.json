[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi people \n\nBeen working on a local agent MVP these 3 last weeks. To summarise newsletters and plugged into your private projects would then offer unique insights and suggestions from the newsletters to keep you competitive and enhance your productivity.\n\nI've implemented a baseline RAG under Ollama using Llama index, ChromaDB for ingestion and indexing, as well as Langchain for the orchestration.\n\nI'm realizing that the insights synthesized by similarity search method (between the newsletters and the ingested user context) is mediocre, and planning on shifting to a knowledge graph for the RAG, to create a more powerful semantic representation of the user context, which should enable a more relevant insight generation.\n\nThe problem is, I have 7 days from now to complete it before submitting the MVP for an investor pitch. How realistic is that ? \n\nThanks for any help ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "GRAPH RAG vs baseline RAG for MVP",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mas4nn",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1tsmkqb3yj",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753637180,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people &lt;/p&gt;\n\n&lt;p&gt;Been working on a local agent MVP these 3 last weeks. To summarise newsletters and plugged into your private projects would then offer unique insights and suggestions from the newsletters to keep you competitive and enhance your productivity.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve implemented a baseline RAG under Ollama using Llama index, ChromaDB for ingestion and indexing, as well as Langchain for the orchestration.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m realizing that the insights synthesized by similarity search method (between the newsletters and the ingested user context) is mediocre, and planning on shifting to a knowledge graph for the RAG, to create a more powerful semantic representation of the user context, which should enable a more relevant insight generation.&lt;/p&gt;\n\n&lt;p&gt;The problem is, I have 7 days from now to complete it before submitting the MVP for an investor pitch. How realistic is that ? &lt;/p&gt;\n\n&lt;p&gt;Thanks for any help &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mas4nn",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ctxgen_founder",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/",
            "subreddit_subscribers": 505617,
            "created_utc": 1753637180,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5h09ml",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "ctxgen_founder",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5gw86y",
                                          "score": 1,
                                          "author_fullname": "t2_1tsmkqb3yj",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks for that.\nChunking is across 512 bytes, and overlap at 50 chars. \nNice idea about assigning authoritative weight, but could shift significant effort on user side though. Definitely will keep it in mind still.\nMulti agent I don't now if it would help, if the problem is the disparate aspect of the user's notes. \nI consider GraphRAG mainly because I read it allows a tighter coupling of all entities mentioned in the datasets, with relationships between them to more effectively navigate the context and glean meaningful data from it",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5h09ml",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for that.\nChunking is across 512 bytes, and overlap at 50 chars. \nNice idea about assigning authoritative weight, but could shift significant effort on user side though. Definitely will keep it in mind still.\nMulti agent I don&amp;#39;t now if it would help, if the problem is the disparate aspect of the user&amp;#39;s notes. \nI consider GraphRAG mainly because I read it allows a tighter coupling of all entities mentioned in the datasets, with relationships between them to more effectively navigate the context and glean meaningful data from it&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mas4nn",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/n5h09ml/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753639650,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753639650,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gw86y",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "jklre",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gull0",
                                "score": 2,
                                "author_fullname": "t2_5ut58",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What is your chunking and overlap set to?. You could try to assign a weight system. Like these documents are more athoritative than those. Or if you are just looking for better ourputs across multiple sources you could go multi agent or multi-trun multi-step with the rag you already have to give higher quality outputs.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gw86y",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What is your chunking and overlap set to?. You could try to assign a weight system. Like these documents are more athoritative than those. Or if you are just looking for better ourputs across multiple sources you could go multi agent or multi-trun multi-step with the rag you already have to give higher quality outputs.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mas4nn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/n5gw86y/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753638476,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753638476,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gull0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ctxgen_founder",
                      "can_mod_post": false,
                      "created_utc": 1753638011,
                      "send_replies": true,
                      "parent_id": "t1_n5gt3o6",
                      "score": 0,
                      "author_fullname": "t2_1tsmkqb3yj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah ChromaDB. Newsletters are also chunked and embedded with the same stack, and then similarity search is performed with the user already indexed context in order to provide the augmented prompt needed for the LLM to generate an insight, if any new tech mentioned is deemed useful for the user's projects.\n\nIssue is insights are not that great, the indexing of the context doesn't seem to allow effective reasoning on the newsletters content",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gull0",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah ChromaDB. Newsletters are also chunked and embedded with the same stack, and then similarity search is performed with the user already indexed context in order to provide the augmented prompt needed for the LLM to generate an insight, if any new tech mentioned is deemed useful for the user&amp;#39;s projects.&lt;/p&gt;\n\n&lt;p&gt;Issue is insights are not that great, the indexing of the context doesn&amp;#39;t seem to allow effective reasoning on the newsletters content&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mas4nn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/n5gull0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753638011,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5gt3o6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jklre",
            "can_mod_post": false,
            "created_utc": 1753637583,
            "send_replies": true,
            "parent_id": "t3_1mas4nn",
            "score": 2,
            "author_fullname": "t2_5ut58",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How are you storing the information in rag and what database are you using? Chroma? is each news letter / user input an independent collection?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gt3o6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How are you storing the information in rag and what database are you using? Chroma? is each news letter / user input an independent collection?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/n5gt3o6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753637583,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mas4nn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gz7bv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ctxgen_founder",
                      "can_mod_post": false,
                      "created_utc": 1753639337,
                      "send_replies": true,
                      "parent_id": "t1_n5gx2vs",
                      "score": 1,
                      "author_fullname": "t2_1tsmkqb3yj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for all that.\nI don't use the summary for the insight generation. For the embedding, chunk are 512 bytes long and overlapping is at 50 chars. I haven't tried out chunk tuning yet, as I've read Microsoft paper and their own implementation of GraphRAG, as well as neo4j python module, and their result point to a significant increase in agentic understanding of the context",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gz7bv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for all that.\nI don&amp;#39;t use the summary for the insight generation. For the embedding, chunk are 512 bytes long and overlapping is at 50 chars. I haven&amp;#39;t tried out chunk tuning yet, as I&amp;#39;ve read Microsoft paper and their own implementation of GraphRAG, as well as neo4j python module, and their result point to a significant increase in agentic understanding of the context&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mas4nn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/n5gz7bv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753639337,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5gx2vs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RapidTangent",
            "can_mod_post": false,
            "created_utc": 1753638721,
            "send_replies": true,
            "parent_id": "t3_1mas4nn",
            "score": 2,
            "author_fullname": "t2_acf0xz26",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's hard to give good advice based on the information you are providing because it is not entirely clear what you are trying to achieve but I will give it a go.\n\nThings to check before changing anything.\n1. Are you able to get useful insight yourself using the same tool as the agent? If yes, then the problem is likely that your agent either is getting too much tokens or you need a more powerful model. It might not have enough iterations to look up all relevant information.\n2. If the results are poor using the tools alone. Why is it? Often chunking can give terrible results and unless you have very long documents. It's almost always better to create a single embedding per document. Modern embeddings can handle 8k tokens easily.\n3. If I understood correctly you summarise the articles firsts. Summaries remove information so don't use it unless you really know up front what information you need the summary to contain.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gx2vs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s hard to give good advice based on the information you are providing because it is not entirely clear what you are trying to achieve but I will give it a go.&lt;/p&gt;\n\n&lt;p&gt;Things to check before changing anything.\n1. Are you able to get useful insight yourself using the same tool as the agent? If yes, then the problem is likely that your agent either is getting too much tokens or you need a more powerful model. It might not have enough iterations to look up all relevant information.\n2. If the results are poor using the tools alone. Why is it? Often chunking can give terrible results and unless you have very long documents. It&amp;#39;s almost always better to create a single embedding per document. Modern embeddings can handle 8k tokens easily.\n3. If I understood correctly you summarise the articles firsts. Summaries remove information so don&amp;#39;t use it unless you really know up front what information you need the summary to contain.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mas4nn/graph_rag_vs_baseline_rag_for_mvp/n5gx2vs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753638721,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mas4nn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]