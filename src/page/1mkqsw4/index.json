[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I've been spending a few days trying to diagnose this...I get the distinct feeling that my H100 is performing much slower than it should, but since it's a non-consumer GPU I find it hard to find reliable numbers online. I figured I'd ask here and maybe some of you have some insight or can point me in the right direction.\n\n\n\nI have access to \"1/2 of a H100\". The H100 is passed into a VM, which I have access to. I get half the VRAM (40GB) at least half the compute - however currently nobody else is using it so I get the full compute of a H100. The VM is receiving the GPU courtes of the Nvidia virtual GPU software version 17.5. The VM is running CUDA 12.4 and driver 550.144.03 (which are the latest compatible cuda/driver versions for this machine).\n\n\n\nNow to the inference speeds. I've tried various inference engines and models and I can't shake the feeling that the H100 is way too slow. Granted, my experience with these things is relatively limited, so it is entirely possible that it is performing as it should...\n\n\n\nI've tried serving the model via a docker container with llama-cpp and with vllm, and also tried with text-generation-webui loading the models (non containerized), as well as vllm serve outside of any containers. Results seem to me to be too slow. See the below chart (each bar was made with 25 inferences of exactly the same prompt). Particularly, I feel like nr #13 (24B FP8 model only giving 60 token/s) or #15 (24B 4-bit quantized model only giving \\~75 token/s) are much slower than what I would hope.\n\nhttps://preview.redd.it/lsnz3ci0nrhf1.png?width=1379&amp;format=png&amp;auto=webp&amp;s=39e14587916711297959e17c8c7a7fe3a6660b70\n\n\n\nAny advice for diagnostics or even solutions would be GREATLY appreciated. If anyone can confirm or deny to me whether in fact the H100 IS performing as it should that would also help.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "H100 performing slower than I think it should....am I right or wrong??",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 63,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "lsnz3ci0nrhf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 49,
                    "x": 108,
                    "u": "https://preview.redd.it/lsnz3ci0nrhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b582c9da6fd91192e4c5a6fdaf69f5152372597c"
                  },
                  {
                    "y": 98,
                    "x": 216,
                    "u": "https://preview.redd.it/lsnz3ci0nrhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fde88318ec95b9f8bef85471257b4716c213acba"
                  },
                  {
                    "y": 145,
                    "x": 320,
                    "u": "https://preview.redd.it/lsnz3ci0nrhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2d018a8a0606becbb41180e16af2a69f86db5fec"
                  },
                  {
                    "y": 291,
                    "x": 640,
                    "u": "https://preview.redd.it/lsnz3ci0nrhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9cfcabdd0c1d37c9eeed9243ae34768017b93b53"
                  },
                  {
                    "y": 437,
                    "x": 960,
                    "u": "https://preview.redd.it/lsnz3ci0nrhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3ea2c2eb9228d88920ae9e95cc0a87c4c500d86"
                  },
                  {
                    "y": 492,
                    "x": 1080,
                    "u": "https://preview.redd.it/lsnz3ci0nrhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=58a2f8e56fdd982795b30365f701850ae4d49de7"
                  }
                ],
                "s": {
                  "y": 629,
                  "x": 1379,
                  "u": "https://preview.redd.it/lsnz3ci0nrhf1.png?width=1379&amp;format=png&amp;auto=webp&amp;s=39e14587916711297959e17c8c7a7fe3a6660b70"
                },
                "id": "lsnz3ci0nrhf1"
              }
            },
            "name": "t3_1mkqsw4",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_n6ybupma",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/qJfZ04fi4l1HgigfS5A2-TAt8jEVpB8Da6hLSkZdJAo.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754646014,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been spending a few days trying to diagnose this...I get the distinct feeling that my H100 is performing much slower than it should, but since it&amp;#39;s a non-consumer GPU I find it hard to find reliable numbers online. I figured I&amp;#39;d ask here and maybe some of you have some insight or can point me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;I have access to &amp;quot;1/2 of a H100&amp;quot;. The H100 is passed into a VM, which I have access to. I get half the VRAM (40GB) at least half the compute - however currently nobody else is using it so I get the full compute of a H100. The VM is receiving the GPU courtes of the Nvidia virtual GPU software version 17.5. The VM is running CUDA 12.4 and driver 550.144.03 (which are the latest compatible cuda/driver versions for this machine).&lt;/p&gt;\n\n&lt;p&gt;Now to the inference speeds. I&amp;#39;ve tried various inference engines and models and I can&amp;#39;t shake the feeling that the H100 is way too slow. Granted, my experience with these things is relatively limited, so it is entirely possible that it is performing as it should...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried serving the model via a docker container with llama-cpp and with vllm, and also tried with text-generation-webui loading the models (non containerized), as well as vllm serve outside of any containers. Results seem to me to be too slow. See the below chart (each bar was made with 25 inferences of exactly the same prompt). Particularly, I feel like nr #13 (24B FP8 model only giving 60 token/s) or #15 (24B 4-bit quantized model only giving ~75 token/s) are much slower than what I would hope.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lsnz3ci0nrhf1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39e14587916711297959e17c8c7a7fe3a6660b70\"&gt;https://preview.redd.it/lsnz3ci0nrhf1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39e14587916711297959e17c8c7a7fe3a6660b70&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any advice for diagnostics or even solutions would be GREATLY appreciated. If anyone can confirm or deny to me whether in fact the H100 IS performing as it should that would also help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mkqsw4",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "PM_ME_UR_THERAPY",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkqsw4/h100_performing_slower_than_i_think_it_shouldam_i/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkqsw4/h100_performing_slower_than_i_think_it_shouldam_i/",
            "subreddit_subscribers": 513815,
            "created_utc": 1754646014,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kn5vu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "helicoptersneeze",
            "can_mod_post": false,
            "created_utc": 1754646426,
            "send_replies": true,
            "parent_id": "t3_1mkqsw4",
            "score": 3,
            "author_fullname": "t2_hla3tqyi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GPU virtualisation has crazy overhead, last I checked",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kn5vu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GPU virtualisation has crazy overhead, last I checked&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkqsw4/h100_performing_slower_than_i_think_it_shouldam_i/n7kn5vu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754646426,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkqsw4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kpk3y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "entsnack",
            "can_mod_post": false,
            "created_utc": 1754647730,
            "send_replies": true,
            "parent_id": "t3_1mkqsw4",
            "score": 2,
            "author_fullname": "t2_1a48h7vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why are you using llama-cpp? H100 supports MXFP4 natively, you're doing all this silly quantization to end up with slower inference. Your CUDA is ancient too!\n\nHere are my H100 benchmarks to compare. I have also posted some installation tips. Let me know if I can help with anything.\n\nhttps://www.reddit.com/r/LocalLLaMA/s/wl3hqIgAfW",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kpk3y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why are you using llama-cpp? H100 supports MXFP4 natively, you&amp;#39;re doing all this silly quantization to end up with slower inference. Your CUDA is ancient too!&lt;/p&gt;\n\n&lt;p&gt;Here are my H100 benchmarks to compare. I have also posted some installation tips. Let me know if I can help with anything.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/wl3hqIgAfW\"&gt;https://www.reddit.com/r/LocalLLaMA/s/wl3hqIgAfW&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkqsw4/h100_performing_slower_than_i_think_it_shouldam_i/n7kpk3y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754647730,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "link_id": "t3_1mkqsw4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "transparent",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kopr9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kinetic_energy28",
            "can_mod_post": false,
            "created_utc": 1754647273,
            "send_replies": true,
            "parent_id": "t3_1mkqsw4",
            "score": 1,
            "author_fullname": "t2_tab3l90i",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Since you mentioned 1/2 H100, which is capped by the nVidia Multi-Instance GPU (MIG) profile settings that may beyond your control, like the profile may limit you from using 1/2 compute cores.\n\nand there is benchmark for 3090/4090/H100, H100 peroforms faster at context and request parsing, but it may slower at token generation, which related to the token per seconds value you concerned.  \n[https://www.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark\\_rtx\\_3090\\_4090\\_and\\_even\\_4080\\_are/](https://www.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/)  \n[https://github.com/DeutscheKI/llm-performance-tests](https://github.com/DeutscheKI/llm-performance-tests)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kopr9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Since you mentioned 1/2 H100, which is capped by the nVidia Multi-Instance GPU (MIG) profile settings that may beyond your control, like the profile may limit you from using 1/2 compute cores.&lt;/p&gt;\n\n&lt;p&gt;and there is benchmark for 3090/4090/H100, H100 peroforms faster at context and request parsing, but it may slower at token generation, which related to the token per seconds value you concerned.&lt;br/&gt;\n&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/DeutscheKI/llm-performance-tests\"&gt;https://github.com/DeutscheKI/llm-performance-tests&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkqsw4/h100_performing_slower_than_i_think_it_shouldam_i/n7kopr9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754647273,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkqsw4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]