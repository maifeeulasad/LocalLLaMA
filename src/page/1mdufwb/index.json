[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "TLDR; Basically, I want llamaswap to expose a \"default\" model to all the clients, and it figures out what is the active model and routes the request to that model in the backend. \n\n------------------------------\n\nI'm running llama-swap and trying to simplify my client integration. Ideally, I want the client to always hit the same endpoint (/v1/chat/completions) without specifying a model name in the payload.\n\nI want to run only one model at a time, and be able to switch that model from the llama-swap Web UI or by reloading the config — without the client having to care or change anything.\n\nRight now it seems llama-swap requires \"model\": \"&lt;id&gt;\" in every request. Is there a way to define a global \"active\" model via UI and the client doesn't need to worry about the specific model. or it can just say the default model and Llamaswap figures out what is the default model set via the UI. \n\nIf this isn’t possible today, is there any known workaround?\n\nI do not want to go back and change the yaml config files. I only want to handle everything (model swapping) from the llamaswap UI itself.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Can llama-swap work without specifying the \"model\" field in API requests?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdufwb",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.29,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_e33mgcbq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753944553,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753944230,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR; Basically, I want llamaswap to expose a &amp;quot;default&amp;quot; model to all the clients, and it figures out what is the active model and routes the request to that model in the backend. &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I&amp;#39;m running llama-swap and trying to simplify my client integration. Ideally, I want the client to always hit the same endpoint (/v1/chat/completions) without specifying a model name in the payload.&lt;/p&gt;\n\n&lt;p&gt;I want to run only one model at a time, and be able to switch that model from the llama-swap Web UI or by reloading the config — without the client having to care or change anything.&lt;/p&gt;\n\n&lt;p&gt;Right now it seems llama-swap requires &amp;quot;model&amp;quot;: &amp;quot;&amp;lt;id&amp;gt;&amp;quot; in every request. Is there a way to define a global &amp;quot;active&amp;quot; model via UI and the client doesn&amp;#39;t need to worry about the specific model. or it can just say the default model and Llamaswap figures out what is the default model set via the UI. &lt;/p&gt;\n\n&lt;p&gt;If this isn’t possible today, is there any known workaround?&lt;/p&gt;\n\n&lt;p&gt;I do not want to go back and change the yaml config files. I only want to handle everything (model swapping) from the llamaswap UI itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdufwb",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "discoveringnature12",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdufwb/can_llamaswap_work_without_specifying_the_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdufwb/can_llamaswap_work_without_specifying_the_model/",
            "subreddit_subscribers": 507575,
            "created_utc": 1753944230,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n659vms",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "viperx7",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n64tq23",
                                "score": 1,
                                "author_fullname": "t2_1f90sij9",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yes   \nif requested model isn't already loaded it loads it  \nif it\\`s already loaded it serves the requests it  \nif something else is loaded it unloads it and then loads the model that is requested \n\nthen it will unload the model after a set timeout \n\nof course there are options where you can keep more than one model in memory simultaneously",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n659vms",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes&lt;br/&gt;\nif requested model isn&amp;#39;t already loaded it loads it&lt;br/&gt;\nif it`s already loaded it serves the requests it&lt;br/&gt;\nif something else is loaded it unloads it and then loads the model that is requested &lt;/p&gt;\n\n&lt;p&gt;then it will unload the model after a set timeout &lt;/p&gt;\n\n&lt;p&gt;of course there are options where you can keep more than one model in memory simultaneously&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdufwb",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdufwb/can_llamaswap_work_without_specifying_the_model/n659vms/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753961233,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753961233,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n64tq23",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "discoveringnature12",
                      "can_mod_post": false,
                      "created_utc": 1753953108,
                      "send_replies": true,
                      "parent_id": "t1_n64sybq",
                      "score": 1,
                      "author_fullname": "t2_e33mgcbq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "When the client gives the model name, does LLaMAswap load that model in the moment?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64tq23",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When the client gives the model name, does LLaMAswap load that model in the moment?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdufwb",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdufwb/can_llamaswap_work_without_specifying_the_model/n64tq23/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753953108,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64sybq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "The_Soul_Collect0r",
            "can_mod_post": false,
            "created_utc": 1753952663,
            "send_replies": true,
            "parent_id": "t3_1mdufwb",
            "score": 2,
            "author_fullname": "t2_13x9rnhkbw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You doing something wrong/backwards here. How are you simplifying the implementation with that, THAT is the implementation... (beside base url...)  \n  \nLlama swap is explicitly intended to ease CLIENT access to multiple models, without having to constantly play with the server side, by allowing clients to switch models by providing MODEL ID (name). Model ID is the routing info here, the only required param as far as I can see,  it tells llama swap which model to use. \n\nEither you don't need llama swap all together or you are misunderstanding something.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64sybq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You doing something wrong/backwards here. How are you simplifying the implementation with that, THAT is the implementation... (beside base url...)  &lt;/p&gt;\n\n&lt;p&gt;Llama swap is explicitly intended to ease CLIENT access to multiple models, without having to constantly play with the server side, by allowing clients to switch models by providing MODEL ID (name). Model ID is the routing info here, the only required param as far as I can see,  it tells llama swap which model to use. &lt;/p&gt;\n\n&lt;p&gt;Either you don&amp;#39;t need llama swap all together or you are misunderstanding something.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdufwb/can_llamaswap_work_without_specifying_the_model/n64sybq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753952663,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdufwb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]