[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Right now I am using Qwen and Gemma (32B and 27B) on my old pc from 2011 where the architecture isn’t compatible and doesn’t even detect my graphics card. \n\nI want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it’s either 30 minutes or 1 hour I get a response .\n\nIs there a logical reason for this? Is there some possible way I can figure this out and keep using the higher version models ? \n\n (I realize i need to get a new pc but now isn’t the best time for that)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Question about my dinosaur computer",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfedua",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_l4qac",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754099099,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now I am using Qwen and Gemma (32B and 27B) on my old pc from 2011 where the architecture isn’t compatible and doesn’t even detect my graphics card. &lt;/p&gt;\n\n&lt;p&gt;I want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it’s either 30 minutes or 1 hour I get a response .&lt;/p&gt;\n\n&lt;p&gt;Is there a logical reason for this? Is there some possible way I can figure this out and keep using the higher version models ? &lt;/p&gt;\n\n&lt;p&gt;(I realize i need to get a new pc but now isn’t the best time for that)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mfedua",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "XiRw",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/",
            "subreddit_subscribers": 509054,
            "created_utc": 1754099099,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6izjru",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "XiRw",
                      "can_mod_post": false,
                      "created_utc": 1754142839,
                      "send_replies": true,
                      "parent_id": "t1_n6gjwxc",
                      "score": 1,
                      "author_fullname": "t2_l4qac",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Input files are the only ones consistent with longer load times. If I tell it a simple command like “I want you to just say the word Yes.” It can still drag on and take forever. I have 64GB of ram and it only uses 20GB max on ollama. Honestly I’m surprised I got this far too with it lol. LM Studio installs but won’t even load for me properly. It opens but everything is empty inside and somewhere in the app it tells me I’m not compatible. AnythingLLM works but for some reason media files never do and times out. Ollama command line has been the only one to give me a chance though I will try llama.cpp to see if it’s any better.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6izjru",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Input files are the only ones consistent with longer load times. If I tell it a simple command like “I want you to just say the word Yes.” It can still drag on and take forever. I have 64GB of ram and it only uses 20GB max on ollama. Honestly I’m surprised I got this far too with it lol. LM Studio installs but won’t even load for me properly. It opens but everything is empty inside and somewhere in the app it tells me I’m not compatible. AnythingLLM works but for some reason media files never do and times out. Ollama command line has been the only one to give me a chance though I will try llama.cpp to see if it’s any better.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfedua",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6izjru/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754142839,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6gjwxc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "cristoper",
            "can_mod_post": false,
            "created_utc": 1754100369,
            "send_replies": true,
            "parent_id": "t3_1mfedua",
            "score": 7,
            "author_fullname": "t2_38xkk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Two thoughts:\n\n* The initial latency should mostly be a function of how long your initial prompt is. For the long ones are you giving it long prompts / input files?\n\n* How much free RAM do you have? If your OS has to swap memory contents to disk in order to load model weights, that can be very slow\n\nBut I'm surprised those models are usable at all on a 14 year old PC with no GPU offloading!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6gjwxc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Two thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The initial latency should mostly be a function of how long your initial prompt is. For the long ones are you giving it long prompts / input files?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How much free RAM do you have? If your OS has to swap memory contents to disk in order to load model weights, that can be very slow&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But I&amp;#39;m surprised those models are usable at all on a 14 year old PC with no GPU offloading!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6gjwxc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754100369,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfedua",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iyi6k",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "XiRw",
                      "can_mod_post": false,
                      "created_utc": 1754142464,
                      "send_replies": true,
                      "parent_id": "t1_n6gqkys",
                      "score": 1,
                      "author_fullname": "t2_l4qac",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the reply, I am using ollama not llama.cpp. What you said sounds interesting though so I will try that later and see if I get any kind of performance difference with the speed. Thanks again",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iyi6k",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the reply, I am using ollama not llama.cpp. What you said sounds interesting though so I will try that later and see if I get any kind of performance difference with the speed. Thanks again&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfedua",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6iyi6k/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754142464,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6gqkys",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754102969,
            "send_replies": true,
            "parent_id": "t3_1mfedua",
            "score": 4,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you running llama.cpp or a derivative?  CPU only?  I'll suppose so.\n\n- Set your number of threads to the number of _physical_ cores - 1.  Inference time can get obliterated if something is hogging a core.\n- Check your available RAM.  llama.cpp uses `mmap` which means basically if there isn't enough memory available it'll swap off the disk (basically like normal swap).  This is slow with an SSD or impossibly slow with a hard drive.  Use `--no-mmap` (or `--mlock` but that requires permissions) to prevent swapping (for llama.cpp at least)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6gqkys",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you running llama.cpp or a derivative?  CPU only?  I&amp;#39;ll suppose so.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set your number of threads to the number of &lt;em&gt;physical&lt;/em&gt; cores - 1.  Inference time can get obliterated if something is hogging a core.&lt;/li&gt;\n&lt;li&gt;Check your available RAM.  llama.cpp uses &lt;code&gt;mmap&lt;/code&gt; which means basically if there isn&amp;#39;t enough memory available it&amp;#39;ll swap off the disk (basically like normal swap).  This is slow with an SSD or impossibly slow with a hard drive.  Use &lt;code&gt;--no-mmap&lt;/code&gt; (or &lt;code&gt;--mlock&lt;/code&gt; but that requires permissions) to prevent swapping (for llama.cpp at least)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6gqkys/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754102969,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfedua",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6h9la5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "randomqhacker",
            "can_mod_post": false,
            "created_utc": 1754111365,
            "send_replies": true,
            "parent_id": "t3_1mfedua",
            "score": 3,
            "author_fullname": "t2_4nw3v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sounds like the performance elbow from disk i/o.  If under linux watch vmstat 1 and see if you have lots of disk blocks in and out, and what your memory usage is.\n\nAlso, try this model in Q4\\_K\\_XL, it will be a game changer for you in tokens/second:\n\n[https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF](https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF)\n\nor specifically for coding:\n\n[https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)\n\nif speed is not paramount, try Q5\\_K\\_XL or higher for better accuracy.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6h9la5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sounds like the performance elbow from disk i/o.  If under linux watch vmstat 1 and see if you have lots of disk blocks in and out, and what your memory usage is.&lt;/p&gt;\n\n&lt;p&gt;Also, try this model in Q4_K_XL, it will be a game changer for you in tokens/second:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;or specifically for coding:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;if speed is not paramount, try Q5_K_XL or higher for better accuracy.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6h9la5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754111365,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfedua",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6gkipz",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "XiRw",
                      "can_mod_post": false,
                      "created_utc": 1754100597,
                      "send_replies": true,
                      "parent_id": "t1_n6gi6wm",
                      "score": 2,
                      "author_fullname": "t2_l4qac",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "No actually, that’s what confuses me. It can be a long or short prompt and it can go either way. However if I input an image that always takes long so I get it with that. Usually with the fast replies it happens in con sessions then you get periods of long again. It’s never (long-short-long-shot) but usually (long, long, long, short short short , long)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6gkipz",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No actually, that’s what confuses me. It can be a long or short prompt and it can go either way. However if I input an image that always takes long so I get it with that. Usually with the fast replies it happens in con sessions then you get periods of long again. It’s never (long-short-long-shot) but usually (long, long, long, short short short , long)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfedua",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6gkipz/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754100597,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6gi6wm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SM8085",
            "can_mod_post": false,
            "created_utc": 1754099734,
            "send_replies": true,
            "parent_id": "t3_1mfedua",
            "score": 2,
            "author_fullname": "t2_14vikjao97",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;I want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it’s either 30 minutes or 1 hour I get a response .\n\nThere's a 'prompt processing' speed/time.  Are the 5-30 second responses much shorter prompts than the 30 minutes to 1 hour?\n\nFor instance, a long transcript or document can take a while on slower machines &amp; high B models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6gi6wm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I want to know why sometimes the performance is (almost) instantly , maybe it will answer after 5-30 seconds. But other times it’s either 30 minutes or 1 hour I get a response .&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;There&amp;#39;s a &amp;#39;prompt processing&amp;#39; speed/time.  Are the 5-30 second responses much shorter prompts than the 30 minutes to 1 hour?&lt;/p&gt;\n\n&lt;p&gt;For instance, a long transcript or document can take a while on slower machines &amp;amp; high B models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6gi6wm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754099734,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfedua",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n6j9l23",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "XiRw",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n6j5lx2",
                                                              "score": 1,
                                                              "author_fullname": "t2_l4qac",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "That’s good to know since I’m still trying to learn all this stuff, thank you.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n6j9l23",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s good to know since I’m still trying to learn all this stuff, thank you.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mfedua",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6j9l23/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754146220,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754146220,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6j5lx2",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Marksta",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6j40fd",
                                                    "score": 2,
                                                    "author_fullname": "t2_559a1",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "The deeper you get into context, slower performance does go. But just setting it higher won't unless you evicted tensors out of VRAM into system ram to make room for KVcache by allocating more context.\n\nBut no, it's really not a great thing unless the intended use case really is a 'sliding window' context for a forever RP story conversation. But a single response can be more than 4k tokens so that's why the default Ollama one is insanely bad. At least at 32k it could know the last few back and froths even if it forgot the start of the convo once context overflowed. But for most people, we set it at 32k+, 128k ideally and just never do work that overflows out of it since things get erratic at that point.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6j5lx2",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The deeper you get into context, slower performance does go. But just setting it higher won&amp;#39;t unless you evicted tensors out of VRAM into system ram to make room for KVcache by allocating more context.&lt;/p&gt;\n\n&lt;p&gt;But no, it&amp;#39;s really not a great thing unless the intended use case really is a &amp;#39;sliding window&amp;#39; context for a forever RP story conversation. But a single response can be more than 4k tokens so that&amp;#39;s why the default Ollama one is insanely bad. At least at 32k it could know the last few back and froths even if it forgot the start of the convo once context overflowed. But for most people, we set it at 32k+, 128k ideally and just never do work that overflows out of it since things get erratic at that point.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfedua",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6j5lx2/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754144907,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754144907,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6j40fd",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "XiRw",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6iy0i5",
                                          "score": 1,
                                          "author_fullname": "t2_l4qac",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Oh that I forgot the name. I played around with the slider . I set it to max and tried a step up with 8. Can’t say I’ve noticed a difference with any of the numbers though. Smaller would give me the best performance anyway wouldn’t it? Since it doesn’t have to keep track of everything I wrote so far? Or are you saying smaller is worse because it has to get creative and think for itself to fill in the missing blanks? But either way I haven’t noticed a difference for me at least.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6j40fd",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh that I forgot the name. I played around with the slider . I set it to max and tried a step up with 8. Can’t say I’ve noticed a difference with any of the numbers though. Smaller would give me the best performance anyway wouldn’t it? Since it doesn’t have to keep track of everything I wrote so far? Or are you saying smaller is worse because it has to get creative and think for itself to fill in the missing blanks? But either way I haven’t noticed a difference for me at least.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfedua",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6j40fd/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754144380,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754144380,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6iy0i5",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Marksta",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6iw8ur",
                                "score": 2,
                                "author_fullname": "t2_559a1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If you don't set how large of a context size, Ollama then defaults to 4k. Which is an insanely small number to set it to. So then that means after 4k tokens, the LLM will no longer remember who it is, what it's doing, and what you told it to do in your prompt. At that point, the AI will lose its sanity and start rifting off whatever it was last talking about writing total non-sense, potentially stuck in a loop forever spamming random junk characters.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6iy0i5",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you don&amp;#39;t set how large of a context size, Ollama then defaults to 4k. Which is an insanely small number to set it to. So then that means after 4k tokens, the LLM will no longer remember who it is, what it&amp;#39;s doing, and what you told it to do in your prompt. At that point, the AI will lose its sanity and start rifting off whatever it was last talking about writing total non-sense, potentially stuck in a loop forever spamming random junk characters.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfedua",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6iy0i5/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754142287,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754142287,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iw8ur",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "XiRw",
                      "can_mod_post": false,
                      "created_utc": 1754141653,
                      "send_replies": true,
                      "parent_id": "t1_n6iokr2",
                      "score": 1,
                      "author_fullname": "t2_l4qac",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes lol what do you mean by default context?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iw8ur",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes lol what do you mean by default context?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfedua",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6iw8ur/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754141653,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6iokr2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1754138732,
            "send_replies": true,
            "parent_id": "t3_1mfedua",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you using Ollama and getting wreckt by the default context?",
            "edited": 1754138941,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6iokr2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you using Ollama and getting wreckt by the default context?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfedua/question_about_my_dinosaur_computer/n6iokr2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754138732,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfedua",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]