[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "**TL;DR: Transactional Outbox pattern makes cheap, unreliable LLMs more reliable than expensive ones.**\n\nI was spending $20 just on local dev tests with OpenAI. After implementing proper reliability patterns, I migrated everything to DeepSeek and now my costs are literally $0.\n\nHere's the thing everyone misses: **reliability isn't about the model - it's about the system design.**\n\nOpenAI gives you 99.9% uptime for $$$. DeepSeek gives you 80% uptime for free. But with the right patterns, 80% becomes 99.9%+ at the system level.\n\n**The magic: Transactional Outbox Pattern**\n\n1. Accept user request â†’ save to DB â†’ return 200 OK immediately\n2. Background scheduler picks up pending jobs\n3. Retry with exponential backoff until success\n4. Never lose a request, even if your service crashes mid-processing\n\nBuilt this into my [reddit-agent](https://github.com/vitalii-honchar/reddit-agent/tree/main) that runs daily AI analysis on Reddit data. Check it out live: [https://insights.vitaliihonchar.com/](https://insights.vitaliihonchar.com/)\n\n**Results:**\n\n* ðŸ”¥ Went from $20 dev costs to $0 production costs\n* ðŸ”¥ Better reliability than when I was using OpenAI\n* ðŸ”¥ Can scale horizontally without hitting rate limits\n\nThe best part? This works with **any** cheap/local model. Ollama, local Qwen, whatever. Make unreliable models reliable through architecture, not by throwing money at \"premium\" APIs.\n\nFull technical implementation guide: [https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems](https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems)\n\nWho else is tired of OpenAI's pricing and ready to go full local/cheap? ðŸš€",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How I cut LLM costs from $20 to $0 by making unreliable APIs reliable (and ditched OpenAI for DeepSeek)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mj6krv",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.21,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_astexpp3e",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754491469,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Transactional Outbox pattern makes cheap, unreliable LLMs more reliable than expensive ones.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I was spending $20 just on local dev tests with OpenAI. After implementing proper reliability patterns, I migrated everything to DeepSeek and now my costs are literally $0.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the thing everyone misses: &lt;strong&gt;reliability isn&amp;#39;t about the model - it&amp;#39;s about the system design.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;OpenAI gives you 99.9% uptime for $$$. DeepSeek gives you 80% uptime for free. But with the right patterns, 80% becomes 99.9%+ at the system level.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The magic: Transactional Outbox Pattern&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Accept user request â†’ save to DB â†’ return 200 OK immediately&lt;/li&gt;\n&lt;li&gt;Background scheduler picks up pending jobs&lt;/li&gt;\n&lt;li&gt;Retry with exponential backoff until success&lt;/li&gt;\n&lt;li&gt;Never lose a request, even if your service crashes mid-processing&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Built this into my &lt;a href=\"https://github.com/vitalii-honchar/reddit-agent/tree/main\"&gt;reddit-agent&lt;/a&gt; that runs daily AI analysis on Reddit data. Check it out live: &lt;a href=\"https://insights.vitaliihonchar.com/\"&gt;https://insights.vitaliihonchar.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ðŸ”¥ Went from $20 dev costs to $0 production costs&lt;/li&gt;\n&lt;li&gt;ðŸ”¥ Better reliability than when I was using OpenAI&lt;/li&gt;\n&lt;li&gt;ðŸ”¥ Can scale horizontally without hitting rate limits&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The best part? This works with &lt;strong&gt;any&lt;/strong&gt; cheap/local model. Ollama, local Qwen, whatever. Make unreliable models reliable through architecture, not by throwing money at &amp;quot;premium&amp;quot; APIs.&lt;/p&gt;\n\n&lt;p&gt;Full technical implementation guide: &lt;a href=\"https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems\"&gt;https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Who else is tired of OpenAI&amp;#39;s pricing and ready to go full local/cheap? ðŸš€&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1mj6krv",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Historical_Wing_9573",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mj6krv/how_i_cut_llm_costs_from_20_to_0_by_making/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj6krv/how_i_cut_llm_costs_from_20_to_0_by_making/",
            "subreddit_subscribers": 512427,
            "created_utc": 1754491469,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]