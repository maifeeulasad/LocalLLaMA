[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have an Asus laptop with 64 GB RAM and Nvidia RTX 5080 with 16 GB. I've been trying to run different LLM models, including Qwen3 30B. While I'm getting around 30 tokens per second, but when I look at my GPU utilization, I see that it's not even reaching 50%, which makes me wonder if there's more I can get from this GPU because when I run other models, it's a lot slower and still not utilizing the GPU completely. How do I fix this? \n\nI'm making sure that I run the laptop on the performance/turbo profile. I have given LM Studio and Olama prioritization in Nvidia settings, and I'm connecting my laptop to the power supply that provides 380 watts, so power is not a challenge here,   \n  \nPlease share your thoughts. . ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "NVIDIA GPU underutilized.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 39,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mh88gg",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ctx41",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/OWjxWUg66tzOtlpxWOTPkx7qVxbSdB7PipMQFFr4FRM.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754297498,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an Asus laptop with 64 GB RAM and Nvidia RTX 5080 with 16 GB. I&amp;#39;ve been trying to run different LLM models, including Qwen3 30B. While I&amp;#39;m getting around 30 tokens per second, but when I look at my GPU utilization, I see that it&amp;#39;s not even reaching 50%, which makes me wonder if there&amp;#39;s more I can get from this GPU because when I run other models, it&amp;#39;s a lot slower and still not utilizing the GPU completely. How do I fix this? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m making sure that I run the laptop on the performance/turbo profile. I have given LM Studio and Olama prioritization in Nvidia settings, and I&amp;#39;m connecting my laptop to the power supply that provides 380 watts, so power is not a challenge here,   &lt;/p&gt;\n\n&lt;p&gt;Please share your thoughts. . &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/04cjwsaguygf1.png",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/04cjwsaguygf1.png?auto=webp&amp;s=9168f311a7d759b657e124b553e1cfda8859a091",
                    "width": 1759,
                    "height": 500
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/04cjwsaguygf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f56e63703c08f1e15b89671c7bfd11462a862e1",
                      "width": 108,
                      "height": 30
                    },
                    {
                      "url": "https://preview.redd.it/04cjwsaguygf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bde0274d328d65fb99f8c8eefd210a420e32f903",
                      "width": 216,
                      "height": 61
                    },
                    {
                      "url": "https://preview.redd.it/04cjwsaguygf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=124eec8080259e7e84fcf50776ca742a122addf6",
                      "width": 320,
                      "height": 90
                    },
                    {
                      "url": "https://preview.redd.it/04cjwsaguygf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dafc1cdf36686a4a46137b4ebb242cead45f40a1",
                      "width": 640,
                      "height": 181
                    },
                    {
                      "url": "https://preview.redd.it/04cjwsaguygf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fd62e3651f0c260756fd79ad25cdbdf8bcfe13c",
                      "width": 960,
                      "height": 272
                    },
                    {
                      "url": "https://preview.redd.it/04cjwsaguygf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=26d69ced0da9a7f7ba9f89fe47b1c879db6cd35e",
                      "width": 1080,
                      "height": 306
                    }
                  ],
                  "variants": {},
                  "id": "de1X23OADpVokjeNjAhtzYtnWQEqH8HGPM3KiMrwDfg"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mh88gg",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "bu3askoor",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/",
            "stickied": false,
            "url": "https://i.redd.it/04cjwsaguygf1.png",
            "subreddit_subscribers": 509911,
            "created_utc": 1754297498,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6u8cwv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Minute_Attempt3063",
            "can_mod_post": false,
            "created_utc": 1754297941,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 5,
            "author_fullname": "t2_t6m6d9my",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Looks like it is also off loading to system ram, which is likely causing slow downs",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6u8cwv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Looks like it is also off loading to system ram, which is likely causing slow downs&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6u8cwv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754297941,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6uiara",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bu3askoor",
                      "can_mod_post": false,
                      "created_utc": 1754303484,
                      "send_replies": true,
                      "parent_id": "t1_n6ua185",
                      "score": 1,
                      "author_fullname": "t2_ctx41",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I've moved on to lm studio",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6uiara",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve moved on to lm studio&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh88gg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6uiara/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754303484,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ua185",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "jwpbe",
            "can_mod_post": false,
            "created_utc": 1754298923,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 6,
            "author_fullname": "t2_1uqfjcqyh3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "stop using ollama and switch to a different backend",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ua185",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;stop using ollama and switch to a different backend&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6ua185/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754298923,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uapdl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mpasila",
            "can_mod_post": false,
            "created_utc": 1754299317,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 2,
            "author_fullname": "t2_lhhagpdw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Make sure all the layers are offloaded to your GPU (assuming you have enough VRAM) otherwise it might be offloading it to your CPU/RAM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uapdl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Make sure all the layers are offloaded to your GPU (assuming you have enough VRAM) otherwise it might be offloading it to your CPU/RAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6uapdl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754299317,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ub30b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RiskyBizz216",
            "can_mod_post": false,
            "created_utc": 1754299541,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 2,
            "author_fullname": "t2_4eu8nupk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "hmmm something is not adding up. that power draw  (82w) looks really weak for performance/turbo mode\n\nwhat size context are you using?\n\njust curious, which quant are you running that will fit the model + context into 16GB?\n\nin lm studio, are you offloading the everything to GPU?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ub30b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;hmmm something is not adding up. that power draw  (82w) looks really weak for performance/turbo mode&lt;/p&gt;\n\n&lt;p&gt;what size context are you using?&lt;/p&gt;\n\n&lt;p&gt;just curious, which quant are you running that will fit the model + context into 16GB?&lt;/p&gt;\n\n&lt;p&gt;in lm studio, are you offloading the everything to GPU?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6ub30b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754299541,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uph0n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "We-are-just-trolling",
            "can_mod_post": false,
            "created_utc": 1754306789,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 2,
            "author_fullname": "t2_b4th8khl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Memory bandwith is always the bottleneck and in your case of qwen3 30b part of the model is likely on RAM. \n\nContext needs space as well 1gb-2gb and ,unless you change it in settings, will always be in vram.\n\nFor your setup with 16gb vram a mistral small model at q4km 13gb would be best as qwen3 30b doesn't fit into your gpu unless you use under q4 which I wouldn't recommend due to the already low active parameters count making it more sensitive to quantisation. \n\nAdditionally you can get:\n 10-20% more performance on ubuntu using ik_llama\n\nand another 10% by overclocking your Vram memory frequency on the gpu\n\nalternatively if it does fit completely in gpu you could try running exl3 instead of gguf which depending on model can give you quite the speed boost on nvidia cards",
            "edited": 1754307915,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uph0n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Memory bandwith is always the bottleneck and in your case of qwen3 30b part of the model is likely on RAM. &lt;/p&gt;\n\n&lt;p&gt;Context needs space as well 1gb-2gb and ,unless you change it in settings, will always be in vram.&lt;/p&gt;\n\n&lt;p&gt;For your setup with 16gb vram a mistral small model at q4km 13gb would be best as qwen3 30b doesn&amp;#39;t fit into your gpu unless you use under q4 which I wouldn&amp;#39;t recommend due to the already low active parameters count making it more sensitive to quantisation. &lt;/p&gt;\n\n&lt;p&gt;Additionally you can get:\n 10-20% more performance on ubuntu using ik_llama&lt;/p&gt;\n\n&lt;p&gt;and another 10% by overclocking your Vram memory frequency on the gpu&lt;/p&gt;\n\n&lt;p&gt;alternatively if it does fit completely in gpu you could try running exl3 instead of gguf which depending on model can give you quite the speed boost on nvidia cards&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6uph0n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754306789,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ubjqp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "boringcynicism",
            "can_mod_post": false,
            "created_utc": 1754299813,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 1,
            "author_fullname": "t2_1hz0lz0k5i",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen3 30B-A3B won't fit in a 16G card. What quant are you using? How are you offloading?\n\n(If the answer to this isn't clear to you, you just learned why you shouldn't use ollama)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ubjqp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 30B-A3B won&amp;#39;t fit in a 16G card. What quant are you using? How are you offloading?&lt;/p&gt;\n\n&lt;p&gt;(If the answer to this isn&amp;#39;t clear to you, you just learned why you shouldn&amp;#39;t use ollama)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6ubjqp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754299813,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ujfts",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bu3askoor",
            "can_mod_post": false,
            "created_utc": 1754304053,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 1,
            "author_fullname": "t2_ctx41",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "for those who asked about quant, i have been using q3. \n\nu/mpasila u/RiskyBizz216 u/boringcynicism \n\n  \nThe solution i found:  disabling \"Automatic Tuning\"  made the gpu load up to 98% . \n\nThank you for responding.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ujfts",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;for those who asked about quant, i have been using q3. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/mpasila\"&gt;u/mpasila&lt;/a&gt; &lt;a href=\"/u/RiskyBizz216\"&gt;u/RiskyBizz216&lt;/a&gt; &lt;a href=\"/u/boringcynicism\"&gt;u/boringcynicism&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;The solution i found:  disabling &amp;quot;Automatic Tuning&amp;quot;  made the gpu load up to 98% . &lt;/p&gt;\n\n&lt;p&gt;Thank you for responding.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6ujfts/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754304053,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ux2vj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bu3askoor",
                      "can_mod_post": false,
                      "created_utc": 1754309911,
                      "send_replies": true,
                      "parent_id": "t1_n6uthp3",
                      "score": 1,
                      "author_fullname": "t2_ctx41",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "is there a preference? I'm on studio drivers",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ux2vj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;is there a preference? I&amp;#39;m on studio drivers&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh88gg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6ux2vj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754309911,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6uthp3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Falcon3132",
            "can_mod_post": false,
            "created_utc": 1754308488,
            "send_replies": true,
            "parent_id": "t3_1mh88gg",
            "score": 1,
            "author_fullname": "t2_82w26a6h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Might be worth checking the drivers you are using as  - are you on the game ready driver or studio drivers? \n\nI recently picked up a 5080 ROG laptop and had massive issues with this until I swapped drivers.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uthp3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Might be worth checking the drivers you are using as  - are you on the game ready driver or studio drivers? &lt;/p&gt;\n\n&lt;p&gt;I recently picked up a 5080 ROG laptop and had massive issues with this until I swapped drivers.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh88gg/nvidia_gpu_underutilized/n6uthp3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754308488,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh88gg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]