[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi :)\n\nI'm a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don't say anything very personal or sensitive. \n\nI have an rtx 5090 (32 gb)\n\nWhat are the best local models I can run? \n\nThanks",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What can I run on my 5090?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m5w8yl",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.36,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_3231b",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753134845,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don&amp;#39;t say anything very personal or sensitive. &lt;/p&gt;\n\n&lt;p&gt;I have an rtx 5090 (32 gb)&lt;/p&gt;\n\n&lt;p&gt;What are the best local models I can run? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m5w8yl",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "hurfery",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/",
            "subreddit_subscribers": 502515,
            "created_utc": 1753134845,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4f5avr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Tbhmaximillian",
            "can_mod_post": false,
            "created_utc": 1753135473,
            "send_replies": true,
            "parent_id": "t3_1m5w8yl",
            "score": 1,
            "author_fullname": "t2_b5a3coy7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "get yourself LM studio and search for models like Kimi. You will get an overview of all the versions on huggingface that are quantisized and work based your GPU and you can download and start them. Get a feeling what is working with some QWEN versions and then you can use that knowledge for your local ollama models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4f5avr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;get yourself LM studio and search for models like Kimi. You will get an overview of all the versions on huggingface that are quantisized and work based your GPU and you can download and start them. Get a feeling what is working with some QWEN versions and then you can use that knowledge for your local ollama models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4f5avr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753135473,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5w8yl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4f73sp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753136060,
            "send_replies": true,
            "parent_id": "t3_1m5w8yl",
            "score": 2,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can run Nemotron Super 49b at Q4 on your system. if you got a lot of ram, large MoE models like dots.llm1 could also be a good option.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4f73sp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can run Nemotron Super 49b at Q4 on your system. if you got a lot of ram, large MoE models like dots.llm1 could also be a good option.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4f73sp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753136060,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5w8yl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4fdxhc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ThinkExtension2328",
            "can_mod_post": false,
            "created_utc": 1753138289,
            "send_replies": true,
            "parent_id": "t3_1m5w8yl",
            "score": 1,
            "author_fullname": "t2_8eneodlk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Definitely crysis! But that’s probably not what you’re asking. \n\nYou’re good for most 32b models with headroom for context windows. You will struggle with anything larger.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4fdxhc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Definitely crysis! But that’s probably not what you’re asking. &lt;/p&gt;\n\n&lt;p&gt;You’re good for most 32b models with headroom for context windows. You will struggle with anything larger.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4fdxhc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753138289,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m5w8yl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4fgfm1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Weary-Wing-6806",
            "can_mod_post": false,
            "created_utc": 1753139105,
            "send_replies": true,
            "parent_id": "t3_1m5w8yl",
            "score": 1,
            "author_fullname": "t2_1t2xvghrcr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nice, you’re chilling. You can run basically anything under 70B dense, and all the big MoE models like Mixtral and DeepSeek fly. 27B and 33B dense models run smooth, and you can even dabble in multimodal stuff like BakLLaVA or MedGemma if you want. Local whisper, TTS, whatever shouldn't be an issues. You’re not limited by hardware, just pick your model and go go go.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4fgfm1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice, you’re chilling. You can run basically anything under 70B dense, and all the big MoE models like Mixtral and DeepSeek fly. 27B and 33B dense models run smooth, and you can even dabble in multimodal stuff like BakLLaVA or MedGemma if you want. Local whisper, TTS, whatever shouldn&amp;#39;t be an issues. You’re not limited by hardware, just pick your model and go go go.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4fgfm1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753139105,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5w8yl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4fxwrx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RiskyBizz216",
            "can_mod_post": false,
            "created_utc": 1753145005,
            "send_replies": true,
            "parent_id": "t3_1m5w8yl",
            "score": 1,
            "author_fullname": "t2_4eu8nupk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "fellow 5090 owner.\n\nI run some smaller quants of llama 70B which is the best imo\n\nfor speed + quality use devstral Q6 or bf16\n\ni play around with the qwen2.5 32B finetunes, theyre pretty good too\n\nI'm testing Kimi Dev",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4fxwrx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;fellow 5090 owner.&lt;/p&gt;\n\n&lt;p&gt;I run some smaller quants of llama 70B which is the best imo&lt;/p&gt;\n\n&lt;p&gt;for speed + quality use devstral Q6 or bf16&lt;/p&gt;\n\n&lt;p&gt;i play around with the qwen2.5 32B finetunes, theyre pretty good too&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m testing Kimi Dev&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4fxwrx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753145005,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5w8yl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]