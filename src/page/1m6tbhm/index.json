[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hello everyone, i was scrolling on LM studio and always saw model like \"model_name_q4_k_m.gguf\" everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some \"dynamic quantization\" but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?\n\nthank by advance ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What does the _K _S _M _L mean behind the quantization of a model?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1m6tbhm",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_jgegifux8",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753226118,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i was scrolling on LM studio and always saw model like &amp;quot;model_name_q4_k_m.gguf&amp;quot; everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some &amp;quot;dynamic quantization&amp;quot; but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?&lt;/p&gt;\n\n&lt;p&gt;thank by advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m6tbhm",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Hurtcraft01",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
            "subreddit_subscribers": 502981,
            "created_utc": 1753226118,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4mjyc0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BumbleSlob",
            "can_mod_post": false,
            "created_utc": 1753230156,
            "send_replies": true,
            "parent_id": "t3_1m6tbhm",
            "score": 3,
            "author_fullname": "t2_1j7fhlcqkp",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "(This refers to K quants\nBelow)\n\nThe basic gist is it is referring to the size of what is called the block “scaling factor”. A block is a collection of weights sitting in a tensor — but importantly, these weights are quantized and thus just representative of an integer (ex: Q4 has 4 bits per weight, so it can represent all values between 0-15).\n\nTo actually use the weights, they have to be unquantized (which is effectively uncompressing them). This is done by applying the formula\n\nFloat weight = Quantized Weight Int * Scaling Factor + Shift\n\nEach block has both the shift and the scaling factor. If I recall correctly, which I might not be, _S refers to using a FP8, _M refers to FP16, and _L refers to using FP32. So you are increasing the accuracy of the recovered weights, which may or may not make a difference depending on the particular model and quantization. Since, IIRC, a block is 256 weights, you don’t really end up saving that much space when you do the math of how many bits you save overall.\n\nSo anyway, now that you’ve uncompressed the weight, you can actually start using it as intended in the tensor (ie matrix). \n\nSource: me, I got deep into reading llama.cpp’s source code while writing my own inference engine and needing to understand how to decode GGUF files\n\nLast thing: for folks who always wondered why you don’t actually get nice round numbers for “bits per weight”, this is the “why”.",
            "edited": 1753230424,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4mjyc0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;(This refers to K quants\nBelow)&lt;/p&gt;\n\n&lt;p&gt;The basic gist is it is referring to the size of what is called the block “scaling factor”. A block is a collection of weights sitting in a tensor — but importantly, these weights are quantized and thus just representative of an integer (ex: Q4 has 4 bits per weight, so it can represent all values between 0-15).&lt;/p&gt;\n\n&lt;p&gt;To actually use the weights, they have to be unquantized (which is effectively uncompressing them). This is done by applying the formula&lt;/p&gt;\n\n&lt;p&gt;Float weight = Quantized Weight Int * Scaling Factor + Shift&lt;/p&gt;\n\n&lt;p&gt;Each block has both the shift and the scaling factor. If I recall correctly, which I might not be, _S refers to using a FP8, _M refers to FP16, and _L refers to using FP32. So you are increasing the accuracy of the recovered weights, which may or may not make a difference depending on the particular model and quantization. Since, IIRC, a block is 256 weights, you don’t really end up saving that much space when you do the math of how many bits you save overall.&lt;/p&gt;\n\n&lt;p&gt;So anyway, now that you’ve uncompressed the weight, you can actually start using it as intended in the tensor (ie matrix). &lt;/p&gt;\n\n&lt;p&gt;Source: me, I got deep into reading llama.cpp’s source code while writing my own inference engine and needing to understand how to decode GGUF files&lt;/p&gt;\n\n&lt;p&gt;Last thing: for folks who always wondered why you don’t actually get nice round numbers for “bits per weight”, this is the “why”.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4mjyc0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753230156,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6tbhm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4ma56b",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "NNN_Throwaway2",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4m9edn",
                                "score": 2,
                                "author_fullname": "t2_8rrihts9",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Models have multiple layers and components, the quantization doesn't need to be homogeneous.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4ma56b",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Models have multiple layers and components, the quantization doesn&amp;#39;t need to be homogeneous.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m6tbhm",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4ma56b/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753226914,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753226914,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4m9edn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Hurtcraft01",
                      "can_mod_post": false,
                      "created_utc": 1753226676,
                      "send_replies": true,
                      "parent_id": "t1_n4m8pno",
                      "score": 1,
                      "author_fullname": "t2_jgegifux8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "But the precision isnt determined by the scale and the quantization ? Like if its q4 the weight will be stored on 4 bit independently of the letter s m or l right?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4m9edn",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;But the precision isnt determined by the scale and the quantization ? Like if its q4 the weight will be stored on 4 bit independently of the letter s m or l right?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m6tbhm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4m9edn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753226676,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4m8pno",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "NNN_Throwaway2",
            "can_mod_post": false,
            "created_utc": 1753226456,
            "send_replies": true,
            "parent_id": "t3_1m6tbhm",
            "score": 2,
            "author_fullname": "t2_8rrihts9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "small, medium, large referring to the precision and thus quality. The technical implications will vary depending on quant author and the exact methods they used.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4m8pno",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;small, medium, large referring to the precision and thus quality. The technical implications will vary depending on quant author and the exact methods they used.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4m8pno/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753226456,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6tbhm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4m91s7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Toooooool",
            "can_mod_post": false,
            "created_utc": 1753226563,
            "send_replies": true,
            "parent_id": "t3_1m6tbhm",
            "score": 2,
            "author_fullname": "t2_8llornh4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The medium and small versions are pruned for lesser used weights in order to save a little extra memory without quantizing the model further.\n\nThink of it like this; You got the word ice cream, but you've also got words like sorbet, gelato, sundae, affogato.. these words are less used variations for the same thing so to save that little extra memory these lesser-used words are removed in favor of just relying on the more commonly used \"ice cream\"\n\n(super oversimplified but ye)\n\n  \nedit: no wait i'm getting it mixed up with the parameter count i think 😭",
            "edited": 1753227171,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4m91s7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The medium and small versions are pruned for lesser used weights in order to save a little extra memory without quantizing the model further.&lt;/p&gt;\n\n&lt;p&gt;Think of it like this; You got the word ice cream, but you&amp;#39;ve also got words like sorbet, gelato, sundae, affogato.. these words are less used variations for the same thing so to save that little extra memory these lesser-used words are removed in favor of just relying on the more commonly used &amp;quot;ice cream&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;(super oversimplified but ye)&lt;/p&gt;\n\n&lt;p&gt;edit: no wait i&amp;#39;m getting it mixed up with the parameter count i think 😭&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4m91s7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753226563,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6tbhm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]