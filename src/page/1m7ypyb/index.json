[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi everyone,\n\nAccording to ArtificialAnalysis data (from their hardware benchmarks, like at [https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1](https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1)), the performance difference between NVIDIA's 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don't show a huge gap despite B200's superior specs on paper.\n\nIs this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I'd love to hear your thoughts or any insights from real-world usage!\n\nThanks!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why is B200 performing similarly to H200? (ArtificialAnalysis)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m7ypyb",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_93zqvlmj",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753345144,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;According to ArtificialAnalysis data (from their hardware benchmarks, like at &lt;a href=\"https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1\"&gt;https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1&lt;/a&gt;), the performance difference between NVIDIA&amp;#39;s 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don&amp;#39;t show a huge gap despite B200&amp;#39;s superior specs on paper.&lt;/p&gt;\n\n&lt;p&gt;Is this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I&amp;#39;d love to hear your thoughts or any insights from real-world usage!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a",
                    "width": 1260,
                    "height": 700
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4",
                      "width": 108,
                      "height": 60
                    },
                    {
                      "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672",
                      "width": 216,
                      "height": 120
                    },
                    {
                      "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e",
                      "width": 320,
                      "height": 177
                    },
                    {
                      "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc",
                      "width": 640,
                      "height": 355
                    },
                    {
                      "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a",
                      "width": 960,
                      "height": 533
                    },
                    {
                      "url": "https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442",
                      "width": 1080,
                      "height": 600
                    }
                  ],
                  "variants": {},
                  "id": "RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m7ypyb",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Cyp9715",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/",
            "subreddit_subscribers": 503759,
            "created_utc": 1753345144,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4v6dzi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "djm07231",
            "can_mod_post": false,
            "created_utc": 1753346123,
            "send_replies": true,
            "parent_id": "t3_1m7ypyb",
            "score": 6,
            "author_fullname": "t2_zuxto",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think that Blackwell place more emphasis on “rack-scale” architectures.\n\nIt does seem to emphasize how the compute scales in a datacenter rather than just a single node.\n\nSo this would probably work best for training of large models where multi-node communications and cohesiveness matters more and Trillion-parameter scale models where multi-node inference is a must.\n\nFor models smaller than this where multi-node scaling isn’t as important it performance might not scale as much.\n\nAnother obvious point is that Blackwell is still pretty new so software inference stack hasn’t been optimized as much compared to Hopper.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4v6dzi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think that Blackwell place more emphasis on “rack-scale” architectures.&lt;/p&gt;\n\n&lt;p&gt;It does seem to emphasize how the compute scales in a datacenter rather than just a single node.&lt;/p&gt;\n\n&lt;p&gt;So this would probably work best for training of large models where multi-node communications and cohesiveness matters more and Trillion-parameter scale models where multi-node inference is a must.&lt;/p&gt;\n\n&lt;p&gt;For models smaller than this where multi-node scaling isn’t as important it performance might not scale as much.&lt;/p&gt;\n\n&lt;p&gt;Another obvious point is that Blackwell is still pretty new so software inference stack hasn’t been optimized as much compared to Hopper.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4v6dzi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753346123,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7ypyb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4vklhm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "shing3232",
            "can_mod_post": false,
            "created_utc": 1753353845,
            "send_replies": true,
            "parent_id": "t3_1m7ypyb",
            "score": 3,
            "author_fullname": "t2_ze4mg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There is not big different in fp8/bf16 performance between b200 and h200",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4vklhm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is not big different in fp8/bf16 performance between b200 and h200&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4vklhm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753353845,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7ypyb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4v71p0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dr4kin",
            "can_mod_post": false,
            "created_utc": 1753346501,
            "send_replies": true,
            "parent_id": "t3_1m7ypyb",
            "score": 1,
            "author_fullname": "t2_x5gqb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A lot of performance also comes from node shrinkage which Blackwell didn't have.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4v71p0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A lot of performance also comes from node shrinkage which Blackwell didn&amp;#39;t have.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4v71p0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753346501,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7ypyb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4vksfe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "shing3232",
            "can_mod_post": false,
            "created_utc": 1753353936,
            "send_replies": true,
            "parent_id": "t3_1m7ypyb",
            "score": 1,
            "author_fullname": "t2_ze4mg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There is not big different in fp8/bf16 performance between b200 and h200",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4vksfe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is not big different in fp8/bf16 performance between b200 and h200&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4vksfe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753353936,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7ypyb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "1c60b73a-72f2-11ee-bdcc-8e2a7b94d6a0",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4vwjpg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Temporary-Size7310",
            "can_mod_post": false,
            "created_utc": 1753358894,
            "send_replies": true,
            "parent_id": "t3_1m7ypyb",
            "score": 1,
            "author_fullname": "t2_v2yh9yxw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "B200 or any Blackwell are brillant with NVFP4 keeping quite the same quality compared to FP8, the document doesn't show what they use to compare ? \nVLLM, TensorRT-LLM, which Quant type and size ?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4vwjpg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "textgen web UI"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;B200 or any Blackwell are brillant with NVFP4 keeping quite the same quality compared to FP8, the document doesn&amp;#39;t show what they use to compare ? \nVLLM, TensorRT-LLM, which Quant type and size ?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4vwjpg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753358894,
            "author_flair_text": "textgen web UI",
            "treatment_tags": [],
            "link_id": "t3_1m7ypyb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]