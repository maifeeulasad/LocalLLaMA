[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I am experimenting with local llms. Have been using the 780m integrated onto the 7840u on my current machine which has 64GB of LPDDR5X memory clocked at 7500 MT/s (16GB allocated to the GPU). I have also been playing with my eGPU over oculink (GPD G1). I am looking at Strix Halo for future dev (especially mobile), and realized that as far as memory bandwidth the GPD G1 should be similar, so I decided to test Qwen3-8b-Q4\\_K\\_M in LM Studio with the Vulkan and ROCm runtimes against it.\n\nI was kind of appalled at the performance. 12.68 tok/sec when asking to write a short story. Interestingly on my iGPU I get 14.39 tok/sec... From my understanding Strix Halo should be getting 35-40 tok/sec on such a model and Strix Halo should have similar or worse memory bandwidth than my eGPU, so why is my eGPU sucking so badly that it's worse than my iGPU? Is Oculink limiting things for some reason or some other part of my system? Any good way to diagnose?\n\nI was hoping I could get an idea of Strix Halo performance from my current rig, even if it came with the caveat of limited context size.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why is my external RX 7600M XT (GPD G1) slow by comparison?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m7i9pl",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_y0abrfm",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753297979,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am experimenting with local llms. Have been using the 780m integrated onto the 7840u on my current machine which has 64GB of LPDDR5X memory clocked at 7500 MT/s (16GB allocated to the GPU). I have also been playing with my eGPU over oculink (GPD G1). I am looking at Strix Halo for future dev (especially mobile), and realized that as far as memory bandwidth the GPD G1 should be similar, so I decided to test Qwen3-8b-Q4_K_M in LM Studio with the Vulkan and ROCm runtimes against it.&lt;/p&gt;\n\n&lt;p&gt;I was kind of appalled at the performance. 12.68 tok/sec when asking to write a short story. Interestingly on my iGPU I get 14.39 tok/sec... From my understanding Strix Halo should be getting 35-40 tok/sec on such a model and Strix Halo should have similar or worse memory bandwidth than my eGPU, so why is my eGPU sucking so badly that it&amp;#39;s worse than my iGPU? Is Oculink limiting things for some reason or some other part of my system? Any good way to diagnose?&lt;/p&gt;\n\n&lt;p&gt;I was hoping I could get an idea of Strix Halo performance from my current rig, even if it came with the caveat of limited context size.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m7i9pl",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "cfogrady",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7i9pl/why_is_my_external_rx_7600m_xt_gpd_g1_slow_by/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7i9pl/why_is_my_external_rx_7600m_xt_gpd_g1_slow_by/",
            "subreddit_subscribers": 503517,
            "created_utc": 1753297979,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4s0j9j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "cfogrady",
                      "can_mod_post": false,
                      "created_utc": 1753302124,
                      "send_replies": true,
                      "parent_id": "t1_n4rxign",
                      "score": 1,
                      "author_fullname": "t2_y0abrfm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It has all layers in GPU (according to LM Studio). And GPU Memory appears to reflect that. I have the 2023 version. Bought in late 2023.\n\nHadn't thought about TDP... I do remember there being some ways to flash it when I first got it, which I had forgotten about... I wouldn't have expected TDP to impact memory bandwidth and token generation by that much. I would have expected it to impact prompt processing more. Wish there was a tool to measure GPU memory bandwidth.\n\nP.S. Looks like it came by default with 100W TDP with an option to flash up to 120W TDP or down to 80W. I've never flashed it, so mine should still be at the default 100W TDP.",
                      "edited": 1753302510,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4s0j9j",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It has all layers in GPU (according to LM Studio). And GPU Memory appears to reflect that. I have the 2023 version. Bought in late 2023.&lt;/p&gt;\n\n&lt;p&gt;Hadn&amp;#39;t thought about TDP... I do remember there being some ways to flash it when I first got it, which I had forgotten about... I wouldn&amp;#39;t have expected TDP to impact memory bandwidth and token generation by that much. I would have expected it to impact prompt processing more. Wish there was a tool to measure GPU memory bandwidth.&lt;/p&gt;\n\n&lt;p&gt;P.S. Looks like it came by default with 100W TDP with an option to flash up to 120W TDP or down to 80W. I&amp;#39;ve never flashed it, so mine should still be at the default 100W TDP.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7i9pl",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7i9pl/why_is_my_external_rx_7600m_xt_gpd_g1_slow_by/n4s0j9j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753302124,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4rxign",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Rich_Repeat_22",
            "can_mod_post": false,
            "created_utc": 1753301283,
            "send_replies": true,
            "parent_id": "t3_1m7i9pl",
            "score": 1,
            "author_fullname": "t2_viufiki6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is this the 2023 version or the 2024 version with the TDP switch? \n\nAlso have you made sure 100% of the dGPU is been used? \n\nWhat's weird with the perf is that Qwen3-8b-Q4\\_K\\_M should fit in the 8GB VRAM. 🤔",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4rxign",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is this the 2023 version or the 2024 version with the TDP switch? &lt;/p&gt;\n\n&lt;p&gt;Also have you made sure 100% of the dGPU is been used? &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s weird with the perf is that Qwen3-8b-Q4_K_M should fit in the 8GB VRAM. 🤔&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7i9pl/why_is_my_external_rx_7600m_xt_gpd_g1_slow_by/n4rxign/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753301283,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7i9pl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]