[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "First time poster, so I'm not sure if this is the right area, but I'm looking for some help troubleshooting performance issues.\n\nWhen using models that fit in VRAM, I get the expected performance or within reason.\n\nThe issues occur when using models that need to spill over into system RAM. Specifically, I've noticed a significant drop in performance with the model **qwen3:30b-a3b-q4\\_K\\_M**, though **Deepseek R1 32B** is showing similar issues.\n\nWhen I run qwen3:30b-a3b-q4\\_K\\_M on CPU with no GPU installed I get \\~19t/s as measured by Open Web UI.\n\nWhen running qwen3:30b-a3b-q4\\_K\\_M on a mix of GPU/CPU I get the worse performance then running on CPU only. The performance degrades even further the more layers I offload to the CPU.\n\nTested the following in Ollama by modifying num_gpu:\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    20 GB    25%/75% CPU/GPU    4096  \neval rate:           10.02 tokens/s\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    20 GB    73%/27% CPU/GPU    4096  \neval rate:            4.35 tokens/s\n\nqwen3:30b-a3b-q4\\_K\\_M    0b28110b7a33    19 GB    100% CPU     4096  \neval rate:            2.49 tokens/s\n\nOS is hosted in Proxmox. Going from 30 cores to 15 cores assigned to the VM had no effect on performance.\n\nSystem Specs:\n\nCPU: Gold 6254\n\nGPU: Nvidia T4 (16gb)\n\nOS: ubuntu 24.04\n\nOllama 0.10.1\n\nNvidia Driver 570.169 Cuda 12.8\n\nAny suggestions would be helpful.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Performance issues when using GPU and CPU",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mffuv0",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_seb9b",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754105472,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754103612,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time poster, so I&amp;#39;m not sure if this is the right area, but I&amp;#39;m looking for some help troubleshooting performance issues.&lt;/p&gt;\n\n&lt;p&gt;When using models that fit in VRAM, I get the expected performance or within reason.&lt;/p&gt;\n\n&lt;p&gt;The issues occur when using models that need to spill over into system RAM. Specifically, I&amp;#39;ve noticed a significant drop in performance with the model &lt;strong&gt;qwen3:30b-a3b-q4_K_M&lt;/strong&gt;, though &lt;strong&gt;Deepseek R1 32B&lt;/strong&gt; is showing similar issues.&lt;/p&gt;\n\n&lt;p&gt;When I run qwen3:30b-a3b-q4_K_M on CPU with no GPU installed I get ~19t/s as measured by Open Web UI.&lt;/p&gt;\n\n&lt;p&gt;When running qwen3:30b-a3b-q4_K_M on a mix of GPU/CPU I get the worse performance then running on CPU only. The performance degrades even further the more layers I offload to the CPU.&lt;/p&gt;\n\n&lt;p&gt;Tested the following in Ollama by modifying num_gpu:&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    20 GB    25%/75% CPU/GPU    4096&lt;br/&gt;\neval rate:           10.02 tokens/s&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    20 GB    73%/27% CPU/GPU    4096&lt;br/&gt;\neval rate:            4.35 tokens/s&lt;/p&gt;\n\n&lt;p&gt;qwen3:30b-a3b-q4_K_M    0b28110b7a33    19 GB    100% CPU     4096&lt;br/&gt;\neval rate:            2.49 tokens/s&lt;/p&gt;\n\n&lt;p&gt;OS is hosted in Proxmox. Going from 30 cores to 15 cores assigned to the VM had no effect on performance.&lt;/p&gt;\n\n&lt;p&gt;System Specs:&lt;/p&gt;\n\n&lt;p&gt;CPU: Gold 6254&lt;/p&gt;\n\n&lt;p&gt;GPU: Nvidia T4 (16gb)&lt;/p&gt;\n\n&lt;p&gt;OS: ubuntu 24.04&lt;/p&gt;\n\n&lt;p&gt;Ollama 0.10.1&lt;/p&gt;\n\n&lt;p&gt;Nvidia Driver 570.169 Cuda 12.8&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mffuv0",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "BabySasquatch1",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/",
            "subreddit_subscribers": 509054,
            "created_utc": 1754103612,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6h3cp8",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "BabySasquatch1",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6gyeku",
                                          "score": 1,
                                          "author_fullname": "t2_seb9b",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Eval tokens is in the 100 t/s range on cpu only. I will have to look into llama.cpp.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6h3cp8",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Eval tokens is in the 100 t/s range on cpu only. I will have to look into llama.cpp.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mffuv0",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/n6h3cp8/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754108372,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754108372,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6gyeku",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Clear-Ad-9312",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6gwkvh",
                                "score": 1,
                                "author_fullname": "t2_13gn4f8kdq",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "are you sure that 19 t/s is not prompt processing and it is the actual eval t/s?\n\nif adding the GPU(even if unusued) is truly tanking performance, then you likely would get better support if you go to the ollama discord, or testing other inference engines like llama.cpp or ik\\_llama.cpp",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6gyeku",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;are you sure that 19 t/s is not prompt processing and it is the actual eval t/s?&lt;/p&gt;\n\n&lt;p&gt;if adding the GPU(even if unusued) is truly tanking performance, then you likely would get better support if you go to the ollama discord, or testing other inference engines like llama.cpp or ik_llama.cpp&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mffuv0",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/n6gyeku/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754106188,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754106188,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6gwkvh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "BabySasquatch1",
                      "can_mod_post": false,
                      "created_utc": 1754105410,
                      "send_replies": true,
                      "parent_id": "t1_n6gvou5",
                      "score": 1,
                      "author_fullname": "t2_seb9b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "With cpu only, no gpu attached to the vm I get 19 t/s. As soon as I add the gpu I get worse performance. With the gpu added, but num_gpu set to 0 which offloads the whole model to cpu, I get 2ish t/s.\n\nThe xeon may only support 2933, but it's 6 channels, so its more memory bandwidth than most consumer ddr5 cpus.\n\nI would prefer to run everything locally.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6gwkvh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;With cpu only, no gpu attached to the vm I get 19 t/s. As soon as I add the gpu I get worse performance. With the gpu added, but num_gpu set to 0 which offloads the whole model to cpu, I get 2ish t/s.&lt;/p&gt;\n\n&lt;p&gt;The xeon may only support 2933, but it&amp;#39;s 6 channels, so its more memory bandwidth than most consumer ddr5 cpus.&lt;/p&gt;\n\n&lt;p&gt;I would prefer to run everything locally.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mffuv0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/n6gwkvh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754105410,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6gvou5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Clear-Ad-9312",
            "can_mod_post": false,
            "created_utc": 1754105034,
            "send_replies": true,
            "parent_id": "t3_1mffuv0",
            "score": 0,
            "author_fullname": "t2_13gn4f8kdq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "you say you are getting 19t/s with no GPU but your 100% CPU shows an eval rate of 2.49 t/s, that is slow...  \nclearly there is a discrepancy you need to address here for us.\n\nalso, no other way around it, you should either upgrade hardware or use the openrouter API for your usage.\n\nmy friends and I built a server that routes our requests through any API that is currently free to use. We are college students, so we make do with what we can.\n\nalso, RAM/VRAM speed is most important factor aside from just raw computational power. your xeon cpu can only handle relatively slower RAM speeds up to DDR4-2933. For better performance, you need DDR5 at 6000, but even then that is less than half the speed of modern GPUs VRAM capabilities. a fully geared out Ryzen AI Max+ 395 with 128GB would be getting better performance, and with the T4 on top you likely would get more than 20 t/s",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6gvou5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you say you are getting 19t/s with no GPU but your 100% CPU shows an eval rate of 2.49 t/s, that is slow...&lt;br/&gt;\nclearly there is a discrepancy you need to address here for us.&lt;/p&gt;\n\n&lt;p&gt;also, no other way around it, you should either upgrade hardware or use the openrouter API for your usage.&lt;/p&gt;\n\n&lt;p&gt;my friends and I built a server that routes our requests through any API that is currently free to use. We are college students, so we make do with what we can.&lt;/p&gt;\n\n&lt;p&gt;also, RAM/VRAM speed is most important factor aside from just raw computational power. your xeon cpu can only handle relatively slower RAM speeds up to DDR4-2933. For better performance, you need DDR5 at 6000, but even then that is less than half the speed of modern GPUs VRAM capabilities. a fully geared out Ryzen AI Max+ 395 with 128GB would be getting better performance, and with the T4 on top you likely would get more than 20 t/s&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/n6gvou5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754105034,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mffuv0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6h32zv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "BabySasquatch1",
                      "can_mod_post": false,
                      "created_utc": 1754108246,
                      "send_replies": true,
                      "parent_id": "t1_n6h01bf",
                      "score": 1,
                      "author_fullname": "t2_seb9b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "AFAIK ollama is just a wrapper for llama.cpp, so it makes sense it would inherit the same issues. I'll give you suggestions a try tomorrow.\n\nThanks!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6h32zv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;AFAIK ollama is just a wrapper for llama.cpp, so it makes sense it would inherit the same issues. I&amp;#39;ll give you suggestions a try tomorrow.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mffuv0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/n6h32zv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754108246,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6h01bf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754106890,
            "send_replies": true,
            "parent_id": "t3_1mffuv0",
            "score": 1,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hrm... I wonder if ollama inherited [this bug](https://github.com/ggml-org/llama.cpp/issues/14201)\n\nYou might want to set up llama.cpp for some easier debugging.  (At least llama-bench.)\n\nSome things to check:\n\n- You aren't dual CPU (you indicate it's one, just sanity checking)\n- Run with `--threads 14`.  Performance tanks if you use more than your physical core count (or if any cores are heavily loaded)\n- Check on the host that the threads seem to be distributed and aren't running on hyper thread cores.  Maybe disable SMT in the bios to rule that out when testing\n- Benchmark with the GPU installed and `CUDA_VISIBLE_DEVICES=-1` to hide the GPU to see if you replicate the \"4090 not installed\" performance.\n- Try `llama-bench -ngl 0 -p 512 -n 128 -r 2 -fa 1 -m Qwen3-30B-A3B-Q4_K_M.gguf --no-kv-offload 0,1 --no-op-offload 0,1` to see if there is any strange performance deviation\n- Check your 4090's connection... is it running at PCIe3x16?  (`lspci -vv` shows `LnkCap: Speed 8GT/s, Width x16`)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6h01bf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hrm... I wonder if ollama inherited &lt;a href=\"https://github.com/ggml-org/llama.cpp/issues/14201\"&gt;this bug&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You might want to set up llama.cpp for some easier debugging.  (At least llama-bench.)&lt;/p&gt;\n\n&lt;p&gt;Some things to check:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You aren&amp;#39;t dual CPU (you indicate it&amp;#39;s one, just sanity checking)&lt;/li&gt;\n&lt;li&gt;Run with &lt;code&gt;--threads 14&lt;/code&gt;.  Performance tanks if you use more than your physical core count (or if any cores are heavily loaded)&lt;/li&gt;\n&lt;li&gt;Check on the host that the threads seem to be distributed and aren&amp;#39;t running on hyper thread cores.  Maybe disable SMT in the bios to rule that out when testing&lt;/li&gt;\n&lt;li&gt;Benchmark with the GPU installed and &lt;code&gt;CUDA_VISIBLE_DEVICES=-1&lt;/code&gt; to hide the GPU to see if you replicate the &amp;quot;4090 not installed&amp;quot; performance.&lt;/li&gt;\n&lt;li&gt;Try &lt;code&gt;llama-bench -ngl 0 -p 512 -n 128 -r 2 -fa 1 -m Qwen3-30B-A3B-Q4_K_M.gguf --no-kv-offload 0,1 --no-op-offload 0,1&lt;/code&gt; to see if there is any strange performance deviation&lt;/li&gt;\n&lt;li&gt;Check your 4090&amp;#39;s connection... is it running at PCIe3x16?  (&lt;code&gt;lspci -vv&lt;/code&gt; shows &lt;code&gt;LnkCap: Speed 8GT/s, Width x16&lt;/code&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mffuv0/performance_issues_when_using_gpu_and_cpu/n6h01bf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754106890,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mffuv0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]