[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I just checked the monthly LLM API costs at my firm, and it's insanely high. I donâ€™t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.\n\nAnd honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Traditional Data Science work is going to be back",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lxvrjm",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 17,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_h1zso7cq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 17,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752310104,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just checked the monthly LLM API costs at my firm, and it&amp;#39;s insanely high. I donâ€™t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.&lt;/p&gt;\n\n&lt;p&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lxvrjm",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Competitive_Push5407",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/",
            "subreddit_subscribers": 497824,
            "created_utc": 1752310104,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2p6ne4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1752310774,
            "send_replies": true,
            "parent_id": "t3_1lxvrjm",
            "score": 8,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "could you share the costs please, is it 3, 4, 5 digits in USD?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2p6ne4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;could you share the costs please, is it 3, 4, 5 digits in USD?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2p6ne4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752310774,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxvrjm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2ph67d",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "meta_voyager7",
                      "can_mod_post": false,
                      "created_utc": 1752317036,
                      "send_replies": true,
                      "parent_id": "t1_n2p9es2",
                      "score": 2,
                      "author_fullname": "t2_1fywh12qqy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Also small open source llms in 1 to 2 years would be as good as gpt 4.1 or O3. So it wont need as much as energy, also harder and software for llm inference is getting optimized like what happened with cpusÂ ",
                      "edited": 1752322387,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2ph67d",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also small open source llms in 1 to 2 years would be as good as gpt 4.1 or O3. So it wont need as much as energy, also harder and software for llm inference is getting optimized like what happened with cpusÂ &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxvrjm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2ph67d/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752317036,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2pw6k4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "annakhouri2150",
                      "can_mod_post": false,
                      "created_utc": 1752324003,
                      "send_replies": true,
                      "parent_id": "t1_n2p9es2",
                      "score": 2,
                      "author_fullname": "t2_1jmvw9zdnb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "There's actually no evidence that LLM Cloud providers are under pricing their product that much:\n\n\n\"\n\nThe LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don't think they are, for a few reasons. I'd instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise.\n\nFirst, there's not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there's no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn't explain it. Note that it's not just that you'd be losing money on each of these queries for no benefit, you're losing the compute that could be spent on training, research, or more useful types of inference.\n\nSecond, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn't even first party hosting -- I sure can't figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting.\n\nThird, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs.\n\nFourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links.\"\n\nhttps://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2pw6k4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s actually no evidence that LLM Cloud providers are under pricing their product that much:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don&amp;#39;t think they are, for a few reasons. I&amp;#39;d instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise.&lt;/p&gt;\n\n&lt;p&gt;First, there&amp;#39;s not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there&amp;#39;s no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn&amp;#39;t explain it. Note that it&amp;#39;s not just that you&amp;#39;d be losing money on each of these queries for no benefit, you&amp;#39;re losing the compute that could be spent on training, research, or more useful types of inference.&lt;/p&gt;\n\n&lt;p&gt;Second, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn&amp;#39;t even first party hosting -- I sure can&amp;#39;t figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting.&lt;/p&gt;\n\n&lt;p&gt;Third, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs.&lt;/p&gt;\n\n&lt;p&gt;Fourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/\"&gt;https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxvrjm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pw6k4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752324003,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2q18rn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "moofunk",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2pgwr5",
                                "score": 1,
                                "author_fullname": "t2_6gpjg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Solar is the easiest to install in sizes small enough for your needs.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2q18rn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Solar is the easiest to install in sizes small enough for your needs.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lxvrjm",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2q18rn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752325955,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752325955,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2pgwr5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "meta_voyager7",
                      "can_mod_post": false,
                      "created_utc": 1752316892,
                      "send_replies": true,
                      "parent_id": "t1_n2p9es2",
                      "score": 1,
                      "author_fullname": "t2_1fywh12qqy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "why solar panel related companies would be winners? there are other green energy companies like wind turbine, hydro, hydrogen etc",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2pgwr5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;why solar panel related companies would be winners? there are other green energy companies like wind turbine, hydro, hydrogen etc&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxvrjm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pgwr5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752316892,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2p9es2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1752312481,
            "send_replies": true,
            "parent_id": "t3_1lxvrjm",
            "score": 17,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Remember cloud LLMs are heavily subsidized by VCs, when that stop prices will get higher.\n\nThen business will start deploying local agents just for their power bill to skyrocket.\n\nThe ultimate winner of AI business will be the solar panel vendors.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2p9es2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Remember cloud LLMs are heavily subsidized by VCs, when that stop prices will get higher.&lt;/p&gt;\n\n&lt;p&gt;Then business will start deploying local agents just for their power bill to skyrocket.&lt;/p&gt;\n\n&lt;p&gt;The ultimate winner of AI business will be the solar panel vendors.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2p9es2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752312481,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1lxvrjm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 17
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2pxhhp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "BidWestern1056",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2ps1id",
                                "score": 1,
                                "author_fullname": "t2_uzxql7po",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "the core npcpy library took me about 6-7 months to get to more or less stable point. past couple months been simplifying and trying to build on top of it. for a while i tried to have this kind of interactivity be a \"data\" mode within npcsh but just couldnt figure out a way that made sense that wasnt just like implicitly assuming pandas or st or other. in the end i realized i could just do the thing i had initially intended which was to apply the npcsh flow (assume bash, otherwise natural language) to python and then the first version of this took me a day or two. been using it for some research consulting ive been doing over the past month or so so trying to ensure as few bugs as possible.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2pxhhp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the core npcpy library took me about 6-7 months to get to more or less stable point. past couple months been simplifying and trying to build on top of it. for a while i tried to have this kind of interactivity be a &amp;quot;data&amp;quot; mode within npcsh but just couldnt figure out a way that made sense that wasnt just like implicitly assuming pandas or st or other. in the end i realized i could just do the thing i had initially intended which was to apply the npcsh flow (assume bash, otherwise natural language) to python and then the first version of this took me a day or two. been using it for some research consulting ive been doing over the past month or so so trying to ensure as few bugs as possible.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lxvrjm",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pxhhp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752324521,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752324521,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2ps1id",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "__JockY__",
                      "can_mod_post": false,
                      "created_utc": 1752322282,
                      "send_replies": true,
                      "parent_id": "t1_n2pfkr3",
                      "score": 1,
                      "author_fullname": "t2_qf8h7ka8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That must have taken a while. ðŸ˜Ž",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2ps1id",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That must have taken a while. ðŸ˜Ž&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxvrjm",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2ps1id/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752322282,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2pfkr3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BidWestern1056",
            "can_mod_post": false,
            "created_utc": 1752316141,
            "send_replies": true,
            "parent_id": "t3_1lxvrjm",
            "score": 1,
            "author_fullname": "t2_uzxql7po",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "one of the key pain points for me as a data scientist was not having AI integrated well into my iterative python lifestyle, so i built a modified python repl which lets you have AI execute code directly and build with you as you go, and the variables and functions it produces you can then inspect directly to build on yourself.\nworks w local or api models, and it is pomodoro inspired to encourage you to occasionally take your experimentation and turn it into automations so you dont get lost in a sea of tinkering.\ncheck it out you may like it as a DS urself\nhttps://github.com/NPC-Worldwide/npcsh?tab=readme-ov-file#guac",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2pfkr3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;one of the key pain points for me as a data scientist was not having AI integrated well into my iterative python lifestyle, so i built a modified python repl which lets you have AI execute code directly and build with you as you go, and the variables and functions it produces you can then inspect directly to build on yourself.\nworks w local or api models, and it is pomodoro inspired to encourage you to occasionally take your experimentation and turn it into automations so you dont get lost in a sea of tinkering.\ncheck it out you may like it as a DS urself\n&lt;a href=\"https://github.com/NPC-Worldwide/npcsh?tab=readme-ov-file#guac\"&gt;https://github.com/NPC-Worldwide/npcsh?tab=readme-ov-file#guac&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pfkr3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752316141,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxvrjm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2p6wbh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Fit-Produce420",
            "can_mod_post": false,
            "created_utc": 1752310928,
            "send_replies": true,
            "parent_id": "t3_1lxvrjm",
            "score": 1,
            "author_fullname": "t2_tewf9bdwg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Bro, this new automation tech works. It's not just some Dragon Naturally Speaking on steroids this time. We're close, we had 10e24 units of compute, it browns out some grids and warms up the salmon, but bro we only need to scale to 10e48 units of compute and the benches will be very close to wiping out jobs across the board!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2p6wbh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Bro, this new automation tech works. It&amp;#39;s not just some Dragon Naturally Speaking on steroids this time. We&amp;#39;re close, we had 10e24 units of compute, it browns out some grids and warms up the salmon, but bro we only need to scale to 10e48 units of compute and the benches will be very close to wiping out jobs across the board!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2p6wbh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752310928,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxvrjm",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]