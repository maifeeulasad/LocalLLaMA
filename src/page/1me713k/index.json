[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’m running OpenWebUI with an Ollama-backed instance of `Qwen3-30B-A3B-Thinking-2507` (just tried the Instruct as well, and ran into the same issue) and running into a frustrating behavior I’m hoping others have seen (or solved).\n\nHere’s the pattern:\n\n1. I give the model a clear task, for example: “Plan a fun day in Washington DC on August 23rd using real events happening that day.”\n2. The model correctly identifies that it needs to use a `search_web` tool (I have this hooked up with SearxNG).\n3. It performs the search and gets HTML content or page summaries back (looks like it's scraping from a site like Washington.org).\n4. Then it completely loses the plot. Instead of using the tool result to fulfill my original request (planning the day), it treats the result as if I had asked it to summarize that page, and shifts focus entirely. It never returns to the original task.\n\nI’ve tried:\n\n* A very explicit system prompt that tells the model to keep the user’s goal in mind and never treat tool results as new input unless instructed.\n* Adding scratchpad-style reminders like “Your job is still to…” before and after tool calls.\n* Swapping out phrasing in the user prompt.\n* Checking for context length issues (not the problem, chats are short).\n\nStill broken. It seems like the model is either:\n\n* Failing to persist intent across tool usage (tool output is overriding memory), or\n* Treating the tool result as if it's coming from me, not as intermediate output.\n\nAnyone else run into this?  \nIs this:\n\n* A quirk of Qwen 30B (tool-use behavior too shallow)?\n* A bug in how OpenWebUI or Ollama routes tool results (e.g., injecting them as raw user-like messages)?\n* Something else?\n\nWould love to know:\n\n* Which models you’ve had better luck with in tool-following or goal persistence\n* Any workarounds you’ve used (prompt engineering, framework config, scratchpad logic, etc.)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Has anyone else seen LLMs lose context after a tool call in OpenWebUI? (Using Qwen 30B)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1me713k",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_egd5l",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753981142,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running OpenWebUI with an Ollama-backed instance of &lt;code&gt;Qwen3-30B-A3B-Thinking-2507&lt;/code&gt; (just tried the Instruct as well, and ran into the same issue) and running into a frustrating behavior I’m hoping others have seen (or solved).&lt;/p&gt;\n\n&lt;p&gt;Here’s the pattern:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I give the model a clear task, for example: “Plan a fun day in Washington DC on August 23rd using real events happening that day.”&lt;/li&gt;\n&lt;li&gt;The model correctly identifies that it needs to use a &lt;code&gt;search_web&lt;/code&gt; tool (I have this hooked up with SearxNG).&lt;/li&gt;\n&lt;li&gt;It performs the search and gets HTML content or page summaries back (looks like it&amp;#39;s scraping from a site like Washington.org).&lt;/li&gt;\n&lt;li&gt;Then it completely loses the plot. Instead of using the tool result to fulfill my original request (planning the day), it treats the result as if I had asked it to summarize that page, and shifts focus entirely. It never returns to the original task.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’ve tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A very explicit system prompt that tells the model to keep the user’s goal in mind and never treat tool results as new input unless instructed.&lt;/li&gt;\n&lt;li&gt;Adding scratchpad-style reminders like “Your job is still to…” before and after tool calls.&lt;/li&gt;\n&lt;li&gt;Swapping out phrasing in the user prompt.&lt;/li&gt;\n&lt;li&gt;Checking for context length issues (not the problem, chats are short).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Still broken. It seems like the model is either:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Failing to persist intent across tool usage (tool output is overriding memory), or&lt;/li&gt;\n&lt;li&gt;Treating the tool result as if it&amp;#39;s coming from me, not as intermediate output.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone else run into this?&lt;br/&gt;\nIs this:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A quirk of Qwen 30B (tool-use behavior too shallow)?&lt;/li&gt;\n&lt;li&gt;A bug in how OpenWebUI or Ollama routes tool results (e.g., injecting them as raw user-like messages)?&lt;/li&gt;\n&lt;li&gt;Something else?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to know:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Which models you’ve had better luck with in tool-following or goal persistence&lt;/li&gt;\n&lt;li&gt;Any workarounds you’ve used (prompt engineering, framework config, scratchpad logic, etc.)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1me713k",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "DVoltaire",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/",
            "subreddit_subscribers": 507935,
            "created_utc": 1753981142,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n68uh70",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Current-Stop7806",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n676w0n",
                                "score": 1,
                                "author_fullname": "t2_8c7clfk1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "How can I set the context length for API external models in Open webUI ? Thanks in advance. 🙏💥",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n68uh70",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How can I set the context length for API external models in Open webUI ? Thanks in advance. 🙏💥&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1me713k",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n68uh70/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753999449,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753999449,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n676w0n",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DVoltaire",
                      "can_mod_post": false,
                      "created_utc": 1753982401,
                      "send_replies": true,
                      "parent_id": "t1_n675ekf",
                      "score": 1,
                      "author_fullname": "t2_egd5l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Set at Default in OpenWebUI for context length. Looking at 5 results, limited to 5000 words for page content on the web\\_search tool.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n676w0n",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Set at Default in OpenWebUI for context length. Looking at 5 results, limited to 5000 words for page content on the web_search tool.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1me713k",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n676w0n/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753982401,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n675ekf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kweglinski",
            "can_mod_post": false,
            "created_utc": 1753981999,
            "send_replies": true,
            "parent_id": "t3_1me713k",
            "score": 2,
            "author_fullname": "t2_7gw03ro",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "what's your context length and how many results it looks at?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n675ekf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;what&amp;#39;s your context length and how many results it looks at?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n675ekf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753981999,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me713k",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n67b8im",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Eugr",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n677dqt",
                                "score": 1,
                                "author_fullname": "t2_f3l6q",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Set it to 16384. I run this model with 32K context on my 4090, but I have set quantized k/v cache (q8 for both k and v) in Ollama as default.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n67b8im",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Set it to 16384. I run this model with 32K context on my 4090, but I have set quantized k/v cache (q8 for both k and v) in Ollama as default.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1me713k",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n67b8im/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753983579,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753983579,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n677dqt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DVoltaire",
                      "can_mod_post": false,
                      "created_utc": 1753982534,
                      "send_replies": true,
                      "parent_id": "t1_n675h6i",
                      "score": 2,
                      "author_fullname": "t2_egd5l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ah I felt this might be the case but ran this issue through ChatGPT earlier (with JSON export of the conversation) and it assured me the context length should be fine. What would you recommend I set it to? The machine running Ollama has an RTX 3090 and 64 GB of RAM.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n677dqt",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah I felt this might be the case but ran this issue through ChatGPT earlier (with JSON export of the conversation) and it assured me the context length should be fine. What would you recommend I set it to? The machine running Ollama has an RTX 3090 and 64 GB of RAM.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1me713k",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n677dqt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753982534,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n675h6i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Eugr",
            "can_mod_post": false,
            "created_utc": 1753982019,
            "send_replies": true,
            "parent_id": "t3_1me713k",
            "score": 1,
            "author_fullname": "t2_f3l6q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "ollama has very small default context size (used to be 2048 tokens, now I think it's 4096). You need to go to chat parameters and set the context size (n\\_ctx) higher. Keep in mind, that retrieved HTML pages are part of the context too!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n675h6i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ollama has very small default context size (used to be 2048 tokens, now I think it&amp;#39;s 4096). You need to go to chat parameters and set the context size (n_ctx) higher. Keep in mind, that retrieved HTML pages are part of the context too!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n675h6i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753982019,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me713k",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n676j4y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "QuackerEnte",
            "can_mod_post": false,
            "created_utc": 1753982305,
            "send_replies": true,
            "parent_id": "t3_1me713k",
            "score": 1,
            "author_fullname": "t2_7trrxhjl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Because that's how OpenWebUI works. It gives a system prompt to the model to perform search, then when it's done the other system prompt tells it to give you (the user) an answer based on the information given, which it does not know anymore that it searched for it on its own. Nothing crazy about it, just the prompts",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n676j4y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because that&amp;#39;s how OpenWebUI works. It gives a system prompt to the model to perform search, then when it&amp;#39;s done the other system prompt tells it to give you (the user) an answer based on the information given, which it does not know anymore that it searched for it on its own. Nothing crazy about it, just the prompts&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n676j4y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753982305,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me713k",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n67dqar",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Capable-Ad-7494",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n678u4q",
                                "score": 2,
                                "author_fullname": "t2_9so78ol2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "6000+ words is almost definitely going over ollama’s 4096 context limit, set it to 12 or 16k and see how that goes",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n67dqar",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;6000+ words is almost definitely going over ollama’s 4096 context limit, set it to 12 or 16k and see how that goes&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1me713k",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n67dqar/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753984252,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753984252,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n678u4q",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DVoltaire",
                      "can_mod_post": false,
                      "created_utc": 1753982927,
                      "send_replies": true,
                      "parent_id": "t1_n676l0u",
                      "score": 1,
                      "author_fullname": "t2_egd5l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I exported the entire conversation (including the search results) as JSON and did a count on it, it's:  \nCharacter count: 40376  \nWord count: 6446  \nByte size: 39.72 KB\n\nIt does not seem significantly large to me, but I'm not an expert at context sizes. What would you recommend I'd need to handle something like this well? My PC is decently beefy so I can definitely increase the context size if that is indeed the issue.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n678u4q",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I exported the entire conversation (including the search results) as JSON and did a count on it, it&amp;#39;s:&lt;br/&gt;\nCharacter count: 40376&lt;br/&gt;\nWord count: 6446&lt;br/&gt;\nByte size: 39.72 KB&lt;/p&gt;\n\n&lt;p&gt;It does not seem significantly large to me, but I&amp;#39;m not an expert at context sizes. What would you recommend I&amp;#39;d need to handle something like this well? My PC is decently beefy so I can definitely increase the context size if that is indeed the issue.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1me713k",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n678u4q/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753982927,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n676l0u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Apprehensive-Emu357",
            "can_mod_post": false,
            "created_utc": 1753982319,
            "send_replies": true,
            "parent_id": "t3_1me713k",
            "score": 1,
            "author_fullname": "t2_m0b14m4v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "When the tool call returns a megabyte worth of HTML it doesn’t just magically go into an “intermediate output” outside of the context window. It’s all one context. Local LLM systems will generally get blown up by this. At best your tool call(s) may be able to return a slimmed down version of a page via accessibility data, but it’s still going to lose the plot fairly fast.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n676l0u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When the tool call returns a megabyte worth of HTML it doesn’t just magically go into an “intermediate output” outside of the context window. It’s all one context. Local LLM systems will generally get blown up by this. At best your tool call(s) may be able to return a slimmed down version of a page via accessibility data, but it’s still going to lose the plot fairly fast.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n676l0u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753982319,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me713k",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n679ffw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GortKlaatu_",
            "can_mod_post": false,
            "created_utc": 1753983084,
            "send_replies": true,
            "parent_id": "t3_1me713k",
            "score": 1,
            "author_fullname": "t2_ixeagk4w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My advice is to either increase the default context length of ollama or create a new model file.\n\nThe easiest way to do this is load up your favorite model, run\n\n`/set parameter num_ctx 32768`\n\n`/save &lt;your_model_name_32K_or_something_unique&gt;`\n\n  \nThe new settings menu in Ollama might make this super easy actually...",
            "edited": 1753987805,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n679ffw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My advice is to either increase the default context length of ollama or create a new model file.&lt;/p&gt;\n\n&lt;p&gt;The easiest way to do this is load up your favorite model, run&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;/set parameter num_ctx 32768&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;/save &amp;lt;your_model_name_32K_or_something_unique&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The new settings menu in Ollama might make this super easy actually...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n679ffw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753983084,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me713k",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n68bhtp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Klutzy-Snow8016",
            "can_mod_post": false,
            "created_utc": 1753993867,
            "send_replies": true,
            "parent_id": "t3_1me713k",
            "score": 1,
            "author_fullname": "t2_1d5l610jz3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Make sure you have the model set to use Native tool calls (not Default) in Open WebUI.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n68bhtp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Make sure you have the model set to use Native tool calls (not Default) in Open WebUI.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me713k/has_anyone_else_seen_llms_lose_context_after_a/n68bhtp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753993867,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me713k",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]