[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey folks,\n\nJust published a deep dive on the full infrastructure stack required to scale LLM inference to billions of users and agents. It goes beyond a single engine and looks at the entire system.\n\nHighlights:\n\n* GKE Inference Gateway: How it cuts tail latency by 60% &amp; boosts throughput 40% with model-aware routing (KV cache, LoRA).\n* vLLM on GPUs &amp; TPUs: Using vLLM as a unified layer to serve models across different hardware, including a look at the insane interconnects on Cloud TPUs.\n* The Future is llm-d: A breakdown of the new Google/Red Hat project for disaggregated inference (separating prefill/decode stages).\n* Planetary-Scale Networking: The role of a global Anycast network and 42+ regions in minimizing latency for users everywhere.\n* Managing Capacity &amp; Cost: Using GKE Custom Compute Classes to build a resilient and cost-effective mix of Spot, On-demand, and Reserved instances.\n\nFull article with architecture diagrams &amp; walkthroughs:\n\n[https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7](https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7)\n\nLet me know what you think!\n\n*(Disclaimer: I work at Google Cloud.)*",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Scaling Inference To Billions of Users And Agents",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9tnj5",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.82,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_43kcp65r",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753537307,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\n\n&lt;p&gt;Just published a deep dive on the full infrastructure stack required to scale LLM inference to billions of users and agents. It goes beyond a single engine and looks at the entire system.&lt;/p&gt;\n\n&lt;p&gt;Highlights:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GKE Inference Gateway: How it cuts tail latency by 60% &amp;amp; boosts throughput 40% with model-aware routing (KV cache, LoRA).&lt;/li&gt;\n&lt;li&gt;vLLM on GPUs &amp;amp; TPUs: Using vLLM as a unified layer to serve models across different hardware, including a look at the insane interconnects on Cloud TPUs.&lt;/li&gt;\n&lt;li&gt;The Future is llm-d: A breakdown of the new Google/Red Hat project for disaggregated inference (separating prefill/decode stages).&lt;/li&gt;\n&lt;li&gt;Planetary-Scale Networking: The role of a global Anycast network and 42+ regions in minimizing latency for users everywhere.&lt;/li&gt;\n&lt;li&gt;Managing Capacity &amp;amp; Cost: Using GKE Custom Compute Classes to build a resilient and cost-effective mix of Spot, On-demand, and Reserved instances.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Full article with architecture diagrams &amp;amp; walkthroughs:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7\"&gt;https://medium.com/google-cloud/scaling-inference-to-billions-of-users-and-agents-516d5d9f5da7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(Disclaimer: I work at Google Cloud.)&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?auto=webp&amp;s=03e21a1bc3ab1617cd4ffe76d71c7c70efbc5cd0",
                    "width": 1200,
                    "height": 1200
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=58f812f9bd6c9bebf70c1af318f89680a57d02cc",
                      "width": 108,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7803a71a8d13a2ea1df78345f28d2b72c03065e1",
                      "width": 216,
                      "height": 216
                    },
                    {
                      "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44d2bb07bca9e3359cb00e3b44c2bb19e82d326a",
                      "width": 320,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63e55fc492279919c4c0a2beee0bfb4315a9c01c",
                      "width": 640,
                      "height": 640
                    },
                    {
                      "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=81d6c21e619a1ba63093f138a5b114a0d152f5f9",
                      "width": 960,
                      "height": 960
                    },
                    {
                      "url": "https://external-preview.redd.it/7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=060190a2d09b97087c4351bc02783504ce618f26",
                      "width": 1080,
                      "height": 1080
                    }
                  ],
                  "variants": {},
                  "id": "7JS3Ya3b6rSOwFZeMG-XxVZkJJv_W5OFh2T4WbpzeG0"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m9tnj5",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "m4r1k_",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/",
            "subreddit_subscribers": 504973,
            "created_utc": 1753537307,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5a5k5p",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "m4r1k_",
                      "can_mod_post": false,
                      "created_utc": 1753544896,
                      "send_replies": true,
                      "parent_id": "t1_n5a2g9l",
                      "score": 4,
                      "author_fullname": "t2_43kcp65r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "👍 I’m one if those that grew up without paywalls and I believe in free information.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5a5k5p",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;👍 I’m one if those that grew up without paywalls and I believe in free information.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9tnj5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5a5k5p/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753544896,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5a2g9l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "RhubarbSimilar1683",
            "can_mod_post": false,
            "created_utc": 1753543930,
            "send_replies": true,
            "parent_id": "t3_1m9tnj5",
            "score": 7,
            "author_fullname": "t2_1k4sjdwzk2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "thanks for not putting it behind a paywall.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5a2g9l",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;thanks for not putting it behind a paywall.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5a2g9l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753543930,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tnj5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n59pywq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "m4r1k_",
                      "can_mod_post": false,
                      "created_utc": 1753539948,
                      "send_replies": true,
                      "parent_id": "t1_n59klid",
                      "score": 4,
                      "author_fullname": "t2_43kcp65r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks a lot! Absolutely, working at Google gives me incredible insight that is very hard to replicate elsewhere. Usually, my papers are much more technical than this one; this time around, I wanted to provide a more high-level view, with lots of references.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n59pywq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks a lot! Absolutely, working at Google gives me incredible insight that is very hard to replicate elsewhere. Usually, my papers are much more technical than this one; this time around, I wanted to provide a more high-level view, with lots of references.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9tnj5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n59pywq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753539948,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n59klid",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "mtmttuan",
            "can_mod_post": false,
            "created_utc": 1753538138,
            "send_replies": true,
            "parent_id": "t3_1m9tnj5",
            "score": 9,
            "author_fullname": "t2_6mjqz0at",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Wow I would probably never work on something like this, but this is super cool. Also about the disclaimer: the fact that you work at google cloud makes the blog much more believable. There are only very few companies that work on that scale and well I will probably not trust a random redditor on this topic.",
            "edited": 1753538390,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n59klid",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow I would probably never work on something like this, but this is super cool. Also about the disclaimer: the fact that you work at google cloud makes the blog much more believable. There are only very few companies that work on that scale and well I will probably not trust a random redditor on this topic.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n59klid/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753538138,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tnj5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5cjmdn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kidupstart",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5biy9l",
                                "score": 1,
                                "author_fullname": "t2_dtzmsoy3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Great insights on the hardware space.   \nThe CUDA ecosystem reminds me of the Windows v Mac v Linux battles. \n\nNVIDIA has a Windows-like dominance through ecosystem lock-in and developer tools. And solutions like vLLM and open-source AI infrastructure are trying to challenge this, but network effects make this displacement difficult. \n\nThe real game changer will likely be a platform that offers comparable performance with more flexibility.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5cjmdn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great insights on the hardware space.&lt;br/&gt;\nThe CUDA ecosystem reminds me of the Windows v Mac v Linux battles. &lt;/p&gt;\n\n&lt;p&gt;NVIDIA has a Windows-like dominance through ecosystem lock-in and developer tools. And solutions like vLLM and open-source AI infrastructure are trying to challenge this, but network effects make this displacement difficult. &lt;/p&gt;\n\n&lt;p&gt;The real game changer will likely be a platform that offers comparable performance with more flexibility.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9tnj5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5cjmdn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753573167,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753573167,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5biy9l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "m4r1k_",
                      "can_mod_post": false,
                      "created_utc": 1753560530,
                      "send_replies": true,
                      "parent_id": "t1_n5aevd9",
                      "score": 2,
                      "author_fullname": "t2_43kcp65r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I’ll try to answer this, but I have a strong bias for openness and non-lock-in solutions.\n\nIn my humble opinion, NVIDIA has such a big advantage (and not just in hardware but most importantly in the CUDA ecosystem – I just went to dinner with a group of friends; one lives in Munich, just did his PhD in something related to fluid dynamics, and now he’s about to co-found a startup; they use CUDA for pretty much everything) that for anyone else, even Google, it’s hard to have a fair shot. And NVIDIA also provides something quite underrated yet extremely important: CUDA will be there, no matter what, for years to come. It provides the long-term predictability business and decision-makers’ dreams of.\n\nBack to the specialized hardware part of the question: I come from the telco world; I was lucky enough to witness firsthand the containerization of the 4G physical functions. At a certain point on the radio side, all vendors figured out that CPU computation for IPSEC wasn’t going to cut it. Now, back then, FPGAs from a few vendors were the answer, but it came at a major integration cost. Now, to me, vLLM has the potential to reduce the superpower NVIDIA has today, but until you can have on-prem or at a different cloud provider the same specialized hardware, NVIDIA will always be the dominant choice. Of course, this assumes no major technological shift happens, or requires to happen, like for mining BTC. GenAI, at the current complexity level, seems a problem nearly solved.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5biy9l",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’ll try to answer this, but I have a strong bias for openness and non-lock-in solutions.&lt;/p&gt;\n\n&lt;p&gt;In my humble opinion, NVIDIA has such a big advantage (and not just in hardware but most importantly in the CUDA ecosystem – I just went to dinner with a group of friends; one lives in Munich, just did his PhD in something related to fluid dynamics, and now he’s about to co-found a startup; they use CUDA for pretty much everything) that for anyone else, even Google, it’s hard to have a fair shot. And NVIDIA also provides something quite underrated yet extremely important: CUDA will be there, no matter what, for years to come. It provides the long-term predictability business and decision-makers’ dreams of.&lt;/p&gt;\n\n&lt;p&gt;Back to the specialized hardware part of the question: I come from the telco world; I was lucky enough to witness firsthand the containerization of the 4G physical functions. At a certain point on the radio side, all vendors figured out that CPU computation for IPSEC wasn’t going to cut it. Now, back then, FPGAs from a few vendors were the answer, but it came at a major integration cost. Now, to me, vLLM has the potential to reduce the superpower NVIDIA has today, but until you can have on-prem or at a different cloud provider the same specialized hardware, NVIDIA will always be the dominant choice. Of course, this assumes no major technological shift happens, or requires to happen, like for mining BTC. GenAI, at the current complexity level, seems a problem nearly solved.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9tnj5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5biy9l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753560530,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5aevd9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kidupstart",
            "can_mod_post": false,
            "created_utc": 1753547835,
            "send_replies": true,
            "parent_id": "t3_1m9tnj5",
            "score": 2,
            "author_fullname": "t2_dtzmsoy3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How do you see the space between specialized hardware (like TPUs) and more generalized GPU infrastructure evolving?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5aevd9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do you see the space between specialized hardware (like TPUs) and more generalized GPU infrastructure evolving?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5aevd9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753547835,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tnj5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5bjjc9",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Accomplished_Mode170",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5abpzd",
                                "score": 1,
                                "author_fullname": "t2_4hfmiefj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It’s not. They literally solved a problem they created by not selling TPUs",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5bjjc9",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s not. They literally solved a problem they created by not selling TPUs&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9tnj5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5bjjc9/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753560722,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753560722,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5abpzd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "mlvnd",
                      "can_mod_post": false,
                      "created_utc": 1753546830,
                      "send_replies": true,
                      "parent_id": "t1_n5a8one",
                      "score": 0,
                      "author_fullname": "t2_bf0fz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What part do you mean, it’s local to him, right? ;)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5abpzd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What part do you mean, it’s local to him, right? ;)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9tnj5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5abpzd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753546830,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5b7n9p",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Recoil42",
                      "can_mod_post": false,
                      "created_utc": 1753556824,
                      "send_replies": true,
                      "parent_id": "t1_n5a8one",
                      "score": -1,
                      "author_fullname": "t2_2kndo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah, this isn't Llama, the large language model by Meta!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5b7n9p",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, this isn&amp;#39;t Llama, the large language model by Meta!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9tnj5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5b7n9p/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753556824,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5a8one",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cleverusernametry",
            "can_mod_post": false,
            "created_utc": 1753545875,
            "send_replies": true,
            "parent_id": "t3_1m9tnj5",
            "score": -1,
            "author_fullname": "t2_17bfjs",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why is this on LOCAL LLAMA?\n\n/u/HOLUPREDICTIONS ?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5a8one",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why is this on LOCAL LLAMA?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/HOLUPREDICTIONS\"&gt;/u/HOLUPREDICTIONS&lt;/a&gt; ?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tnj5/scaling_inference_to_billions_of_users_and_agents/n5a8one/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753545875,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tnj5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        }
      ],
      "before": null
    }
  }
]