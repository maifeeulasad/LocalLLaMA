[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Why everyone release models under Apache 2.0 and MIT if none of them claim that output is not a derivative work? We actually need a new license for this new era",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "local AI Licenses",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjt5hw",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_8dnu3hmd",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754550016,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why everyone release models under Apache 2.0 and MIT if none of them claim that output is not a derivative work? We actually need a new license for this new era&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mjt5hw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "AleksHop",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjt5hw/local_ai_licenses/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjt5hw/local_ai_licenses/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754550016,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7dxazo",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "plankalkul-z1",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7dsoeo",
                                          "score": 1,
                                          "author_fullname": "t2_w73n3yrsx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "&gt; You missed this:\n\n\nNope, I didn't.\n\n\n&gt; models trained on Gemma output is considered derivative of Gemma\n\n\nYes, sure.\n\n\nBut there is a world of difference between \"models trained on Gemma output\" and \"Gemma output\". So your original statement\n\n\n&gt; explicitly state that *output* is a derivative work\n\n\n\n(emphasis is mine) is still wrong.\n\n\nEDIT: I think that this academic exercise that you and me are having here is moot...\n\n\nThe license obviously contains a contradiction, and I firmly believe that the \"models trained on Gemma output is considered derivative of Gemma\" would be undefendable in an *unbiased* court.\n\n\nUnfortunately, I also believe no-one will challenge Google on that, especially given that there are way better models to use for training... So we will never find out whether that license holds any water at all.",
                                          "edited": 1754558883,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7dxazo",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;You missed this:&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Nope, I didn&amp;#39;t.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;models trained on Gemma output is considered derivative of Gemma&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yes, sure.&lt;/p&gt;\n\n&lt;p&gt;But there is a world of difference between &amp;quot;models trained on Gemma output&amp;quot; and &amp;quot;Gemma output&amp;quot;. So your original statement&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;explicitly state that &lt;em&gt;output&lt;/em&gt; is a derivative work&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;(emphasis is mine) is still wrong.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I think that this academic exercise that you and me are having here is moot...&lt;/p&gt;\n\n&lt;p&gt;The license obviously contains a contradiction, and I firmly believe that the &amp;quot;models trained on Gemma output is considered derivative of Gemma&amp;quot; would be undefendable in an &lt;em&gt;unbiased&lt;/em&gt; court.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, I also believe no-one will challenge Google on that, especially given that there are way better models to use for training... So we will never find out whether that license holds any water at all.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mjt5hw",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mjt5hw/local_ai_licenses/n7dxazo/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754558269,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754558269,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7dsoeo",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ttkciar",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7drxmm",
                                "score": 1,
                                "author_fullname": "t2_cpegz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You missed this:\n\n&gt; (e) \"Model Derivatives\" means all [...] (iii) any other machine learning model which is created by transfer of patterns of the weights, parameters, operations, or **Output of Gemma**\n\n(emphasis mine)\n\nThus models trained on Gemma output is considered derivative of Gemma, and is burdened by the Gemma license.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7dsoeo",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You missed this:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;(e) &amp;quot;Model Derivatives&amp;quot; means all [...] (iii) any other machine learning model which is created by transfer of patterns of the weights, parameters, operations, or &lt;strong&gt;Output of Gemma&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;(emphasis mine)&lt;/p&gt;\n\n&lt;p&gt;Thus models trained on Gemma output is considered derivative of Gemma, and is burdened by the Gemma license.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjt5hw",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjt5hw/local_ai_licenses/n7dsoeo/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754555575,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754555575,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7drxmm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "plankalkul-z1",
                      "can_mod_post": false,
                      "created_utc": 1754555142,
                      "send_replies": true,
                      "parent_id": "t1_n7dn3bw",
                      "score": 1,
                      "author_fullname": "t2_w73n3yrsx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Restrictive/invasive license agreements like Gemma's explicitly state that output is a derivative work.\n\n\n\nWhile I agree that [Gemma's license](https://ai.google.dev/gemma/termsis) is both restrictive and invasive, it does *not* claim that output is a derivative work.\n\n\nHere's how it defines output:\n\n\n&gt; (f) \"**Output**\" means the information content output of Gemma or a Model Derivative that results from operating or otherwise using Gemma or the Model Derivative, including via a Hosted Service.\n\n\nIt merely claims that output of a Gemma model derivative is also considered to be Gemma model output, nothing more.\n\n\nHere's what it says about the ownership of the model output:\n\n\n&gt; 3.3 **Generated Output**\nGoogle claims no rights in Outputs you generate using Gemma. You and your users are solely responsible for Outputs and their subsequent uses.\n\n\nI do not think there is any room for interpreting it differently.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7drxmm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Restrictive/invasive license agreements like Gemma&amp;#39;s explicitly state that output is a derivative work.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;While I agree that &lt;a href=\"https://ai.google.dev/gemma/termsis\"&gt;Gemma&amp;#39;s license&lt;/a&gt; is both restrictive and invasive, it does &lt;em&gt;not&lt;/em&gt; claim that output is a derivative work.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it defines output:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;(f) &amp;quot;&lt;strong&gt;Output&lt;/strong&gt;&amp;quot; means the information content output of Gemma or a Model Derivative that results from operating or otherwise using Gemma or the Model Derivative, including via a Hosted Service.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It merely claims that output of a Gemma model derivative is also considered to be Gemma model output, nothing more.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what it says about the ownership of the model output:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;3.3 &lt;strong&gt;Generated Output&lt;/strong&gt;\nGoogle claims no rights in Outputs you generate using Gemma. You and your users are solely responsible for Outputs and their subsequent uses.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I do not think there is any room for interpreting it differently.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjt5hw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjt5hw/local_ai_licenses/n7drxmm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754555142,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7dn3bw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754552349,
            "send_replies": true,
            "parent_id": "t3_1mjt5hw",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's the other way around.  If they don't claim output is a derivative work, then it isn't.\n\nRestrictive/invasive license agreements like Gemma's explicitly state that output is a derivative work.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dn3bw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s the other way around.  If they don&amp;#39;t claim output is a derivative work, then it isn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;Restrictive/invasive license agreements like Gemma&amp;#39;s explicitly state that output is a derivative work.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjt5hw/local_ai_licenses/n7dn3bw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754552349,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mjt5hw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]