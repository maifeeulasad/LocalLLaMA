[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "A while ago I bought a new computer. 32GB of RAM (two sticks) and 16GB of VRAM. Now I'm considering buying 32GB more RAM. Would that help with running local models in any significant way? Or is really only a stronger GPU going to help with that?\n\nFor the record, I use LMStudio to run my models.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Extra RAM Useful?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjrlge",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_6inwf8q4",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754544339,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while ago I bought a new computer. 32GB of RAM (two sticks) and 16GB of VRAM. Now I&amp;#39;m considering buying 32GB more RAM. Would that help with running local models in any significant way? Or is really only a stronger GPU going to help with that?&lt;/p&gt;\n\n&lt;p&gt;For the record, I use LMStudio to run my models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjrlge",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "OneOnOne6211",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": false,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjrlge/extra_ram_useful/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjrlge/extra_ram_useful/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754544339,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dafqi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Only-Letterhead-3411",
            "can_mod_post": false,
            "created_utc": 1754545452,
            "send_replies": true,
            "parent_id": "t3_1mjrlge",
            "score": 6,
            "author_fullname": "t2_pbfqmgf8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you have a good cpu, having lots of RAM lets you run huge MoE models with low active parameters at usable speeds",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dafqi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you have a good cpu, having lots of RAM lets you run huge MoE models with low active parameters at usable speeds&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjrlge/extra_ram_useful/n7dafqi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754545452,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjrlge",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dad2f",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "deathcom65",
            "can_mod_post": false,
            "created_utc": 1754545412,
            "send_replies": true,
            "parent_id": "t3_1mjrlge",
            "score": 3,
            "author_fullname": "t2_15o3gy1oht",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "yeah you can load larger models such as MOE where only some parameters are loaded onto the gpu. i just did the exact same thing and it helps a ton, even though when things get loaded onto ram its slower, u can still run larger models. without the extra ram u cant even run them. Imo its a cheap upgrade for a good return. I kind of regret not getting 128gb ram directly",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dad2f",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah you can load larger models such as MOE where only some parameters are loaded onto the gpu. i just did the exact same thing and it helps a ton, even though when things get loaded onto ram its slower, u can still run larger models. without the extra ram u cant even run them. Imo its a cheap upgrade for a good return. I kind of regret not getting 128gb ram directly&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjrlge/extra_ram_useful/n7dad2f/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754545412,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjrlge",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dco80",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OneOnOne6211",
                      "can_mod_post": false,
                      "created_utc": 1754546608,
                      "send_replies": false,
                      "parent_id": "t1_n7dbck9",
                      "score": 1,
                      "author_fullname": "t2_6inwf8q4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;You may get some speed boost from the extra memory bandwidth of using more slots, but please research, and match the memory and timings config properly otherwise you could make speed worse.\n\nIf I do this, I'm going to just get more of the exact same RAM sticks that are currently inside. So it shouldn't be a problem.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dco80",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;You may get some speed boost from the extra memory bandwidth of using more slots, but please research, and match the memory and timings config properly otherwise you could make speed worse.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If I do this, I&amp;#39;m going to just get more of the exact same RAM sticks that are currently inside. So it shouldn&amp;#39;t be a problem.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjrlge",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjrlge/extra_ram_useful/n7dco80/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754546608,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7dbck9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Bus9917",
            "can_mod_post": false,
            "created_utc": 1754545922,
            "send_replies": true,
            "parent_id": "t3_1mjrlge",
            "score": 2,
            "author_fullname": "t2_1u3ele924k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "More ram and vram gives you the options;  \nto host a larger model with more parameters than you could previously  \nalso to run models at higher quantisations than you could previously  \nand or longer context.\n\nYou may get some speed boost from the extra memory bandwidth of using more slots, but please research, and match the memory and timings config properly otherwise you could make speed worse.\n\n64GB+12GB should be enough for GLM 4.5 Air Q4 with good context?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dbck9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;More ram and vram gives you the options;&lt;br/&gt;\nto host a larger model with more parameters than you could previously&lt;br/&gt;\nalso to run models at higher quantisations than you could previously&lt;br/&gt;\nand or longer context.&lt;/p&gt;\n\n&lt;p&gt;You may get some speed boost from the extra memory bandwidth of using more slots, but please research, and match the memory and timings config properly otherwise you could make speed worse.&lt;/p&gt;\n\n&lt;p&gt;64GB+12GB should be enough for GLM 4.5 Air Q4 with good context?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjrlge/extra_ram_useful/n7dbck9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754545922,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjrlge",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7debd0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zRevengee",
            "can_mod_post": false,
            "created_utc": 1754547471,
            "send_replies": true,
            "parent_id": "t3_1mjrlge",
            "score": 2,
            "author_fullname": "t2_10vdoj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes, you can store context for smaller models or run partially loaded on CPU\n\ni got 128gb ddr4 , the 235b qwen3 run at 2-3tk/s with a small amount running on a 5080, both k and v quantized to q4_0, and flash attention enabled.\n\n(Rember to offload cache to CPU instead of GPU for higher parameters models)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7debd0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, you can store context for smaller models or run partially loaded on CPU&lt;/p&gt;\n\n&lt;p&gt;i got 128gb ddr4 , the 235b qwen3 run at 2-3tk/s with a small amount running on a 5080, both k and v quantized to q4_0, and flash attention enabled.&lt;/p&gt;\n\n&lt;p&gt;(Rember to offload cache to CPU instead of GPU for higher parameters models)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjrlge/extra_ram_useful/n7debd0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754547471,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjrlge",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]