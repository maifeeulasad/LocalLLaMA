[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I would expect the download size to be proportional to quantization, but Q2\\_K is 11.47GB, while Q8\\_0 is 12.11GB. Even F16 and BF16 are only 13.79GB.\n\nThe only one that's significantly different is F32, which is 41.86GB.\n\nAre only some layers being quantized or something?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why are all the unsloth GPT-OSS-20b quants basically the same size?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjf25w",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.46,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_3s1bp",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754510576,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would expect the download size to be proportional to quantization, but Q2_K is 11.47GB, while Q8_0 is 12.11GB. Even F16 and BF16 are only 13.79GB.&lt;/p&gt;\n\n&lt;p&gt;The only one that&amp;#39;s significantly different is F32, which is 41.86GB.&lt;/p&gt;\n\n&lt;p&gt;Are only some layers being quantized or something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjf25w",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "meatmanek",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/",
            "subreddit_subscribers": 512875,
            "created_utc": 1754510576,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7alohp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "nmkd",
            "can_mod_post": false,
            "created_utc": 1754511171,
            "send_replies": true,
            "parent_id": "t3_1mjf25w",
            "score": 10,
            "author_fullname": "t2_rg6rx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Because F16 and BF16 are incorrectly named, they are MXFP4 weights, which is the original format OpenAI published.\n\nDownload those if you have the VRAM; the Q8 quant is just a conversion from MXFP4 to F32 and back to Q8 which is basically the same size but with a quality loss.\n\nF32 is an upcast and is only useful for making quants.\n\nI think unsloth mentioned that they kept the naming scheme for compatibility reasons, but yeah it's wrong.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7alohp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because F16 and BF16 are incorrectly named, they are MXFP4 weights, which is the original format OpenAI published.&lt;/p&gt;\n\n&lt;p&gt;Download those if you have the VRAM; the Q8 quant is just a conversion from MXFP4 to F32 and back to Q8 which is basically the same size but with a quality loss.&lt;/p&gt;\n\n&lt;p&gt;F32 is an upcast and is only useful for making quants.&lt;/p&gt;\n\n&lt;p&gt;I think unsloth mentioned that they kept the naming scheme for compatibility reasons, but yeah it&amp;#39;s wrong.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/n7alohp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754511171,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjf25w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7avd1u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "SuperChewbacca",
            "can_mod_post": false,
            "created_utc": 1754513897,
            "send_replies": true,
            "parent_id": "t3_1mjf25w",
            "score": 4,
            "author_fullname": "t2_8lufz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The unsloth guys mentioned that llama.cpp needs updates before smaller quants are supported.  I think they are waiting on that and those are basically placeholders, but yes it's confusing.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7avd1u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The unsloth guys mentioned that llama.cpp needs updates before smaller quants are supported.  I think they are waiting on that and those are basically placeholders, but yes it&amp;#39;s confusing.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/n7avd1u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754513897,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjf25w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7bk4w5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "meatmanek",
            "can_mod_post": false,
            "created_utc": 1754521661,
            "send_replies": true,
            "parent_id": "t3_1mjf25w",
            "score": 3,
            "author_fullname": "t2_3s1bp",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ah, I missed this on the [unsloth guide](https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune):\n\n&gt;Any quant smaller than f16, including 2-bit — has minimal accuracy loss, since only some parts (e.g., attention layers) are lower bit while most remain full-precision. That’s why sizes are close to the f16 model; for example, the 2-bit (11.5 GB) version performs nearly the same as the full 16-bit (14 GB) one. Once llama.cpp supports better quantization for these models, we'll upload them ASAP.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bk4w5",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah, I missed this on the &lt;a href=\"https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune\"&gt;unsloth guide&lt;/a&gt;:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Any quant smaller than f16, including 2-bit — has minimal accuracy loss, since only some parts (e.g., attention layers) are lower bit while most remain full-precision. That’s why sizes are close to the f16 model; for example, the 2-bit (11.5 GB) version performs nearly the same as the full 16-bit (14 GB) one. Once llama.cpp supports better quantization for these models, we&amp;#39;ll upload them ASAP.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/n7bk4w5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754521661,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjf25w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7avwyv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "llama-impersonator",
            "can_mod_post": false,
            "created_utc": 1754514057,
            "send_replies": true,
            "parent_id": "t3_1mjf25w",
            "score": 1,
            "author_fullname": "t2_14usv0hw3h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "the ffn/mlp came 4 bit, so the thing that's changing size is the attention tensors. it varies from model to model, but usually the attn tensors are much smaller than the mlp and only contribute something like 20% of the total parameters.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7avwyv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the ffn/mlp came 4 bit, so the thing that&amp;#39;s changing size is the attention tensors. it varies from model to model, but usually the attn tensors are much smaller than the mlp and only contribute something like 20% of the total parameters.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/n7avwyv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754514057,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjf25w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7awwny",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1754514348,
            "send_replies": true,
            "parent_id": "t3_1mjf25w",
            "score": 1,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Because only some weights are quantized at the type it says on the tin. Specifically, the attention blocks. The vast majority of weights (feed-forward networks) are in the original quant.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7awwny",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because only some weights are quantized at the type it says on the tin. Specifically, the attention blocks. The vast majority of weights (feed-forward networks) are in the original quant.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/n7awwny/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754514348,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjf25w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]