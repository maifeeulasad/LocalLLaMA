[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have a mac and whenever a new model launches, I see MLX quants available in a day or two. However GGUF takes more time due to llama.cpp support.\nRecent example is GLM 4.5\n\nI’m just genuinely curious to know, what makes it easy or faster to add support in MLX.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "I’m curious to know how does MLX adds support for models faster than llama.cpp",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdgjmk",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.84,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_jqxb4pte",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753904976,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a mac and whenever a new model launches, I see MLX quants available in a day or two. However GGUF takes more time due to llama.cpp support.\nRecent example is GLM 4.5&lt;/p&gt;\n\n&lt;p&gt;I’m just genuinely curious to know, what makes it easy or faster to add support in MLX.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mdgjmk",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "No_Conversation9561",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/",
            "subreddit_subscribers": 507275,
            "created_utc": 1753904976,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n61ycli",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AlwaysInconsistant",
                      "can_mod_post": false,
                      "created_utc": 1753910977,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 4,
                      "author_fullname": "t2_65e1spw7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is a great write up, I’d also add that so long as it isn’t an architecture change, the community can generate their own quant files from safetensors fairly easily - it’s way easier than creating a gguf, I think, I haven’t created my own gguf but I have uploaded quants to mlx community.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n61ycli",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a great write up, I’d also add that so long as it isn’t an architecture change, the community can generate their own quant files from safetensors fairly easily - it’s way easier than creating a gguf, I think, I haven’t created my own gguf but I have uploaded quants to mlx community.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n61ycli/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753910977,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62wo7r",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Accomplished_Ad9530",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62nqme",
                                          "score": 1,
                                          "author_fullname": "t2_88fma001",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yep, here's a colab notebook showing how easy it is: [https://colab.research.google.com/drive/1orolX420Esc3Gcz\\_vOrYoR1V2k-yJTdP](https://colab.research.google.com/drive/1orolX420Esc3Gcz_vOrYoR1V2k-yJTdP)\n\nNot sure how it performs compared to llama.cpp and vllm and such, but it's still early days and optimizations are ongoing.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62wo7r",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep, here&amp;#39;s a colab notebook showing how easy it is: &lt;a href=\"https://colab.research.google.com/drive/1orolX420Esc3Gcz_vOrYoR1V2k-yJTdP\"&gt;https://colab.research.google.com/drive/1orolX420Esc3Gcz_vOrYoR1V2k-yJTdP&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Not sure how it performs compared to llama.cpp and vllm and such, but it&amp;#39;s still early days and optimizations are ongoing.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdgjmk",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62wo7r/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753922057,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753922057,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n62nqme",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Few-Yam9901",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n620gxc",
                                "score": 2,
                                "author_fullname": "t2_1rhlf3bcfk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Wait can I use mlx on Linux with Nvidia gpu?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62nqme",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wait can I use mlx on Linux with Nvidia gpu?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgjmk",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62nqme/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753918999,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753918999,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n620gxc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Accomplished_Ad9530",
                      "can_mod_post": false,
                      "created_utc": 1753911609,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 3,
                      "author_fullname": "t2_88fma001",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's a pretty good rundown!\n\nMy only correction is the last part about ASi-- MLX supports CUDA on linux now (also whatever CPUs on linux and windows). AMD ROCm support should also not be too difficult to add now that CUDA is in.\n\nThink of MLX as what Modular Mojo has been trying to achieve, but openly developed and fully available from the start.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n620gxc",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s a pretty good rundown!&lt;/p&gt;\n\n&lt;p&gt;My only correction is the last part about ASi-- MLX supports CUDA on linux now (also whatever CPUs on linux and windows). AMD ROCm support should also not be too difficult to add now that CUDA is in.&lt;/p&gt;\n\n&lt;p&gt;Think of MLX as what Modular Mojo has been trying to achieve, but openly developed and fully available from the start.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n620gxc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753911609,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n62sc9z",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No-Refrigerator-1672",
                      "can_mod_post": false,
                      "created_utc": 1753920547,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 2,
                      "author_fullname": "t2_baavelp5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Fun fact: you actually don't need a llama.cpp support to make a gguf - i.e. ComfyUI community [uses GGUFs](https://github.com/city96/ComfyUI-GGUF) for image generation models which are, needless to say, completely unsupported in llama.cpp. And they are easily pulling this off because, just as you said, Python libraries handle every computational corcern for them. So you can generate a correct GGUF the moment the model is released; it's just that you have nothing to feed it into until days or weeks later.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62sc9z",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fun fact: you actually don&amp;#39;t need a llama.cpp support to make a gguf - i.e. ComfyUI community &lt;a href=\"https://github.com/city96/ComfyUI-GGUF\"&gt;uses GGUFs&lt;/a&gt; for image generation models which are, needless to say, completely unsupported in llama.cpp. And they are easily pulling this off because, just as you said, Python libraries handle every computational corcern for them. So you can generate a correct GGUF the moment the model is released; it&amp;#39;s just that you have nothing to feed it into until days or weeks later.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62sc9z/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920547,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n62sb9g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pristine-Woodpecker",
                      "can_mod_post": false,
                      "created_utc": 1753920538,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 1,
                      "author_fullname": "t2_5b972ieo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;But then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.\n\nUh, if you look at the GLM 4.5 patch for MLX, it's doing exactly this. It's slightly less verbose than the same thing in the llama.cpp patch, but they're really doing the exact same thing. In fact, defining this is the *only* thing the patch does. Most of the extras llama.cpp needs are shuffling everything in and out of GGUF format and especially mapping GGUF's naming convention, whereas MLX shares the original Python one.\n\n[https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46](https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46)",
                      "edited": 1753920993,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62sb9g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;But then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Uh, if you look at the GLM 4.5 patch for MLX, it&amp;#39;s doing exactly this. It&amp;#39;s slightly less verbose than the same thing in the llama.cpp patch, but they&amp;#39;re really doing the exact same thing. In fact, defining this is the &lt;em&gt;only&lt;/em&gt; thing the patch does. Most of the extras llama.cpp needs are shuffling everything in and out of GGUF format and especially mapping GGUF&amp;#39;s naming convention, whereas MLX shares the original Python one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46\"&gt;https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62sb9g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920538,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61jxix",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "lothariusdark",
            "can_mod_post": false,
            "created_utc": 1753906917,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": 19,
            "author_fullname": "t2_idhb522c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "As far as I know MLX LM is abstracted far higher than llama.cpp, this means they dont really have to worry about low level stuff like memory management for example.\n\nIts also python and closely related to the way models are run using transformers with pytorch, which every model supports when it comes out.\n\nAs such they have far less work to do to adapt it.\n\nIn llama.cpp the developers need to define everything, the model first needs to be convertable to gguf properly, then you need to define the model, like model parameters, memory for tensors, mapping names etc.\n\nBut then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.\n\nIn the end it also needs to be tested in all the different backends llama.cpp supports, vulkan/cuda/hip/cpu/etc.\n\nMLX only needs to care about apple silicon.\n\n*If I wrote something really wrong, feel free to correct me.*",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61jxix",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As far as I know MLX LM is abstracted far higher than llama.cpp, this means they dont really have to worry about low level stuff like memory management for example.&lt;/p&gt;\n\n&lt;p&gt;Its also python and closely related to the way models are run using transformers with pytorch, which every model supports when it comes out.&lt;/p&gt;\n\n&lt;p&gt;As such they have far less work to do to adapt it.&lt;/p&gt;\n\n&lt;p&gt;In llama.cpp the developers need to define everything, the model first needs to be convertable to gguf properly, then you need to define the model, like model parameters, memory for tensors, mapping names etc.&lt;/p&gt;\n\n&lt;p&gt;But then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.&lt;/p&gt;\n\n&lt;p&gt;In the end it also needs to be tested in all the different backends llama.cpp supports, vulkan/cuda/hip/cpu/etc.&lt;/p&gt;\n\n&lt;p&gt;MLX only needs to care about apple silicon.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If I wrote something really wrong, feel free to correct me.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n61jxix/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753906917,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 19
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62pj25",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Gold_Scholar1111",
            "can_mod_post": false,
            "created_utc": 1753919592,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": 1,
            "author_fullname": "t2_ccc788to",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are they deliver results very differently? in terms like resource using efficiency or output quality for equivalent hardwares.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62pj25",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are they deliver results very differently? in terms like resource using efficiency or output quality for equivalent hardwares.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62pj25/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753919592,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62s06w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pristine-Woodpecker",
            "can_mod_post": false,
            "created_utc": 1753920433,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": -1,
            "author_fullname": "t2_5b972ieo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For one, GLM 4.5 support in MLX was added by Apple. They have \"a few\" more resources than the random hobbyist that's trying to get it into llama.cpp.\n\nllama.cpp support is marginally more messy because of the need to translate to GGUF. But it's not fundamentally different.\n\nMLX still doesn't have any vision support for \\*any\\* of these models, so I'm not sure they really \"add support faster\".",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62s06w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For one, GLM 4.5 support in MLX was added by Apple. They have &amp;quot;a few&amp;quot; more resources than the random hobbyist that&amp;#39;s trying to get it into llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;llama.cpp support is marginally more messy because of the need to translate to GGUF. But it&amp;#39;s not fundamentally different.&lt;/p&gt;\n\n&lt;p&gt;MLX still doesn&amp;#39;t have any vision support for *any* of these models, so I&amp;#39;m not sure they really &amp;quot;add support faster&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62s06w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753920433,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        }
      ],
      "before": null
    }
  }
]