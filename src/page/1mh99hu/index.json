[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:\n\n* Some requests randomly timed out or took way longer than others, even with the same provider\n* Logs didn’t show much, and tracing failures across providers was difficult\n* When we tried running it behind a load balancer, we ran into strange behavior with state\n* Fallbacks didn’t always trigger reliably when a provider was down or rate-limited\n* We tried plugging in Prometheus, but visibility into request flow was limited\n\nThe architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.\n\nWhile looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "LiteLLM started breaking down for us past 300 RPS, what are folks using in prod?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mh99hu",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 21,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1p9vds0za6",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 21,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754301478,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some requests randomly timed out or took way longer than others, even with the same provider&lt;/li&gt;\n&lt;li&gt;Logs didn’t show much, and tracing failures across providers was difficult&lt;/li&gt;\n&lt;li&gt;When we tried running it behind a load balancer, we ran into strange behavior with state&lt;/li&gt;\n&lt;li&gt;Fallbacks didn’t always trigger reliably when a provider was down or rate-limited&lt;/li&gt;\n&lt;li&gt;We tried plugging in Prometheus, but visibility into request flow was limited&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.&lt;/p&gt;\n\n&lt;p&gt;While looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mh99hu",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Otherwise_Flan7339",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/",
            "subreddit_subscribers": 510541,
            "created_utc": 1754301478,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6yr6z7",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "any41",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6unpoo",
                                "score": 2,
                                "author_fullname": "t2_16yhcjni",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah so many of them have suggested bifrost.\nMoreover litellm is too bloated for my usecase. I dont want, user management, other tracking stuff.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6yr6z7",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah so many of them have suggested bifrost.\nMoreover litellm is too bloated for my usecase. I dont want, user management, other tracking stuff.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mh99hu",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6yr6z7/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754351807,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754351807,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6unpoo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Otherwise_Flan7339",
                      "can_mod_post": false,
                      "created_utc": 1754306014,
                      "send_replies": true,
                      "parent_id": "t1_n6uf73a",
                      "score": 4,
                      "author_fullname": "t2_1p9vds0za6",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "planning to look at bifrost. there is one long comment about that. seems better than most i have seen",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6unpoo",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;planning to look at bifrost. there is one long comment about that. seems better than most i have seen&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh99hu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6unpoo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754306014,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6uf73a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "any41",
            "can_mod_post": false,
            "created_utc": 1754301875,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 10,
            "author_fullname": "t2_16yhcjni",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Even i would like to know.  I have a similar requirement and was considering litellm.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uf73a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Even i would like to know.  I have a similar requirement and was considering litellm.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6uf73a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754301875,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n71glt1",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "EngineConstant1900",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6v7qpn",
                                "score": 1,
                                "author_fullname": "t2_1uzlexoydj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Hey [u/SigmoidGrindset](https://www.reddit.com/user/SigmoidGrindset/), I’m from the team behind Bifrost and really happy to see someone digging deep into the feature implementation of Bifrost. We were actually planning the exact features you mentioned, model/provider-specific budgeting and a gossip protocol for communication between Bifrost instances. You can find more about our v1.2 milestone here: [https://github.com/maximhq/bifrost/discussions/118](https://github.com/maximhq/bifrost/discussions/118).\n\nFor added robustness, we're also launching our circuit breaker plugin and key blacklisting algorithms in upcoming releases. Happy to answer more of your queries, and always open to feedback!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n71glt1",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"https://www.reddit.com/user/SigmoidGrindset/\"&gt;u/SigmoidGrindset&lt;/a&gt;, I’m from the team behind Bifrost and really happy to see someone digging deep into the feature implementation of Bifrost. We were actually planning the exact features you mentioned, model/provider-specific budgeting and a gossip protocol for communication between Bifrost instances. You can find more about our v1.2 milestone here: &lt;a href=\"https://github.com/maximhq/bifrost/discussions/118\"&gt;https://github.com/maximhq/bifrost/discussions/118&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;For added robustness, we&amp;#39;re also launching our circuit breaker plugin and key blacklisting algorithms in upcoming releases. Happy to answer more of your queries, and always open to feedback!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mh99hu",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n71glt1/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754396153,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754396153,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6v7qpn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "SigmoidGrindset",
                      "can_mod_post": false,
                      "created_utc": 1754313723,
                      "send_replies": true,
                      "parent_id": "t1_n6uggyc",
                      "score": 4,
                      "author_fullname": "t2_jldqthqa3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm currently using LiteLLM (as a proxy, in Docker, with some minor customizations patched in). It did the job, but I'm not a huge fan - especially after digging through the codebase to figure out how to patch things, it feels very messy / hacky. It's also trying to do far more things (user + billing management etc) than what I need from it (internal microservice consumed by backend systems that already have auth etc).\n\nI just took a look at Bifrost and it looks very promising. The benchmarking results look encouraging, it supports most of the features I need without too much bloat, and the Golang codebase looks better engineered and fairly straightforward to work with. It does seem to lack the level of Datadog integration I need, but I've had to implement that myself for LiteLLM already, and the existing Maxim plugin gives me some confidence that it should be even easier to integrate Datadog with Bifrost.\n\nThe one place it does seem a bit lacking is around managing provider rate limits and horizontal scalability. In LiteLLM proxy, I can set rpm/tpm rates per provider, matching these values to the ones documented / configured for the provider implementation, and when loadbalancing instances, it can sync values between instances to ensure that we stay within rate limit budgets for each provider even when requests are spread across the instances. I haven't dug deep into Bifrost yet, but it looks like the rate limiting approach is limited to setting values for concurrency and buffer size, and when load balancing, there's no communication between instances. It does support retry and exponential backoff configuration though, so in practice maybe this would be sufficient to prevent overloading provider APIs.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6v7qpn",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently using LiteLLM (as a proxy, in Docker, with some minor customizations patched in). It did the job, but I&amp;#39;m not a huge fan - especially after digging through the codebase to figure out how to patch things, it feels very messy / hacky. It&amp;#39;s also trying to do far more things (user + billing management etc) than what I need from it (internal microservice consumed by backend systems that already have auth etc).&lt;/p&gt;\n\n&lt;p&gt;I just took a look at Bifrost and it looks very promising. The benchmarking results look encouraging, it supports most of the features I need without too much bloat, and the Golang codebase looks better engineered and fairly straightforward to work with. It does seem to lack the level of Datadog integration I need, but I&amp;#39;ve had to implement that myself for LiteLLM already, and the existing Maxim plugin gives me some confidence that it should be even easier to integrate Datadog with Bifrost.&lt;/p&gt;\n\n&lt;p&gt;The one place it does seem a bit lacking is around managing provider rate limits and horizontal scalability. In LiteLLM proxy, I can set rpm/tpm rates per provider, matching these values to the ones documented / configured for the provider implementation, and when loadbalancing instances, it can sync values between instances to ensure that we stay within rate limit budgets for each provider even when requests are spread across the instances. I haven&amp;#39;t dug deep into Bifrost yet, but it looks like the rate limiting approach is limited to setting values for concurrency and buffer size, and when load balancing, there&amp;#39;s no communication between instances. It does support retry and exponential backoff configuration though, so in practice maybe this would be sufficient to prevent overloading provider APIs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh99hu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6v7qpn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754313723,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6x84ig",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "EngineConstant1900",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6uh51z",
                                "score": 2,
                                "author_fullname": "t2_1uzlexoydj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "t3 might throttle for high response times, ig t3 large should do",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6x84ig",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;t3 might throttle for high response times, ig t3 large should do&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mh99hu",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6x84ig/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754334765,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754334765,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6uj30p",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "any41",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6ui69t",
                                          "score": 1,
                                          "author_fullname": "t2_16yhcjni",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Ok thank you.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6uj30p",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ok thank you.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mh99hu",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6uj30p/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754303882,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754303882,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ui69t",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AdSpecialist4154",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6uh51z",
                                "score": 1,
                                "author_fullname": "t2_fu1jln6z",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "t3 medium should be good, btw you can check their official benchmarks at 5k rps here - [https://github.com/maximhq/bifrost/blob/main/docs/benchmarks.md](https://github.com/maximhq/bifrost/blob/main/docs/benchmarks.md)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ui69t",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;t3 medium should be good, btw you can check their official benchmarks at 5k rps here - &lt;a href=\"https://github.com/maximhq/bifrost/blob/main/docs/benchmarks.md\"&gt;https://github.com/maximhq/bifrost/blob/main/docs/benchmarks.md&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mh99hu",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6ui69t/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754303420,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754303420,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6uh51z",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "any41",
                      "can_mod_post": false,
                      "created_utc": 1754302896,
                      "send_replies": true,
                      "parent_id": "t1_n6uggyc",
                      "score": 1,
                      "author_fullname": "t2_16yhcjni",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "If i want to handle 2-2.5k rps, what should the min specs of the vm?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6uh51z",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If i want to handle 2-2.5k rps, what should the min specs of the vm?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mh99hu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6uh51z/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754302896,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6uggyc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AdSpecialist4154",
            "can_mod_post": false,
            "created_utc": 1754302550,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 7,
            "author_fullname": "t2_fu1jln6z",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "LiteLLm is okay but you should try Bifrost, I believe its the fastest AI Gateway \n\nI’ve been using LiteLLM for a while as a quick way to unify OpenAI, Claude, Mistral, etc. It’s solid for dev or low-RPS workloads, but I kept running into issues as we started scaling:\n\n* Latency spiked heavily past 2K RPS\n* CPU and memory usage climbed fast under load\n* Observability was limited, making debugging a pain\n* P99 latency would jump to 40–50 ms even with caching\n\nStarted looking for alternatives and randomly came across Bifrost in a Reddit comment. Decided to try it out and I’m honestly blown away.\n\nI tested it under similar conditions and here’s what I saw:\n\n* 5K RPS sustained on a mid-tier VM\n* 11µs mean overhead, flat across load tests\n* P99 latency at 0.87 ms (LiteLLM was around 47 ms)\n* \\~9.5x higher throughput, 54x lower P99, 68% lower memory use\n* Native Prometheus support, visual dashboard\n* No wrappers, just clean, fast HTTP API\n* Handles dynamic provider routing, timeouts, retries, structured logs\n\nIt was plug-and-play with our existing setup. Genuinely feels like infra-grade tooling, not a Python wrapper trying to do too much.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uggyc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LiteLLm is okay but you should try Bifrost, I believe its the fastest AI Gateway &lt;/p&gt;\n\n&lt;p&gt;I’ve been using LiteLLM for a while as a quick way to unify OpenAI, Claude, Mistral, etc. It’s solid for dev or low-RPS workloads, but I kept running into issues as we started scaling:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Latency spiked heavily past 2K RPS&lt;/li&gt;\n&lt;li&gt;CPU and memory usage climbed fast under load&lt;/li&gt;\n&lt;li&gt;Observability was limited, making debugging a pain&lt;/li&gt;\n&lt;li&gt;P99 latency would jump to 40–50 ms even with caching&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Started looking for alternatives and randomly came across Bifrost in a Reddit comment. Decided to try it out and I’m honestly blown away.&lt;/p&gt;\n\n&lt;p&gt;I tested it under similar conditions and here’s what I saw:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5K RPS sustained on a mid-tier VM&lt;/li&gt;\n&lt;li&gt;11µs mean overhead, flat across load tests&lt;/li&gt;\n&lt;li&gt;P99 latency at 0.87 ms (LiteLLM was around 47 ms)&lt;/li&gt;\n&lt;li&gt;~9.5x higher throughput, 54x lower P99, 68% lower memory use&lt;/li&gt;\n&lt;li&gt;Native Prometheus support, visual dashboard&lt;/li&gt;\n&lt;li&gt;No wrappers, just clean, fast HTTP API&lt;/li&gt;\n&lt;li&gt;Handles dynamic provider routing, timeouts, retries, structured logs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It was plug-and-play with our existing setup. Genuinely feels like infra-grade tooling, not a Python wrapper trying to do too much.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6uggyc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754302550,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xs6zh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ExcuseAccomplished97",
            "can_mod_post": false,
            "created_utc": 1754340555,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 3,
            "author_fullname": "t2_73xg2fw4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's written in Python, so I'm not surprised that it can't handle a heavy load on a single node. I contributed to their project several times, and the codebase was somewhat messy. They didn't care much about usability; they focused more on enterprise features.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xs6zh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s written in Python, so I&amp;#39;m not surprised that it can&amp;#39;t handle a heavy load on a single node. I contributed to their project several times, and the codebase was somewhat messy. They didn&amp;#39;t care much about usability; they focused more on enterprise features.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6xs6zh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754340555,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uhm9h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lonely_Pea_7748",
            "can_mod_post": false,
            "created_utc": 1754303139,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 3,
            "author_fullname": "t2_tk8cux0tq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We recently benchmarked against LiteLLM. Do check TrueFoundry out. Being used by multiple enterprises at higher RPS than 600 - [https://www.truefoundry.com/blog/truefoundry-llm-gateway-is-blazing-fast](https://www.truefoundry.com/blog/truefoundry-llm-gateway-is-blazing-fast)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uhm9h",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We recently benchmarked against LiteLLM. Do check TrueFoundry out. Being used by multiple enterprises at higher RPS than 600 - &lt;a href=\"https://www.truefoundry.com/blog/truefoundry-llm-gateway-is-blazing-fast\"&gt;https://www.truefoundry.com/blog/truefoundry-llm-gateway-is-blazing-fast&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6uhm9h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754303139,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ugqo7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ViRROOO",
            "can_mod_post": false,
            "created_utc": 1754302691,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 1,
            "author_fullname": "t2_11nncq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It depends on which features matter most to you. If it's only about serving and observing, you can use VLLM (and similar) to serve, and your preferred observation tool to monitor (e.g., Datadog and Grafana support LLM monitoring). If you care about the failover and cost tracking, then try [Portkey](https://portkey.ai/) or [host your own littlellm](https://crates.io/crates/litellm-rs)/[Bifrost](https://github.com/maximhq/bifrost) and handle yourself the horizontal scaling",
            "edited": 1754303122,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ugqo7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends on which features matter most to you. If it&amp;#39;s only about serving and observing, you can use VLLM (and similar) to serve, and your preferred observation tool to monitor (e.g., Datadog and Grafana support LLM monitoring). If you care about the failover and cost tracking, then try &lt;a href=\"https://portkey.ai/\"&gt;Portkey&lt;/a&gt; or &lt;a href=\"https://crates.io/crates/litellm-rs\"&gt;host your own littlellm&lt;/a&gt;/&lt;a href=\"https://github.com/maximhq/bifrost\"&gt;Bifrost&lt;/a&gt; and handle yourself the horizontal scaling&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6ugqo7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754302691,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ukhah",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BadSkater0729",
            "can_mod_post": false,
            "created_utc": 1754304551,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 1,
            "author_fullname": "t2_u4deq6r0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is a shot in the dark but do you have debug output enabled? We noticed OOMs when this setting was on weirdly enough",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ukhah",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a shot in the dark but do you have debug output enabled? We noticed OOMs when this setting was on weirdly enough&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6ukhah/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754304551,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uqo5a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Blackvz",
            "can_mod_post": false,
            "created_utc": 1754307305,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 1,
            "author_fullname": "t2_dkxx8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You could try vercel ai. It should be very similar.\n\nDid not use this in prod yet",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uqo5a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You could try vercel ai. It should be very similar.&lt;/p&gt;\n\n&lt;p&gt;Did not use this in prod yet&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6uqo5a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754307305,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6vq2mz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TheTerrasque",
            "can_mod_post": false,
            "created_utc": 1754319466,
            "send_replies": true,
            "parent_id": "t3_1mh99hu",
            "score": 1,
            "author_fullname": "t2_9uv8v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; When we tried running it behind a load balancer, we ran into strange behavior with state \n\n\nDid you try pinning by ip or similar? That might help",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6vq2mz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;When we tried running it behind a load balancer, we ran into strange behavior with state &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Did you try pinning by ip or similar? That might help&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/n6vq2mz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754319466,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh99hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]