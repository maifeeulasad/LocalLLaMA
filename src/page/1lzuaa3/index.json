[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Obviously this is a silly question. 4k context is limiting to the point where even dumber models are \"better\" for almost any pipeline and use case.\n\nBut for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG's)? What model sizes now beat Llama2-70B in:\n\n- instruction following\n\n- depth of knowledge \n\n- writing skill\n\n- coding \n\n- logic",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "If you limit context to 4k tokens, which models today beat Llama2-70B from 2 years ago?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1lzuaa3",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_w2gxqd6i2",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752517599,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously this is a silly question. 4k context is limiting to the point where even dumber models are &amp;quot;better&amp;quot; for almost any pipeline and use case.&lt;/p&gt;\n\n&lt;p&gt;But for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG&amp;#39;s)? What model sizes now beat Llama2-70B in:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;instruction following&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;depth of knowledge &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;writing skill&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;coding &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;logic&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lzuaa3",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "EmPips",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/",
            "subreddit_subscribers": 498850,
            "created_utc": 1752517599,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34eqt0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "BigRepresentative731",
            "can_mod_post": false,
            "created_utc": 1752518057,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 7,
            "author_fullname": "t2_8a78x7h6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen 2.5 14b",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34eqt0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen 2.5 14b&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34eqt0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752518057,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n34hmxm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Efficiency_1144",
                      "can_mod_post": false,
                      "created_utc": 1752518885,
                      "send_replies": true,
                      "parent_id": "t1_n34fh00",
                      "score": 1,
                      "author_fullname": "t2_1nkj9l14b0",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah it doesn’t have to be 4k tokens of English\n\n\n4k tokens of a domain specific language or encoded data can be loads",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n34hmxm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah it doesn’t have to be 4k tokens of English&lt;/p&gt;\n\n&lt;p&gt;4k tokens of a domain specific language or encoded data can be loads&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lzuaa3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34hmxm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752518885,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n34fh00",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1752518266,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 3,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Actually, 4k context is \\*a lot\\* in the context of a broader system; you'd be surprised at what can be done with 4k context and a carefully engineered setup.\n\nRegardless, in my humble opinion:\n\nOlmo 2 32B.\n\nIt's a pretty remarkable model and really does feel like the mini Claude at home, its only limitation being its context window of 4k (which probably could be alleviated with things like Rope or Alibi if the model were well supported in inference backends).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34fh00",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Actually, 4k context is *a lot* in the context of a broader system; you&amp;#39;d be surprised at what can be done with 4k context and a carefully engineered setup.&lt;/p&gt;\n\n&lt;p&gt;Regardless, in my humble opinion:&lt;/p&gt;\n\n&lt;p&gt;Olmo 2 32B.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a pretty remarkable model and really does feel like the mini Claude at home, its only limitation being its context window of 4k (which probably could be alleviated with things like Rope or Alibi if the model were well supported in inference backends).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34fh00/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752518266,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34hab2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1752518787,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 1,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Almost all of my usage of LLMs is below 4k token context",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34hab2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Almost all of my usage of LLMs is below 4k token context&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34hab2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752518787,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34j8ok",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Red_Redditor_Reddit",
            "can_mod_post": false,
            "created_utc": 1752519354,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 1,
            "author_fullname": "t2_8eelmfjg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My experience is that the newer models are generally better at everything except writing with human like verbiage from being over trained. Llama 2 also seems to do a better job following some system prompts. Newer models seem to be hard trained to be an AI assistant. If I give a prompt to llama 2 that says \"this is a conversation between an ant and a user\", it will hallucinate being an ant. Newer models will insist that it's an AI assistant or at least an ant AI assistant. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34j8ok",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My experience is that the newer models are generally better at everything except writing with human like verbiage from being over trained. Llama 2 also seems to do a better job following some system prompts. Newer models seem to be hard trained to be an AI assistant. If I give a prompt to llama 2 that says &amp;quot;this is a conversation between an ant and a user&amp;quot;, it will hallucinate being an ant. Newer models will insist that it&amp;#39;s an AI assistant or at least an ant AI assistant. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34j8ok/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752519354,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34l16b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1752519885,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 1,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Never tried llama2-70B, but if you offer sample prompt and the produced result, that would make it easier to  answer. I'd think all 24b and bigger models of 2025 will beat llama2 at everything but writing skill, which is hit and miss with modern models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34l16b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Never tried llama2-70B, but if you offer sample prompt and the produced result, that would make it easier to  answer. I&amp;#39;d think all 24b and bigger models of 2025 will beat llama2 at everything but writing skill, which is hit and miss with modern models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34l16b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752519885,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34np65",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1752520635,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 1,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Plain llama-2 was pretty meh.. what models would beat miqu?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34np65",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Plain llama-2 was pretty meh.. what models would beat miqu?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34np65/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752520635,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34ilph",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DinoAmino",
            "can_mod_post": false,
            "created_utc": 1752519161,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 1,
            "author_fullname": "t2_j1v7f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'd hazard a subjective guess that anything \"recent\" 32B and above are better in all regards due to more training tokens, improved training methods, and higher quality datasets. Codellama 70B was a fine-tune of Llama 2. Since then the only coder fine-tune above 70B was the DS 236B. So I'm assuming later models 70B and above have also been trained on coding datasets. Like Qwen 2.5 32B coder probably was fine-tuned on the same coding datasets used in their 72B. And that coder  certainly beats codellama - codestral 22B did.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34ilph",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d hazard a subjective guess that anything &amp;quot;recent&amp;quot; 32B and above are better in all regards due to more training tokens, improved training methods, and higher quality datasets. Codellama 70B was a fine-tune of Llama 2. Since then the only coder fine-tune above 70B was the DS 236B. So I&amp;#39;m assuming later models 70B and above have also been trained on coding datasets. Like Qwen 2.5 32B coder probably was fine-tuned on the same coding datasets used in their 72B. And that coder  certainly beats codellama - codestral 22B did.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34ilph/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752519161,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34gory",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ravenpest",
            "can_mod_post": false,
            "created_utc": 1752518618,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": -1,
            "author_fullname": "t2_21lhc1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Deepseek (R1 \\\\ V3) at Q1 at 4k context shits on Llama2 70b any day of the week for whatever task your heart desires.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34gory",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Deepseek (R1 \\ V3) at Q1 at 4k context shits on Llama2 70b any day of the week for whatever task your heart desires.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34gory/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752518618,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n34ntgq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mikael110",
            "can_mod_post": false,
            "created_utc": 1752520670,
            "send_replies": true,
            "parent_id": "t3_1lzuaa3",
            "score": 1,
            "author_fullname": "t2_4amlo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If we are talking about vanilla Llama-2 and not a finetune, then honestly pretty much any modern model that is 12B or above will likely beat it on anything other that creative writing.\n\nLlama-2 always felt like it was undertrained. It was not very good at instruction following, and it certainly wasn't a fountain of knowledge either. It was also one of the first official instruction models that had been red-teamed to such an extend that it was basically unusable for most tasks.  It was the origin of the whole \"Refusing to kill a Linux process\" which was a meme for a bit in this community.\n\nThat's part of why very few actually used the official instruct model, and finetunes flourished. I'm pretty sure more finetunes came out of Llama-2 than any other models before or since.\n\nCoding was also terrible, it came out before coding was a big focus among LLMs, and it shows. I remember there was a big push to create coding finetunes from it back then specifically because the base model was so bad at it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n34ntgq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If we are talking about vanilla Llama-2 and not a finetune, then honestly pretty much any modern model that is 12B or above will likely beat it on anything other that creative writing.&lt;/p&gt;\n\n&lt;p&gt;Llama-2 always felt like it was undertrained. It was not very good at instruction following, and it certainly wasn&amp;#39;t a fountain of knowledge either. It was also one of the first official instruction models that had been red-teamed to such an extend that it was basically unusable for most tasks.  It was the origin of the whole &amp;quot;Refusing to kill a Linux process&amp;quot; which was a meme for a bit in this community.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s part of why very few actually used the official instruct model, and finetunes flourished. I&amp;#39;m pretty sure more finetunes came out of Llama-2 than any other models before or since.&lt;/p&gt;\n\n&lt;p&gt;Coding was also terrible, it came out before coding was a big focus among LLMs, and it shows. I remember there was a big push to create coding finetunes from it back then specifically because the base model was so bad at it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34ntgq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752520670,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lzuaa3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]