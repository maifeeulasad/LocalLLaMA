[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I completed my local LLM rig in May, just after Qwen3's release (thanks to r/LocalLLaMA 's folks for the invaluable guidance!). Now that I've settled into the setup, I'm excited to share my build and how it's performing with local LLMs.\n\nhttps://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\n\n  \n\n\nThis is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let's dive in!\n\n# Key Specs\n\n|Component|Specs|\n|:-|:-|\n|**CPU**|AMD Ryzen 7 7700 (8C/16T)|\n|**GPU**|2 x NVIDIA RTX 3090 (48GB VRAM total)|\n|**RAM**|64GB DDR5 @ 6400 MHz|\n|**Storage**|2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)|\n|**Motherboard**|ASUS TUF B650-PLUS|\n|**PSU**|850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)|\n|**Case**|Lian Li LANCOOL 216|\n|**Cooling**|a lot of fans ðŸ’¨|\n\nTried to run the following:\n\n* **30B-A3B Q4\\_K\\_XL**, **32B Q4\\_K\\_XL** â€“ fit into one GPU with ample context window\n* **32B Q8\\_K\\_XL** â€“ runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference\n* **30B-A3B Q8\\_K\\_XL** â€“ now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model's context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I havenâ€™t found an optimal way to store weights once and manage contexts separately, so this remains a WiP).\n\nPrimary use: Running Qwen3-30B-A3B models with `llama.cpp`. The performance for this model is \\~ 1000 pp512 / 100 tg128\n\nWhat's next? I think I will play with this one for a while. But... I'm already eyeing an EPYC-based system with 4x 4090s (48GB each). ðŸ˜Ž",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "My (practical) dual 3090 setup for local inference",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 105,
            "top_awarded_type": null,
            "hide_score": true,
            "media_metadata": {
              "wiim1ouai7ef1": {
                "status": "valid",
                "e": "Image",
                "m": "image/jpg",
                "p": [
                  {
                    "y": 81,
                    "x": 108,
                    "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4174de20838a1b6b769288eb5b1b5f70f0f9602"
                  },
                  {
                    "y": 162,
                    "x": 216,
                    "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c538aa32a3f813976d5ce771e34c0d48b0440eaa"
                  },
                  {
                    "y": 240,
                    "x": 320,
                    "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=122f1edd42a0419bd780aa64f40f29f91c0ed509"
                  },
                  {
                    "y": 480,
                    "x": 640,
                    "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41b8c09a47c3c8926c73db4b4303c394b1b45f6f"
                  },
                  {
                    "y": 720,
                    "x": 960,
                    "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1eb4a1cb38cbbf6d104e970f4b00623ccd4033db"
                  },
                  {
                    "y": 810,
                    "x": 1080,
                    "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=720ffaef64226dedbcf3e6ecd6efd90763b1edcf"
                  }
                ],
                "s": {
                  "y": 961,
                  "x": 1280,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf"
                },
                "id": "wiim1ouai7ef1"
              }
            },
            "name": "t3_1m5fkts",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1nkpqiujlm",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://a.thumbs.redditmedia.com/vvE88N6ubX7Gsux9A2O-Hn5WBZJNPCIpKoUwojUDik4.jpg",
            "edited": 1753095094,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753094600,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I completed my local LLM rig in May, just after Qwen3&amp;#39;s release (thanks to &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; &amp;#39;s folks for the invaluable guidance!). Now that I&amp;#39;ve settled into the setup, I&amp;#39;m excited to share my build and how it&amp;#39;s performing with local LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\"&gt;https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let&amp;#39;s dive in!&lt;/p&gt;\n\n&lt;h1&gt;Key Specs&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Component&lt;/th&gt;\n&lt;th align=\"left\"&gt;Specs&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 7 7700 (8C/16T)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2 x NVIDIA RTX 3090 (48GB VRAM total)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;64GB DDR5 @ 6400 MHz&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;ASUS TUF B650-PLUS&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Lian Li LANCOOL 216&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Cooling&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;a lot of fans ðŸ’¨&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Tried to run the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;30B-A3B Q4_K_XL&lt;/strong&gt;, &lt;strong&gt;32B Q4_K_XL&lt;/strong&gt; â€“ fit into one GPU with ample context window&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;32B Q8_K_XL&lt;/strong&gt; â€“ runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;30B-A3B Q8_K_XL&lt;/strong&gt; â€“ now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model&amp;#39;s context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I havenâ€™t found an optimal way to store weights once and manage contexts separately, so this remains a WiP).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Primary use: Running Qwen3-30B-A3B models with &lt;code&gt;llama.cpp&lt;/code&gt;. The performance for this model is ~ 1000 pp512 / 100 tg128&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s next? I think I will play with this one for a while. But... I&amp;#39;m already eyeing an EPYC-based system with 4x 4090s (48GB each). ðŸ˜Ž&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m5fkts",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ColdImplement1319",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/",
            "subreddit_subscribers": 502273,
            "created_utc": 1753094600,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4bk6mg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "jacek2023",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4bk0zy",
                                "score": 2,
                                "author_fullname": "t2_vqgbql9w",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Jamba, Dots, Hunyuan, Llama Scout",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4bk6mg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Jamba, Dots, Hunyuan, Llama Scout&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5fkts",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bk6mg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753096022,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753096022,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bk0zy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ColdImplement1319",
                      "can_mod_post": false,
                      "created_utc": 1753095950,
                      "send_replies": true,
                      "parent_id": "t1_n4bjcw3",
                      "score": 1,
                      "author_fullname": "t2_1nkpqiujlm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Do you have any recommendations? I'm currently running Qwen3 30B-A3B, which is an MoE model and quite up-to-date.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bk0zy",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you have any recommendations? I&amp;#39;m currently running Qwen3 30B-A3B, which is an MoE model and quite up-to-date.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fkts",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bk0zy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753095950,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4bjcw3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1753095632,
            "send_replies": true,
            "parent_id": "t3_1m5fkts",
            "score": 3,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try some modern MoE models",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bjcw3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try some modern MoE models&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bjcw3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753095632,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m5fkts",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4bm04e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fizzy1242",
            "can_mod_post": false,
            "created_utc": 1753096860,
            "send_replies": true,
            "parent_id": "t3_1m5fkts",
            "score": 2,
            "author_fullname": "t2_16zcsx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "try some 70b models in exl2 format. they're very fast, even with 200W powerlimit.\n\n3rd one lets you run 4.0bpw mistral large, wink.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bm04e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;try some 70b models in exl2 format. they&amp;#39;re very fast, even with 200W powerlimit.&lt;/p&gt;\n\n&lt;p&gt;3rd one lets you run 4.0bpw mistral large, wink.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bm04e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753096860,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fkts",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]