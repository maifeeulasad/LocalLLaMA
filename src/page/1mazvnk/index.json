[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I am running an ollama server as a container in unraid, but I am running up against some problems where models are failing for some use cases. I have several different clients connecting to the server. But I don't know the best way to monitor ollama, for example even just for token usage. But really I want to have some way to monitor what ollama is doing, how models are performing, and to help diagnose problems. But I am having trouble finding a good way to do it. How are you monitoring your ollama server and checking model performance? ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How do you monitor your Ollama instance?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mazvnk",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_4ibsm",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753656282,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am running an ollama server as a container in unraid, but I am running up against some problems where models are failing for some use cases. I have several different clients connecting to the server. But I don&amp;#39;t know the best way to monitor ollama, for example even just for token usage. But really I want to have some way to monitor what ollama is doing, how models are performing, and to help diagnose problems. But I am having trouble finding a good way to do it. How are you monitoring your ollama server and checking model performance? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mazvnk",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ishbuggy",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/",
            "subreddit_subscribers": 505879,
            "created_utc": 1753656282,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5iize7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CtrlAltDelve",
            "can_mod_post": false,
            "created_utc": 1753656756,
            "send_replies": true,
            "parent_id": "t3_1mazvnk",
            "score": 1,
            "author_fullname": "t2_1f1tptkzcs",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It looks like maybe OpenLIT might be what you're looking for? https://docs.openlit.io/latest/integrations/ollama\n\nI have not used it, sorry, but I hope it points you in the right direction!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5iize7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It looks like maybe OpenLIT might be what you&amp;#39;re looking for? &lt;a href=\"https://docs.openlit.io/latest/integrations/ollama\"&gt;https://docs.openlit.io/latest/integrations/ollama&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have not used it, sorry, but I hope it points you in the right direction!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/n5iize7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753656756,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mazvnk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5j5uy6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HistorianPotential48",
            "can_mod_post": false,
            "created_utc": 1753665017,
            "send_replies": true,
            "parent_id": "t3_1mazvnk",
            "score": 1,
            "author_fullname": "t2_4dzthia7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Without streaming, Ollama's response contains a \\`eval\\_count\\` which in official repo discussions is mentioned as output token count. With streaming i guess it's 1 token per chunk? idk\n\nFor monitoring, we wrap our ollama with a REST api. People who want to use LLM should call through that api, then in the api we can do despicable things like recording token counts or prompts in DB.\n\nWe also connect it to a Loki, so we can later analise the logs sent by api.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5j5uy6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Without streaming, Ollama&amp;#39;s response contains a `eval_count` which in official repo discussions is mentioned as output token count. With streaming i guess it&amp;#39;s 1 token per chunk? idk&lt;/p&gt;\n\n&lt;p&gt;For monitoring, we wrap our ollama with a REST api. People who want to use LLM should call through that api, then in the api we can do despicable things like recording token counts or prompts in DB.&lt;/p&gt;\n\n&lt;p&gt;We also connect it to a Loki, so we can later analise the logs sent by api.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mazvnk/how_do_you_monitor_your_ollama_instance/n5j5uy6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753665017,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mazvnk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]