[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Scenario 1:  \nWe’re currently hosting our image‐generation model on Replicate and calling its API from the customer’s side. Unfortunately, each cold start can take up to 15 minutes to produce a single image, which is totally unacceptable for end users. (If only a very small number of customers use it, the delay is less noticeable—but as soon as usage scales to dozens, it becomes a real problem.)\n\nScenario 2:  \nMy boss suggested we buy a dedicated PC equipped with an NVIDIA RTX 4090, install the model locally, and expose our own API endpoint to customers. That would eliminate the cold-start penalty—but if many customers submit requests simultaneously, they’ll still queue up and wait, and we don’t even have a proper server room to house the hardware.\n\nWhat I’d really like is a cost-effective cloud solution that avoids long cold-start times. Does anyone have recommendations for how we could deploy this model so that image requests spin up almost instantly?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Buy a Local PC with GPU or Go for Cloud",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1ltidhz",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.36,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_bm7appuf",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1751853908,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Scenario 1:&lt;br/&gt;\nWe’re currently hosting our image‐generation model on Replicate and calling its API from the customer’s side. Unfortunately, each cold start can take up to 15 minutes to produce a single image, which is totally unacceptable for end users. (If only a very small number of customers use it, the delay is less noticeable—but as soon as usage scales to dozens, it becomes a real problem.)&lt;/p&gt;\n\n&lt;p&gt;Scenario 2:&lt;br/&gt;\nMy boss suggested we buy a dedicated PC equipped with an NVIDIA RTX 4090, install the model locally, and expose our own API endpoint to customers. That would eliminate the cold-start penalty—but if many customers submit requests simultaneously, they’ll still queue up and wait, and we don’t even have a proper server room to house the hardware.&lt;/p&gt;\n\n&lt;p&gt;What I’d really like is a cost-effective cloud solution that avoids long cold-start times. Does anyone have recommendations for how we could deploy this model so that image requests spin up almost instantly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1ltidhz",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "visionkhawar512",
            "discussion_type": null,
            "num_comments": 29,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/",
            "subreddit_subscribers": 495647,
            "created_utc": 1751853908,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n1qrc1v",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Buey",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n1qq52x",
                                                    "score": 3,
                                                    "author_fullname": "t2_6nde1",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I'm not sure, my experience is pretty limited.  I've been playing around with image models (Stable Diffusion) locally on an AMD gpu using diffusers and my own custom loading code, and model loading itself to GPU is always on the order of maybe 15 seconds max.  This is with the model already locally on disk.\n\nLong model loads for me means that it's trying to access the local already downloaded model, but is using the wrong path, and instead tries to download the model from HuggingFace fresh.\n\nSDXL models are maybe 8-9GB max on disk.  If you've got a super fancy custom model, the load time could be longer, but still would be expected &lt; 1 min if a 4090 (24GB VRAM) is able to fit the model.\n\nAlso, if it's a custom model, it's probably not on HuggingFace?  Do you have fallback code that pulls the model from cloud storage?\n\n  \nI've never used a hosted platform but 15 mins is \\*extremely\\* long, as in \"something is wrong\" long.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n1qrc1v",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure, my experience is pretty limited.  I&amp;#39;ve been playing around with image models (Stable Diffusion) locally on an AMD gpu using diffusers and my own custom loading code, and model loading itself to GPU is always on the order of maybe 15 seconds max.  This is with the model already locally on disk.&lt;/p&gt;\n\n&lt;p&gt;Long model loads for me means that it&amp;#39;s trying to access the local already downloaded model, but is using the wrong path, and instead tries to download the model from HuggingFace fresh.&lt;/p&gt;\n\n&lt;p&gt;SDXL models are maybe 8-9GB max on disk.  If you&amp;#39;ve got a super fancy custom model, the load time could be longer, but still would be expected &amp;lt; 1 min if a 4090 (24GB VRAM) is able to fit the model.&lt;/p&gt;\n\n&lt;p&gt;Also, if it&amp;#39;s a custom model, it&amp;#39;s probably not on HuggingFace?  Do you have fallback code that pulls the model from cloud storage?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never used a hosted platform but 15 mins is *extremely* long, as in &amp;quot;something is wrong&amp;quot; long.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1ltidhz",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qrc1v/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1751855936,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1751855936,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1qq52x",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "visionkhawar512",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1qp6gy",
                                          "score": 1,
                                          "author_fullname": "t2_bm7appuf",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "It means that there is problem in code, I will check thatm still you think that cool start will take only 20 sec not 15 minutes on replicate.com?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1qq52x",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It means that there is problem in code, I will check thatm still you think that cool start will take only 20 sec not 15 minutes on replicate.com?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltidhz",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qq52x/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751855472,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751855472,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1qp6gy",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Buey",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1qo48j",
                                "score": 3,
                                "author_fullname": "t2_6nde1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah I hear ya those things are expensive.\n\nFrom my limited experience long cold starts or runtimes are some combination of:\n\n1. Downloading models at runtime\n2. Running on CPU instead of GPU\n3. CPU layer offloading\n\nLoading up a model into VRAM should be on the order of seconds, not minutes, if everything is set up right.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1qp6gy",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah I hear ya those things are expensive.&lt;/p&gt;\n\n&lt;p&gt;From my limited experience long cold starts or runtimes are some combination of:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Downloading models at runtime&lt;/li&gt;\n&lt;li&gt;Running on CPU instead of GPU&lt;/li&gt;\n&lt;li&gt;CPU layer offloading&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Loading up a model into VRAM should be on the order of seconds, not minutes, if everything is set up right.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltidhz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qp6gy/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751855099,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751855099,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1qo48j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "visionkhawar512",
                      "can_mod_post": false,
                      "created_utc": 1751854693,
                      "send_replies": true,
                      "parent_id": "t1_n1qn564",
                      "score": 2,
                      "author_fullname": "t2_bm7appuf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Extremely expensive to buy RTX 6000 Pro Blackwell, as customer do not have enough users at the moment. We have docker image on replicate with already downloaded models, we are not downloading any model.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1qo48j",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Extremely expensive to buy RTX 6000 Pro Blackwell, as customer do not have enough users at the moment. We have docker image on replicate with already downloaded models, we are not downloading any model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qo48j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751854693,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1qn564",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Buey",
            "can_mod_post": false,
            "created_utc": 1751854323,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 4,
            "author_fullname": "t2_6nde1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "15 minutes is a super long time for a cold start - are you pulling your entire model at startup time?  Are you using a shared volume with the model already on it to read from?  Or, are you baking your model directly into containers and then pre-pulling container images onto nodes?\n\nOtherwise, if this is your own model, I don't really know of proper solutions to this other than running more warm instances ready to serve requests.\n\nModal has a writeup of cold starts on its platform but again it boils down to prewarmed instances: [https://modal.com/docs/guide/cold-start](https://modal.com/docs/guide/cold-start)\n\nInstead of a 4090 you could spring for a RTX 6000 Pro Blackwell (96GB VRAM) to be able to run even more permanently warm instances at once.",
            "edited": 1751854654,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1qn564",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;15 minutes is a super long time for a cold start - are you pulling your entire model at startup time?  Are you using a shared volume with the model already on it to read from?  Or, are you baking your model directly into containers and then pre-pulling container images onto nodes?&lt;/p&gt;\n\n&lt;p&gt;Otherwise, if this is your own model, I don&amp;#39;t really know of proper solutions to this other than running more warm instances ready to serve requests.&lt;/p&gt;\n\n&lt;p&gt;Modal has a writeup of cold starts on its platform but again it boils down to prewarmed instances: &lt;a href=\"https://modal.com/docs/guide/cold-start\"&gt;https://modal.com/docs/guide/cold-start&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Instead of a 4090 you could spring for a RTX 6000 Pro Blackwell (96GB VRAM) to be able to run even more permanently warm instances at once.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qn564/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751854323,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n1qpren",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "secopsml",
            "can_mod_post": false,
            "created_utc": 1751855323,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 3,
            "author_fullname": "t2_pmniwf57y",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "start with 100% public APIs and add local hardware. Replicate/Modal are only good choice when you process in batches so big that it is cheaper to host models locally or use lots of small requests and you face rate limits.\n\nI use modal when i need to classify images/text and exceed 1k RPM. Then rented hardware excels as I have steady flow that can scale down to 0 after I finish.\n\nI was so fascinated with that concept that I reimplemented KEDA (Kubernetes event driven autoscaling as modal use?) and raspberry pi to remotely manage servers that I boot remotely and then scale down to 0 after batches are done.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1qpren",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;start with 100% public APIs and add local hardware. Replicate/Modal are only good choice when you process in batches so big that it is cheaper to host models locally or use lots of small requests and you face rate limits.&lt;/p&gt;\n\n&lt;p&gt;I use modal when i need to classify images/text and exceed 1k RPM. Then rented hardware excels as I have steady flow that can scale down to 0 after I finish.&lt;/p&gt;\n\n&lt;p&gt;I was so fascinated with that concept that I reimplemented KEDA (Kubernetes event driven autoscaling as modal use?) and raspberry pi to remotely manage servers that I boot remotely and then scale down to 0 after batches are done.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qpren/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751855323,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1r04cu",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "visionkhawar512",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1qro4a",
                                          "score": 1,
                                          "author_fullname": "t2_bm7appuf",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Alright, I understand your point. Also, I would also prefer local PC as there are less users in beginning.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1r04cu",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Alright, I understand your point. Also, I would also prefer local PC as there are less users in beginning.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltidhz",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1r04cu/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751859539,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751859539,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1qro4a",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "buildmine10",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1qpust",
                                "score": 1,
                                "author_fullname": "t2_9zuu2802",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Oh. So it's an automatic scaling thing on replicates end that allows them to better utilize their servers.\n\nTry providing a dummy request every so often just to keep priority. Though that might be more expensive than simply buying enough gpus to bridge the gap. It might be cheaper to use 30 series cards. If my knowledge from local LLMs is applicable, then vram is the main bottleneck of inference, not compute. So high vram low compute cards like the 3060 12GB or 4060 16GB might be a better value. I'm not sure, but it's simple math to verify. Using multiple cards in tandem for models that are too large should also be possible.\n\n\nI'm biased towards the local option, if you haven't noticed.",
                                "edited": 1751856558,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1qro4a",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh. So it&amp;#39;s an automatic scaling thing on replicates end that allows them to better utilize their servers.&lt;/p&gt;\n\n&lt;p&gt;Try providing a dummy request every so often just to keep priority. Though that might be more expensive than simply buying enough gpus to bridge the gap. It might be cheaper to use 30 series cards. If my knowledge from local LLMs is applicable, then vram is the main bottleneck of inference, not compute. So high vram low compute cards like the 3060 12GB or 4060 16GB might be a better value. I&amp;#39;m not sure, but it&amp;#39;s simple math to verify. Using multiple cards in tandem for models that are too large should also be possible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m biased towards the local option, if you haven&amp;#39;t noticed.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltidhz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qro4a/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751856067,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751856067,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1qpust",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "visionkhawar512",
                      "can_mod_post": false,
                      "created_utc": 1751855359,
                      "send_replies": true,
                      "parent_id": "t1_n1qnugt",
                      "score": 1,
                      "author_fullname": "t2_bm7appuf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "We host our model on [replicate.com](http://replicate.com) where if there is no customer hitting our API it goes into cool it means that when you start second time after some hours then replicate will find the GPU and give it you it's called \"cool start\" that's why it takes 15 minutes replicate only charge when we hit the API.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1qpust",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We host our model on &lt;a href=\"http://replicate.com\"&gt;replicate.com&lt;/a&gt; where if there is no customer hitting our API it goes into cool it means that when you start second time after some hours then replicate will find the GPU and give it you it&amp;#39;s called &amp;quot;cool start&amp;quot; that&amp;#39;s why it takes 15 minutes replicate only charge when we hit the API.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qpust/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751855359,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1qnugt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "buildmine10",
            "can_mod_post": false,
            "created_utc": 1751854590,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 1,
            "author_fullname": "t2_9zuu2802",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Those don't seem like two scenarios. That is context and possible solution.\n\nI somewhat doubt that a single 4090 would be sufficient, but I haven't looked into the vram requirements of image generation in a while.\n\nThe proposed solution is good. Yes, the proposed issue is a worry, but all solutions have tradeoffs. It kind of seems like you are just wrapping another company's service and reselling it. Either that or you are using a more general serverless solution and it takes 15 minutes to configure the server before you can provide your service.\n\nWithout knowing why it takes 15 minutes to start for each user, I don't think I can provide good advice. What is happening during those 15 minutes.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1qnugt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Those don&amp;#39;t seem like two scenarios. That is context and possible solution.&lt;/p&gt;\n\n&lt;p&gt;I somewhat doubt that a single 4090 would be sufficient, but I haven&amp;#39;t looked into the vram requirements of image generation in a while.&lt;/p&gt;\n\n&lt;p&gt;The proposed solution is good. Yes, the proposed issue is a worry, but all solutions have tradeoffs. It kind of seems like you are just wrapping another company&amp;#39;s service and reselling it. Either that or you are using a more general serverless solution and it takes 15 minutes to configure the server before you can provide your service.&lt;/p&gt;\n\n&lt;p&gt;Without knowing why it takes 15 minutes to start for each user, I don&amp;#39;t think I can provide good advice. What is happening during those 15 minutes.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qnugt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751854590,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n1skfy4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ajmusic15",
                      "can_mod_post": false,
                      "created_utc": 1751889447,
                      "send_replies": true,
                      "parent_id": "t1_n1qputk",
                      "score": 2,
                      "author_fullname": "t2_62puzq0k",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I don't even want to hear about RunPod. It's slow with storage, and they charge you as if they were selling you dedicated NVMe storage with reserved IOPS. With \"Secure Cloud,\" I've had to wait up to 30 minutes to decompress a measly 12 GB.\n\nEverything else works fine except for storage. It reminded me of when I used to use HDD. With DataCrunch is the same.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1skfy4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "Ollama"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t even want to hear about RunPod. It&amp;#39;s slow with storage, and they charge you as if they were selling you dedicated NVMe storage with reserved IOPS. With &amp;quot;Secure Cloud,&amp;quot; I&amp;#39;ve had to wait up to 30 minutes to decompress a measly 12 GB.&lt;/p&gt;\n\n&lt;p&gt;Everything else works fine except for storage. It reminded me of when I used to use HDD. With DataCrunch is the same.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1skfy4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751889447,
                      "author_flair_text": "Ollama",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n1radar",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Altruistic_Heat_9531",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1r1sv7",
                                "score": 2,
                                "author_fullname": "t2_9sh0mgya",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "oh my god i forgot. in my another comment i forgot that my runpod serverless is running 70B model so of course it takes 1 minutes after a cold start. for SD serverless only 3 second. And 11 second for total generation\n\nhttps://preview.redd.it/66mic32pvdbf1.png?width=1551&amp;format=png&amp;auto=webp&amp;s=69a5e26bed9645d4d7162d5eeaca2809d4014188",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1radar",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;oh my god i forgot. in my another comment i forgot that my runpod serverless is running 70B model so of course it takes 1 minutes after a cold start. for SD serverless only 3 second. And 11 second for total generation&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/66mic32pvdbf1.png?width=1551&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a5e26bed9645d4d7162d5eeaca2809d4014188\"&gt;https://preview.redd.it/66mic32pvdbf1.png?width=1551&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a5e26bed9645d4d7162d5eeaca2809d4014188&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltidhz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1radar/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751864244,
                                "media_metadata": {
                                  "66mic32pvdbf1": {
                                    "status": "valid",
                                    "e": "Image",
                                    "m": "image/png",
                                    "p": [
                                      {
                                        "y": 36,
                                        "x": 108,
                                        "u": "https://preview.redd.it/66mic32pvdbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5050358bb207612fa3630928cee40b885ea70aed"
                                      },
                                      {
                                        "y": 72,
                                        "x": 216,
                                        "u": "https://preview.redd.it/66mic32pvdbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c302a9d3888c14641b425ae4c357d0276bb1813"
                                      },
                                      {
                                        "y": 106,
                                        "x": 320,
                                        "u": "https://preview.redd.it/66mic32pvdbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4866070e601fad77744351fe7a4696eaa58beb7a"
                                      },
                                      {
                                        "y": 213,
                                        "x": 640,
                                        "u": "https://preview.redd.it/66mic32pvdbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=229c7a4a3a337639076a3a61b322d21c1c6f1a17"
                                      },
                                      {
                                        "y": 320,
                                        "x": 960,
                                        "u": "https://preview.redd.it/66mic32pvdbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=075b4df8fb5014903e858aa4e8252023d7e3aa58"
                                      },
                                      {
                                        "y": 360,
                                        "x": 1080,
                                        "u": "https://preview.redd.it/66mic32pvdbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba15c1395d64a84360795c0379510e6edade53ce"
                                      }
                                    ],
                                    "s": {
                                      "y": 518,
                                      "x": 1551,
                                      "u": "https://preview.redd.it/66mic32pvdbf1.png?width=1551&amp;format=png&amp;auto=webp&amp;s=69a5e26bed9645d4d7162d5eeaca2809d4014188"
                                    },
                                    "id": "66mic32pvdbf1"
                                  }
                                },
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751864244,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n1t2y04",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "PermanentLiminality",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1r1sv7",
                                "score": 1,
                                "author_fullname": "t2_19zqycaf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You pay for storage as long as you are using it.  It isn't that expensive, but it isn't zero and depends on just how much you need.  Then you pay for the time your instance is active. \n\nYou set the idle time before it shuts down again.  Usually a few minutes.  It only makes sense if your service does not have constant usage.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1t2y04",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You pay for storage as long as you are using it.  It isn&amp;#39;t that expensive, but it isn&amp;#39;t zero and depends on just how much you need.  Then you pay for the time your instance is active. &lt;/p&gt;\n\n&lt;p&gt;You set the idle time before it shuts down again.  Usually a few minutes.  It only makes sense if your service does not have constant usage.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltidhz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1t2y04/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751896333,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751896333,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1r1sv7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "visionkhawar512",
                      "can_mod_post": false,
                      "created_utc": 1751860265,
                      "send_replies": true,
                      "parent_id": "t1_n1qputk",
                      "score": 1,
                      "author_fullname": "t2_bm7appuf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "We only have to pay once we hit the service?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1r1sv7",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We only have to pay once we hit the service?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1r1sv7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751860265,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1qputk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "PermanentLiminality",
            "can_mod_post": false,
            "created_utc": 1751855360,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 1,
            "author_fullname": "t2_19zqycaf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Take a look at the Runpod \"serverless.\". You will have to pay for storage, but it has very fast startup.  More like a second instead of 15 minutes.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1qputk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Take a look at the Runpod &amp;quot;serverless.&amp;quot;. You will have to pay for storage, but it has very fast startup.  More like a second instead of 15 minutes.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qputk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751855360,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n1r4558",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "visionkhawar512",
                      "can_mod_post": false,
                      "created_utc": 1751861311,
                      "send_replies": true,
                      "parent_id": "t1_n1r3led",
                      "score": 1,
                      "author_fullname": "t2_bm7appuf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I got your idea, thanks",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1r4558",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I got your idea, thanks&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1r4558/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751861311,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n1sxd9k",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "WrongAtom_43",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1rl96s",
                                "score": 1,
                                "author_fullname": "t2_1lhj88pz7v",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It was for something internal to my client's company, basically, he wanted to migrate to a completely local solution, so we did that with some requests to test things. While the client prepared a place for the server and got the rest of the graphics cards.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1sxd9k",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It was for something internal to my client&amp;#39;s company, basically, he wanted to migrate to a completely local solution, so we did that with some requests to test things. While the client prepared a place for the server and got the rest of the graphics cards.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltidhz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1sxd9k/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751894418,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751894418,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1rl96s",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "wahnsinnwanscene",
                      "can_mod_post": false,
                      "created_utc": 1751870030,
                      "send_replies": true,
                      "parent_id": "t1_n1r3led",
                      "score": 1,
                      "author_fullname": "t2_43v98jplp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Maybe a strange question. Which industry and use case is this? And couldn't you preseed requests to the cloud service to get past the cold start?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1rl96s",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe a strange question. Which industry and use case is this? And couldn&amp;#39;t you preseed requests to the cloud service to get past the cold start?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1rl96s/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751870030,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n1t14cq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "WrongAtom_43",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1sk2vq",
                                "score": 2,
                                "author_fullname": "t2_1lhj88pz7v",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It was temporary while the client obtained the rest of the components. And yes, the server was put into sleep state and \"turned on\" using WoL. I was just simplifying a bit.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1t14cq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It was temporary while the client obtained the rest of the components. And yes, the server was put into sleep state and &amp;quot;turned on&amp;quot; using WoL. I was just simplifying a bit.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltidhz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1t14cq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751895729,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751895729,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1sk2vq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ajmusic15",
                      "can_mod_post": false,
                      "created_utc": 1751889292,
                      "send_replies": true,
                      "parent_id": "t1_n1r3led",
                      "score": 1,
                      "author_fullname": "t2_62puzq0k",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It may seem strange to you, but... What do you mean, \"server shut down\"? If electricity is expensive, it would be much faster to put the equipment into sleep mode.\n\nOf course, I'm just guessing at your real reasons for doing so.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1sk2vq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "Ollama"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It may seem strange to you, but... What do you mean, &amp;quot;server shut down&amp;quot;? If electricity is expensive, it would be much faster to put the equipment into sleep mode.&lt;/p&gt;\n\n&lt;p&gt;Of course, I&amp;#39;m just guessing at your real reasons for doing so.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1sk2vq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751889292,
                      "author_flair_text": "Ollama",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1r3led",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "WrongAtom_43",
            "can_mod_post": false,
            "created_utc": 1751861060,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 1,
            "author_fullname": "t2_1lhj88pz7v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'll tell you what we did with a text generation model, in my case.\n\nWe have a local server with a 4090 running the model, from turning it on until it is ready to receive requests takes approximately 60 seconds. It didn't stay on all the time, only when a request came in. But, 60 seconds for a response was too long. Customers began to complain.\n\nOur solution was to create an intermediate script that receives the requests and sends them either to the local API or to the cloud. So, by default the first request goes to the cloud server, so the response arrives in around 4 or 6 seconds. If there are any other requests before the local server is ready, it is sent to the cloud as well. Once the local server is up and running, requests are sent to it for the next hour. If there are no new requests during that time, it shuts down and the cycle repeats.\n\nI think you could do something similar in your case, leaving the first requests to the local server while you turn on the slow but higher capacity server for the other requests.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1r3led",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll tell you what we did with a text generation model, in my case.&lt;/p&gt;\n\n&lt;p&gt;We have a local server with a 4090 running the model, from turning it on until it is ready to receive requests takes approximately 60 seconds. It didn&amp;#39;t stay on all the time, only when a request came in. But, 60 seconds for a response was too long. Customers began to complain.&lt;/p&gt;\n\n&lt;p&gt;Our solution was to create an intermediate script that receives the requests and sends them either to the local API or to the cloud. So, by default the first request goes to the cloud server, so the response arrives in around 4 or 6 seconds. If there are any other requests before the local server is ready, it is sent to the cloud as well. Once the local server is up and running, requests are sent to it for the next hour. If there are no new requests during that time, it shuts down and the cycle repeats.&lt;/p&gt;\n\n&lt;p&gt;I think you could do something similar in your case, leaving the first requests to the local server while you turn on the slow but higher capacity server for the other requests.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1r3led/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751861060,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n1s5eb9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "k_means_clusterfuck",
            "can_mod_post": false,
            "created_utc": 1751881962,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 1,
            "author_fullname": "t2_4bby1cv5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "cloud 100%. Only go local if privacy is must or you can prove that it is cheaper",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1s5eb9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;cloud 100%. Only go local if privacy is must or you can prove that it is cheaper&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1s5eb9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751881962,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n1smxpu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ajmusic15",
            "can_mod_post": false,
            "created_utc": 1751890495,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 1,
            "author_fullname": "t2_62puzq0k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "15 minutes for a cold start... It's exactly the same experience I have with RunPod and DataCrunch when starting an instance with large models. It's as if the network architecture of your storage system were working on RJ45 dozens of meters away from the server you rent.\n\nBasically everything is running at HDD speed, unfortunately.\n\nFor that reason, I use Vultr (low variety of GPUs, but everything works very quickly there). I create the entire system in a block volume, and the rest is just a matter of indicating which hardware to use and starting it up. In less than 90 seconds, you are already able to receive requests.\n\nSimilarly, I suggest having your own infrastructure to cushion the cool start load while the serverless starts up. In fact, it's the same scenario: the GPU will use almost no electricity while it's not processing requests. For example... I have my 5080 with the models loaded in VRAM 24/7, and the consumption of the entire equipment without receiving any load is between 47-80W, which is what my phone charger consumes and a cost that a company can easily afford.\n\nIf you need a MoDT for very low power consumption while the GPU is responsible for the entire load, I recommend the Minisforum BD795M (or if you want PCIe 5.0, the BD795i SE). It's the MoDT I use in my small cluster.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1smxpu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Ollama"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;15 minutes for a cold start... It&amp;#39;s exactly the same experience I have with RunPod and DataCrunch when starting an instance with large models. It&amp;#39;s as if the network architecture of your storage system were working on RJ45 dozens of meters away from the server you rent.&lt;/p&gt;\n\n&lt;p&gt;Basically everything is running at HDD speed, unfortunately.&lt;/p&gt;\n\n&lt;p&gt;For that reason, I use Vultr (low variety of GPUs, but everything works very quickly there). I create the entire system in a block volume, and the rest is just a matter of indicating which hardware to use and starting it up. In less than 90 seconds, you are already able to receive requests.&lt;/p&gt;\n\n&lt;p&gt;Similarly, I suggest having your own infrastructure to cushion the cool start load while the serverless starts up. In fact, it&amp;#39;s the same scenario: the GPU will use almost no electricity while it&amp;#39;s not processing requests. For example... I have my 5080 with the models loaded in VRAM 24/7, and the consumption of the entire equipment without receiving any load is between 47-80W, which is what my phone charger consumes and a cost that a company can easily afford.&lt;/p&gt;\n\n&lt;p&gt;If you need a MoDT for very low power consumption while the GPU is responsible for the entire load, I recommend the Minisforum BD795M (or if you want PCIe 5.0, the BD795i SE). It&amp;#39;s the MoDT I use in my small cluster.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1smxpu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751890495,
            "author_flair_text": "Ollama",
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n1sv1wy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GoldCompetition7722",
            "can_mod_post": false,
            "created_utc": 1751893604,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 1,
            "author_fullname": "t2_banbmed5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A100 80GB consumes about 60W in while idle and 32b 70b models running up in 3-5 seconds.. time to first token hugely depends on size of context window - this is what I'm having with local ollama server. Basic comfyui template is generating 512x512 image in about 0.5-0.8 seconds.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1sv1wy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A100 80GB consumes about 60W in while idle and 32b 70b models running up in 3-5 seconds.. time to first token hugely depends on size of context window - this is what I&amp;#39;m having with local ollama server. Basic comfyui template is generating 512x512 image in about 0.5-0.8 seconds.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1sv1wy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751893604,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n1ue0ea",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "powasky",
            "can_mod_post": false,
            "created_utc": 1751910153,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 1,
            "author_fullname": "t2_erzgo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The cold start problem is exactly why we built Runpod the way we did. Replicate's 15 minute cold starts are brutal for user experience and you're right that it gets worse as you scale.\n\nLocal hardware like the 4090 your boss suggested would solve cold starts but you're gonna hit the queuing issue pretty quick, plus maintaining hardware is a pain. Also if something breaks you're down until you fix it.\n\nFor Runpod you can keep instances warm to avoid cold starts entirely - just pay a bit more to keep the workers running vs spinning up from scratch each time. Way cheaper than buying hardware outright and you can scale up/down based on demand. We see a lot of image generation workloads doing exactly this.\n\nYou could also set up multiple smaller instances instead of one big one to handle concurrent requests better. Like 3-4 RTX 4090 pods instead of one massive instance, that way multiple users aren't waiting in the same queue.\n\nWhat kind of model are you running? Some are more optimized for quick inference than others, might be worth looking at model optimization alongside the infrastructure piece.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1ue0ea",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The cold start problem is exactly why we built Runpod the way we did. Replicate&amp;#39;s 15 minute cold starts are brutal for user experience and you&amp;#39;re right that it gets worse as you scale.&lt;/p&gt;\n\n&lt;p&gt;Local hardware like the 4090 your boss suggested would solve cold starts but you&amp;#39;re gonna hit the queuing issue pretty quick, plus maintaining hardware is a pain. Also if something breaks you&amp;#39;re down until you fix it.&lt;/p&gt;\n\n&lt;p&gt;For Runpod you can keep instances warm to avoid cold starts entirely - just pay a bit more to keep the workers running vs spinning up from scratch each time. Way cheaper than buying hardware outright and you can scale up/down based on demand. We see a lot of image generation workloads doing exactly this.&lt;/p&gt;\n\n&lt;p&gt;You could also set up multiple smaller instances instead of one big one to handle concurrent requests better. Like 3-4 RTX 4090 pods instead of one massive instance, that way multiple users aren&amp;#39;t waiting in the same queue.&lt;/p&gt;\n\n&lt;p&gt;What kind of model are you running? Some are more optimized for quick inference than others, might be worth looking at model optimization alongside the infrastructure piece.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1ue0ea/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751910153,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n1r950k",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Altruistic_Heat_9531",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1qz2z5",
                                "score": 1,
                                "author_fullname": "t2_9sh0mgya",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "oh nice SDXL, 4090 can batch inference easily. I already **synchronous batch** inferenced 8 image about 30 sec using SD Forge. My vram can handle more request but forge only limited to 8 batch.\n\nThe only problem is well..... asynchronus batch\n\n[https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/10205](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/10205)\n\nYou know what. just use RunPod.... my bad... [https://www.runpod.io/?ref=yruu07gh](https://www.runpod.io/?ref=yruu07gh) (you might get free credit for testing, and so do i)\n\nhttps://preview.redd.it/letyr761wdbf1.png?width=1551&amp;format=png&amp;auto=webp&amp;s=7ddaea3a772bf31e286963291884e3c325dfcef4",
                                "edited": 1751864367,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1r950k",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;oh nice SDXL, 4090 can batch inference easily. I already &lt;strong&gt;synchronous batch&lt;/strong&gt; inferenced 8 image about 30 sec using SD Forge. My vram can handle more request but forge only limited to 8 batch.&lt;/p&gt;\n\n&lt;p&gt;The only problem is well..... asynchronus batch&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/10205\"&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/10205&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You know what. just use RunPod.... my bad... &lt;a href=\"https://www.runpod.io/?ref=yruu07gh\"&gt;https://www.runpod.io/?ref=yruu07gh&lt;/a&gt; (you might get free credit for testing, and so do i)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/letyr761wdbf1.png?width=1551&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ddaea3a772bf31e286963291884e3c325dfcef4\"&gt;https://preview.redd.it/letyr761wdbf1.png?width=1551&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ddaea3a772bf31e286963291884e3c325dfcef4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltidhz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1r950k/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751863650,
                                "media_metadata": {
                                  "letyr761wdbf1": {
                                    "status": "valid",
                                    "e": "Image",
                                    "m": "image/png",
                                    "p": [
                                      {
                                        "y": 36,
                                        "x": 108,
                                        "u": "https://preview.redd.it/letyr761wdbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f60f26c2d1d6bb0f32446a0c7d7103726d8c1131"
                                      },
                                      {
                                        "y": 72,
                                        "x": 216,
                                        "u": "https://preview.redd.it/letyr761wdbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf315287abdd80f129c3f3e8dab184a7383607cb"
                                      },
                                      {
                                        "y": 106,
                                        "x": 320,
                                        "u": "https://preview.redd.it/letyr761wdbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a2ba04a0c6b1c84cf8deac46da17b8c3f4deb8f"
                                      },
                                      {
                                        "y": 213,
                                        "x": 640,
                                        "u": "https://preview.redd.it/letyr761wdbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=23dedf594b561e20643321bdc8b1f80f047ff702"
                                      },
                                      {
                                        "y": 320,
                                        "x": 960,
                                        "u": "https://preview.redd.it/letyr761wdbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b3310753f8fb3501b3a08dd31109026b10352d7"
                                      },
                                      {
                                        "y": 360,
                                        "x": 1080,
                                        "u": "https://preview.redd.it/letyr761wdbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0914fb9fe8b79d8c626d158b3cdc8eb9fafb6f12"
                                      }
                                    ],
                                    "s": {
                                      "y": 518,
                                      "x": 1551,
                                      "u": "https://preview.redd.it/letyr761wdbf1.png?width=1551&amp;format=png&amp;auto=webp&amp;s=7ddaea3a772bf31e286963291884e3c325dfcef4"
                                    },
                                    "id": "letyr761wdbf1"
                                  }
                                },
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751863650,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1qz2z5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "visionkhawar512",
                      "can_mod_post": false,
                      "created_utc": 1751859087,
                      "send_replies": true,
                      "parent_id": "t1_n1qpkvq",
                      "score": 1,
                      "author_fullname": "t2_bm7appuf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I am running SDXL model along with more small models, also the problem is that they have 1000 users, and maybe these users will try our model. When there 10 users at same time, the 4090 takes more time to generate it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1qz2z5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am running SDXL model along with more small models, also the problem is that they have 1000 users, and maybe these users will try our model. When there 10 users at same time, the 4090 takes more time to generate it.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltidhz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qz2z5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751859087,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1qpkvq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Altruistic_Heat_9531",
            "can_mod_post": false,
            "created_utc": 1751855253,
            "send_replies": true,
            "parent_id": "t3_1ltidhz",
            "score": 0,
            "author_fullname": "t2_9sh0mgya",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What is the image generation model? Is it Flux or SDXL? At the end of the day, I suggest hosting it locally. Cold start time for any GPU model is long, and it is simply unavoidable. Try another serving platform, for example, runpod or Massed. I have a referral link for runpod if you want to try it  [https://runpod.io?ref=yruu07gh](https://runpod.io?ref=yruu07gh)  (You get free tokens for renting an L40.)\n\nAnyway, if you are using Flux or any heavy DiT model like HiDream, you will need at most a dual 4090 setup with a ComfyUI API backend. You can run different ports for different GPU instances. To manage the traffic, you can use Nginx or any reverse proxy. If you are using Flux, you can also try a speed boost LoRA to reduce image generation time and open up capacity for more clients. For example, you can look at this model: https://huggingface.co/alimama-creative/FLUX.1-Turbo-Alpha.\n\nAnd additional info. also what is your client frequency, if you so happened already launch simmilar product? so you can avoid cold start alltogether\n\nI am on 3090, and generating flux image with bunch of optimization (Sage, Turbo Flux), i need 16 second per image on avg ( 2s / it of 8 iteration). And 4090 is twice of 3090 in performance",
            "edited": 1751855485,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1qpkvq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What is the image generation model? Is it Flux or SDXL? At the end of the day, I suggest hosting it locally. Cold start time for any GPU model is long, and it is simply unavoidable. Try another serving platform, for example, runpod or Massed. I have a referral link for runpod if you want to try it  &lt;a href=\"https://runpod.io?ref=yruu07gh\"&gt;https://runpod.io?ref=yruu07gh&lt;/a&gt;  (You get free tokens for renting an L40.)&lt;/p&gt;\n\n&lt;p&gt;Anyway, if you are using Flux or any heavy DiT model like HiDream, you will need at most a dual 4090 setup with a ComfyUI API backend. You can run different ports for different GPU instances. To manage the traffic, you can use Nginx or any reverse proxy. If you are using Flux, you can also try a speed boost LoRA to reduce image generation time and open up capacity for more clients. For example, you can look at this model: &lt;a href=\"https://huggingface.co/alimama-creative/FLUX.1-Turbo-Alpha\"&gt;https://huggingface.co/alimama-creative/FLUX.1-Turbo-Alpha&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;And additional info. also what is your client frequency, if you so happened already launch simmilar product? so you can avoid cold start alltogether&lt;/p&gt;\n\n&lt;p&gt;I am on 3090, and generating flux image with bunch of optimization (Sage, Turbo Flux), i need 16 second per image on avg ( 2s / it of 8 iteration). And 4090 is twice of 3090 in performance&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltidhz/buy_a_local_pc_with_gpu_or_go_for_cloud/n1qpkvq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751855253,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltidhz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]