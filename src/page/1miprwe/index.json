[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "**Full specs**:\n\n**GPU**: RTX 4070 TI Super (16 GB VRAM)\n\n**CPU**: i7 14700K\n\n**System RAM**: 96 GB DDR5 @ 6200 MT/s (total usage, including all Windows processes, is 61 GB, so only having 64GB RAM is probably sufficient)\n\n**OS**: Windows 11\n\n**Model runner**: LM Studio (see settings in third screenshot)\n\n  \nWhen I saw that OpenAI released a 120b parameter model, my assumption was that running it wouldn't be realistic for people with consumer-grade hardware. After some experimentation, I was *partly* proven wrong- 13 t/s is a speed that I'd consider \"usable\" on days where I'm feeling relatively patient. I'd imagine that people running RTX 5090's and/or faster system RAM are getting speeds that are truly usable for a lot of people, a lot of the time. If anyone has this setup, I'd love to hear what kind of speeds you're getting. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "is_gallery": true,
            "title": "gpt-oss-120b performance with only 16 GB VRAM- surprisingly decent",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "New Model"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 114,
            "top_awarded_type": null,
            "name": "t3_1miprwe",
            "media_metadata": {
              "i0atmotcgahf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 88,
                    "x": 108,
                    "u": "https://preview.redd.it/i0atmotcgahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ae2fdd72226ff436307c407112f2825a6a2885d"
                  },
                  {
                    "y": 176,
                    "x": 216,
                    "u": "https://preview.redd.it/i0atmotcgahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b006d9e18fb382960d218f8475eda8d6af7a4e5"
                  },
                  {
                    "y": 262,
                    "x": 320,
                    "u": "https://preview.redd.it/i0atmotcgahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2334591ddfde6c24b6127e8e024c358eb7c96f23"
                  }
                ],
                "s": {
                  "y": 344,
                  "x": 420,
                  "u": "https://preview.redd.it/i0atmotcgahf1.png?width=420&amp;format=png&amp;auto=webp&amp;s=82d5ca666a8ad3f6bc713112ec84cd299bb4abc7"
                },
                "id": "i0atmotcgahf1"
              },
              "i1yjttaggahf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 147,
                    "x": 108,
                    "u": "https://preview.redd.it/i1yjttaggahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=139ffdbef2afdfb2f27c515606e13b62126698ba"
                  },
                  {
                    "y": 295,
                    "x": 216,
                    "u": "https://preview.redd.it/i1yjttaggahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=89d9ce76d1ff7eebf94964ec2de7f9b6d5ce8060"
                  },
                  {
                    "y": 437,
                    "x": 320,
                    "u": "https://preview.redd.it/i1yjttaggahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6c628f2b1dab58f75b68c746437470e5a64e628"
                  }
                ],
                "s": {
                  "y": 592,
                  "x": 433,
                  "u": "https://preview.redd.it/i1yjttaggahf1.png?width=433&amp;format=png&amp;auto=webp&amp;s=bf4c25d2d3d63ecc3d632e14af48ae2fdd260edd"
                },
                "id": "i1yjttaggahf1"
              },
              "uin3n9rjgahf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 60,
                    "x": 108,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f6a117776c5a38995291509599b083970cbab99"
                  },
                  {
                    "y": 120,
                    "x": 216,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ac27f81fd04f331e34425b0686ef826313fd641"
                  },
                  {
                    "y": 178,
                    "x": 320,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a9f2eeac062cba3d74ea30788fd59f2b770f304"
                  },
                  {
                    "y": 356,
                    "x": 640,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b812cd15ad1a75cfab386d5c7ee8b7720cb298d7"
                  },
                  {
                    "y": 534,
                    "x": 960,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7b252d0d7d01a708c51fb555d6da0ba7a275e22"
                  },
                  {
                    "y": 601,
                    "x": 1080,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=894f3af2e68991bca0d2b8352c7e80d3e3d0871a"
                  }
                ],
                "s": {
                  "y": 684,
                  "x": 1229,
                  "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=1229&amp;format=png&amp;auto=webp&amp;s=b28d0dca527df3e3dac4939295d592c858f6aec2"
                },
                "id": "uin3n9rjgahf1"
              }
            },
            "hide_score": true,
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.74,
            "author_flair_background_color": null,
            "ups": 11,
            "domain": "reddit.com",
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1t2n2s9f",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "gallery_data": {
              "items": [
                {
                  "media_id": "i0atmotcgahf1",
                  "id": 722144194
                },
                {
                  "media_id": "i1yjttaggahf1",
                  "id": 722144195
                },
                {
                  "media_id": "uin3n9rjgahf1",
                  "id": 722144196
                }
              ]
            },
            "link_flair_text": "New Model",
            "can_mod_post": false,
            "score": 11,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/3XwIvatwftLg0e--y7jCH8mLR2VOGeduF77wVTFjpug.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754438808,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Full specs&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: RTX 4070 TI Super (16 GB VRAM)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;: i7 14700K&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;System RAM&lt;/strong&gt;: 96 GB DDR5 @ 6200 MT/s (total usage, including all Windows processes, is 61 GB, so only having 64GB RAM is probably sufficient)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Windows 11&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model runner&lt;/strong&gt;: LM Studio (see settings in third screenshot)&lt;/p&gt;\n\n&lt;p&gt;When I saw that OpenAI released a 120b parameter model, my assumption was that running it wouldn&amp;#39;t be realistic for people with consumer-grade hardware. After some experimentation, I was &lt;em&gt;partly&lt;/em&gt; proven wrong- 13 t/s is a speed that I&amp;#39;d consider &amp;quot;usable&amp;quot; on days where I&amp;#39;m feeling relatively patient. I&amp;#39;d imagine that people running RTX 5090&amp;#39;s and/or faster system RAM are getting speeds that are truly usable for a lot of people, a lot of the time. If anyone has this setup, I&amp;#39;d love to hear what kind of speeds you&amp;#39;re getting. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://www.reddit.com/gallery/1miprwe",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ffb000",
            "id": "1miprwe",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "gigaflops_",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/",
            "stickied": false,
            "url": "https://www.reddit.com/gallery/1miprwe",
            "subreddit_subscribers": 511363,
            "created_utc": 1754438808,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n75apvr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "random-tomato",
            "can_mod_post": false,
            "created_utc": 1754440589,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 3,
            "author_fullname": "t2_fmd6oq5v6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just posting my numbers too! 5090 + 60GB of DDR5, 22 cpu moe layers offloaded:\n\n    srv  params_from_: Chat format: GPT-OSS\n    slot launch_slot_: id  0 | task 5966 | processing task\n    slot update_slots: id  0 | task 5966 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 112\n    slot update_slots: id  0 | task 5966 | kv cache rm [104, end)\n    slot update_slots: id  0 | task 5966 | prompt processing progress, n_past = 112, n_tokens = 8, progress = 0.071429\n    slot update_slots: id  0 | task 5966 | prompt done, n_past = 112, n_tokens = 8\n    slot      release: id  0 | task 5966 | stop processing: n_past = 970, truncated = 0\n    slot print_timing: id  0 | task 5966 | \n    prompt eval time =     167.26 ms /     8 tokens (   20.91 ms per token,    47.83 tokens per second)\n           eval time =   23464.92 ms /   859 tokens (   27.32 ms per token,    36.61 tokens per second)\n          total time =   23632.18 ms /   867 tokens",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75apvr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just posting my numbers too! 5090 + 60GB of DDR5, 22 cpu moe layers offloaded:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;srv  params_from_: Chat format: GPT-OSS\nslot launch_slot_: id  0 | task 5966 | processing task\nslot update_slots: id  0 | task 5966 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 112\nslot update_slots: id  0 | task 5966 | kv cache rm [104, end)\nslot update_slots: id  0 | task 5966 | prompt processing progress, n_past = 112, n_tokens = 8, progress = 0.071429\nslot update_slots: id  0 | task 5966 | prompt done, n_past = 112, n_tokens = 8\nslot      release: id  0 | task 5966 | stop processing: n_past = 970, truncated = 0\nslot print_timing: id  0 | task 5966 | \nprompt eval time =     167.26 ms /     8 tokens (   20.91 ms per token,    47.83 tokens per second)\n       eval time =   23464.92 ms /   859 tokens (   27.32 ms per token,    36.61 tokens per second)\n      total time =   23632.18 ms /   867 tokens\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75apvr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754440589,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n757i8t",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Admirable-Star7088",
            "can_mod_post": false,
            "created_utc": 1754439481,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 2,
            "author_fullname": "t2_qhlcbiy3k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yup, pretty fast model for its size, and despite that I do not use a fully functional quant yet it has performed overall really well for me, especially in creative writing where it's quite impressive.\n\nWill download a more stable and bug-free quant tomorrow and test this model some more.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n757i8t",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup, pretty fast model for its size, and despite that I do not use a fully functional quant yet it has performed overall really well for me, especially in creative writing where it&amp;#39;s quite impressive.&lt;/p&gt;\n\n&lt;p&gt;Will download a more stable and bug-free quant tomorrow and test this model some more.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n757i8t/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754439481,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n75h3zc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gigaflops_",
                      "can_mod_post": false,
                      "created_utc": 1754442777,
                      "send_replies": true,
                      "parent_id": "t1_n75berx",
                      "score": 1,
                      "author_fullname": "t2_1t2n2s9f",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "MXFP4\n\nThis one: [https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF](https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF) \n\nhttps://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;format=png&amp;auto=webp&amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c\n\n  \nWhat CPU and system RAM speed do you have? Since a substantial amount of the model is still run on the CPU, I wonder if that could be your bottleneck?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n75h3zc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MXFP4&lt;/p&gt;\n\n&lt;p&gt;This one: &lt;a href=\"https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF\"&gt;https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c\"&gt;https://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What CPU and system RAM speed do you have? Since a substantial amount of the model is still run on the CPU, I wonder if that could be your bottleneck?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miprwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75h3zc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754442777,
                      "media_metadata": {
                        "rp0hn4aruahf1": {
                          "status": "valid",
                          "e": "Image",
                          "m": "image/png",
                          "p": [
                            {
                              "y": 83,
                              "x": 108,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a01bcee1c17aae70164b8aed2b7c73ff98ff6031"
                            },
                            {
                              "y": 166,
                              "x": 216,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec7833521fc8619c7cb1ff04a4d057159b7f97c8"
                            },
                            {
                              "y": 246,
                              "x": 320,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38e13e3b3100510917d7c261c63af7fc5ea62e41"
                            },
                            {
                              "y": 493,
                              "x": 640,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44f6495b75b1034f57ae02d6bf402a62aaf1cb69"
                            },
                            {
                              "y": 739,
                              "x": 960,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2842f10d7c8645f13e701176463afb1d98e29e15"
                            }
                          ],
                          "s": {
                            "y": 785,
                            "x": 1019,
                            "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;format=png&amp;auto=webp&amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c"
                          },
                          "id": "rp0hn4aruahf1"
                        }
                      },
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n75berx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pro-editor-1105",
            "can_mod_post": false,
            "created_utc": 1754440827,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 1,
            "author_fullname": "t2_uptissiz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What quant are you using? I am using llama.cpp with unsloth and getting 8tps on a 4090 with 16k context. 64GB of ram.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75berx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What quant are you using? I am using llama.cpp with unsloth and getting 8tps on a 4090 with 16k context. 64GB of ram.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75berx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754440827,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n757kzp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "IxinDow",
            "can_mod_post": false,
            "created_utc": 1754439507,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 1,
            "author_fullname": "t2_lt5ci87n",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "seems pretty safe",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n757kzp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;seems pretty safe&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n757kzp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754439507,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]