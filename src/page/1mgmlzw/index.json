[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I know a lot of people are fine-tuning their models using their codebase but I couldn't find that many resources on how to build this dataset.\n\nSure you could dump your codebase and that's it but there must be a better way to teach the model how to interact with this codebase right?\n\nWhat did you try? And did it work?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How would you generate a dataset to fine-tune a llm to your codebase?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgmlzw",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_5219n8dd",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754236148,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of people are fine-tuning their models using their codebase but I couldn&amp;#39;t find that many resources on how to build this dataset.&lt;/p&gt;\n\n&lt;p&gt;Sure you could dump your codebase and that&amp;#39;s it but there must be a better way to teach the model how to interact with this codebase right?&lt;/p&gt;\n\n&lt;p&gt;What did you try? And did it work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgmlzw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ThisIsBartRick",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/",
            "subreddit_subscribers": 509625,
            "created_utc": 1754236148,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6px00j",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Ok_Appearance3584",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6pq0xt",
                                "score": 1,
                                "author_fullname": "t2_oyxj85n1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah, that's plausible. My work codebase is too old for that so I just started from the common functions and worked my way up. You can build a dependency tree and have LLM create the synthetic data on its own. You need to play around with it to get good results though. But the basic idea is simple enough.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6px00j",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, that&amp;#39;s plausible. My work codebase is too old for that so I just started from the common functions and worked my way up. You can build a dependency tree and have LLM create the synthetic data on its own. You need to play around with it to get good results though. But the basic idea is simple enough.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgmlzw",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6px00j/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754239170,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754239170,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6pq0xt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ThisIsBartRick",
                      "can_mod_post": false,
                      "created_utc": 1754237024,
                      "send_replies": true,
                      "parent_id": "t1_n6ppm86",
                      "score": 1,
                      "author_fullname": "t2_5219n8dd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's kind of what I wanted to do with the git history but I wanted to see if anybody either had a better approach or had any success with this one",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6pq0xt",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s kind of what I wanted to do with the git history but I wanted to see if anybody either had a better approach or had any success with this one&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgmlzw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6pq0xt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754237024,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ppm86",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ok_Appearance3584",
            "can_mod_post": false,
            "created_utc": 1754236898,
            "send_replies": true,
            "parent_id": "t3_1mgmlzw",
            "score": 1,
            "author_fullname": "t2_oyxj85n1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can create a synthetic dataset that forms a conversation log as if the LLM created the codebase (assuming codebase is in the scale of less than 100k lines). \n\n\nThen finetune the LLM on it and keep adding to the training dataset every day and updating the weights every night or however often you like or can.\n\n\nLLMs are extremely good at retaining knowledge, including which functions already exist. But you need to create a synthetic dataset for it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ppm86",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can create a synthetic dataset that forms a conversation log as if the LLM created the codebase (assuming codebase is in the scale of less than 100k lines). &lt;/p&gt;\n\n&lt;p&gt;Then finetune the LLM on it and keep adding to the training dataset every day and updating the weights every night or however often you like or can.&lt;/p&gt;\n\n&lt;p&gt;LLMs are extremely good at retaining knowledge, including which functions already exist. But you need to create a synthetic dataset for it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6ppm86/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754236898,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmlzw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ru7dh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Working-Magician-823",
            "can_mod_post": false,
            "created_utc": 1754260674,
            "send_replies": true,
            "parent_id": "t3_1mgmlzw",
            "score": 1,
            "author_fullname": "t2_1ubcg2z4eb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A developer can easily modify any codebase that is structured like a tree with independent nodes, and documented at each level of what it does, what it is ok to do and what to avoid \n\nAnd if you store it in graph db and let the developer ask for detailed questions about every line in the file they are modifying, then they will always give you the right solution, well, most of the time, unless it is visual logic \n\nExample prototype done 100% with AI and still under development: https://eworker.ca",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ru7dh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A developer can easily modify any codebase that is structured like a tree with independent nodes, and documented at each level of what it does, what it is ok to do and what to avoid &lt;/p&gt;\n\n&lt;p&gt;And if you store it in graph db and let the developer ask for detailed questions about every line in the file they are modifying, then they will always give you the right solution, well, most of the time, unless it is visual logic &lt;/p&gt;\n\n&lt;p&gt;Example prototype done 100% with AI and still under development: &lt;a href=\"https://eworker.ca\"&gt;https://eworker.ca&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6ru7dh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754260674,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmlzw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ppinq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Coldaine",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6po49h",
                                "score": 1,
                                "author_fullname": "t2_pt9wv",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Good indexing solves both of those problems. For something like kilo (sorry, I don't know cline) add to the system prompt that [GUIDETOTHISCODEbase.md](http://GUIDETOTHISCODEbase.md) has a comprehensive index of where all the functions are, and have the prompt read your master archetecture document before doing anything.\n\nWhen you say good extensions like cline, do you do anything to them? or just take the base settings? \n\nAn example in my claude code setup, the first time claude reads any file, it calls my RAG agent who delivers them a short summary of what functions are in that file, and what similar files there are. Solves all the problems like this.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ppinq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Good indexing solves both of those problems. For something like kilo (sorry, I don&amp;#39;t know cline) add to the system prompt that &lt;a href=\"http://GUIDETOTHISCODEbase.md\"&gt;GUIDETOTHISCODEbase.md&lt;/a&gt; has a comprehensive index of where all the functions are, and have the prompt read your master archetecture document before doing anything.&lt;/p&gt;\n\n&lt;p&gt;When you say good extensions like cline, do you do anything to them? or just take the base settings? &lt;/p&gt;\n\n&lt;p&gt;An example in my claude code setup, the first time claude reads any file, it calls my RAG agent who delivers them a short summary of what functions are in that file, and what similar files there are. Solves all the problems like this.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgmlzw",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6ppinq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754236867,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754236867,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6po49h",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ThisIsBartRick",
                      "can_mod_post": false,
                      "created_utc": 1754236439,
                      "send_replies": true,
                      "parent_id": "t1_n6pnj69",
                      "score": 1,
                      "author_fullname": "t2_5219n8dd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I've tried that but sometimes I would need the model to \"know where to look\" and also to use the same functions if they already exist. Which is not something they can do even with good extensions like cline",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6po49h",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried that but sometimes I would need the model to &amp;quot;know where to look&amp;quot; and also to use the same functions if they already exist. Which is not something they can do even with good extensions like cline&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgmlzw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6po49h/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754236439,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6quc5g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Double_Cause4609",
                      "can_mod_post": false,
                      "created_utc": 1754249311,
                      "send_replies": true,
                      "parent_id": "t1_n6pnj69",
                      "score": 1,
                      "author_fullname": "t2_1kubzxt2ww",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ehhhh, it sort of depends on which way you want to take it.\n\nGood synthetic data generation pipelines (which require good indexing, to your point) can absolutely outperform in-context only performance.\n\nIn truth, though, it's ML. You can solve it however you want. Pretty much every discipline lets you solve every problem, it's just a question of how much time you need to put in your specific use case.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6quc5g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ehhhh, it sort of depends on which way you want to take it.&lt;/p&gt;\n\n&lt;p&gt;Good synthetic data generation pipelines (which require good indexing, to your point) can absolutely outperform in-context only performance.&lt;/p&gt;\n\n&lt;p&gt;In truth, though, it&amp;#39;s ML. You can solve it however you want. Pretty much every discipline lets you solve every problem, it&amp;#39;s just a question of how much time you need to put in your specific use case.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgmlzw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6quc5g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754249311,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6pnj69",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Coldaine",
            "can_mod_post": false,
            "created_utc": 1754236259,
            "send_replies": true,
            "parent_id": "t3_1mgmlzw",
            "score": 1,
            "author_fullname": "t2_pt9wv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You will get more out of smart context/indexing/structuring your codebase right, then you will from any sort of finetune.\n\n  \nLet me give you an example here, lets say your code was in rust. You'd probably benefit from a rust finetune of some coding models, but you won't be able to finetune a model on your specific codebase.",
            "edited": 1754236372,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pnj69",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You will get more out of smart context/indexing/structuring your codebase right, then you will from any sort of finetune.&lt;/p&gt;\n\n&lt;p&gt;Let me give you an example here, lets say your code was in rust. You&amp;#39;d probably benefit from a rust finetune of some coding models, but you won&amp;#39;t be able to finetune a model on your specific codebase.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmlzw/how_would_you_generate_a_dataset_to_finetune_a/n6pnj69/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754236259,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmlzw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]