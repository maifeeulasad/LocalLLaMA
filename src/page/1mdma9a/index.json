[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm setting up a local LLM to run in the background on my MacBook Pro (M3 Pro). The main use case is this: I use a dictation app (like SuperWhisper or Spokenly) to convert my voice to text, and then send that text to a local LLM server for processing. Think: summarizing, answering, rephrasing, correction, or responding intelligently to the text input.\n\nI want something:\n\n- Fast (low latency for near-real-time dictation use)\n\n- Reasonably accurate\n\n- Local (no cloud APIs)\n\n- Ideally OpenAI-compatible API so it's easier to integrate with other tools\n\nWith some flexibility for future use cases beyond just dictation\n\nSo far I'm looking at:\n\n- llama.cpp (via llama-server)\n\n- Ollama\n\nAnd what Llama model would you recommend? I was thinking of Gemma 3, but are there better ones? \n\nWould love to hear from others who've done similar setups. Which stack do you recommend and why?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Help choosing between Ollama, llama.cpp, or something else for background LLM server (used with dictation)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdma9a",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.55,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_e33mgcbq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753918996,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m setting up a local LLM to run in the background on my MacBook Pro (M3 Pro). The main use case is this: I use a dictation app (like SuperWhisper or Spokenly) to convert my voice to text, and then send that text to a local LLM server for processing. Think: summarizing, answering, rephrasing, correction, or responding intelligently to the text input.&lt;/p&gt;\n\n&lt;p&gt;I want something:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Fast (low latency for near-real-time dictation use)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Reasonably accurate&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Local (no cloud APIs)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ideally OpenAI-compatible API so it&amp;#39;s easier to integrate with other tools&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With some flexibility for future use cases beyond just dictation&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;m looking at:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;llama.cpp (via llama-server)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ollama&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And what Llama model would you recommend? I was thinking of Gemma 3, but are there better ones? &lt;/p&gt;\n\n&lt;p&gt;Would love to hear from others who&amp;#39;ve done similar setups. Which stack do you recommend and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdma9a",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "discoveringnature12",
            "discussion_type": null,
            "num_comments": 23,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/",
            "subreddit_subscribers": 507574,
            "created_utc": 1753918996,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64eijy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "splitheaddawg",
                      "can_mod_post": false,
                      "created_utc": 1753944509,
                      "send_replies": true,
                      "parent_id": "t1_n62si6d",
                      "score": 1,
                      "author_fullname": "t2_88xb21d",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I second both these options. I haven't tried koboldcpp lately but jan.ai is pretty nice to use.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64eijy",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I second both these options. I haven&amp;#39;t tried koboldcpp lately but jan.ai is pretty nice to use.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n64eijy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753944509,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62si6d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "The_frozen_one",
            "can_mod_post": false,
            "created_utc": 1753920604,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 3,
            "author_fullname": "t2_5rvpo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Have you looked at koboldcpp or jan.ai? Just throwing some different options out there. gemma 3 is a solid model for its size, and it handles images well. I'd stick with whisper for transcription, if keeping the text accurate is important especially with low latency.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62si6d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Have you looked at koboldcpp or jan.ai? Just throwing some different options out there. gemma 3 is a solid model for its size, and it handles images well. I&amp;#39;d stick with whisper for transcription, if keeping the text accurate is important especially with low latency.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62si6d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753920604,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n63q83i",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "discoveringnature12",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n63pw77",
                                          "score": 1,
                                          "author_fullname": "t2_e33mgcbq",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Great, thank you for the details. I'll try llama.cpp with slightly more patience to set it up.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n63q83i",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great, thank you for the details. I&amp;#39;ll try llama.cpp with slightly more patience to set it up.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdma9a",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63q83i/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753932830,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753932830,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n63pw77",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ArsNeph",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n63laq2",
                                "score": 2,
                                "author_fullname": "t2_vt0xkv60d",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Since you're on mac, there are only two inference engines you can use, llama.cpp and MLX. MLX is better optimized for Mac, but in all honesty, I don't know how to use it since I've never tried.\n\nChanging models would just require an instance of llama swap, but you would have to manually configure each model.\nThere's no way to have maximum performance without manually adjusting the parameters based on your device, Ollama just decides for you. \n\nYes, there are other llama.cpp wrappers like KoboldCPP and LM studio. LM Studio is very ready to use and works as an API server, but is closed source, so you can't guarantee anything if you're concerned about privacy. \n\nI'm running a RTX 3090, not a mac, so our experiences will be different, but I've seen up to a 30% difference in speeds between Ollama and vanilla llama.cpp. In your case, you would probably get better speeds running mlx though honestly",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n63pw77",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Since you&amp;#39;re on mac, there are only two inference engines you can use, llama.cpp and MLX. MLX is better optimized for Mac, but in all honesty, I don&amp;#39;t know how to use it since I&amp;#39;ve never tried.&lt;/p&gt;\n\n&lt;p&gt;Changing models would just require an instance of llama swap, but you would have to manually configure each model.\nThere&amp;#39;s no way to have maximum performance without manually adjusting the parameters based on your device, Ollama just decides for you. &lt;/p&gt;\n\n&lt;p&gt;Yes, there are other llama.cpp wrappers like KoboldCPP and LM studio. LM Studio is very ready to use and works as an API server, but is closed source, so you can&amp;#39;t guarantee anything if you&amp;#39;re concerned about privacy. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running a RTX 3090, not a mac, so our experiences will be different, but I&amp;#39;ve seen up to a 30% difference in speeds between Ollama and vanilla llama.cpp. In your case, you would probably get better speeds running mlx though honestly&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdma9a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63pw77/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753932695,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753932695,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n63laq2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "discoveringnature12",
                      "can_mod_post": false,
                      "created_utc": 1753930882,
                      "send_replies": true,
                      "parent_id": "t1_n63gsqe",
                      "score": 1,
                      "author_fullname": "t2_e33mgcbq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Ollama is a wrapper of llama.cpp\n\nVery interesting, I did not know this. \n\nmakes sense. I just started diving into llama.cpp and seems like there is a few configs to manage. And changing models seems like I have to run a bunch of commands, or have them setup properly, which I assume requires some time. \n\nA few questions. \n\n1. Is there a way to automate all this? Like is there a utility or an app which can automate all this? Like how it's so simple to do it in Olama\n\n2. Are there any other alternatives for this? \n\n3. How much of a performance difference have you noticed between ollama and llama.cpp?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63laq2",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Ollama is a wrapper of llama.cpp&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Very interesting, I did not know this. &lt;/p&gt;\n\n&lt;p&gt;makes sense. I just started diving into llama.cpp and seems like there is a few configs to manage. And changing models seems like I have to run a bunch of commands, or have them setup properly, which I assume requires some time. &lt;/p&gt;\n\n&lt;p&gt;A few questions. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Is there a way to automate all this? Like is there a utility or an app which can automate all this? Like how it&amp;#39;s so simple to do it in Olama&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any other alternatives for this? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How much of a performance difference have you noticed between ollama and llama.cpp?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63laq2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753930882,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n63gsqe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ArsNeph",
            "can_mod_post": false,
            "created_utc": 1753929225,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 4,
            "author_fullname": "t2_vt0xkv60d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ollama is a wrapper of llama.cpp, but significantly slower, and has terrible default settings which are hard to edit. It also updates much more slowly, meaningless support for the newest models, though vision models are better supported. If you're a beginner, Ollama is fine, but if you're not afraid of command line, llama-server for sure. The downside is Ollama model swapping is automatic, but for llama.cpp you would want llama-swap set up.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63gsqe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama is a wrapper of llama.cpp, but significantly slower, and has terrible default settings which are hard to edit. It also updates much more slowly, meaningless support for the newest models, though vision models are better supported. If you&amp;#39;re a beginner, Ollama is fine, but if you&amp;#39;re not afraid of command line, llama-server for sure. The downside is Ollama model swapping is automatic, but for llama.cpp you would want llama-swap set up.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63gsqe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753929225,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n63mlgh",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "triynizzles1",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n63arp6",
                                "score": 1,
                                "author_fullname": "t2_zr0g49ixt",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Doing experiment is how all of us learn. You can ask Claude or Gemini to write you a python script for the dictation and ask to include in the script a toggle between ollama’s api and llama cpp serve. Then install both and see for yourself what works best / is most intuitive. It would be less than an hour to get up, running, and test what works.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n63mlgh",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Doing experiment is how all of us learn. You can ask Claude or Gemini to write you a python script for the dictation and ask to include in the script a toggle between ollama’s api and llama cpp serve. Then install both and see for yourself what works best / is most intuitive. It would be less than an hour to get up, running, and test what works.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdma9a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63mlgh/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753931382,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753931382,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n63arp6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": "LOW_SCORE",
                      "no_follow": true,
                      "author": "discoveringnature12",
                      "can_mod_post": false,
                      "created_utc": 1753927090,
                      "send_replies": true,
                      "parent_id": "t1_n62shjv",
                      "score": -6,
                      "author_fullname": "t2_e33mgcbq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I mean how hard it would be to do everything that every post shares in this forum? What kind of a silly comment is this? \n\nIf you don't have anything to contribute, just move on. Nobody's forcing your hand to comment here.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63arp6",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I mean how hard it would be to do everything that every post shares in this forum? What kind of a silly comment is this? &lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t have anything to contribute, just move on. Nobody&amp;#39;s forcing your hand to comment here.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": "comment score below threshold",
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63arp6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753927090,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": true,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -6
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62shjv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753920598,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 3,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is a silly question, but how hard would it be to test both?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62shjv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a silly question, but how hard would it be to test both?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62shjv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753920598,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62vmkx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SM8085",
            "can_mod_post": false,
            "created_utc": 1753921690,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 1,
            "author_fullname": "t2_14vikjao97",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;I was thinking of Gemma 3, but are there better ones?\n\nSounds fine.  It *should* be able to summarize things you give it more or less.  You could try a Qwen3 but the thinking might be a pain (just from the added inference time), unless you turn thinking off.  The smaller Llama's should be fine too, like a llama 3.2, if you want to try them.  \n\n&gt;Which stack do you recommend and why?\n\nI was already using [Obsidian](https://obsidian.md/) as a markdown text editor so the Obsidian plugins for [Whisper](https://obsidian.md/plugins?search=whisper) and [local GPT](https://obsidian.md/plugins?search=local)s were handy for me.  So I can have a whisper-server up at one port on a machine &amp; llama-server on another port.  My keybindings are alt-x to start/stop whisper transcription from my mic and then alt-L to bring up the LLM options for the text, like summarize, create bulletpoints, fix errors, etc.\n\nIf there's a different combo for your mac that does basically the same thing then that's cool.\n\nI even bought a cheap non-USB mic so I could walk around the room and rant into Obsidian then see if the bot could make a coherent narrative.  My USB webcam mic was tripping out occasionally on my Linux for some reason, lol linux.\n\nggml-base.en-q5\\_1.bin has been doing fine for my whisper-server for my English.  I still have to go back and fix certain inaccuracies, like if it's something it wasn't trained on, or if it gets a name incorrect, etc.  The LLM can fix some errors from context clues, but if whisper gets a name wrong then the LLM doesn't necessarily know it's wrong.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62vmkx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I was thinking of Gemma 3, but are there better ones?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Sounds fine.  It &lt;em&gt;should&lt;/em&gt; be able to summarize things you give it more or less.  You could try a Qwen3 but the thinking might be a pain (just from the added inference time), unless you turn thinking off.  The smaller Llama&amp;#39;s should be fine too, like a llama 3.2, if you want to try them.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Which stack do you recommend and why?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I was already using &lt;a href=\"https://obsidian.md/\"&gt;Obsidian&lt;/a&gt; as a markdown text editor so the Obsidian plugins for &lt;a href=\"https://obsidian.md/plugins?search=whisper\"&gt;Whisper&lt;/a&gt; and &lt;a href=\"https://obsidian.md/plugins?search=local\"&gt;local GPT&lt;/a&gt;s were handy for me.  So I can have a whisper-server up at one port on a machine &amp;amp; llama-server on another port.  My keybindings are alt-x to start/stop whisper transcription from my mic and then alt-L to bring up the LLM options for the text, like summarize, create bulletpoints, fix errors, etc.&lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;s a different combo for your mac that does basically the same thing then that&amp;#39;s cool.&lt;/p&gt;\n\n&lt;p&gt;I even bought a cheap non-USB mic so I could walk around the room and rant into Obsidian then see if the bot could make a coherent narrative.  My USB webcam mic was tripping out occasionally on my Linux for some reason, lol linux.&lt;/p&gt;\n\n&lt;p&gt;ggml-base.en-q5_1.bin has been doing fine for my whisper-server for my English.  I still have to go back and fix certain inaccuracies, like if it&amp;#39;s something it wasn&amp;#39;t trained on, or if it gets a name incorrect, etc.  The LLM can fix some errors from context clues, but if whisper gets a name wrong then the LLM doesn&amp;#39;t necessarily know it&amp;#39;s wrong.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62vmkx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753921690,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n63g9o0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "exaknight21",
            "can_mod_post": false,
            "created_utc": 1753929037,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 2,
            "author_fullname": "t2_1nprbkmy5x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I use ollama + openwebui for myself. Tbh, it is the easiest. I have a headless server running 3060 12gb + 16gb ram on an old intel i7 4th gen.\n\nI also use that same headless server as my development server to test-deploy my apps. \n\nOllama because its easier to test.\n\nOn that same server, I have experiment with docker vLLM. So, choose your poison.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63g9o0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use ollama + openwebui for myself. Tbh, it is the easiest. I have a headless server running 3060 12gb + 16gb ram on an old intel i7 4th gen.&lt;/p&gt;\n\n&lt;p&gt;I also use that same headless server as my development server to test-deploy my apps. &lt;/p&gt;\n\n&lt;p&gt;Ollama because its easier to test.&lt;/p&gt;\n\n&lt;p&gt;On that same server, I have experiment with docker vLLM. So, choose your poison.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63g9o0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753929037,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62uzln",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DepthHour1669",
            "can_mod_post": false,
            "created_utc": 1753921466,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 1,
            "author_fullname": "t2_t6glzswk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You want LM Studio, that's what most beginners use. It uses llama.cpp underneath, and runs a openai-compatible api server for you. \n\nhttps://lmstudio.ai/docs/app/api\n\nOtherwise just run llama.cpp (llama-server) directly, but LM Studio is a lot easier for beginners. LM Studio also autoupdates their version of llama.cpp just a few hours after the official updates, so you're never far behind. \n\nYou need 32GB or more RAM on your Macbook Pro, the ~16GB MacBook models won't cut it (they can't run most useful models). \n\nIdeally you will run models at Q4 or higher (Q5, Q6, Q8). Q3 or smaller will destroy the quality of a model. \n\nYour best bet for top of the line models are Qwen3-32B, Qwen3-30B-A3B-Instruct-2507, Gemma3 27B, and Mistral Small 3.2 24B.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62uzln",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You want LM Studio, that&amp;#39;s what most beginners use. It uses llama.cpp underneath, and runs a openai-compatible api server for you. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lmstudio.ai/docs/app/api\"&gt;https://lmstudio.ai/docs/app/api&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Otherwise just run llama.cpp (llama-server) directly, but LM Studio is a lot easier for beginners. LM Studio also autoupdates their version of llama.cpp just a few hours after the official updates, so you&amp;#39;re never far behind. &lt;/p&gt;\n\n&lt;p&gt;You need 32GB or more RAM on your Macbook Pro, the ~16GB MacBook models won&amp;#39;t cut it (they can&amp;#39;t run most useful models). &lt;/p&gt;\n\n&lt;p&gt;Ideally you will run models at Q4 or higher (Q5, Q6, Q8). Q3 or smaller will destroy the quality of a model. &lt;/p&gt;\n\n&lt;p&gt;Your best bet for top of the line models are Qwen3-32B, Qwen3-30B-A3B-Instruct-2507, Gemma3 27B, and Mistral Small 3.2 24B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62uzln/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753921466,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n62sn29",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DepthHour1669",
                      "can_mod_post": false,
                      "created_utc": 1753920651,
                      "send_replies": true,
                      "parent_id": "t1_n62p5g9",
                      "score": 1,
                      "author_fullname": "t2_t6glzswk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ollama is usually slow at supporting new models and performs slower than llama.cpp (a few git commits behind in optimizations), though.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62sn29",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama is usually slow at supporting new models and performs slower than llama.cpp (a few git commits behind in optimizations), though.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62sn29/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920651,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62p5g9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "o5mfiHTNsH748KVq",
            "can_mod_post": false,
            "created_utc": 1753919466,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 0,
            "author_fullname": "t2_e11zi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "local app just for you? I’d just use ollama",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62p5g9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;local app just for you? I’d just use ollama&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62p5g9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753919466,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n63amxp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "discoveringnature12",
                      "can_mod_post": false,
                      "created_utc": 1753927042,
                      "send_replies": true,
                      "parent_id": "t1_n639wkt",
                      "score": 4,
                      "author_fullname": "t2_e33mgcbq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Great, thank you for a detailed answer.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63amxp",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great, thank you for a detailed answer.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n63amxp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753927042,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n639wkt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bharattrader",
            "can_mod_post": false,
            "created_utc": 1753926787,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": -1,
            "author_fullname": "t2_9dgykr47",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Anything but ollama",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n639wkt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Anything but ollama&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n639wkt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753926787,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62v55q",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "DepthHour1669",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62u1gi",
                                          "score": 3,
                                          "author_fullname": "t2_t6glzswk",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Ollama works equally well for windows/linux/mac. But it's certainly not developed as windows-first.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62v55q",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama works equally well for windows/linux/mac. But it&amp;#39;s certainly not developed as windows-first.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdma9a",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62v55q/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753921520,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753921520,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n62u1gi",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "JellyfishAutomatic25",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62swuw",
                                "score": 1,
                                "author_fullname": "t2_clyuifd5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "See....  the info i had was that ollama was like, like I said, built to be on windows.  I'll give my know it all buddy a ton of crap for this. \n\nBut to be fair, I'm new and learning, so thank you for correcting the misinformation I had.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62u1gi",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;See....  the info i had was that ollama was like, like I said, built to be on windows.  I&amp;#39;ll give my know it all buddy a ton of crap for this. &lt;/p&gt;\n\n&lt;p&gt;But to be fair, I&amp;#39;m new and learning, so thank you for correcting the misinformation I had.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdma9a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62u1gi/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753921134,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753921134,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62swuw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DepthHour1669",
                      "can_mod_post": false,
                      "created_utc": 1753920745,
                      "send_replies": true,
                      "parent_id": "t1_n62p9fc",
                      "score": 2,
                      "author_fullname": "t2_t6glzswk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's entirely incorrect. Most software using CUDA works better on linux machines compared to windows. \n\nOllama tries to be platform agnostic, but it's built on llama.cpp which is mostly developed for linux. All the llama.cpp devs make changes and test it on linux machines, and then if it breaks anything on windows they fix it as an afterthought.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62swuw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s entirely incorrect. Most software using CUDA works better on linux machines compared to windows. &lt;/p&gt;\n\n&lt;p&gt;Ollama tries to be platform agnostic, but it&amp;#39;s built on llama.cpp which is mostly developed for linux. All the llama.cpp devs make changes and test it on linux machines, and then if it breaks anything on windows they fix it as an afterthought.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62swuw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920745,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62uk0x",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "JellyfishAutomatic25",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62rg0o",
                                "score": 1,
                                "author_fullname": "t2_clyuifd5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Nah I loaded WSL.  I think it was part of the docker install.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62uk0x",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nah I loaded WSL.  I think it was part of the docker install.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdma9a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62uk0x/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753921315,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753921315,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62rg0o",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "The_frozen_one",
                      "can_mod_post": false,
                      "created_utc": 1753920243,
                      "send_replies": true,
                      "parent_id": "t1_n62p9fc",
                      "score": 0,
                      "author_fullname": "t2_5rvpo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I run ollama on Linux, Windows and macOS. ollama actually came out on Windows last. It runs the same on most systems with some small differences, but it's just a service that provides an API.\n\n&gt; Ironically, I then turned around and loaded a Linux shell so.......... yeah.\n\nWhat you're talking about is the terminal or command line, that exists on all OSes. If you want to avoid this, you should go with something like jan.ai or LM Studio. People seem to like LM Studio quite a lot, and it seemed nice when I tried it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62rg0o",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I run ollama on Linux, Windows and macOS. ollama actually came out on Windows last. It runs the same on most systems with some small differences, but it&amp;#39;s just a service that provides an API.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Ironically, I then turned around and loaded a Linux shell so.......... yeah.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;What you&amp;#39;re talking about is the terminal or command line, that exists on all OSes. If you want to avoid this, you should go with something like jan.ai or LM Studio. People seem to like LM Studio quite a lot, and it seemed nice when I tried it.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62rg0o/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920243,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62p9fc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "JellyfishAutomatic25",
            "can_mod_post": false,
            "created_utc": 1753919503,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 0,
            "author_fullname": "t2_clyuifd5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm new to all this so please don't take what I say as actual advice....\n\nBut I think the OS makes a big difference here.   I'm running ollama on my windows pc because as in understand it, it is designed to run on windows without the need for Linux shell programs.\n\nIronically, I then turned around and loaded a Linux shell so..........  yeah.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62p9fc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to all this so please don&amp;#39;t take what I say as actual advice....&lt;/p&gt;\n\n&lt;p&gt;But I think the OS makes a big difference here.   I&amp;#39;m running ollama on my windows pc because as in understand it, it is designed to run on windows without the need for Linux shell programs.&lt;/p&gt;\n\n&lt;p&gt;Ironically, I then turned around and loaded a Linux shell so..........  yeah.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62p9fc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753919503,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62ycko",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Jatilq",
            "can_mod_post": false,
            "created_utc": 1753922654,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 0,
            "author_fullname": "t2_3txs3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Koboldcpp when I want something easy. LM Studio is easy and allows you to search and download models. Both have worked well with dual AMD or NVidia cards. This is in windows.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62ycko",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Koboldcpp when I want something easy. LM Studio is easy and allows you to search and download models. Both have worked well with dual AMD or NVidia cards. This is in windows.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62ycko/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753922654,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]