[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm setting up a local LLM to run in the background on my MacBook Pro (M3 Pro). The main use case is this: I use a dictation app (like SuperWhisper or Spokenly) to convert my voice to text, and then send that text to a local LLM server for processing. Think: summarizing, answering, rephrasing, correction, or responding intelligently to the text input.\n\nI want something:\n\n- Fast (low latency for near-real-time dictation use)\n\n- Reasonably accurate\n\n- Local (no cloud APIs)\n\n- Ideally OpenAI-compatible API so it's easier to integrate with other tools\n\nWith some flexibility for future use cases beyond just dictation\n\nSo far I'm looking at:\n\n- llama.cpp (via llama-server)\n\n- Ollama\n\nAnd what Llama model would you recommend? I was thinking of Gemma 3, but are there better ones? \n\nWould love to hear from others who've done similar setups. Which stack do you recommend and why?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Help choosing between Ollama, llama.cpp, or something else for background LLM server (used with dictation)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mdma9a",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_e33mgcbq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753918996,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m setting up a local LLM to run in the background on my MacBook Pro (M3 Pro). The main use case is this: I use a dictation app (like SuperWhisper or Spokenly) to convert my voice to text, and then send that text to a local LLM server for processing. Think: summarizing, answering, rephrasing, correction, or responding intelligently to the text input.&lt;/p&gt;\n\n&lt;p&gt;I want something:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Fast (low latency for near-real-time dictation use)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Reasonably accurate&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Local (no cloud APIs)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ideally OpenAI-compatible API so it&amp;#39;s easier to integrate with other tools&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With some flexibility for future use cases beyond just dictation&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;m looking at:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;llama.cpp (via llama-server)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ollama&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And what Llama model would you recommend? I was thinking of Gemma 3, but are there better ones? &lt;/p&gt;\n\n&lt;p&gt;Would love to hear from others who&amp;#39;ve done similar setups. Which stack do you recommend and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdma9a",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "discoveringnature12",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/",
            "subreddit_subscribers": 507274,
            "created_utc": 1753918996,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62shjv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753920598,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 3,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is a silly question, but how hard would it be to test both?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62shjv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a silly question, but how hard would it be to test both?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62shjv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753920598,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62si6d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "The_frozen_one",
            "can_mod_post": false,
            "created_utc": 1753920604,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 2,
            "author_fullname": "t2_5rvpo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Have you looked at koboldcpp or jan.ai? Just throwing some different options out there. gemma 3 is a solid model for its size, and it handles images well. I'd stick with whisper for transcription, if keeping the text accurate is important especially with low latency.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62si6d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Have you looked at koboldcpp or jan.ai? Just throwing some different options out there. gemma 3 is a solid model for its size, and it handles images well. I&amp;#39;d stick with whisper for transcription, if keeping the text accurate is important especially with low latency.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62si6d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753920604,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n62sn29",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DepthHour1669",
                      "can_mod_post": false,
                      "created_utc": 1753920651,
                      "send_replies": true,
                      "parent_id": "t1_n62p5g9",
                      "score": 2,
                      "author_fullname": "t2_t6glzswk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ollama is usually slow at supporting new models and performs slower than llama.cpp (a few git commits behind in optimizations), though.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62sn29",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama is usually slow at supporting new models and performs slower than llama.cpp (a few git commits behind in optimizations), though.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62sn29/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920651,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62p5g9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "o5mfiHTNsH748KVq",
            "can_mod_post": false,
            "created_utc": 1753919466,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 1,
            "author_fullname": "t2_e11zi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "local app just for you? I’d just use ollama",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62p5g9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;local app just for you? I’d just use ollama&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62p5g9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753919466,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62uzln",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DepthHour1669",
            "can_mod_post": false,
            "created_utc": 1753921466,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 1,
            "author_fullname": "t2_t6glzswk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You want LM Studio, that's what most beginners use. It uses llama.cpp underneath, and runs a openai-compatible api server for you. \n\nhttps://lmstudio.ai/docs/app/api\n\nOtherwise just run llama.cpp (llama-server) directly, but LM Studio is a lot easier for beginners. LM Studio also autoupdates their version of llama.cpp just a few hours after the official updates, so you're never far behind. \n\nYou need 32GB or more RAM on your Macbook Pro, the ~16GB MacBook models won't cut it (they can't run most useful models). \n\nIdeally you will run models at Q4 or higher (Q5, Q6, Q8). Q3 or smaller will destroy the quality of a model. \n\nYour best bet for top of the line models are Qwen3-32B, Qwen3-30B-A3B-Instruct-2507, Gemma3 27B, and Mistral Small 3.2 24B.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62uzln",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You want LM Studio, that&amp;#39;s what most beginners use. It uses llama.cpp underneath, and runs a openai-compatible api server for you. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lmstudio.ai/docs/app/api\"&gt;https://lmstudio.ai/docs/app/api&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Otherwise just run llama.cpp (llama-server) directly, but LM Studio is a lot easier for beginners. LM Studio also autoupdates their version of llama.cpp just a few hours after the official updates, so you&amp;#39;re never far behind. &lt;/p&gt;\n\n&lt;p&gt;You need 32GB or more RAM on your Macbook Pro, the ~16GB MacBook models won&amp;#39;t cut it (they can&amp;#39;t run most useful models). &lt;/p&gt;\n\n&lt;p&gt;Ideally you will run models at Q4 or higher (Q5, Q6, Q8). Q3 or smaller will destroy the quality of a model. &lt;/p&gt;\n\n&lt;p&gt;Your best bet for top of the line models are Qwen3-32B, Qwen3-30B-A3B-Instruct-2507, Gemma3 27B, and Mistral Small 3.2 24B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62uzln/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753921466,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62vmkx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SM8085",
            "can_mod_post": false,
            "created_utc": 1753921690,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 1,
            "author_fullname": "t2_14vikjao97",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;I was thinking of Gemma 3, but are there better ones?\n\nSounds fine.  It *should* be able to summarize things you give it more or less.  You could try a Qwen3 but the thinking might be a pain (just from the added inference time), unless you turn thinking off.  The smaller Llama's should be fine too, like a llama 3.2, if you want to try them.  \n\n&gt;Which stack do you recommend and why?\n\nI was already using [Obsidian](https://obsidian.md/) as a markdown text editor so the Obsidian plugins for [Whisper](https://obsidian.md/plugins?search=whisper) and [local GPT](https://obsidian.md/plugins?search=local)s were handy for me.  So I can have a whisper-server up at one port on a machine &amp; llama-server on another port.  My keybindings are alt-x to start/stop whisper transcription from my mic and then alt-L to bring up the LLM options for the text, like summarize, create bulletpoints, fix errors, etc.\n\nIf there's a different combo for your mac that does basically the same thing then that's cool.\n\nI even bought a cheap non-USB mic so I could walk around the room and rant into Obsidian then see if the bot could make a coherent narrative.  My USB webcam mic was tripping out occasionally on my Linux for some reason, lol linux.\n\nggml-base.en-q5\\_1.bin has been doing fine for my whisper-server for my English.  I still have to go back and fix certain inaccuracies, like if it's something it wasn't trained on, or if it gets a name incorrect, etc.  The LLM can fix some errors from context clues, but if whisper gets a name wrong then the LLM doesn't necessarily know it's wrong.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62vmkx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I was thinking of Gemma 3, but are there better ones?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Sounds fine.  It &lt;em&gt;should&lt;/em&gt; be able to summarize things you give it more or less.  You could try a Qwen3 but the thinking might be a pain (just from the added inference time), unless you turn thinking off.  The smaller Llama&amp;#39;s should be fine too, like a llama 3.2, if you want to try them.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Which stack do you recommend and why?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I was already using &lt;a href=\"https://obsidian.md/\"&gt;Obsidian&lt;/a&gt; as a markdown text editor so the Obsidian plugins for &lt;a href=\"https://obsidian.md/plugins?search=whisper\"&gt;Whisper&lt;/a&gt; and &lt;a href=\"https://obsidian.md/plugins?search=local\"&gt;local GPT&lt;/a&gt;s were handy for me.  So I can have a whisper-server up at one port on a machine &amp;amp; llama-server on another port.  My keybindings are alt-x to start/stop whisper transcription from my mic and then alt-L to bring up the LLM options for the text, like summarize, create bulletpoints, fix errors, etc.&lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;s a different combo for your mac that does basically the same thing then that&amp;#39;s cool.&lt;/p&gt;\n\n&lt;p&gt;I even bought a cheap non-USB mic so I could walk around the room and rant into Obsidian then see if the bot could make a coherent narrative.  My USB webcam mic was tripping out occasionally on my Linux for some reason, lol linux.&lt;/p&gt;\n\n&lt;p&gt;ggml-base.en-q5_1.bin has been doing fine for my whisper-server for my English.  I still have to go back and fix certain inaccuracies, like if it&amp;#39;s something it wasn&amp;#39;t trained on, or if it gets a name incorrect, etc.  The LLM can fix some errors from context clues, but if whisper gets a name wrong then the LLM doesn&amp;#39;t necessarily know it&amp;#39;s wrong.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62vmkx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753921690,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62ycko",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Jatilq",
            "can_mod_post": false,
            "created_utc": 1753922654,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 1,
            "author_fullname": "t2_3txs3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Koboldcpp when I want something easy. LM Studio is easy and allows you to search and download models. Both have worked well with dual AMD or NVidia cards. This is in windows.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62ycko",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Koboldcpp when I want something easy. LM Studio is easy and allows you to search and download models. Both have worked well with dual AMD or NVidia cards. This is in windows.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62ycko/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753922654,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62uk0x",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "JellyfishAutomatic25",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62rg0o",
                                "score": 1,
                                "author_fullname": "t2_clyuifd5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Nah I loaded WSL.  I think it was part of the docker install.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62uk0x",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nah I loaded WSL.  I think it was part of the docker install.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdma9a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62uk0x/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753921315,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753921315,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62rg0o",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "The_frozen_one",
                      "can_mod_post": false,
                      "created_utc": 1753920243,
                      "send_replies": true,
                      "parent_id": "t1_n62p9fc",
                      "score": 1,
                      "author_fullname": "t2_5rvpo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I run ollama on Linux, Windows and macOS. ollama actually came out on Windows last. It runs the same on most systems with some small differences, but it's just a service that provides an API.\n\n&gt; Ironically, I then turned around and loaded a Linux shell so.......... yeah.\n\nWhat you're talking about is the terminal or command line, that exists on all OSes. If you want to avoid this, you should go with something like jan.ai or LM Studio. People seem to like LM Studio quite a lot, and it seemed nice when I tried it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62rg0o",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I run ollama on Linux, Windows and macOS. ollama actually came out on Windows last. It runs the same on most systems with some small differences, but it&amp;#39;s just a service that provides an API.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Ironically, I then turned around and loaded a Linux shell so.......... yeah.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;What you&amp;#39;re talking about is the terminal or command line, that exists on all OSes. If you want to avoid this, you should go with something like jan.ai or LM Studio. People seem to like LM Studio quite a lot, and it seemed nice when I tried it.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62rg0o/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920243,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62v55q",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "DepthHour1669",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62u1gi",
                                          "score": 2,
                                          "author_fullname": "t2_t6glzswk",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Ollama works equally well for windows/linux/mac. But it's certainly not developed as windows-first.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62v55q",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama works equally well for windows/linux/mac. But it&amp;#39;s certainly not developed as windows-first.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdma9a",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62v55q/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753921520,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753921520,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n62u1gi",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "JellyfishAutomatic25",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62swuw",
                                "score": 1,
                                "author_fullname": "t2_clyuifd5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "See....  the info i had was that ollama was like, like I said, built to be on windows.  I'll give my know it all buddy a ton of crap for this. \n\nBut to be fair, I'm new and learning, so thank you for correcting the misinformation I had.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62u1gi",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;See....  the info i had was that ollama was like, like I said, built to be on windows.  I&amp;#39;ll give my know it all buddy a ton of crap for this. &lt;/p&gt;\n\n&lt;p&gt;But to be fair, I&amp;#39;m new and learning, so thank you for correcting the misinformation I had.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdma9a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62u1gi/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753921134,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753921134,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62swuw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DepthHour1669",
                      "can_mod_post": false,
                      "created_utc": 1753920745,
                      "send_replies": true,
                      "parent_id": "t1_n62p9fc",
                      "score": 2,
                      "author_fullname": "t2_t6glzswk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's entirely incorrect. Most software using CUDA works better on linux machines compared to windows. \n\nOllama tries to be platform agnostic, but it's built on llama.cpp which is mostly developed for linux. All the llama.cpp devs make changes and test it on linux machines, and then if it breaks anything on windows they fix it as an afterthought.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62swuw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s entirely incorrect. Most software using CUDA works better on linux machines compared to windows. &lt;/p&gt;\n\n&lt;p&gt;Ollama tries to be platform agnostic, but it&amp;#39;s built on llama.cpp which is mostly developed for linux. All the llama.cpp devs make changes and test it on linux machines, and then if it breaks anything on windows they fix it as an afterthought.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdma9a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62swuw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920745,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62p9fc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "JellyfishAutomatic25",
            "can_mod_post": false,
            "created_utc": 1753919503,
            "send_replies": true,
            "parent_id": "t3_1mdma9a",
            "score": 0,
            "author_fullname": "t2_clyuifd5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm new to all this so please don't take what I say as actual advice....\n\nBut I think the OS makes a big difference here.   I'm running ollama on my windows pc because as in understand it, it is designed to run on windows without the need for Linux shell programs.\n\nIronically, I then turned around and loaded a Linux shell so..........  yeah.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62p9fc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to all this so please don&amp;#39;t take what I say as actual advice....&lt;/p&gt;\n\n&lt;p&gt;But I think the OS makes a big difference here.   I&amp;#39;m running ollama on my windows pc because as in understand it, it is designed to run on windows without the need for Linux shell programs.&lt;/p&gt;\n\n&lt;p&gt;Ironically, I then turned around and loaded a Linux shell so..........  yeah.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdma9a/help_choosing_between_ollama_llamacpp_or/n62p9fc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753919503,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdma9a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]