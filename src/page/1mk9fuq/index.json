[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have been running ChatGPT and other AI chatbots for a while and have been blown away by their capabilities. When I discovered I could run LLM (Large Language Models) on my computer, I was intrigued.\n\nFor one thing, it would give me all the privacy I desire, as I would not have to expose my data to the Internet. It would also allow me to run a wide array of open-source models at zero cost. And, I would have total control of the system and would not have to worry about Internet issues or provider outages.\n\nMy current PC is a Ryzen 5700G with 32 GB of RAM. It is an APU with onboard graphics. The downside is the graphics processor does not have enough speed or memory to do LLM inference, as it shares memory with the CPU. The results are slow output speed compared to a graphics card and model size limitations.\n\nI spent hours learning platforms like Ollama and LM Studio and did a lot of testing and benchmarking a variety of LLMs.\n\nI also looked at a variety of upgrade options, including rebuilding my present system and adding a graphics card, building a new system from scratch, or buying one of those cool new mini computers loaded with 64GB of memory and support for dual nVME drives.\n\nIn addition, Ichecked out the X99 motherboard/Xeon processor/memory combos that you can get really cheap on various sites on the internet. Plus, all of the available graphic card options for LLM inference.\n\nThe end result is my new book: LLM Hardware Unlocked. It will show you the benefits and limitations of running LLMs at home as well as exposing the realities of heat, noise, and power draw if you decide to go “all in”.\n\nI invite you to check it out. It is a quick read with a low sticker price. And, hopefully, it will save you time and frustration if you want to unlock the power of local LLMs.\n\n*Here is the link to my ebook on Amazon for Kindle:*\n\n[https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/](https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/)\n\nMedium Article: [https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66](https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Unlocking the Power of Local LLMs",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mk9fuq",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.29,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ycgc2",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754594701,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been running ChatGPT and other AI chatbots for a while and have been blown away by their capabilities. When I discovered I could run LLM (Large Language Models) on my computer, I was intrigued.&lt;/p&gt;\n\n&lt;p&gt;For one thing, it would give me all the privacy I desire, as I would not have to expose my data to the Internet. It would also allow me to run a wide array of open-source models at zero cost. And, I would have total control of the system and would not have to worry about Internet issues or provider outages.&lt;/p&gt;\n\n&lt;p&gt;My current PC is a Ryzen 5700G with 32 GB of RAM. It is an APU with onboard graphics. The downside is the graphics processor does not have enough speed or memory to do LLM inference, as it shares memory with the CPU. The results are slow output speed compared to a graphics card and model size limitations.&lt;/p&gt;\n\n&lt;p&gt;I spent hours learning platforms like Ollama and LM Studio and did a lot of testing and benchmarking a variety of LLMs.&lt;/p&gt;\n\n&lt;p&gt;I also looked at a variety of upgrade options, including rebuilding my present system and adding a graphics card, building a new system from scratch, or buying one of those cool new mini computers loaded with 64GB of memory and support for dual nVME drives.&lt;/p&gt;\n\n&lt;p&gt;In addition, Ichecked out the X99 motherboard/Xeon processor/memory combos that you can get really cheap on various sites on the internet. Plus, all of the available graphic card options for LLM inference.&lt;/p&gt;\n\n&lt;p&gt;The end result is my new book: LLM Hardware Unlocked. It will show you the benefits and limitations of running LLMs at home as well as exposing the realities of heat, noise, and power draw if you decide to go “all in”.&lt;/p&gt;\n\n&lt;p&gt;I invite you to check it out. It is a quick read with a low sticker price. And, hopefully, it will save you time and frustration if you want to unlock the power of local LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Here is the link to my ebook on Amazon for Kindle:&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/\"&gt;https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Medium Article: &lt;a href=\"https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66\"&gt;https://medium.com/@tthomas1000/unlocking-the-power-of-local-llms-07c9cf4c3f66&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mk9fuq",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "tony10000",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mk9fuq/unlocking_the_power_of_local_llms/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk9fuq/unlocking_the_power_of_local_llms/",
            "subreddit_subscribers": 513417,
            "created_utc": 1754594701,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]