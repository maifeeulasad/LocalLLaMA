[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "MoE LLMs (like Mixtral) have set a new bar for efficient scaling. But all open MoEs route at the token level, with expert specialization emerging implicitly.\n\n**Recent research (TaskMoE, DomainMoE, THOR-MoE, GLaM) explores explicit routing by domain and even subdomain. This enables:**\n\n* Targeted upgrades (swap in a better “math” or “literature” expert without retraining the whole model)\n* More interpretable model internals\n* Modularity that aligns with how orchestrators (AutoGen, CrewAI, MCP) are evolving\n\n**What might this look like for Mistral?**\n\n* Expert groups per domain (English, math, code, etc.)\n* Hierarchies within domains (e.g., arithmetic → algebra → calculus), potentially with meta-experts that arbitrate or combine outputs\n* A possible “expert registry” for community or enterprise swapping/upgrading\n\n**This isn’t trivial. Some questions:**\n\n* How should the gating and training be handled to avoid catastrophic forgetting or interface mismatch?\n* What’s the best way to benchmark performance of swapped modules?\n* Are there security or trust issues with open expert modules, and how do other plugin/package systems handle it?\n* Who’s working on this already? Any public code, experiments, or ideas?\n\n**Links:**\n\n* TaskMoE: \n* DomainMoE: \n* THOR-MoE: \n* AutoGen: [https://github.com/microsoft/autogen](https://github.com/microsoft/autogen)\n* CrewAI: [https://github.com/joaomdmoura/crewAI](https://github.com/joaomdmoura/crewAI)\n* ModelContextProtocol: [https://github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)\n\nWould love thoughts, critique, and collaboration. Is this plausible as the next step for Mixtral (or other open MoEs)? What would it take to make this real?\n\n**TL;DR**  \nIs it time for modular, upgradeable, domain-aware MoE in open models like Mistral? What’s missing—and who’s already working on it?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Proposal: Modular, Domain &amp; Subdomain-Aware MoE for Mistral—Next Steps?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lvyvmw",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_gsms20cv",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1752109442,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752106983,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MoE LLMs (like Mixtral) have set a new bar for efficient scaling. But all open MoEs route at the token level, with expert specialization emerging implicitly.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Recent research (TaskMoE, DomainMoE, THOR-MoE, GLaM) explores explicit routing by domain and even subdomain. This enables:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Targeted upgrades (swap in a better “math” or “literature” expert without retraining the whole model)&lt;/li&gt;\n&lt;li&gt;More interpretable model internals&lt;/li&gt;\n&lt;li&gt;Modularity that aligns with how orchestrators (AutoGen, CrewAI, MCP) are evolving&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What might this look like for Mistral?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Expert groups per domain (English, math, code, etc.)&lt;/li&gt;\n&lt;li&gt;Hierarchies within domains (e.g., arithmetic → algebra → calculus), potentially with meta-experts that arbitrate or combine outputs&lt;/li&gt;\n&lt;li&gt;A possible “expert registry” for community or enterprise swapping/upgrading&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;This isn’t trivial. Some questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How should the gating and training be handled to avoid catastrophic forgetting or interface mismatch?&lt;/li&gt;\n&lt;li&gt;What’s the best way to benchmark performance of swapped modules?&lt;/li&gt;\n&lt;li&gt;Are there security or trust issues with open expert modules, and how do other plugin/package systems handle it?&lt;/li&gt;\n&lt;li&gt;Who’s working on this already? Any public code, experiments, or ideas?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;TaskMoE: &lt;/li&gt;\n&lt;li&gt;DomainMoE: &lt;/li&gt;\n&lt;li&gt;THOR-MoE: &lt;/li&gt;\n&lt;li&gt;AutoGen: &lt;a href=\"https://github.com/microsoft/autogen\"&gt;https://github.com/microsoft/autogen&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;CrewAI: &lt;a href=\"https://github.com/joaomdmoura/crewAI\"&gt;https://github.com/joaomdmoura/crewAI&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;ModelContextProtocol: &lt;a href=\"https://github.com/modelcontextprotocol/servers\"&gt;https://github.com/modelcontextprotocol/servers&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love thoughts, critique, and collaboration. Is this plausible as the next step for Mixtral (or other open MoEs)? What would it take to make this real?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;br/&gt;\nIs it time for modular, upgradeable, domain-aware MoE in open models like Mistral? What’s missing—and who’s already working on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I.png?auto=webp&amp;s=de3134207bdb03a40ec3213a5d5b9fb37889323a",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=31e91b273e10834ecfcddb025f68ca9f4f99b01a",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=89fb36caeea12436cd5c89e5d3bfd80bc7b9cb0e",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bb0fc79bc35dd2a9959ef70620022f2cd786c18",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3826f7d5805b8e0c2e690a3fc014819dbb0f3a75",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee91359de4703e558b82221abc9df059055ce593",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9bfef0db7f581de06741d5f1886fe0b491c2ab4f",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "ssJ7ZThFxiDtWap2rh1Sw3u3noHXRIviaqaqgePE26I"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lvyvmw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "the_sturgill",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lvyvmw/proposal_modular_domain_subdomainaware_moe_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvyvmw/proposal_modular_domain_subdomainaware_moe_for/",
            "subreddit_subscribers": 497022,
            "created_utc": 1752106983,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2a75j9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1752109492,
            "send_replies": true,
            "parent_id": "t3_1lvyvmw",
            "score": 1,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nope, screw that noise. Domain experts are gross, only perform well in benchmarks and not in real use cases, and ignore a really important point:\n\nPer training flop, standard MoE models are just really good.\n\nIf you want domain specific functionality, use adapters like LoRA, or provide rich embeddings from another model at the start of the pipeline.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2a75j9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nope, screw that noise. Domain experts are gross, only perform well in benchmarks and not in real use cases, and ignore a really important point:&lt;/p&gt;\n\n&lt;p&gt;Per training flop, standard MoE models are just really good.&lt;/p&gt;\n\n&lt;p&gt;If you want domain specific functionality, use adapters like LoRA, or provide rich embeddings from another model at the start of the pipeline.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvyvmw/proposal_modular_domain_subdomainaware_moe_for/n2a75j9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752109492,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvyvmw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2a67mw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "the_sturgill",
                      "can_mod_post": false,
                      "created_utc": 1752109167,
                      "send_replies": true,
                      "parent_id": "t1_n2a5ywi",
                      "score": 1,
                      "author_fullname": "t2_gsms20cv",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Correction/clarification:  \nA number of the links I posted—especially in the research section—aren’t as directly relevant as I’d hoped. There are strong papers on domain/task-aware routing in MoE (like TaskMoE and DomainMoE), but *none* describe a working, hot-swappable modular expert registry.  \nMy post was meant as a “next step” proposal, connecting emerging ideas in model and agent modularity (as seen in things like CrewAI/AutoGen/MCP) to MoE, but this is ahead of the literature.\n\nIf anyone knows of closer prior art, failed attempts, or practical code, please link it!  \nOtherwise, I think this highlights a true gap worth talking about.\n\nApologies for any confusion—I want the technical debate, not just the hype.",
                      "edited": 1752109716,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2a67mw",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Correction/clarification:&lt;br/&gt;\nA number of the links I posted—especially in the research section—aren’t as directly relevant as I’d hoped. There are strong papers on domain/task-aware routing in MoE (like TaskMoE and DomainMoE), but &lt;em&gt;none&lt;/em&gt; describe a working, hot-swappable modular expert registry.&lt;br/&gt;\nMy post was meant as a “next step” proposal, connecting emerging ideas in model and agent modularity (as seen in things like CrewAI/AutoGen/MCP) to MoE, but this is ahead of the literature.&lt;/p&gt;\n\n&lt;p&gt;If anyone knows of closer prior art, failed attempts, or practical code, please link it!&lt;br/&gt;\nOtherwise, I think this highlights a true gap worth talking about.&lt;/p&gt;\n\n&lt;p&gt;Apologies for any confusion—I want the technical debate, not just the hype.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lvyvmw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lvyvmw/proposal_modular_domain_subdomainaware_moe_for/n2a67mw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752109167,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2a5ywi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Accomplished_Ad9530",
            "can_mod_post": false,
            "created_utc": 1752109083,
            "send_replies": true,
            "parent_id": "t3_1lvyvmw",
            "score": 2,
            "author_fullname": "t2_88fma001",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "TaskMoE and DomainMoE arxiv links are just to random papers. Bot post?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2a5ywi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;TaskMoE and DomainMoE arxiv links are just to random papers. Bot post?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvyvmw/proposal_modular_domain_subdomainaware_moe_for/n2a5ywi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752109083,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvyvmw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2a2sze",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No-Refrigerator-1672",
            "can_mod_post": false,
            "created_utc": 1752107992,
            "send_replies": true,
            "parent_id": "t3_1lvyvmw",
            "score": 1,
            "author_fullname": "t2_baavelp5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "More than a year ago, it was substantially popular to mix multiple pretrained finetunes into a makeshift MoE for RP, with a router layer trained from scratch. This sounds extremely similar to rhe said research. If this concent were explored by amateurs relatively long ago, yet wasn't ever employed by AI researchers, then it means that there's something fundamentally flawed there. I predict that this new research, while being better than previously described tech, will not take off due to sharing the same funtamental flaws.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2a2sze",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;More than a year ago, it was substantially popular to mix multiple pretrained finetunes into a makeshift MoE for RP, with a router layer trained from scratch. This sounds extremely similar to rhe said research. If this concent were explored by amateurs relatively long ago, yet wasn&amp;#39;t ever employed by AI researchers, then it means that there&amp;#39;s something fundamentally flawed there. I predict that this new research, while being better than previously described tech, will not take off due to sharing the same funtamental flaws.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvyvmw/proposal_modular_domain_subdomainaware_moe_for/n2a2sze/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752107992,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvyvmw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]