[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Sorry if this is a basic question, but I seem to be really struggling :/\n\nConsider a typical, text-in text-out use case. If I'm using an offline model API via e.g. REST, how can I incorporate tool  use? Is \"tool use\" some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?\n\nIs there some basic, no-nonsense code or tutorial to get a concrete idea?\n\nThanks",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What is \"tool use\", exactly?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfrq3v",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 18,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_127kho",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 18,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754144468,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a basic question, but I seem to be really struggling :/&lt;/p&gt;\n\n&lt;p&gt;Consider a typical, text-in text-out use case. If I&amp;#39;m using an offline model API via e.g. REST, how can I incorporate tool  use? Is &amp;quot;tool use&amp;quot; some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?&lt;/p&gt;\n\n&lt;p&gt;Is there some basic, no-nonsense code or tutorial to get a concrete idea?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mfrq3v",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ihatebeinganonymous",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
            "subreddit_subscribers": 509293,
            "created_utc": 1754144468,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6kqee6",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "_moria_",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6jdtkh",
                                                    "score": 3,
                                                    "author_fullname": "t2_10lot4",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Yes!\n\nIf you want a super clear explanation the hf tutorial on smolabent library is a good mine\n\nEdit: to clarify one round is the invocation \"\"I would really need the output of this tool\" and one round is the same message with \"this tool reports this output\" in the context",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6kqee6",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes!&lt;/p&gt;\n\n&lt;p&gt;If you want a super clear explanation the hf tutorial on smolabent library is a good mine&lt;/p&gt;\n\n&lt;p&gt;Edit: to clarify one round is the invocation &amp;quot;&amp;quot;I would really need the output of this tool&amp;quot; and one round is the same message with &amp;quot;this tool reports this output&amp;quot; in the context&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfrq3v",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6kqee6/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754163180,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754163180,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6jdtkh",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "ihatebeinganonymous",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6j6b8z",
                                          "score": 5,
                                          "author_fullname": "t2_127kho",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Many thanks. It was very insightful.\n\n\nSo far, what I get is that one can simply look for function call tokens in the output and replace them with function output to get useful result, e.g. weather check.\n\n\nBut what if the LLM has to make a decision based on the function call output? Will that be a second LLM prompt/call?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6jdtkh",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Many thanks. It was very insightful.&lt;/p&gt;\n\n&lt;p&gt;So far, what I get is that one can simply look for function call tokens in the output and replace them with function output to get useful result, e.g. weather check.&lt;/p&gt;\n\n&lt;p&gt;But what if the LLM has to make a decision based on the function call output? Will that be a second LLM prompt/call?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfrq3v",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6jdtkh/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754147549,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754147549,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 5
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6j6b8z",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "entsnack",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6j5c33",
                                "score": 7,
                                "author_fullname": "t2_1a48h7vf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "They can be defined during training, and you can post-train the language model to tool-call.\n\nThey can also be defined at test time, by including the tool in the prompt. This is how MCP and a lot of tool-calling works. There was an article recently about Claude's massive system prompt that had 100s of tools.\n\nThe implementation is by the LLM user, or by the LLM provider (e.g., OpenAI for web search, git patches, etc.), or by the MCP provider. It's a pretty flexible framework.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6j6b8z",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "a": ":X:",
                                    "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                                    "e": "emoji"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They can be defined during training, and you can post-train the language model to tool-call.&lt;/p&gt;\n\n&lt;p&gt;They can also be defined at test time, by including the tool in the prompt. This is how MCP and a lot of tool-calling works. There was an article recently about Claude&amp;#39;s massive system prompt that had 100s of tools.&lt;/p&gt;\n\n&lt;p&gt;The implementation is by the LLM user, or by the LLM provider (e.g., OpenAI for web search, git patches, etc.), or by the MCP provider. It&amp;#39;s a pretty flexible framework.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfrq3v",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "dark",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6j6b8z/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754145141,
                                "author_flair_text": ":X:",
                                "treatment_tags": [],
                                "created_utc": 1754145141,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "transparent",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 7
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6j5c33",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "ihatebeinganonymous",
                      "can_mod_post": false,
                      "created_utc": 1754144817,
                      "send_replies": true,
                      "parent_id": "t1_n6j4uzv",
                      "score": 5,
                      "author_fullname": "t2_127kho",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Many thanks. How are the functions/tokens defined (I assume during training?), and who implements them? The LLM user?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6j5c33",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Many thanks. How are the functions/tokens defined (I assume during training?), and who implements them? The LLM user?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfrq3v",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6j5c33/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754144817,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6j4uzv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "entsnack",
            "can_mod_post": false,
            "created_utc": 1754144659,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 22,
            "author_fullname": "t2_1a48h7vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A tool call or function call is a sequence of tokens, just like natural language. But it is formatted in a way that you can recognize that a tool call has been mode, stop generating, map the name of the tool to some function (which can be any piece of code, bash command, anything), run that function, add the result to the context, and then continue generating tokens.\n\nSo yes the interaction is multi-step. The best resource to understand tools is Meta's \"Toolformer\" paper, which invented the idea of tool-calling agents.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6j4uzv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A tool call or function call is a sequence of tokens, just like natural language. But it is formatted in a way that you can recognize that a tool call has been mode, stop generating, map the name of the tool to some function (which can be any piece of code, bash command, anything), run that function, add the result to the context, and then continue generating tokens.&lt;/p&gt;\n\n&lt;p&gt;So yes the interaction is multi-step. The best resource to understand tools is Meta&amp;#39;s &amp;quot;Toolformer&amp;quot; paper, which invented the idea of tool-calling agents.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6j4uzv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754144659,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "transparent",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 22
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6kl9iv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "acasto",
            "can_mod_post": false,
            "created_utc": 1754161459,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 7,
            "author_fullname": "t2_1itasc0o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would ignore the stuff about MCP for now. That's just a standardized way to implement generalized tool use but more applicable to the application layer than what's going on with the LLM itself. It's neat, but also another level of complexity you probably don't need to worry about at the moment. \n\nTool use can be confusing because it's a mix of model behavior and backend support. You supply the tools definitions via the API tools parameter, but it ultimately just gets turned into a system prompt basically saying \"you have x,y,z tools available and this is how you use them...\". When the model needs to use a tool, it does do by responding in a JSON format and usually the backend will flag the response as a tool call (e.g., 'stop\\_reason' == 'tool\\_use' on Anthropic) which you then detect on your end and run the tool and submit the results back to the LLM.\n\nYou can see where the line is getting blurred there. There's no reason you can't just put tool use instructions in your system prompt and then try to detect if a response is proper JSON. Most models that are decent at following instructions can take it even further. I wrote my own local chat app back before tool calling was widely available, especially locally, and just created a system where it formats calls in particular tags and then I parse the responses for them. So far every modern model both local and remote has been able to use them flawlessly. Occasionally with a small local model it'll trigger the JSON response though and you'll see the tool calling they were trained on. \n\nSo, the models are just trained that here are your tools in format X and this is how you would respond in format Y. Then the backends are designed to present the tools in a standardized way in format X and detect the responses in format Y so that systems can use them programmatically.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kl9iv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would ignore the stuff about MCP for now. That&amp;#39;s just a standardized way to implement generalized tool use but more applicable to the application layer than what&amp;#39;s going on with the LLM itself. It&amp;#39;s neat, but also another level of complexity you probably don&amp;#39;t need to worry about at the moment. &lt;/p&gt;\n\n&lt;p&gt;Tool use can be confusing because it&amp;#39;s a mix of model behavior and backend support. You supply the tools definitions via the API tools parameter, but it ultimately just gets turned into a system prompt basically saying &amp;quot;you have x,y,z tools available and this is how you use them...&amp;quot;. When the model needs to use a tool, it does do by responding in a JSON format and usually the backend will flag the response as a tool call (e.g., &amp;#39;stop_reason&amp;#39; == &amp;#39;tool_use&amp;#39; on Anthropic) which you then detect on your end and run the tool and submit the results back to the LLM.&lt;/p&gt;\n\n&lt;p&gt;You can see where the line is getting blurred there. There&amp;#39;s no reason you can&amp;#39;t just put tool use instructions in your system prompt and then try to detect if a response is proper JSON. Most models that are decent at following instructions can take it even further. I wrote my own local chat app back before tool calling was widely available, especially locally, and just created a system where it formats calls in particular tags and then I parse the responses for them. So far every modern model both local and remote has been able to use them flawlessly. Occasionally with a small local model it&amp;#39;ll trigger the JSON response though and you&amp;#39;ll see the tool calling they were trained on. &lt;/p&gt;\n\n&lt;p&gt;So, the models are just trained that here are your tools in format X and this is how you would respond in format Y. Then the backends are designed to present the tools in a standardized way in format X and detect the responses in format Y so that systems can use them programmatically.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6kl9iv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754161459,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6lwofa",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "FuguSandwich",
            "can_mod_post": false,
            "created_utc": 1754177567,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 6,
            "author_fullname": "t2_r3xju",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "An \"agent\" is just an LLM running inside of a loop written in some programming language where the output keeps getting appended to the prompt for the next iteration of the loop.  Tool usage means the LLM can produce structured output (like JSON) that you can intercept in the loop and do something with (like call an API passing it as the parameters).  This stuff is nowhere near as complicated and mystical as people make it out to be.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lwofa",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;An &amp;quot;agent&amp;quot; is just an LLM running inside of a loop written in some programming language where the output keeps getting appended to the prompt for the next iteration of the loop.  Tool usage means the LLM can produce structured output (like JSON) that you can intercept in the loop and do something with (like call an API passing it as the parameters).  This stuff is nowhere near as complicated and mystical as people make it out to be.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6lwofa/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754177567,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6lmri7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Lesser-than",
            "can_mod_post": false,
            "created_utc": 1754174058,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 4,
            "author_fullname": "t2_98d256k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Its not as magic as the hype machine tries to make it out, in fact some apps call tools with llms that do not inherently support tool calling, its just parsing the output of the llm for the parameters to programs that are executed by your chat client. llms are just good at following directions and discovering intent of a users query.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lmri7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Its not as magic as the hype machine tries to make it out, in fact some apps call tools with llms that do not inherently support tool calling, its just parsing the output of the llm for the parameters to programs that are executed by your chat client. llms are just good at following directions and discovering intent of a users query.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6lmri7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754174058,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6jnlmn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Sea-Replacement7541",
            "can_mod_post": false,
            "created_utc": 1754150650,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 7,
            "author_fullname": "t2_hacyzki7e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Im not sure either. But I always viewed tool calling as a prompt where you tell the ai something like\n\n”you have access to a google search tool. Use it if you think it can benefit you in helping the user. You access the tool by answering [google: your question].”.\n\nThen when ppl here say model x is great for tool calling, they mean that the model is smart enough to understand your instructions and spit out a message that is in the right format.\n\nBut I could be wrong.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jnlmn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Im not sure either. But I always viewed tool calling as a prompt where you tell the ai something like&lt;/p&gt;\n\n&lt;p&gt;”you have access to a google search tool. Use it if you think it can benefit you in helping the user. You access the tool by answering [google: your question].”.&lt;/p&gt;\n\n&lt;p&gt;Then when ppl here say model x is great for tool calling, they mean that the model is smart enough to understand your instructions and spit out a message that is in the right format.&lt;/p&gt;\n\n&lt;p&gt;But I could be wrong.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6jnlmn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754150650,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6j5fcg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "2BucChuck",
            "can_mod_post": false,
            "created_utc": 1754144847,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 3,
            "author_fullname": "t2_7ztdve2r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Check the Claude 3.7 documentation - you can build your own version of this in other LLMs but it’s hard to get a reliable response from an LLM to parse and call a specific tool:  for example I tell my agent I’d like to know some current compliance related fact and it needs to search the web.  You’d have a program that evaluates the LLM initial response and decide whether it needs to do another pass using that search tools before responding to the user.  Most LLMs by default are only geared at user input -&gt; LLM -&gt; response to user and breaking that cycle to check for extra steps and tool uses before a response back to the user requires some additional coding to intercept and evaluate the answers",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6j5fcg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Check the Claude 3.7 documentation - you can build your own version of this in other LLMs but it’s hard to get a reliable response from an LLM to parse and call a specific tool:  for example I tell my agent I’d like to know some current compliance related fact and it needs to search the web.  You’d have a program that evaluates the LLM initial response and decide whether it needs to do another pass using that search tools before responding to the user.  Most LLMs by default are only geared at user input -&amp;gt; LLM -&amp;gt; response to user and breaking that cycle to check for extra steps and tool uses before a response back to the user requires some additional coding to intercept and evaluate the answers&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6j5fcg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754144847,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6kehj4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "toothpastespiders",
            "can_mod_post": false,
            "created_utc": 1754159191,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 3,
            "author_fullname": "t2_a2uzegb8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You've more or less got it. MCP (Model Context Protocol) is probably the most popular implementation right now.\n\n&gt;or tutorial to get a concrete idea?\n\nI haven't run their example, but I think the \"add(a: int, b: int)\" one on [factmcp's repository](https://github.com/jlowin/fastmcp) and their [documentation](https://gofastmcp.com/getting-started/welcome) is a good hello world type example of the non-LLM part of it. But like you guessed, on the LLM side it's basically watching for a token, calling an external script,  and looping back to process the results along with the prior context. \n\nThat said the token isn't really something you 'need'. I've been using my own janky version of tool use since the llama 2 days. If you're using a framework that gives you a fine level of control over everything it's not that hard to work with a text buffer and manually scan for a ton of different custom suggestions that mean you need to stop the generation and call a tool. In some ways that can work far better than the more polished methods since you're not locked into \"the\" one true way to do something - which a lot of LLMs aren't happy with. Or use more complex criteria to generate what to pass to the tool. Or even modify samplers before and after calls.\n\n&gt;If I'm using an offline model API via e.g. REST\n\nI think fastmcp is probably the easiest way to get started. Their docs aren't 'great' in terms of conveying things. At least in my opinion. But they're serviceable. LLama.cpp using the jinja and the openai api is pretty good with tool use even if a little finicky at times like with name length restrictions.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kehj4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;ve more or less got it. MCP (Model Context Protocol) is probably the most popular implementation right now.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;or tutorial to get a concrete idea?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I haven&amp;#39;t run their example, but I think the &amp;quot;add(a: int, b: int)&amp;quot; one on &lt;a href=\"https://github.com/jlowin/fastmcp\"&gt;factmcp&amp;#39;s repository&lt;/a&gt; and their &lt;a href=\"https://gofastmcp.com/getting-started/welcome\"&gt;documentation&lt;/a&gt; is a good hello world type example of the non-LLM part of it. But like you guessed, on the LLM side it&amp;#39;s basically watching for a token, calling an external script,  and looping back to process the results along with the prior context. &lt;/p&gt;\n\n&lt;p&gt;That said the token isn&amp;#39;t really something you &amp;#39;need&amp;#39;. I&amp;#39;ve been using my own janky version of tool use since the llama 2 days. If you&amp;#39;re using a framework that gives you a fine level of control over everything it&amp;#39;s not that hard to work with a text buffer and manually scan for a ton of different custom suggestions that mean you need to stop the generation and call a tool. In some ways that can work far better than the more polished methods since you&amp;#39;re not locked into &amp;quot;the&amp;quot; one true way to do something - which a lot of LLMs aren&amp;#39;t happy with. Or use more complex criteria to generate what to pass to the tool. Or even modify samplers before and after calls.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;If I&amp;#39;m using an offline model API via e.g. REST&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I think fastmcp is probably the easiest way to get started. Their docs aren&amp;#39;t &amp;#39;great&amp;#39; in terms of conveying things. At least in my opinion. But they&amp;#39;re serviceable. LLama.cpp using the jinja and the openai api is pretty good with tool use even if a little finicky at times like with name length restrictions.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6kehj4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754159191,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ks37v",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "HugeFinger8311",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6kpcsi",
                                "score": -4,
                                "author_fullname": "t2_t7arsuq7f",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Claude is actually pretty good at telling you exactly how it’s took use works and how outputs get injected with it. So todo lists are just a json object of entire todo, old versions remain in context even after updating (which personally I think can send it off course) and there are xml wrappers around system alerts for things like file changes on disk in folders being monitored which it receives before each response but not during internal working. So whilst yes LLMs absolutely can and do hallucinate - Claude is pretty good at letting you see some of the innards with targeted prompting.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ks37v",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Claude is actually pretty good at telling you exactly how it’s took use works and how outputs get injected with it. So todo lists are just a json object of entire todo, old versions remain in context even after updating (which personally I think can send it off course) and there are xml wrappers around system alerts for things like file changes on disk in folders being monitored which it receives before each response but not during internal working. So whilst yes LLMs absolutely can and do hallucinate - Claude is pretty good at letting you see some of the innards with targeted prompting.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfrq3v",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6ks37v/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754163755,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754163755,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": -4
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6kpcsi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "True_Program1542",
                      "can_mod_post": false,
                      "created_utc": 1754162824,
                      "send_replies": true,
                      "parent_id": "t1_n6j4xgd",
                      "score": 21,
                      "author_fullname": "t2_1fbjr5uac5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "And never be able to trust if that's a hallucination or the actual technical explanation. But do go on, condescension is so easy.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6kpcsi",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And never be able to trust if that&amp;#39;s a hallucination or the actual technical explanation. But do go on, condescension is so easy.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfrq3v",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6kpcsi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754162824,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 21
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6j4xgd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ExactSeaworthiness34",
            "can_mod_post": false,
            "created_utc": 1754144682,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 14,
            "author_fullname": "t2_ddhvy03s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The irony here is that you can copy paste this question into an LLM like Claude and you'll get your answer",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6j4xgd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The irony here is that you can copy paste this question into an LLM like Claude and you&amp;#39;ll get your answer&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6j4xgd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754144682,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 14
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ld0zc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "121507090301",
            "can_mod_post": false,
            "created_utc": 1754170691,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 2,
            "author_fullname": "t2_i0hlc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here's an example (I have cut some of the thinking for brevity):\n\n----------\n\nMe:\n\n\nPlease check the machine's system performance.\n\nAI:\n\n&lt;think&gt;\nOkay, the user is asking me to check the machine's system performance. Let me think about how to approach this.\n\nFirst, I need to recall what tools I have available. The user mentioned running Python programs. So maybe I can write a Python script that checks system performance metrics like CPU usage, memory usage, disk usage, etc.\n\n...\n\n&lt;tool_call&gt;\nimport psutil\nprint(f\"CPU Usage: {psutil.cpu_percent(interval=1)}%\")\nprint(f\"Memory Usage: {psutil.virtual_memory().percent}%\")\nprint(f\"Disk Usage: {psutil.disk_usage('/').percent}%\")\n&lt;/tool_call&gt;\n```Tool use response\nCPU Usage: 6.4%\nMemory Usage: 49.3%\nDisk Usage: 92.6%\n'''\n&lt;/think&gt;\n\nThe machine's system performance is as follows:\n\n- **CPU Usage**: 6.4%\n- **Memory Usage**: 49.3%\n- **Disk Usage**: 92.6%\n\nThe system appears to be running efficiently, with moderate CPU and memory usage, but the disk is nearly full (92.6%). You may want to consider cleaning up storage or adding more storage if the disk usage is a concern.\n\n----------\n\nSo, in short, I asked for something, the AI thought about what to do, made a program and because this program was between &lt;tool_call&gt; and &lt;/tool_call&gt; llama.cpp's llama-server stopped the conversation at &lt;/tool_call&gt; and returned it to my \"UI program\" that took the code inside &lt;tool_call&gt; and &lt;/tool_call&gt; and ran it, saving whatever was printed to be returned to the AI that then took the answer and gave me a response based on it. \n\nIn my case the \"tool use\" is literally making and running python programs, although normally people just give some pre-made programs to the AI to \"call\" with some arguments when needed.\n\nIn my case at least I made it like this mostly because I thought it's cooler but also so I don't have to make/prepare the programs for the AI as they can be made when needed (if the AI is capable of it, of course). So when I needed to extract a file in a format I didn't know a few days ago I just gave the AI the path to the file and then the AI made a python script to extract it for me, or when I needed a few folders to sort a few things I just said more or less what I wanted and the AI came up with the name of the folders and made them for me with python.\n\nThis is how I did it but I'm sure there are better ways of doing it too. Hope this clears things up...\n\nEdit: Forgot to say that I did give the AI instructions about how to make and use the programs as well in the system prompt.",
            "edited": 1754171328,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ld0zc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s an example (I have cut some of the thinking for brevity):&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Me:&lt;/p&gt;\n\n&lt;p&gt;Please check the machine&amp;#39;s system performance.&lt;/p&gt;\n\n&lt;p&gt;AI:&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;think&amp;gt;\nOkay, the user is asking me to check the machine&amp;#39;s system performance. Let me think about how to approach this.&lt;/p&gt;\n\n&lt;p&gt;First, I need to recall what tools I have available. The user mentioned running Python programs. So maybe I can write a Python script that checks system performance metrics like CPU usage, memory usage, disk usage, etc.&lt;/p&gt;\n\n&lt;p&gt;...&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;tool_call&amp;gt;\nimport psutil\nprint(f&amp;quot;CPU Usage: {psutil.cpu_percent(interval=1)}%&amp;quot;)\nprint(f&amp;quot;Memory Usage: {psutil.virtual_memory().percent}%&amp;quot;)\nprint(f&amp;quot;Disk Usage: {psutil.disk_usage(&amp;#39;/&amp;#39;).percent}%&amp;quot;)\n&amp;lt;/tool_call&amp;gt;\n```Tool use response\nCPU Usage: 6.4%\nMemory Usage: 49.3%\nDisk Usage: 92.6%\n&amp;#39;&amp;#39;&amp;#39;\n&amp;lt;/think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;The machine&amp;#39;s system performance is as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU Usage&lt;/strong&gt;: 6.4%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;: 49.3%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Disk Usage&lt;/strong&gt;: 92.6%&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The system appears to be running efficiently, with moderate CPU and memory usage, but the disk is nearly full (92.6%). You may want to consider cleaning up storage or adding more storage if the disk usage is a concern.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;So, in short, I asked for something, the AI thought about what to do, made a program and because this program was between &amp;lt;tool_call&amp;gt; and &amp;lt;/tool_call&amp;gt; llama.cpp&amp;#39;s llama-server stopped the conversation at &amp;lt;/tool_call&amp;gt; and returned it to my &amp;quot;UI program&amp;quot; that took the code inside &amp;lt;tool_call&amp;gt; and &amp;lt;/tool_call&amp;gt; and ran it, saving whatever was printed to be returned to the AI that then took the answer and gave me a response based on it. &lt;/p&gt;\n\n&lt;p&gt;In my case the &amp;quot;tool use&amp;quot; is literally making and running python programs, although normally people just give some pre-made programs to the AI to &amp;quot;call&amp;quot; with some arguments when needed.&lt;/p&gt;\n\n&lt;p&gt;In my case at least I made it like this mostly because I thought it&amp;#39;s cooler but also so I don&amp;#39;t have to make/prepare the programs for the AI as they can be made when needed (if the AI is capable of it, of course). So when I needed to extract a file in a format I didn&amp;#39;t know a few days ago I just gave the AI the path to the file and then the AI made a python script to extract it for me, or when I needed a few folders to sort a few things I just said more or less what I wanted and the AI came up with the name of the folders and made them for me with python.&lt;/p&gt;\n\n&lt;p&gt;This is how I did it but I&amp;#39;m sure there are better ways of doing it too. Hope this clears things up...&lt;/p&gt;\n\n&lt;p&gt;Edit: Forgot to say that I did give the AI instructions about how to make and use the programs as well in the system prompt.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6ld0zc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754170691,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6o7u4q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "j_miskov",
            "can_mod_post": false,
            "created_utc": 1754216667,
            "send_replies": true,
            "parent_id": "t3_1mfrq3v",
            "score": 1,
            "author_fullname": "t2_1gq3sva6gj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The interesting discussion is, why is tool calling so popular and why the model itself needs to support it?\n\nThe LLM is supposed to generate text, not to be in control of the flow. The framework offers tools and dumbly executes the calls and feeds the results back into LLM. The LLM on the other hand is trained to stop after tool call is specified and to reason about the results. Does it handle the failure or warnings in the output? Does it retry the call if not satisfied with the result? It depends on the model and the training it receives so you can hardly just switch away to another tool-calling model and expect it to handle the control flow in the same way.\n\nThe current mechanism in place is just too basic and doesn't scale. Anything more complex than the basic case, you have to hack it in and work around the existing design and against the training. Imagine you want multiple tools being executing in parallel, and the generation can only continue after all outputs are collected. Or maybe the called tools can give you realtime progress and some intermediate low fidelity results. Or that you want to insert another block to review the tool call for security before executing the call. These would be simpler to execute if LLM wasn't \"in charge\". I don't mean literally in charge of course, more that the LLM was trained to give a command and to expect results. I haven't tested it but I have strong suspicions that any deflection from this flow will worsen the performance; for example if you tried to roll out your own tool calling and made it too similar to supported one, the LLM would ignore your instructions and slide into the groove that was imprinted during the training.\n\nIMO the focus tool calling during training is misguided. It is a sign of bad design (a code smell) that the tool calling needs to be implemented both within the framework and also inside the model. The model makers should have instead focused on better instruction-following and better reasoning skills; and the tool use should be implemented by the LLM client. The client could automatically prompt-inject the available commands and the desired text format when a tool is supposed to be called. This could be much more advanced than what &lt;tool_call&gt; can express. It could include multiple parallel calls, error handling instructions, expected call duration, cost of the call, whatever the client supports. Similarly the result injection would be much more flexible.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6o7u4q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The interesting discussion is, why is tool calling so popular and why the model itself needs to support it?&lt;/p&gt;\n\n&lt;p&gt;The LLM is supposed to generate text, not to be in control of the flow. The framework offers tools and dumbly executes the calls and feeds the results back into LLM. The LLM on the other hand is trained to stop after tool call is specified and to reason about the results. Does it handle the failure or warnings in the output? Does it retry the call if not satisfied with the result? It depends on the model and the training it receives so you can hardly just switch away to another tool-calling model and expect it to handle the control flow in the same way.&lt;/p&gt;\n\n&lt;p&gt;The current mechanism in place is just too basic and doesn&amp;#39;t scale. Anything more complex than the basic case, you have to hack it in and work around the existing design and against the training. Imagine you want multiple tools being executing in parallel, and the generation can only continue after all outputs are collected. Or maybe the called tools can give you realtime progress and some intermediate low fidelity results. Or that you want to insert another block to review the tool call for security before executing the call. These would be simpler to execute if LLM wasn&amp;#39;t &amp;quot;in charge&amp;quot;. I don&amp;#39;t mean literally in charge of course, more that the LLM was trained to give a command and to expect results. I haven&amp;#39;t tested it but I have strong suspicions that any deflection from this flow will worsen the performance; for example if you tried to roll out your own tool calling and made it too similar to supported one, the LLM would ignore your instructions and slide into the groove that was imprinted during the training.&lt;/p&gt;\n\n&lt;p&gt;IMO the focus tool calling during training is misguided. It is a sign of bad design (a code smell) that the tool calling needs to be implemented both within the framework and also inside the model. The model makers should have instead focused on better instruction-following and better reasoning skills; and the tool use should be implemented by the LLM client. The client could automatically prompt-inject the available commands and the desired text format when a tool is supposed to be called. This could be much more advanced than what &amp;lt;tool_call&amp;gt; can express. It could include multiple parallel calls, error handling instructions, expected call duration, cost of the call, whatever the client supports. Similarly the result injection would be much more flexible.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/n6o7u4q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754216667,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfrq3v",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]