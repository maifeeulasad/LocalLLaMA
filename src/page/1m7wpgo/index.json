[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm a beginner building a RAG system and running into a strange issue with large Excel files.\n\n**The problem:**  \nWhen I ingest large Excel files, the system appears to extract and process the data correctly during ingestion. However, when I later query the system for specific information from those files, it responds as if the data doesn’t exist.\n\n**Details of my tech stack and setup:**\n\n* **Backend:**\n   * Django\n* **RAG/LLM Orchestration:**\n   * LangChain for managing LLM calls, embeddings, and retrieval\n* **Vector Store:**\n   * Qdrant (accessed via langchain-qdrant + qdrant-client)\n* **File Parsing:**\n   * Excel/CSV: `pandas`, `openpyxl`\n* **LLM Details:**\n* **Chat Model:**\n   * `gpt-4o`\n* **Embedding Model:**\n   * `text-embedding-ada-002`",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "RAG project fails to retrieve info from large Excel files – data ingested but not found at query time. Need help debugging.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m7wpgo",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1ko7k822rj",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753337622,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a beginner building a RAG system and running into a strange issue with large Excel files.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br/&gt;\nWhen I ingest large Excel files, the system appears to extract and process the data correctly during ingestion. However, when I later query the system for specific information from those files, it responds as if the data doesn’t exist.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Details of my tech stack and setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Django&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAG/LLM Orchestration:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;LangChain for managing LLM calls, embeddings, and retrieval&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qdrant (accessed via langchain-qdrant + qdrant-client)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;File Parsing:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Excel/CSV: &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;openpyxl&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;LLM Details:&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Chat Model:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Embedding Model:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;text-embedding-ada-002&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m7wpgo",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "One-Will5139",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7wpgo/rag_project_fails_to_retrieve_info_from_large/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7wpgo/rag_project_fails_to_retrieve_info_from_large/",
            "subreddit_subscribers": 503759,
            "created_utc": 1753337622,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4uwi05",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Additional-Bet7074",
            "can_mod_post": false,
            "created_utc": 1753340518,
            "send_replies": true,
            "parent_id": "t3_1m7wpgo",
            "score": 1,
            "author_fullname": "t2_wr4fiywpt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you are vectorizing the tabular data the retrieving it?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4uwi05",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you are vectorizing the tabular data the retrieving it?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7wpgo/rag_project_fails_to_retrieve_info_from_large/n4uwi05/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753340518,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7wpgo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4v4akm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wfgy_engine",
            "can_mod_post": false,
            "created_utc": 1753344906,
            "send_replies": true,
            "parent_id": "t3_1m7wpgo",
            "score": 1,
            "author_fullname": "t2_1tgp8l87vk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Been there. Looks like your data *thinks* it got ingested, but at query time it’s like, “what Excel file?” Classic RAG ghosting behavior.\n\nHere’s what might be going sideways:\n\n1. **You indexed ghosts** Just because the ingestion step didn’t throw doesn’t mean the chunks got embedded *properly*. Especially with Excel + `pandas` \\+ `openpyxl`, it’s easy to end up indexing empty rows, titles, or weird merged cells. Dump a few entries directly from Qdrant and see what’s *actually* in there.\n2. **Your chunking logic is lying to you** Are you feeding entire rows as single chunks? Or dumping the whole sheet as one blob? If you didn’t aggressively control your chunk granularity, your model probably made spaghetti out of it. Long tables need *manual slicing*—models aren’t great at auto-chunking structured data.\n3. **Mismatched embeddings** You’re using `text-embedding-ada-002`—make sure you used the *same model* for both indexing and query. Mixing up models (e.g. Ada index vs OpenAI default search embedding) gives you garbage recall with 0 errors. No one warns you. You just suffer.\n4. **GPT-4o is too confident** Sometimes your retrieval fails silently, and the LLM just… makes something up. Which looks like a “retrieval miss,” but is actually a *hallucination overwrite*. Test your queries *without* passing them to the LLM—just do a raw similarity search and print your top hits. You’ll learn a lot.\n\nDebug tip:  \nBefore querying, try manually embedding your prompt and doing a vector search directly in Qdrant. If nothing relevant comes back, it’s not the LLM’s fault—it’s your chunker or embedder being lazy.\n\nAlso: don’t trust the success logs from LangChain. They lie to make you feel better.\n\nLet me know if you want a brutal Q&amp;A checklist for Excel-based ingestion. Seen this too many times. You’re not alone, friend.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4v4akm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Been there. Looks like your data &lt;em&gt;thinks&lt;/em&gt; it got ingested, but at query time it’s like, “what Excel file?” Classic RAG ghosting behavior.&lt;/p&gt;\n\n&lt;p&gt;Here’s what might be going sideways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;You indexed ghosts&lt;/strong&gt; Just because the ingestion step didn’t throw doesn’t mean the chunks got embedded &lt;em&gt;properly&lt;/em&gt;. Especially with Excel + &lt;code&gt;pandas&lt;/code&gt; + &lt;code&gt;openpyxl&lt;/code&gt;, it’s easy to end up indexing empty rows, titles, or weird merged cells. Dump a few entries directly from Qdrant and see what’s &lt;em&gt;actually&lt;/em&gt; in there.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Your chunking logic is lying to you&lt;/strong&gt; Are you feeding entire rows as single chunks? Or dumping the whole sheet as one blob? If you didn’t aggressively control your chunk granularity, your model probably made spaghetti out of it. Long tables need &lt;em&gt;manual slicing&lt;/em&gt;—models aren’t great at auto-chunking structured data.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mismatched embeddings&lt;/strong&gt; You’re using &lt;code&gt;text-embedding-ada-002&lt;/code&gt;—make sure you used the &lt;em&gt;same model&lt;/em&gt; for both indexing and query. Mixing up models (e.g. Ada index vs OpenAI default search embedding) gives you garbage recall with 0 errors. No one warns you. You just suffer.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GPT-4o is too confident&lt;/strong&gt; Sometimes your retrieval fails silently, and the LLM just… makes something up. Which looks like a “retrieval miss,” but is actually a &lt;em&gt;hallucination overwrite&lt;/em&gt;. Test your queries &lt;em&gt;without&lt;/em&gt; passing them to the LLM—just do a raw similarity search and print your top hits. You’ll learn a lot.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Debug tip:&lt;br/&gt;\nBefore querying, try manually embedding your prompt and doing a vector search directly in Qdrant. If nothing relevant comes back, it’s not the LLM’s fault—it’s your chunker or embedder being lazy.&lt;/p&gt;\n\n&lt;p&gt;Also: don’t trust the success logs from LangChain. They lie to make you feel better.&lt;/p&gt;\n\n&lt;p&gt;Let me know if you want a brutal Q&amp;amp;A checklist for Excel-based ingestion. Seen this too many times. You’re not alone, friend.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7wpgo/rag_project_fails_to_retrieve_info_from_large/n4v4akm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753344906,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7wpgo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]