[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi. Again a \"non-local\" question, but maybe also relevant for local use.\n\nDo you think the current per-token prices of inference service providers are \"dumped\" (is that the right word?) or somehow sustainable in the long term? How do you think the prices will converge after commoditisation, if it will happen?\n\nThanks",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Isn't price per token of LLMs too low?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjud6n",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.62,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_127kho",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754554675,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Again a &amp;quot;non-local&amp;quot; question, but maybe also relevant for local use.&lt;/p&gt;\n\n&lt;p&gt;Do you think the current per-token prices of inference service providers are &amp;quot;dumped&amp;quot; (is that the right word?) or somehow sustainable in the long term? How do you think the prices will converge after commoditisation, if it will happen?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mjud6n",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ihatebeinganonymous",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjud6n/isnt_price_per_token_of_llms_too_low/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjud6n/isnt_price_per_token_of_llms_too_low/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754554675,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7drzpl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "05032-MendicantBias",
            "can_mod_post": false,
            "created_utc": 1754555175,
            "send_replies": true,
            "parent_id": "t3_1mjud6n",
            "score": 2,
            "author_fullname": "t2_6id3lwou",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Subsidized is the right word.\n\nThe answer is yes. It's the same wonnabe monopolistic play that Silicon Valley venture capital loves: make a service, subsidize it to push competitor out of business, then raise prices.\n\nRight now it's not working because local models aren't that far off proprietary models, despite all the hundreds of billions Silicon Valley is throwing at the problem. OpenAI released their first open source model since GPT 2 because it was silly, with Qwen, Deepseek, Hunyuian, Kimi, etc... being open for everyone to host.\n\nIt's also likely it will not work in the future unless Silicon Valley figures out AGI and ASI and can monetize that and prevent other from developing that. It's why you see closed AI labs going all in on spending all the money to build super intelligence teams.\n\nFor contrast, Chinese especially are making open source models. Tencent makes money from publishing games. Their calculus is they make open source, free, models, that their dev studio use to make more, better games, faster and cheaper, so the return for them is in using the tools, not selling subsidized access.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7drzpl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Subsidized is the right word.&lt;/p&gt;\n\n&lt;p&gt;The answer is yes. It&amp;#39;s the same wonnabe monopolistic play that Silicon Valley venture capital loves: make a service, subsidize it to push competitor out of business, then raise prices.&lt;/p&gt;\n\n&lt;p&gt;Right now it&amp;#39;s not working because local models aren&amp;#39;t that far off proprietary models, despite all the hundreds of billions Silicon Valley is throwing at the problem. OpenAI released their first open source model since GPT 2 because it was silly, with Qwen, Deepseek, Hunyuian, Kimi, etc... being open for everyone to host.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also likely it will not work in the future unless Silicon Valley figures out AGI and ASI and can monetize that and prevent other from developing that. It&amp;#39;s why you see closed AI labs going all in on spending all the money to build super intelligence teams.&lt;/p&gt;\n\n&lt;p&gt;For contrast, Chinese especially are making open source models. Tencent makes money from publishing games. Their calculus is they make open source, free, models, that their dev studio use to make more, better games, faster and cheaper, so the return for them is in using the tools, not selling subsidized access.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjud6n/isnt_price_per_token_of_llms_too_low/n7drzpl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754555175,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjud6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ennbh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BobbyL2k",
            "can_mod_post": false,
            "created_utc": 1754569599,
            "send_replies": true,
            "parent_id": "t3_1mjud6n",
            "score": 1,
            "author_fullname": "t2_ghoyg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "LLM inference economy of scale is insane. The main bottleneck is memory bandwidth because the model has to move into the chip for a single forward pass, one token predicted.\n\nSo if you can batch hundreds of requests into a single forward pass, you can push disproportionately massive amount of tokens on a high-end hardware.\n\nMost LLM inference on cloud are on either NVIDIA AI Chips (A100, H100, Bx00, GB300) or specialized hardware, why? Why not buy a bunch of consumer grade GPUs like us on LocalLlama? Simple, it’s cheaper. Given that you need the capacity, and fully load it.\n\nSo no, it’s not subsidized. Maybe ones that tell you, we will train on your data; those are subsidized by your data. But the providers with hardware that host open models on OpenRouter, or Azure selling tokens to enterprises with guaranteed zero data retention? Those are real costs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ennbh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LLM inference economy of scale is insane. The main bottleneck is memory bandwidth because the model has to move into the chip for a single forward pass, one token predicted.&lt;/p&gt;\n\n&lt;p&gt;So if you can batch hundreds of requests into a single forward pass, you can push disproportionately massive amount of tokens on a high-end hardware.&lt;/p&gt;\n\n&lt;p&gt;Most LLM inference on cloud are on either NVIDIA AI Chips (A100, H100, Bx00, GB300) or specialized hardware, why? Why not buy a bunch of consumer grade GPUs like us on LocalLlama? Simple, it’s cheaper. Given that you need the capacity, and fully load it.&lt;/p&gt;\n\n&lt;p&gt;So no, it’s not subsidized. Maybe ones that tell you, we will train on your data; those are subsidized by your data. But the providers with hardware that host open models on OpenRouter, or Azure selling tokens to enterprises with guaranteed zero data retention? Those are real costs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjud6n/isnt_price_per_token_of_llms_too_low/n7ennbh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754569599,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjud6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dtb4d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LostMitosis",
            "can_mod_post": false,
            "created_utc": 1754555943,
            "send_replies": true,
            "parent_id": "t3_1mjud6n",
            "score": 1,
            "author_fullname": "t2_n7j3v3oq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We could also have been successfully brainwashed to believe it has to be expensive. The west has played this game for long, with popular electronics being a good example.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dtb4d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We could also have been successfully brainwashed to believe it has to be expensive. The west has played this game for long, with popular electronics being a good example.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjud6n/isnt_price_per_token_of_llms_too_low/n7dtb4d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754555943,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjud6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]