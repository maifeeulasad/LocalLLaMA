[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I was contemplating buying an RTX PRO 6000 Blackwell, but after conducting some research on [YouTube](https://youtu.be/bAao58hXo9w?si=F1vCh4gSJxrgYqo2&amp;t=832), I was disappointed with its performance. The prompt processing speed didn't meet my expectations, and token generation decreased notably when context was added. It didn't seem to outperform regular consumer GPUs, which left me wondering why it's so expensive. Is this normal behavior, or was the YouTuber not using it properly?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Dissatisfied with how the RTX PRO 6000 Blackwell is performing during AI inference",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9dur7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_qhk9kpc",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753484979,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was contemplating buying an RTX PRO 6000 Blackwell, but after conducting some research on &lt;a href=\"https://youtu.be/bAao58hXo9w?si=F1vCh4gSJxrgYqo2&amp;amp;t=832\"&gt;YouTube&lt;/a&gt;, I was disappointed with its performance. The prompt processing speed didn&amp;#39;t meet my expectations, and token generation decreased notably when context was added. It didn&amp;#39;t seem to outperform regular consumer GPUs, which left me wondering why it&amp;#39;s so expensive. Is this normal behavior, or was the YouTuber not using it properly?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?auto=webp&amp;s=2de38e488ed6896adcf13bb7582775e8e37a7e8f",
                    "width": 480,
                    "height": 360
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8af79e10d322a89c0acdc8c1e95b10d83feae63",
                      "width": 108,
                      "height": 81
                    },
                    {
                      "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be02e726d7db29c9b5cf7e4489a55edc87699ca8",
                      "width": 216,
                      "height": 162
                    },
                    {
                      "url": "https://external-preview.redd.it/Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b878f3cb454ef94f2549f8cf1ea5da83fe5038ba",
                      "width": 320,
                      "height": 240
                    }
                  ],
                  "variants": {},
                  "id": "Bg367mTzk4i869SXNHiikTP1RjxNeVibkNvwuMAACGo"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m9dur7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "d00m_sayer",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753484979,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n58kk3f",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "GPTrack_ai",
                      "can_mod_post": false,
                      "created_utc": 1753521732,
                      "send_replies": true,
                      "parent_id": "t1_n56fu7v",
                      "score": 1,
                      "author_fullname": "t2_1tpuoj72sa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Are the folks still using windows , even though linux is soooooooooooooooooooo much better? Are the folks still using windows , even after SARS-Covid-2? Are .... the list goes on an on and on....",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n58kk3f",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are the folks still using windows , even though linux is soooooooooooooooooooo much better? Are the folks still using windows , even after SARS-Covid-2? Are .... the list goes on an on and on....&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9dur7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n58kk3f/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753521732,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n58yewl",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "nostriluu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n56k1lp",
                                          "score": 1,
                                          "author_fullname": "t2_l10vk",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "This is a helpful guide. I use Linux but use containers whenever possible. Just wanted to correct one point, containers are not VMs; [https://www.reddit.com/r/compsci/comments/f2d3a6/eli5\\_what\\_is\\_the\\_difference\\_between\\_a\\_container/](https://www.reddit.com/r/compsci/comments/f2d3a6/eli5_what_is_the_difference_between_a_container/)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n58yewl",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a helpful guide. I use Linux but use containers whenever possible. Just wanted to correct one point, containers are not VMs; &lt;a href=\"https://www.reddit.com/r/compsci/comments/f2d3a6/eli5_what_is_the_difference_between_a_container/\"&gt;https://www.reddit.com/r/compsci/comments/f2d3a6/eli5_what_is_the_difference_between_a_container/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m9dur7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n58yewl/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753529304,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753529304,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n56k1lp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "DorphinPack",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n56h04j",
                                "score": 6,
                                "author_fullname": "t2_zebuyjw9s",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "CUDA may perform well but the inference engine that issues the CUDA instructions to the GPU still matters.\n\nYou have a bleeding edge card and are running an inference engine aimed at supporting a large number of much older configurations.\n\nTry looking for native windows engines that target the bleeding edge. You’ll have to dig a bit.\n\nI can tell you for a fact on the Linux side you have a lot of options.\n\nIt may sound daunting, but WSL2+Docker is your friend if you need to stick on Windows. \n\nWSL2 uses a special Linux VM that can share the GPU with Windows so you don’t have to fiddle to try native engines after setting this up. Nvidia has a guide for WSL2. They also have one for container toolkit you should go ahead and knock out inside the WSL2 environment (you can follow it like it’s a regular Linux system once you do the WSL2 CUDA setup in the previous guide).\n\nDocker provides isolated mini-VMs called containers that launch from an image. Find an image that has the latest greatest engine of your choice (with CUDA) and you don’t have to fuss with installing software or swapping versions.  Update by pulling the new image and relaunching.  Also, inference engines avoid the biggest newbie trap in Docker — persistent storage. Your engine doesn’t need to keep a database or any files. You just have to tell Docker to attach the folder containing the model inside the container.\n\n(This final bit is more of a hint at the future to save on wordcount — feel free to DM me about any of this but ESPECIALLY the following if you need it)\n\nShould someone say “ah you’ve got a Blackwell? You’ll need to build it yourself with XYZ flags…” DON’T PANIC. You don’t need to risk your WSL2 environment installing packages and fiddling trying to get the software built. Instead, you can modify the existing Dockerfile and build your own custom container images right on your machine.\n\nContainers are THE way to balance stability with fast moving software.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n56k1lp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;CUDA may perform well but the inference engine that issues the CUDA instructions to the GPU still matters.&lt;/p&gt;\n\n&lt;p&gt;You have a bleeding edge card and are running an inference engine aimed at supporting a large number of much older configurations.&lt;/p&gt;\n\n&lt;p&gt;Try looking for native windows engines that target the bleeding edge. You’ll have to dig a bit.&lt;/p&gt;\n\n&lt;p&gt;I can tell you for a fact on the Linux side you have a lot of options.&lt;/p&gt;\n\n&lt;p&gt;It may sound daunting, but WSL2+Docker is your friend if you need to stick on Windows. &lt;/p&gt;\n\n&lt;p&gt;WSL2 uses a special Linux VM that can share the GPU with Windows so you don’t have to fiddle to try native engines after setting this up. Nvidia has a guide for WSL2. They also have one for container toolkit you should go ahead and knock out inside the WSL2 environment (you can follow it like it’s a regular Linux system once you do the WSL2 CUDA setup in the previous guide).&lt;/p&gt;\n\n&lt;p&gt;Docker provides isolated mini-VMs called containers that launch from an image. Find an image that has the latest greatest engine of your choice (with CUDA) and you don’t have to fuss with installing software or swapping versions.  Update by pulling the new image and relaunching.  Also, inference engines avoid the biggest newbie trap in Docker — persistent storage. Your engine doesn’t need to keep a database or any files. You just have to tell Docker to attach the folder containing the model inside the container.&lt;/p&gt;\n\n&lt;p&gt;(This final bit is more of a hint at the future to save on wordcount — feel free to DM me about any of this but ESPECIALLY the following if you need it)&lt;/p&gt;\n\n&lt;p&gt;Should someone say “ah you’ve got a Blackwell? You’ll need to build it yourself with XYZ flags…” DON’T PANIC. You don’t need to risk your WSL2 environment installing packages and fiddling trying to get the software built. Instead, you can modify the existing Dockerfile and build your own custom container images right on your machine.&lt;/p&gt;\n\n&lt;p&gt;Containers are THE way to balance stability with fast moving software.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9dur7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56k1lp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753488159,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753488159,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n56kp4q",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DorphinPack",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n56h04j",
                                "score": 2,
                                "author_fullname": "t2_zebuyjw9s",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "P.S. - I’m sure there are bleeding edge Windows users who will have better info there.\n\nLinux def has the biggest user base for that generation, though. I am no wizard, just curious and motivated. Following the big user base often helps avoid problems that are out of my depth.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n56kp4q",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;P.S. - I’m sure there are bleeding edge Windows users who will have better info there.&lt;/p&gt;\n\n&lt;p&gt;Linux def has the biggest user base for that generation, though. I am no wizard, just curious and motivated. Following the big user base often helps avoid problems that are out of my depth.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9dur7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56kp4q/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753488392,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753488392,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n56ronk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "panchovix",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n56h04j",
                                "score": 1,
                                "author_fullname": "t2_j1kqr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It is ok but on native Linux is still better (sadly or not, depending on your liking). WSL2 is better but still behind native Linux.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n56ronk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 405B"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is ok but on native Linux is still better (sadly or not, depending on your liking). WSL2 is better but still behind native Linux.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9dur7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56ronk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753490907,
                                "author_flair_text": "Llama 405B",
                                "treatment_tags": [],
                                "created_utc": 1753490907,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n56h04j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "d00m_sayer",
                      "can_mod_post": false,
                      "created_utc": 1753487104,
                      "send_replies": true,
                      "parent_id": "t1_n56fu7v",
                      "score": -4,
                      "author_fullname": "t2_qhk9kpc",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I thought CUDA works well on windows ?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n56h04j",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I thought CUDA works well on windows ?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9dur7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56h04j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753487104,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n56fu7v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "koushd",
            "can_mod_post": false,
            "created_utc": 1753486709,
            "send_replies": true,
            "parent_id": "t3_1m9dur7",
            "score": 13,
            "author_fullname": "t2_4yut6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have the card. it's much faster than the YouTuber guy running it in lmstudio on windows. not a great combo.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n56fu7v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have the card. it&amp;#39;s much faster than the YouTuber guy running it in lmstudio on windows. not a great combo.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56fu7v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753486709,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9dur7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 13
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n56c9k9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1753485497,
            "send_replies": true,
            "parent_id": "t3_1m9dur7",
            "score": 10,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think that the selling point is the VRAM, not the speed",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n56c9k9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think that the selling point is the VRAM, not the speed&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56c9k9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753485497,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m9dur7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n58g4px",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "NebulaBetter",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n58da21",
                                "score": 2,
                                "author_fullname": "t2_2m055nvr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Jensen appreciate our commitment to let him buy new fashion jackets",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n58g4px",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Jensen appreciate our commitment to let him buy new fashion jackets&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m9dur7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n58g4px/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753519080,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753519080,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n58da21",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MelodicRecognition7",
                      "can_mod_post": false,
                      "created_utc": 1753517369,
                      "send_replies": true,
                      "parent_id": "t1_n56xays",
                      "score": 2,
                      "author_fullname": "t2_1eex9ug5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;  unless you’re running two or three of these cards, you end up stuck in this awkward middle ground \n\n&gt; for LLMs, it doesn’t really stand out in a meaningful way, imo.\n\nexactly, that's why I'm getting a second one...",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n58da21",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;unless you’re running two or three of these cards, you end up stuck in this awkward middle ground &lt;/p&gt;\n\n&lt;p&gt;for LLMs, it doesn’t really stand out in a meaningful way, imo.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;exactly, that&amp;#39;s why I&amp;#39;m getting a second one...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9dur7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n58da21/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753517369,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n56xays",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "NebulaBetter",
            "can_mod_post": false,
            "created_utc": 1753492977,
            "send_replies": true,
            "parent_id": "t3_1m9dur7",
            "score": 3,
            "author_fullname": "t2_2m055nvr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have the card too, and I mainly use it for image and video diffusion, plus some other tools that require a lot of VRAM. For that kind of workload, it’s honestly excellent.  \nBut when it comes to LLMs, unless you’re running two or three of these cards, you end up stuck in this awkward middle ground.. not enough VRAM for the biggest models, but quite enough than what’s needed for the smaller ones. So for LLMs, it doesn’t really stand out in a meaningful way, imo.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n56xays",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have the card too, and I mainly use it for image and video diffusion, plus some other tools that require a lot of VRAM. For that kind of workload, it’s honestly excellent.&lt;br/&gt;\nBut when it comes to LLMs, unless you’re running two or three of these cards, you end up stuck in this awkward middle ground.. not enough VRAM for the biggest models, but quite enough than what’s needed for the smaller ones. So for LLMs, it doesn’t really stand out in a meaningful way, imo.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56xays/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753492977,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9dur7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n56lrsg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "____vladrad",
                      "can_mod_post": false,
                      "created_utc": 1753488773,
                      "send_replies": true,
                      "parent_id": "t1_n56dsns",
                      "score": 1,
                      "author_fullname": "t2_u6i8a0ay",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Also I would not run llamacpp or ollama. Vllm is enterprise ready for cards like this. The difference is big between the two. \n\nWith two I hit 75 tokens a sec at 131k context for qwen 235",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n56lrsg",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also I would not run llamacpp or ollama. Vllm is enterprise ready for cards like this. The difference is big between the two. &lt;/p&gt;\n\n&lt;p&gt;With two I hit 75 tokens a sec at 131k context for qwen 235&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9dur7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56lrsg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753488773,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n56dsns",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Prestigious_Thing797",
            "can_mod_post": false,
            "created_utc": 1753486014,
            "send_replies": true,
            "parent_id": "t3_1m9dur7",
            "score": 7,
            "author_fullname": "t2_1anh6qztwr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The youtuber has misconfigured something (likely has some CPU offloading). I've run Qwen 32B on this and get drastically better speeds even at float16",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n56dsns",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The youtuber has misconfigured something (likely has some CPU offloading). I&amp;#39;ve run Qwen 32B on this and get drastically better speeds even at float16&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56dsns/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753486014,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9dur7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n58d82q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1753517337,
            "send_replies": true,
            "parent_id": "t3_1m9dur7",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "6000 is just the 5090 with 3x VRAM, not 3x power",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n58d82q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;6000 is just the 5090 with 3x VRAM, not 3x power&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n58d82q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753517337,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9dur7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n56n6ws",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1753489280,
            "send_replies": true,
            "parent_id": "t3_1m9dur7",
            "score": 0,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GPU performance is a blend of many factors. Number of kernel launches, data movement between levels of the memory hierarchy, control flow divergence, occupancy, register usage, register spilling, arithmetic intensity and instruction choice etc.\n\n\n Lot of these conflict. For example the popular method of optimising performance by spamming kernel fusions is highly, highly problematic because larger kernels are more likely to cause register spills and crucially are less likely to fit or distribute nicely across the smaller but faster levels at the bottom of the memory hierarchy.\n\n\nAs a different example, trying to optimise by maximising occupancy at all costs (also a common mistake) places enormous demands on the memory management system. Achieving a high occupancy can require very specific data movements at very specific times and this can end up being less robust to outliers in the path space of the execution graph (this means rare routes in the program.) It can be better to optimise for a system which gives good results most of the time but handles outliers more gracefully.\n\n\nAll of this is made worse by the fact that machine learning models cover an exceptionally wide range of execution dynamics including some of the most extreme ends, for example machine learning can have both models that are extremely low arithmetic intensity or extremely high. To become good at optimising over both you essentially have to become good at the full spectrum.\n\n\nIt is an exceptionally hard task and the learning materials out there are actually zero for a lot of this. Nvidia doesn’t even document many PTX instructions. The conclusion of all of this is that GPUs being used properly (i.e. optimally) is very difficult at the best of times, and essentially never occurs with typical users. For this reason you don’t necessarily need to be concerned if you see lots of people showing poor performance, the actual optimised performance can be far higher.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n56n6ws",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GPU performance is a blend of many factors. Number of kernel launches, data movement between levels of the memory hierarchy, control flow divergence, occupancy, register usage, register spilling, arithmetic intensity and instruction choice etc.&lt;/p&gt;\n\n&lt;p&gt;Lot of these conflict. For example the popular method of optimising performance by spamming kernel fusions is highly, highly problematic because larger kernels are more likely to cause register spills and crucially are less likely to fit or distribute nicely across the smaller but faster levels at the bottom of the memory hierarchy.&lt;/p&gt;\n\n&lt;p&gt;As a different example, trying to optimise by maximising occupancy at all costs (also a common mistake) places enormous demands on the memory management system. Achieving a high occupancy can require very specific data movements at very specific times and this can end up being less robust to outliers in the path space of the execution graph (this means rare routes in the program.) It can be better to optimise for a system which gives good results most of the time but handles outliers more gracefully.&lt;/p&gt;\n\n&lt;p&gt;All of this is made worse by the fact that machine learning models cover an exceptionally wide range of execution dynamics including some of the most extreme ends, for example machine learning can have both models that are extremely low arithmetic intensity or extremely high. To become good at optimising over both you essentially have to become good at the full spectrum.&lt;/p&gt;\n\n&lt;p&gt;It is an exceptionally hard task and the learning materials out there are actually zero for a lot of this. Nvidia doesn’t even document many PTX instructions. The conclusion of all of this is that GPUs being used properly (i.e. optimally) is very difficult at the best of times, and essentially never occurs with typical users. For this reason you don’t necessarily need to be concerned if you see lots of people showing poor performance, the actual optimised performance can be far higher.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n56n6ws/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753489280,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9dur7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n58kqqk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTrack_ai",
            "can_mod_post": false,
            "created_utc": 1753521845,
            "send_replies": true,
            "parent_id": "t3_1m9dur7",
            "score": 0,
            "author_fullname": "t2_1tpuoj72sa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you want somthing better (with high-band-width memory) go visit: [GPTrack.ai](http://GPTrack.ai) and [GPTshop.ai](http://GPTshop.ai)",
            "edited": 1753523069,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n58kqqk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you want somthing better (with high-band-width memory) go visit: &lt;a href=\"http://GPTrack.ai\"&gt;GPTrack.ai&lt;/a&gt; and &lt;a href=\"http://GPTshop.ai\"&gt;GPTshop.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9dur7/dissatisfied_with_how_the_rtx_pro_6000_blackwell/n58kqqk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753521845,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9dur7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]