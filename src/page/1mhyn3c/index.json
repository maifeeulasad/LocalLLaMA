[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have a beast of a machine: [https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s\\_2x6000\\_ada\\_4xrtx\\_6000\\_pro\\_build/](https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s_2x6000_ada_4xrtx_6000_pro_build/)\n\n  \nHowever, most of the time I am running heavy CUDA workloads.  I want to run LLM locally on CPU and only use the GPU if it isn't currently being used.  I've been using ollama but I think its giving me some memory troubles (segfaults) when running two workloads at the same time.\n\n  \nIs there an inference engine that I can prioritize what resources to use and when?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best tool to prioritize workloads sharing with LLM?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhyn3c",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_15x8q5kwup",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754364878,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a beast of a machine: &lt;a href=\"https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s_2x6000_ada_4xrtx_6000_pro_build/\"&gt;https://www.reddit.com/r/nvidia/comments/1mf0yal/2xl40s_2x6000_ada_4xrtx_6000_pro_build/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However, most of the time I am running heavy CUDA workloads.  I want to run LLM locally on CPU and only use the GPU if it isn&amp;#39;t currently being used.  I&amp;#39;ve been using ollama but I think its giving me some memory troubles (segfaults) when running two workloads at the same time.&lt;/p&gt;\n\n&lt;p&gt;Is there an inference engine that I can prioritize what resources to use and when?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhyn3c",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Ill_Recipe7620",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754364878,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ztqft",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "itsmebcc",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ztevm",
                                "score": 0,
                                "author_fullname": "t2_43j7l",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Nah - For that gguf is correct. I mean it should offload to CPU if the GPU is full, but I would not chance it. Yu can use LM Studio, and set it to not offload any layers to gpu at all.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ztqft",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nah - For that gguf is correct. I mean it should offload to CPU if the GPU is full, but I would not chance it. Yu can use LM Studio, and set it to not offload any layers to gpu at all.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhyn3c",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/n6ztqft/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754365986,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754365986,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ztevm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Ill_Recipe7620",
                      "can_mod_post": false,
                      "created_utc": 1754365850,
                      "send_replies": true,
                      "parent_id": "t1_n6zt3l3",
                      "score": 0,
                      "author_fullname": "t2_15x8q5kwup",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah, I figured ollama wasn't the right choice, but I need to make sure I don't accidentally segfault my other workloads.  Can I setup vLLM to use CPU RAM if GPU RAM is available?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ztevm",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, I figured ollama wasn&amp;#39;t the right choice, but I need to make sure I don&amp;#39;t accidentally segfault my other workloads.  Can I setup vLLM to use CPU RAM if GPU RAM is available?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhyn3c",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/n6ztevm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754365850,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6zt3l3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "itsmebcc",
            "can_mod_post": false,
            "created_utc": 1754365716,
            "send_replies": true,
            "parent_id": "t3_1mhyn3c",
            "score": 0,
            "author_fullname": "t2_43j7l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Dude -- Do not run ollama on something like that! GGUF is great and easy and all, but man.... Setup vllm and see how quickly these models really are when NOTHING but the VRAM is being used. It is night and day. Prompt Processing tokens a second will be 20x or more what you are getting with ollama. I do use llama-server for some models that are too large for me, but if it will fit in VRAM use vllm. You will thank me later.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6zt3l3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Dude -- Do not run ollama on something like that! GGUF is great and easy and all, but man.... Setup vllm and see how quickly these models really are when NOTHING but the VRAM is being used. It is night and day. Prompt Processing tokens a second will be 20x or more what you are getting with ollama. I do use llama-server for some models that are too large for me, but if it will fit in VRAM use vllm. You will thank me later.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/n6zt3l3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754365716,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhyn3c",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6zthpn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "itsmebcc",
            "can_mod_post": false,
            "created_utc": 1754365882,
            "send_replies": true,
            "parent_id": "t3_1mhyn3c",
            "score": 0,
            "author_fullname": "t2_43j7l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yea I pretty much just looked at the other post and saw the machine and did not read the rest of what you said lol. Try ik_llama if you want to use cpu only.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6zthpn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yea I pretty much just looked at the other post and saw the machine and did not read the rest of what you said lol. Try ik_llama if you want to use cpu only.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/n6zthpn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754365882,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhyn3c",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70vk9e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LoSboccacc",
            "can_mod_post": false,
            "created_utc": 1754386204,
            "send_replies": true,
            "parent_id": "t3_1mhyn3c",
            "score": 1,
            "author_fullname": "t2_dievh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Skypilot support local kubernetes cluster and preemptible workloads. https://docs.skypilot.co/en/v0.6.0/reference/kubernetes/kubernetes-setup.html#kubernetes-setup-kind",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70vk9e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Skypilot support local kubernetes cluster and preemptible workloads. &lt;a href=\"https://docs.skypilot.co/en/v0.6.0/reference/kubernetes/kubernetes-setup.html#kubernetes-setup-kind\"&gt;https://docs.skypilot.co/en/v0.6.0/reference/kubernetes/kubernetes-setup.html#kubernetes-setup-kind&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhyn3c/best_tool_to_prioritize_workloads_sharing_with_llm/n70vk9e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754386204,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhyn3c",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]