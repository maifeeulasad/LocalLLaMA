[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi all,\n\nI've been exploring the idea of running a local LLM (like Mistral, LLaMA, GPT4All, etc.) and I’m curious about what actual advantages people are seeing *beyond* the usual arguments like \"offline\" or \"data privacy\".\n\nWhat I'm specifically wondering:\n\n* Are there any noticeable *workflow or performance benefits* compared to ChatGPT, Claude, or Gemini?\n* Can I create something that's more flexible or more powerful for specific use cases?\n* Is it possible to build a personal assistant that’s smarter or more integrated than what's possible with cloud tools?\n\nTo put it differently:  \nCan I build a local setup that combines features from ChatGPT and NotebookLM—just more customizable and without the limits?\n\nI’m imagining a tool that can:\n\n* Load and analyze 300+ personal documents (PDFs, Markdown, etc.)\n* Respond with references or citations from those files\n* Help me write, summarize, or analyze complex material\n* Integrate into my note-taking or research workflows\n* Run entirely on my machine, without having to send anything to the cloud\n\nI’m not a developer, but I’m comfortable installing tools, downloading models, and doing some basic setup. I’ve seen names like LM Studio, Ollama, LangChain, RAG, etc., floating around—some look beginner-friendly, some a bit more technical.\n\nSo my questions are:\n\n1. Have you managed to build a setup like this? If so, what tools or combinations worked best for you?\n2. What do local LLMs *actually* do better than GPT-4 or Claude in your day-to-day usage?\n3. Are there real workflow gains—like lower latency, better integration, or more control?\n\nI’d love to hear what others have built. Links, screenshots, tool names, practical examples—all appreciated.\n\nThanks in advance.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Local LLMs – What are the real advantages beyond privacy ?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjdz2a",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.44,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_huuvmqlo",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754508070,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been exploring the idea of running a local LLM (like Mistral, LLaMA, GPT4All, etc.) and I’m curious about what actual advantages people are seeing &lt;em&gt;beyond&lt;/em&gt; the usual arguments like &amp;quot;offline&amp;quot; or &amp;quot;data privacy&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m specifically wondering:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are there any noticeable &lt;em&gt;workflow or performance benefits&lt;/em&gt; compared to ChatGPT, Claude, or Gemini?&lt;/li&gt;\n&lt;li&gt;Can I create something that&amp;#39;s more flexible or more powerful for specific use cases?&lt;/li&gt;\n&lt;li&gt;Is it possible to build a personal assistant that’s smarter or more integrated than what&amp;#39;s possible with cloud tools?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To put it differently:&lt;br/&gt;\nCan I build a local setup that combines features from ChatGPT and NotebookLM—just more customizable and without the limits?&lt;/p&gt;\n\n&lt;p&gt;I’m imagining a tool that can:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Load and analyze 300+ personal documents (PDFs, Markdown, etc.)&lt;/li&gt;\n&lt;li&gt;Respond with references or citations from those files&lt;/li&gt;\n&lt;li&gt;Help me write, summarize, or analyze complex material&lt;/li&gt;\n&lt;li&gt;Integrate into my note-taking or research workflows&lt;/li&gt;\n&lt;li&gt;Run entirely on my machine, without having to send anything to the cloud&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’m not a developer, but I’m comfortable installing tools, downloading models, and doing some basic setup. I’ve seen names like LM Studio, Ollama, LangChain, RAG, etc., floating around—some look beginner-friendly, some a bit more technical.&lt;/p&gt;\n\n&lt;p&gt;So my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Have you managed to build a setup like this? If so, what tools or combinations worked best for you?&lt;/li&gt;\n&lt;li&gt;What do local LLMs &lt;em&gt;actually&lt;/em&gt; do better than GPT-4 or Claude in your day-to-day usage?&lt;/li&gt;\n&lt;li&gt;Are there real workflow gains—like lower latency, better integration, or more control?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’d love to hear what others have built. Links, screenshots, tool names, practical examples—all appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mjdz2a",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "agent007653",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/",
            "subreddit_subscribers": 512875,
            "created_utc": 1754508070,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7be5mm",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "a_beautiful_rhind",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7b41wm",
                                "score": 1,
                                "author_fullname": "t2_h5utwre7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I don't disagree.. you may not need those smarts for some things either. Makes no sense to throw opus at tasks qwen 30b can do. Up to op to try it out.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7be5mm",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t disagree.. you may not need those smarts for some things either. Makes no sense to throw opus at tasks qwen 30b can do. Up to op to try it out.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjdz2a",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7be5mm/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754519654,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754519654,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7b41wm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "burner_sb",
                      "can_mod_post": false,
                      "created_utc": 1754516491,
                      "send_replies": true,
                      "parent_id": "t1_n7addhv",
                      "score": 2,
                      "author_fullname": "t2_1ez41r3ib1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'll push back on \"cloud models are going to be smarter\" -- yes, for SOTA. But right now, we're in a space where cloud providers are offering below-cost access to SOTA models. We're already seeing the beginning stages of that being unsustainable, with Anthropic's recent cuts to access. There are already local models that are competitive with or better than lower tier cloud-based models, and hardware/software are continuing to improve.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7b41wm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll push back on &amp;quot;cloud models are going to be smarter&amp;quot; -- yes, for SOTA. But right now, we&amp;#39;re in a space where cloud providers are offering below-cost access to SOTA models. We&amp;#39;re already seeing the beginning stages of that being unsustainable, with Anthropic&amp;#39;s recent cuts to access. There are already local models that are competitive with or better than lower tier cloud-based models, and hardware/software are continuing to improve.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjdz2a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7b41wm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754516491,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7addhv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1754508807,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 4,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "They lewd better, they don't refuse. No internet connection required or limits beyond your hardware.\n\nBut yea, cloud models are going to be smarter. Tools you build for the API are probably going to work similar to local ones. You need to find something that does your little RAG setup in general and *then* go looking for the model to power it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7addhv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They lewd better, they don&amp;#39;t refuse. No internet connection required or limits beyond your hardware.&lt;/p&gt;\n\n&lt;p&gt;But yea, cloud models are going to be smarter. Tools you build for the API are probably going to work similar to local ones. You need to find something that does your little RAG setup in general and &lt;em&gt;then&lt;/em&gt; go looking for the model to power it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7addhv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754508807,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7adte8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "PermanentLiminality",
            "can_mod_post": false,
            "created_utc": 1754508938,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 2,
            "author_fullname": "t2_19zqycaf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I restrict my usage due to costs.  I'm not against spending money on things like openrouter, but I want decent value for my money.  I have some use cases that will use several million tokens per day.  That adds up.  The value proposition is unclear.\n\nLocal models are not necessarily cheaper.  The cost of buying the hardware and my 40 cents per kwh electricity make it hard to save.  I have cheap GPUs.  I picked up four P102-100 for $40 each.  They do OK, but I'm only running two at this time.  \n\nFor professional use one reason for local models is reliability.  I am working on adding a LLM do drive some new functionality.  It will be a business critical application for our customers.  We provide our nines on our existing products, and it has actually been five nines for the last couple of years.  You can't get close to that with any API provider.\n\nIf you are not a developer, take a look at Agent Zero.  It works a lot better with GPT 4.1, but I need to try the new Qwen 30B and yesterdays openai 20b model.    It kind of needs a vision model for some stuff.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7adte8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I restrict my usage due to costs.  I&amp;#39;m not against spending money on things like openrouter, but I want decent value for my money.  I have some use cases that will use several million tokens per day.  That adds up.  The value proposition is unclear.&lt;/p&gt;\n\n&lt;p&gt;Local models are not necessarily cheaper.  The cost of buying the hardware and my 40 cents per kwh electricity make it hard to save.  I have cheap GPUs.  I picked up four P102-100 for $40 each.  They do OK, but I&amp;#39;m only running two at this time.  &lt;/p&gt;\n\n&lt;p&gt;For professional use one reason for local models is reliability.  I am working on adding a LLM do drive some new functionality.  It will be a business critical application for our customers.  We provide our nines on our existing products, and it has actually been five nines for the last couple of years.  You can&amp;#39;t get close to that with any API provider.&lt;/p&gt;\n\n&lt;p&gt;If you are not a developer, take a look at Agent Zero.  It works a lot better with GPT 4.1, but I need to try the new Qwen 30B and yesterdays openai 20b model.    It kind of needs a vision model for some stuff.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7adte8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754508938,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7adst1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Sad_Comfortable1819",
            "can_mod_post": false,
            "created_utc": 1754508933,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 2,
            "author_fullname": "t2_j5ibijl8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Local users are accepting hallucinations only because local models can audit every token and keep all private data on their device. My point: cloud models still lead on raw reasoning and reliability.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7adst1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Local users are accepting hallucinations only because local models can audit every token and keep all private data on their device. My point: cloud models still lead on raw reasoning and reliability.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7adst1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754508933,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7afsoe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Red_Redditor_Reddit",
            "can_mod_post": false,
            "created_utc": 1754509511,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 1,
            "author_fullname": "t2_8eelmfjg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have only used online LLM's twice.  Once when GPT 3 first came out, and once with grok when someone told me you could look up censored websites.  I made up my mind a long time ago that I don't use online services unless it legitimately needs to be online.  Every time I've used something that was \"free\" online service, it always ended up doing something weird.  I'll wake up and it's doing something different or weird.  It will demand payment once I've become accustomed to it.  It will randomly quit working or change radically.  It doesn't start doing politics or recently started this wacky age verification.  There's just always been a catch when it's a service online.  I don't even do the fucking apps on my phone unless they're just supplementary.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7afsoe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have only used online LLM&amp;#39;s twice.  Once when GPT 3 first came out, and once with grok when someone told me you could look up censored websites.  I made up my mind a long time ago that I don&amp;#39;t use online services unless it legitimately needs to be online.  Every time I&amp;#39;ve used something that was &amp;quot;free&amp;quot; online service, it always ended up doing something weird.  I&amp;#39;ll wake up and it&amp;#39;s doing something different or weird.  It will demand payment once I&amp;#39;ve become accustomed to it.  It will randomly quit working or change radically.  It doesn&amp;#39;t start doing politics or recently started this wacky age verification.  There&amp;#39;s just always been a catch when it&amp;#39;s a service online.  I don&amp;#39;t even do the fucking apps on my phone unless they&amp;#39;re just supplementary.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7afsoe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754509511,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7aidyb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "entsnack",
            "can_mod_post": false,
            "created_utc": 1754510241,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 1,
            "author_fullname": "t2_1a48h7vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I like local LLMs for fine-tuning because I know whether I am doing PEFT or full-paramater fine-tuning. With APIs it's a black-box. Also, local LLMs are pretty much the only economical go-to for reinforcement fine-tuning, training agents for example. OpenAI has a *very expensive* reinforcement fine-tuning API ($100 per *compute hour!*).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7aidyb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I like local LLMs for fine-tuning because I know whether I am doing PEFT or full-paramater fine-tuning. With APIs it&amp;#39;s a black-box. Also, local LLMs are pretty much the only economical go-to for reinforcement fine-tuning, training agents for example. OpenAI has a &lt;em&gt;very expensive&lt;/em&gt; reinforcement fine-tuning API ($100 per &lt;em&gt;compute hour!&lt;/em&gt;).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7aidyb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754510241,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "transparent",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dsv89",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mart-McUH",
            "can_mod_post": false,
            "created_utc": 1754555685,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 1,
            "author_fullname": "t2_q3eqbw2b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Consistency (no one is going to take/replace your model).\n\nAvailability (don't even need internet).\n\nCustomization.\n\nPossibility to finetune/train.\n\nBut the Privacy is probably the main one. I don't use clouds either for anything really private like photos. IMO lot of the advantages are similar to using cloud/service or local HW/SW.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dsv89",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Consistency (no one is going to take/replace your model).&lt;/p&gt;\n\n&lt;p&gt;Availability (don&amp;#39;t even need internet).&lt;/p&gt;\n\n&lt;p&gt;Customization.&lt;/p&gt;\n\n&lt;p&gt;Possibility to finetune/train.&lt;/p&gt;\n\n&lt;p&gt;But the Privacy is probably the main one. I don&amp;#39;t use clouds either for anything really private like photos. IMO lot of the advantages are similar to using cloud/service or local HW/SW.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7dsv89/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754555685,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7e07gw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Toooooool",
            "can_mod_post": false,
            "created_utc": 1754559917,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 1,
            "author_fullname": "t2_8llornh4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The grammar file comes to mind.  \nI'm seeing an increase in game devs including small LLM's in their games and then they let the LLM output not just the text replies but also the situational status of the conversation. This requires a grammar file to force a coherent output, something which cloud LLM's doesn't allow.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7e07gw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The grammar file comes to mind.&lt;br/&gt;\nI&amp;#39;m seeing an increase in game devs including small LLM&amp;#39;s in their games and then they let the LLM output not just the text replies but also the situational status of the conversation. This requires a grammar file to force a coherent output, something which cloud LLM&amp;#39;s doesn&amp;#39;t allow.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7e07gw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754559917,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7af48d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1754509314,
            "send_replies": true,
            "parent_id": "t3_1mjdz2a",
            "score": 0,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;Can I build a local setup that combines features from ChatGPT and NotebookLM—just more customizable and without the limits?\n\nYes. Easy yes if we ignore you mentioning you're not a developer. I think anythingLLM has the most batteries included RAG solution atm? There's also N8n as a no code build stuff solution that's converged heavily on LLM usage as nodes.\n\nThe sky is your limit if it's local. If it's API or web-chat your limit is what they tell you. \n\nJust seriously don't be person number 1000000 who wants to make an AI that remembers stuff. Make reasonable work flows, like 1 time running processing on your incoming email and filtering to a folder for priority sorting. Don't try to build Jarvis, respect current LLMs are a temporary instance, they do something, then they're deleted.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7af48d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Can I build a local setup that combines features from ChatGPT and NotebookLM—just more customizable and without the limits?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yes. Easy yes if we ignore you mentioning you&amp;#39;re not a developer. I think anythingLLM has the most batteries included RAG solution atm? There&amp;#39;s also N8n as a no code build stuff solution that&amp;#39;s converged heavily on LLM usage as nodes.&lt;/p&gt;\n\n&lt;p&gt;The sky is your limit if it&amp;#39;s local. If it&amp;#39;s API or web-chat your limit is what they tell you. &lt;/p&gt;\n\n&lt;p&gt;Just seriously don&amp;#39;t be person number 1000000 who wants to make an AI that remembers stuff. Make reasonable work flows, like 1 time running processing on your incoming email and filtering to a folder for priority sorting. Don&amp;#39;t try to build Jarvis, respect current LLMs are a temporary instance, they do something, then they&amp;#39;re deleted.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/n7af48d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754509314,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdz2a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]