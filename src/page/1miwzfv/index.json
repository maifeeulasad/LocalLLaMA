[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Was downloading the new open ai model and noticed that there is a difference in model size and claimed model size. \n\nI thought there was a confusion between download size and model params but they also don't seem to match. \n\nBoth the repos have model size(params) stating half their claimed model params. \n\nWhat is going on?? ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why is there a difference in size gpt oss.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 115,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1miwzfv",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.38,
            "author_flair_background_color": null,
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_l5x3qnnez",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/sFnv_TPbzeyUoJC0hEPSRAOjMfVzSzUR3ldlNT1JLuE.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754460742,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was downloading the new open ai model and noticed that there is a difference in model size and claimed model size. &lt;/p&gt;\n\n&lt;p&gt;I thought there was a confusion between download size and model params but they also don&amp;#39;t seem to match. &lt;/p&gt;\n\n&lt;p&gt;Both the repos have model size(params) stating half their claimed model params. &lt;/p&gt;\n\n&lt;p&gt;What is going on?? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/6jjgtwpjcchf1.jpeg",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/6jjgtwpjcchf1.jpeg?auto=webp&amp;s=f0e2662c07e10a44561a20a832d56af506590fcd",
                    "width": 856,
                    "height": 705
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/6jjgtwpjcchf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7c1bd68e06a4a62376b5a14b0a4bc4a7295d32",
                      "width": 108,
                      "height": 88
                    },
                    {
                      "url": "https://preview.redd.it/6jjgtwpjcchf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=68d3a2a51d7335f743cf624d8f02c5c89a990d17",
                      "width": 216,
                      "height": 177
                    },
                    {
                      "url": "https://preview.redd.it/6jjgtwpjcchf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=006f149667e2f5132e69a890dd5d830b256d4952",
                      "width": 320,
                      "height": 263
                    },
                    {
                      "url": "https://preview.redd.it/6jjgtwpjcchf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b390177e2185f5c9a1f9606d4d783adedcb3b965",
                      "width": 640,
                      "height": 527
                    }
                  ],
                  "variants": {},
                  "id": "niNOoUeO5IUjUva5PUwS8z1FEGT8FvOQj5e730fH0_g"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1miwzfv",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "According_Fig_4784",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1miwzfv/why_is_there_a_difference_in_size_gpt_oss/",
            "stickied": false,
            "url": "https://i.redd.it/6jjgtwpjcchf1.jpeg",
            "subreddit_subscribers": 511884,
            "created_utc": 1754460742,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n76trh9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "droptableadventures",
                      "can_mod_post": false,
                      "created_utc": 1754463273,
                      "send_replies": true,
                      "parent_id": "t1_n76r1qj",
                      "score": 3,
                      "author_fullname": "t2_52zg0eoq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "MXFP4 is also a brand new format so it's likely HuggingFace still needs to fix the code to provide the correct tensor count for these files.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n76trh9",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MXFP4 is also a brand new format so it&amp;#39;s likely HuggingFace still needs to fix the code to provide the correct tensor count for these files.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miwzfv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miwzfv/why_is_there_a_difference_in_size_gpt_oss/n76trh9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754463273,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n76r1qj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754461792,
            "send_replies": true,
            "parent_id": "t3_1miwzfv",
            "score": 3,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Look what HF says the type is: \"BF16, U8\".  Functionally the model is actually BF16, MXFP4 but that data is stored as blocks of U8 (i.e. bytes).  So the calculation is off.  \n\nThe nitty gritty: if you click on one of the safetensors it shows the dump\n\n    model.layers.0.mlp.experts.down_proj_bias       [32, 2 880]          BF16\n    model.layers.0.mlp.experts.down_proj_blocks     [32, 2 880, 90, 16]  U8\n    model.layers.0.mlp.experts.down_proj_scales     [32, 2 880, 90]      U8\n    model.layers.0.mlp.experts.gate_up_proj_bias    [32, 5 760]          BF16\n    model.layers.0.mlp.experts.gate_up_proj_blocks  [32, 5 760, 90, 16]  U8\n    model.layers.0.mlp.experts.gate_up_proj_scales  [32, 5 760, 90]      U8\n\nMXFP4 is blocks of 32 FP4 (E2M1) values with a quasi-FP8 (E8M0) scale.  You can see in the tensor layout you have blocks of 16 U8 and scales of U8.  Because the 32 fp4 are described as 16 U8 (16 is the 4th dimension of the tensor) HF only counts these for half of what they really are",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76r1qj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Look what HF says the type is: &amp;quot;BF16, U8&amp;quot;.  Functionally the model is actually BF16, MXFP4 but that data is stored as blocks of U8 (i.e. bytes).  So the calculation is off.  &lt;/p&gt;\n\n&lt;p&gt;The nitty gritty: if you click on one of the safetensors it shows the dump&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;model.layers.0.mlp.experts.down_proj_bias       [32, 2 880]          BF16\nmodel.layers.0.mlp.experts.down_proj_blocks     [32, 2 880, 90, 16]  U8\nmodel.layers.0.mlp.experts.down_proj_scales     [32, 2 880, 90]      U8\nmodel.layers.0.mlp.experts.gate_up_proj_bias    [32, 5 760]          BF16\nmodel.layers.0.mlp.experts.gate_up_proj_blocks  [32, 5 760, 90, 16]  U8\nmodel.layers.0.mlp.experts.gate_up_proj_scales  [32, 5 760, 90]      U8\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;MXFP4 is blocks of 32 FP4 (E2M1) values with a quasi-FP8 (E8M0) scale.  You can see in the tensor layout you have blocks of 16 U8 and scales of U8.  Because the 32 fp4 are described as 16 U8 (16 is the 4th dimension of the tensor) HF only counts these for half of what they really are&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzfv/why_is_there_a_difference_in_size_gpt_oss/n76r1qj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754461792,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76q509",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "iddar",
            "can_mod_post": false,
            "created_utc": 1754461303,
            "send_replies": true,
            "parent_id": "t3_1miwzfv",
            "score": 1,
            "author_fullname": "t2_e4mk9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The 20b can be run under 24gb of ram/vram on my MacBook M3 work great using LM Studio. Avg memory around 14gb",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76q509",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The 20b can be run under 24gb of ram/vram on my MacBook M3 work great using LM Studio. Avg memory around 14gb&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzfv/why_is_there_a_difference_in_size_gpt_oss/n76q509/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754461303,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76uzlt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MaxKruse96",
            "can_mod_post": false,
            "created_utc": 1754463931,
            "send_replies": true,
            "parent_id": "t3_1miwzfv",
            "score": 2,
            "author_fullname": "t2_pfi81",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "reading comprehension. you are on the 120b page. you are highlighting the 20b. Also, probably a visual bug",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76uzlt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;reading comprehension. you are on the 120b page. you are highlighting the 20b. Also, probably a visual bug&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzfv/why_is_there_a_difference_in_size_gpt_oss/n76uzlt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754463931,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]