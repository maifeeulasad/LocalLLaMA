[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.\n\nI now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB\n\nI would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.\n\nBased on my research I couldn’t find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:\n\n* 80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500\n* 21 t/s - 120 GB/s Mac Mini M4 32GB 1000$\n* 46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$\n* 43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro\n* 410 GB/s Mac Studio M4 Max 36-128 GB tons of $\n* 819 GB/s Mac Studio M3 Ultra up to 512GB tons of $\n* 132 t/s - 750 GB/s RTX 4090 24GB\n\nBecause I’m considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I’ve reached the following conclusions that I hope to validate:\n\n* I need something between M4 and the GPU in terms of speed.\n* I need 32GB but could do with more if it’s not a ripoff.\n* Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale\n* For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can’t justify getting a second GPU.\n* M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.\n\nSo is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn’t noisy and power hungry?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Looking to build or buy a mini pc for LLM",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgvbw6",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_12uzcl",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754259032,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754256996,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to run an entry level local LLM of 15-35B Q4 with context of at least 10k that I want to train with local pdfs using RAG.&lt;/p&gt;\n\n&lt;p&gt;I now have a Mac Mini M4 16GB and a 4K gaming rig with RTX 4090 24GB&lt;/p&gt;\n\n&lt;p&gt;I would use the AI infrequently throughout the day, so anything above 10-20 t/s would be acceptable for me for a 15-35B LLM.&lt;/p&gt;\n\n&lt;p&gt;Based on my research I couldn’t find one centralized benchmarking solution but I could find the following approximate values for bandwidth and performance for Gemma 3 12B LLM:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;80gb/s DDR5 5200 MHZ, cheapest way to 128GB - $500&lt;/li&gt;\n&lt;li&gt;21 t/s - 120 GB/s Mac Mini M4 32GB 1000$&lt;/li&gt;\n&lt;li&gt;46 t/s - 273 GB/s Mac Mini M4 Pro 32-64GB 2500$&lt;/li&gt;\n&lt;li&gt;43 t/s - 267 GB/s Mini PC Ryzen AI HX 395+ 64-128 GB comparable $ with Mac Mini M4 Pro&lt;/li&gt;\n&lt;li&gt;410 GB/s Mac Studio M4 Max 36-128 GB tons of $&lt;/li&gt;\n&lt;li&gt;819 GB/s Mac Studio M3 Ultra up to 512GB tons of $&lt;/li&gt;\n&lt;li&gt;132 t/s - 750 GB/s RTX 4090 24GB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Because I’m considering the entire thing like purchasing price, ownership, efficiency (electricity, noise, heat) and resale value, I’ve reached the following conclusions that I hope to validate:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I need something between M4 and the GPU in terms of speed.&lt;/li&gt;\n&lt;li&gt;I need 32GB but could do with more if it’s not a ripoff.&lt;/li&gt;\n&lt;li&gt;Since Ryzen and M4 pro both have soldered RAM and have roughly the same bandwidth and Ryzen has much higher power draw, so the mac mini narrowly wins for me, especially when it comes to noise and resale&lt;/li&gt;\n&lt;li&gt;For my use case GPU is OP and not worth it when considering power draw and heat. The same goes for maxing ram on my desktop since it would use 600W, and I can’t justify getting a second GPU.&lt;/li&gt;\n&lt;li&gt;M4 appears the best choice if I want to stay at 32-64 GB. I would like a mini pc with user replaceable ram and bandwidth as high as the AI HX, but there is no such thing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So is there anything between an M4 mini and a GPU that is a good price to performance ratio and isn’t noisy and power hungry?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgvbw6",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "nemuro87",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/",
            "subreddit_subscribers": 509913,
            "created_utc": 1754256996,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ugimh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "nemuro87",
                      "can_mod_post": false,
                      "created_utc": 1754302575,
                      "send_replies": true,
                      "parent_id": "t1_n6sqn2f",
                      "score": 1,
                      "author_fullname": "t2_12uzcl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thank you for the detailed response. \n\nThe token per second are what I actually experienced running Gemma 3 12B on each hardware spec. \nYou’re right about the mistake with the 4090 bandwidth. \n\nVery interesting POV with the 4090 being power hungry but several times faster and somehow not justifying the cost to buy new hardware since it will be used occasionally. \nI didn’t think of that. \n\nI’m a bit bitter about the 4090 costing an arm and a leg and having “only” 24GB, when what I want would be at around the 32GB mark or a bit above. \nI would gladly trade this for a 50% slower one but with 50% more ram. I also believe it would end up being more power efficient too. \n\nUnthinkable how a couple of years later of AI boom later and there’s still no RX 480 or mid range equivalent with 32GB or 48GB, and 500gb/s costing say around 1k. \nThey’d fly off the shelves. ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ugimh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for the detailed response. &lt;/p&gt;\n\n&lt;p&gt;The token per second are what I actually experienced running Gemma 3 12B on each hardware spec. \nYou’re right about the mistake with the 4090 bandwidth. &lt;/p&gt;\n\n&lt;p&gt;Very interesting POV with the 4090 being power hungry but several times faster and somehow not justifying the cost to buy new hardware since it will be used occasionally. \nI didn’t think of that. &lt;/p&gt;\n\n&lt;p&gt;I’m a bit bitter about the 4090 costing an arm and a leg and having “only” 24GB, when what I want would be at around the 32GB mark or a bit above. \nI would gladly trade this for a 50% slower one but with 50% more ram. I also believe it would end up being more power efficient too. &lt;/p&gt;\n\n&lt;p&gt;Unthinkable how a couple of years later of AI boom later and there’s still no RX 480 or mid range equivalent with 32GB or 48GB, and 500gb/s costing say around 1k. \nThey’d fly off the shelves. &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgvbw6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/n6ugimh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754302575,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6sqn2f",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754272090,
            "send_replies": true,
            "parent_id": "t3_1mgvbw6",
            "score": 3,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; I now have a ... RTX 4090 24GB\n\nProblem solved?!  You have a $2k piece of hardware already, why not use it?  Particularly since it sort of sounds like a one-off job.  The heat and power aren't a valid concern, practically speaking.  Like I lose 5% of tg128 performance on my 4090 with the power limited to 250W and CPU is basically idle so maybe 300W all-in.  It's also 4x faster than the mini-PC options meaning it runs for 4x less time.  While the mini PCs probably do win in terms of token/Wh, I can't imagine it's enough to buy a whole machine.  Remember even it you buy it used and sell it for the same sticker price, the middle men are taking like 20% or so (depending on region and middle men) which buys a lot of electrons.\n\nThat said, your numbers are off.  Small models tend to only be partially bottlenecked by memory bandwidth but your calculations seem to assume a 6GB model (Q4_K_M is 6.8GB) and 100% utilization of memory bandwidth.  The reality is much worse, about half your numbers.  E.g. my CPU (Epyc) is theoretically 450GBps and my GPU is 1000GBps (you have the 4090 incorrectly listed as 750GBps) and yet:\n\n| model                          |       size |     params | backend    | ngl | threads |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | --------------: | -------------------: |\n| gemma3 12B Q4_K - Medium       |   6.79 GiB |    11.77 B | CPU        |     |      40 |           tg256 |         28.01 ± 0.00 |\n| gemma3 12B Q8_0                |  11.64 GiB |    11.77 B | CPU        |     |      40 |           tg256 |         19.26 ± 0.00 |\n| gemma3 12B BF16                |  21.92 GiB |    11.77 B | CPU        |     |      40 |           tg256 |         11.70 ± 0.00 |\n| gemma3 12B Q4_K - Medium       |   6.79 GiB |    11.77 B | CUDA       |  99 |         |           tg256 |         87.36 ± 0.00 |\n| gemma3 12B Q8_0                |  11.64 GiB |    11.77 B | CUDA       |  99 |         |           tg256 |         58.43 ± 0.00 |\n| gemma3 12B BF16                |  21.92 GiB |    11.77 B | CUDA       |  99 |         |           tg256 |         35.51 ± 0.00 |\n\nNote the scaling, the bf16 on CPU reaches about ~60% theoretical on CPU or ~80% on GPU, but the Q4_K_M is only 40% and 60% on GPU.  This is llama.cpp, some engines might be able to do better with GPU but that's probably about what to expect in terms of efficiency off theoretical.\n\nAnyways, the AI 395+ might be the best choice if you what cheap and tolerable performance.  It should also only cost $2000 for the 128GB model and that extra RAM will put a number of very good MoE models in reach once you realize that gemma3-12B isn't very good.  (I only somewhat kid, I found it to be significantly worse the the 27B which will also run on a 4090 FWIW.)  I also suspect that the power difference isn't as high as it looks in the specs when both machines are running maxed out.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6sqn2f",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I now have a ... RTX 4090 24GB&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Problem solved?!  You have a $2k piece of hardware already, why not use it?  Particularly since it sort of sounds like a one-off job.  The heat and power aren&amp;#39;t a valid concern, practically speaking.  Like I lose 5% of tg128 performance on my 4090 with the power limited to 250W and CPU is basically idle so maybe 300W all-in.  It&amp;#39;s also 4x faster than the mini-PC options meaning it runs for 4x less time.  While the mini PCs probably do win in terms of token/Wh, I can&amp;#39;t imagine it&amp;#39;s enough to buy a whole machine.  Remember even it you buy it used and sell it for the same sticker price, the middle men are taking like 20% or so (depending on region and middle men) which buys a lot of electrons.&lt;/p&gt;\n\n&lt;p&gt;That said, your numbers are off.  Small models tend to only be partially bottlenecked by memory bandwidth but your calculations seem to assume a 6GB model (Q4_K_M is 6.8GB) and 100% utilization of memory bandwidth.  The reality is much worse, about half your numbers.  E.g. my CPU (Epyc) is theoretically 450GBps and my GPU is 1000GBps (you have the 4090 incorrectly listed as 750GBps) and yet:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;model&lt;/th&gt;\n&lt;th align=\"right\"&gt;size&lt;/th&gt;\n&lt;th align=\"right\"&gt;params&lt;/th&gt;\n&lt;th&gt;backend&lt;/th&gt;\n&lt;th align=\"right\"&gt;ngl&lt;/th&gt;\n&lt;th align=\"right\"&gt;threads&lt;/th&gt;\n&lt;th align=\"right\"&gt;test&lt;/th&gt;\n&lt;th align=\"right\"&gt;t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;gemma3 12B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;6.79 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.77 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;&lt;/td&gt;\n&lt;td align=\"right\"&gt;40&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg256&lt;/td&gt;\n&lt;td align=\"right\"&gt;28.01 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gemma3 12B Q8_0&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.64 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.77 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;&lt;/td&gt;\n&lt;td align=\"right\"&gt;40&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg256&lt;/td&gt;\n&lt;td align=\"right\"&gt;19.26 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gemma3 12B BF16&lt;/td&gt;\n&lt;td align=\"right\"&gt;21.92 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.77 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;&lt;/td&gt;\n&lt;td align=\"right\"&gt;40&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg256&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.70 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gemma3 12B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;6.79 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.77 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td align=\"right\"&gt;&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg256&lt;/td&gt;\n&lt;td align=\"right\"&gt;87.36 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gemma3 12B Q8_0&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.64 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.77 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td align=\"right\"&gt;&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg256&lt;/td&gt;\n&lt;td align=\"right\"&gt;58.43 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gemma3 12B BF16&lt;/td&gt;\n&lt;td align=\"right\"&gt;21.92 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.77 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td align=\"right\"&gt;&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg256&lt;/td&gt;\n&lt;td align=\"right\"&gt;35.51 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Note the scaling, the bf16 on CPU reaches about ~60% theoretical on CPU or ~80% on GPU, but the Q4_K_M is only 40% and 60% on GPU.  This is llama.cpp, some engines might be able to do better with GPU but that&amp;#39;s probably about what to expect in terms of efficiency off theoretical.&lt;/p&gt;\n\n&lt;p&gt;Anyways, the AI 395+ might be the best choice if you what cheap and tolerable performance.  It should also only cost $2000 for the 128GB model and that extra RAM will put a number of very good MoE models in reach once you realize that gemma3-12B isn&amp;#39;t very good.  (I only somewhat kid, I found it to be significantly worse the the 27B which will also run on a 4090 FWIW.)  I also suspect that the power difference isn&amp;#39;t as high as it looks in the specs when both machines are running maxed out.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/n6sqn2f/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754272090,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvbw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6rstkq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "UltralKent",
            "can_mod_post": false,
            "created_utc": 1754260203,
            "send_replies": true,
            "parent_id": "t3_1mgvbw6",
            "score": 2,
            "author_fullname": "t2_nm7qj8vc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "ai max 395+",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rstkq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ai max 395+&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/n6rstkq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754260203,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvbw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6rzfs3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "entsnack",
            "can_mod_post": false,
            "created_utc": 1754262465,
            "send_replies": true,
            "parent_id": "t3_1mgvbw6",
            "score": 2,
            "author_fullname": "t2_1a48h7vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Don't have advice but that's some solid research you've done there.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rzfs3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t have advice but that&amp;#39;s some solid research you&amp;#39;ve done there.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/n6rzfs3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754262465,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "link_id": "t3_1mgvbw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "transparent",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6uft7x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "nemuro87",
                      "can_mod_post": false,
                      "created_utc": 1754302202,
                      "send_replies": true,
                      "parent_id": "t1_n6s2uhf",
                      "score": 1,
                      "author_fullname": "t2_12uzcl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for your response.\nI like in EU so it’s not easy to get one either.\nUnaware of the issues of the GMKtec.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6uft7x",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for your response.\nI like in EU so it’s not easy to get one either.\nUnaware of the issues of the GMKtec.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgvbw6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/n6uft7x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754302202,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6s2uhf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MrTag_42",
            "can_mod_post": false,
            "created_utc": 1754263625,
            "send_replies": true,
            "parent_id": "t3_1mgvbw6",
            "score": 1,
            "author_fullname": "t2_4xb4yifo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I am a bit in the same waters as you are. As I'm using Mac for other things as well and as my main platform, I have concluded that for my use case (replacing Claude Code with local models) studio with M3 ultra with 96GB ram or studio with M4 max and 128GB ram will be the \"most\" bang for bucks. I still haven't pull a plug to buy one to replace current M4 mini with 32GB ram. For half of the money, you could get Ryzen AI HX with 128GB and is also a second option I'm evaluating but they are hard to get and there are some issues with them, at least with the GMKtec EVO-X2.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6s2uhf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am a bit in the same waters as you are. As I&amp;#39;m using Mac for other things as well and as my main platform, I have concluded that for my use case (replacing Claude Code with local models) studio with M3 ultra with 96GB ram or studio with M4 max and 128GB ram will be the &amp;quot;most&amp;quot; bang for bucks. I still haven&amp;#39;t pull a plug to buy one to replace current M4 mini with 32GB ram. For half of the money, you could get Ryzen AI HX with 128GB and is also a second option I&amp;#39;m evaluating but they are hard to get and there are some issues with them, at least with the GMKtec EVO-X2.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/n6s2uhf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754263625,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvbw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6rm9e7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SmChocolateBunnies",
            "can_mod_post": false,
            "created_utc": 1754258008,
            "send_replies": true,
            "parent_id": "t3_1mgvbw6",
            "score": 0,
            "author_fullname": "t2_6ay2csfe",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Trade the mini in for M4 Max Studio 128.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rm9e7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Trade the mini in for M4 Max Studio 128.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvbw6/looking_to_build_or_buy_a_mini_pc_for_llm/n6rm9e7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754258008,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvbw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]