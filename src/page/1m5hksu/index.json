[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I keep wondering how Meta could have screwed up llama4 so bad and then released it?   At this point, everyone knows how to train a model and if you have the data and compute you can really release something good.  The bigger the \"smarter\"    They obviously know what to do based on what we saw with llama3.3, we even saw they improved by the smaller models from the 3.2 series.   Llama4 was so bad, they could have gone back, fixed things, retrained and we won't even know.  It wasn't like there was an aggressive release timeline.\n\nSo what if the original Llama4 was so damn good it shook and shocked them?   What if they decided that it's so close to AGI to release?  So instead of that,  they decided to make some really bad models, then released those models so we could laugh and not suspect how good and close they were.   Then Zuck decides to triple down and hire the best to now get the unreleased Llama4 to AGI?  \n\nThis is the only thing that makes sense to me.   There's no way they could have fucked up Llama4 that bad and then released it.  No way.   ... alright in some parallel universe, this is what happened right?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What if Meta really has the best AI?  Hear me out.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Other"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1m5hksu",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": "#bbbdbf",
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "is_original_content": false,
            "author_fullname": "t2_ah13x",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Other",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753100920,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I keep wondering how Meta could have screwed up llama4 so bad and then released it?   At this point, everyone knows how to train a model and if you have the data and compute you can really release something good.  The bigger the &amp;quot;smarter&amp;quot;    They obviously know what to do based on what we saw with llama3.3, we even saw they improved by the smaller models from the 3.2 series.   Llama4 was so bad, they could have gone back, fixed things, retrained and we won&amp;#39;t even know.  It wasn&amp;#39;t like there was an aggressive release timeline.&lt;/p&gt;\n\n&lt;p&gt;So what if the original Llama4 was so damn good it shook and shocked them?   What if they decided that it&amp;#39;s so close to AGI to release?  So instead of that,  they decided to make some really bad models, then released those models so we could laugh and not suspect how good and close they were.   Then Zuck decides to triple down and hire the best to now get the unreleased Llama4 to AGI?  &lt;/p&gt;\n\n&lt;p&gt;This is the only thing that makes sense to me.   There&amp;#39;s no way they could have fucked up Llama4 that bad and then released it.  No way.   ... alright in some parallel universe, this is what happened right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#94e044",
            "id": "1m5hksu",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "segmond",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "light",
            "permalink": "/r/LocalLLaMA/comments/1m5hksu/what_if_meta_really_has_the_best_ai_hear_me_out/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5hksu/what_if_meta_really_has_the_best_ai_hear_me_out/",
            "subreddit_subscribers": 502273,
            "created_utc": 1753100920,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]