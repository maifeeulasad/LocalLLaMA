[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi all  \n  \nAs the title says  \nI am interested in whether or not a few more thousands of bucks, will be useful to me in the next couple of years having more memory in a Mac.  \n(Please no \"why are you running LLMs on Macs\" questions, I have a PC with huge GPUs too).\n\n128gb of unified memory is the most I can get on an M4 Max Mac Studio.  \nvs 256 or higher on an M3 Ultra - personally 256 is the highest I would go.\n\nGiven good models are improving, and getting smaller... ? Maybe M3 Ultra is overkill.  \n  \nEG Given we can run the 120b new GPT OSS model on 64gb, I think? Do I really need 256 or 512 and the (very) high price of that memory from Apple.\n\nThanks for your opinions.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Mac LLM users: What models can't I run with 128gb (M4 Max) vs 256gb (M3 Ultra)?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mkip7t",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_uqbevs3g",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754618150,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all  &lt;/p&gt;\n\n&lt;p&gt;As the title says&lt;br/&gt;\nI am interested in whether or not a few more thousands of bucks, will be useful to me in the next couple of years having more memory in a Mac.&lt;br/&gt;\n(Please no &amp;quot;why are you running LLMs on Macs&amp;quot; questions, I have a PC with huge GPUs too).&lt;/p&gt;\n\n&lt;p&gt;128gb of unified memory is the most I can get on an M4 Max Mac Studio.&lt;br/&gt;\nvs 256 or higher on an M3 Ultra - personally 256 is the highest I would go.&lt;/p&gt;\n\n&lt;p&gt;Given good models are improving, and getting smaller... ? Maybe M3 Ultra is overkill.  &lt;/p&gt;\n\n&lt;p&gt;EG Given we can run the 120b new GPT OSS model on 64gb, I think? Do I really need 256 or 512 and the (very) high price of that memory from Apple.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mkip7t",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "TheWebbster",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/",
            "subreddit_subscribers": 513813,
            "created_utc": 1754618150,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7jp5kb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "TheWebbster",
                      "can_mod_post": false,
                      "created_utc": 1754628022,
                      "send_replies": true,
                      "parent_id": "t1_n7j7frp",
                      "score": 1,
                      "author_fullname": "t2_uqbevs3g",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for going into this detail, really appreciate it",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7jp5kb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for going into this detail, really appreciate it&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkip7t",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/n7jp5kb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754628022,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7kpmjo",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "chisleu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7jrri5",
                                "score": 1,
                                "author_fullname": "t2_cbxyn",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'm getting the same tok/sec from my m3 ultra 512 GB as I am out of my M4 Max Macbook Pro 128GB.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7kpmjo",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m getting the same tok/sec from my m3 ultra 512 GB as I am out of my M4 Max Macbook Pro 128GB.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mkip7t",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/n7kpmjo/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754647766,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754647766,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7jrri5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "docgok",
                      "can_mod_post": false,
                      "created_utc": 1754629238,
                      "send_replies": true,
                      "parent_id": "t1_n7j7frp",
                      "score": 1,
                      "author_fullname": "t2_4v5bb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Technically the M3 Ultra has only about 1.5x more the mem bandwidth than M4 Max (812 vs 546 GB/s)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7jrri5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Technically the M3 Ultra has only about 1.5x more the mem bandwidth than M4 Max (812 vs 546 GB/s)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": true,
                      "can_gild": false,
                      "link_id": "t3_1mkip7t",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/n7jrri5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754629238,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7kphxd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "chisleu",
                      "can_mod_post": false,
                      "created_utc": 1754647698,
                      "send_replies": true,
                      "parent_id": "t1_n7j7frp",
                      "score": 1,
                      "author_fullname": "t2_cbxyn",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I bought the 512 and then bought a macbook pro 128GB machine.\n\nSlight buyers remorse on the mac studio because of one reason. The smaller models are so good. There is only a certain amount of GPU offload with these smaller, fast models and I'm getting the same tok/sec out of the studio as I am out of the macbook pro on those models.  \n\n\nSure, the 512 GB studio can run much larger models, but they run so slowly that I won't use them over anthropic to bail me out when the small model codes itself into a problem.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7kphxd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I bought the 512 and then bought a macbook pro 128GB machine.&lt;/p&gt;\n\n&lt;p&gt;Slight buyers remorse on the mac studio because of one reason. The smaller models are so good. There is only a certain amount of GPU offload with these smaller, fast models and I&amp;#39;m getting the same tok/sec out of the studio as I am out of the macbook pro on those models.  &lt;/p&gt;\n\n&lt;p&gt;Sure, the 512 GB studio can run much larger models, but they run so slowly that I won&amp;#39;t use them over anthropic to bail me out when the small model codes itself into a problem.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkip7t",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/n7kphxd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754647698,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7j7frp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "VegetaTheGrump",
            "can_mod_post": false,
            "created_utc": 1754620616,
            "send_replies": true,
            "parent_id": "t3_1mkip7t",
            "score": 3,
            "author_fullname": "t2_taorh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://preview.redd.it/ekwv9djviphf1.png?width=1376&amp;format=png&amp;auto=webp&amp;s=1b7e356d3833eaff63d45408b1fd31e3e89b4b3b\n\nI wanted the 512GB, but it was too expensive, so I got the 256GB. Remember that the Ultra will have twice the memory bandwidth of the M4 for more speed.  \n  \nI can only run 1bit DeepSeek or Kimi K2, so I pretty much don't. However you can see the models above that do fit. You will be able to run GLM 4.5 Air at 4bit on 128GB RAM, but most of the other models that you see here won't load. I hate running the very largest, because it makes it so I can't run image generation at the same time with a decent sized context.  \n  \nNot all of the models above run very quickly, but they all run, and they just keep getting better as you mentioned.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7j7frp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/ekwv9djviphf1.png?width=1376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1b7e356d3833eaff63d45408b1fd31e3e89b4b3b\"&gt;https://preview.redd.it/ekwv9djviphf1.png?width=1376&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1b7e356d3833eaff63d45408b1fd31e3e89b4b3b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wanted the 512GB, but it was too expensive, so I got the 256GB. Remember that the Ultra will have twice the memory bandwidth of the M4 for more speed.  &lt;/p&gt;\n\n&lt;p&gt;I can only run 1bit DeepSeek or Kimi K2, so I pretty much don&amp;#39;t. However you can see the models above that do fit. You will be able to run GLM 4.5 Air at 4bit on 128GB RAM, but most of the other models that you see here won&amp;#39;t load. I hate running the very largest, because it makes it so I can&amp;#39;t run image generation at the same time with a decent sized context.  &lt;/p&gt;\n\n&lt;p&gt;Not all of the models above run very quickly, but they all run, and they just keep getting better as you mentioned.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/n7j7frp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754620616,
            "media_metadata": {
              "ekwv9djviphf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 61,
                    "x": 108,
                    "u": "https://preview.redd.it/ekwv9djviphf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52bfc086741c440d957002013a8291b634e5ef06"
                  },
                  {
                    "y": 122,
                    "x": 216,
                    "u": "https://preview.redd.it/ekwv9djviphf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0303e6abccba3b267bbe6db994a66581cca9bee5"
                  },
                  {
                    "y": 180,
                    "x": 320,
                    "u": "https://preview.redd.it/ekwv9djviphf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9f087068153ce81cd741d5810b23bf4a9995265"
                  },
                  {
                    "y": 361,
                    "x": 640,
                    "u": "https://preview.redd.it/ekwv9djviphf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=41c6a885733b8dc86595cf97688d4c1d07ac4365"
                  },
                  {
                    "y": 542,
                    "x": 960,
                    "u": "https://preview.redd.it/ekwv9djviphf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa53b1e0f490101950f3d2fbfe7280709a6dd983"
                  },
                  {
                    "y": 610,
                    "x": 1080,
                    "u": "https://preview.redd.it/ekwv9djviphf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ca274d0e51170c7201714fbfccda54675026d458"
                  }
                ],
                "s": {
                  "y": 778,
                  "x": 1376,
                  "u": "https://preview.redd.it/ekwv9djviphf1.png?width=1376&amp;format=png&amp;auto=webp&amp;s=1b7e356d3833eaff63d45408b1fd31e3e89b4b3b"
                },
                "id": "ekwv9djviphf1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkip7t",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7kpuh0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "chisleu",
                      "can_mod_post": false,
                      "created_utc": 1754647881,
                      "send_replies": true,
                      "parent_id": "t1_n7jpw8e",
                      "score": 1,
                      "author_fullname": "t2_cbxyn",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have the 512 and the problem is even worse. It can run really big models for chat purposes, fast enough for chat purposes. But that's it. \n\nIf you are patient, it will run Qwen 3 coder 480b which can power Cline.\n\nBut I just run the Qwen3 coder 30b a3b locally until it runs into a problem it can't handle.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7kpuh0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have the 512 and the problem is even worse. It can run really big models for chat purposes, fast enough for chat purposes. But that&amp;#39;s it. &lt;/p&gt;\n\n&lt;p&gt;If you are patient, it will run Qwen 3 coder 480b which can power Cline.&lt;/p&gt;\n\n&lt;p&gt;But I just run the Qwen3 coder 30b a3b locally until it runs into a problem it can&amp;#39;t handle.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkip7t",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/n7kpuh0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754647881,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7jpw8e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Jbbrack03",
            "can_mod_post": false,
            "created_utc": 1754628367,
            "send_replies": true,
            "parent_id": "t3_1mkip7t",
            "score": 3,
            "author_fullname": "t2_zc0gl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have the M3 Ultra 256 gb. The problem with really large models is that inference speed is super slow. However, you can load several smaller models concurrently. Which can be useful.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7jpw8e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have the M3 Ultra 256 gb. The problem with really large models is that inference speed is super slow. However, you can load several smaller models concurrently. Which can be useful.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/n7jpw8e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754628367,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkip7t",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]