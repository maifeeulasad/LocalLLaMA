[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Honestly, pretty much the question in the Header. Specifically, I'm trying to run InternVL3-78B or the new Intern-S1 model locally, but it's a challenge. VLLM and lmserve support the InternVL models, but appear to be GPU-only, and llama.cpp seems flaky at best when it comes to running them. (Massive hallucinations, errors with the model thinking there's no image attached, etc.) I'm mostly looking to do image tagging with something more accurate than the (still quite good, but aging) wd14 model found in kohya\\_ss. I could probably step down to InternVL3-38B and still get some pretty great results, but I would need a 4 bit quant to fit into my GPU's VRAM if using an engine that doesn't support CPU offloading. Most quants for the model outside of GGUFs appear to be 8 bit. I could quantize it myself if I truly need to, but I'm hoping there's a simpler solution I'm just unfamiliar with. I'm quite used to running LLMs locally, but multimodal models with image processing are new to me. Any help or insight for a good way to handle image tagging locally would be greatly appreciated!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How Are You Running Multimodal (Text-Image) Models Locally?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mad6sy",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_crdgh",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753590152,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Honestly, pretty much the question in the Header. Specifically, I&amp;#39;m trying to run InternVL3-78B or the new Intern-S1 model locally, but it&amp;#39;s a challenge. VLLM and lmserve support the InternVL models, but appear to be GPU-only, and llama.cpp seems flaky at best when it comes to running them. (Massive hallucinations, errors with the model thinking there&amp;#39;s no image attached, etc.) I&amp;#39;m mostly looking to do image tagging with something more accurate than the (still quite good, but aging) wd14 model found in kohya_ss. I could probably step down to InternVL3-38B and still get some pretty great results, but I would need a 4 bit quant to fit into my GPU&amp;#39;s VRAM if using an engine that doesn&amp;#39;t support CPU offloading. Most quants for the model outside of GGUFs appear to be 8 bit. I could quantize it myself if I truly need to, but I&amp;#39;m hoping there&amp;#39;s a simpler solution I&amp;#39;m just unfamiliar with. I&amp;#39;m quite used to running LLMs locally, but multimodal models with image processing are new to me. Any help or insight for a good way to handle image tagging locally would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mad6sy",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Stickman561",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/",
            "subreddit_subscribers": 505251,
            "created_utc": 1753590152,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5dty1c",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "OutlandishnessIll466",
            "can_mod_post": false,
            "created_utc": 1753592245,
            "send_replies": true,
            "parent_id": "t3_1mad6sy",
            "score": 1,
            "author_fullname": "t2_e4ru5ouw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The best way I found is to just run the full Qwen VL 7B in transformers, it fits in 24 GB vram. Ofcourse this doesn't help you if you want larger models and do not have much vram. The dynamically quantized unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit also works almost as good. Unsloth might have more of them.\n\nThe gguf's that I tried performed terrible, although I did not try the final official implementation.\n\nI think vllm can run full models, but they don't support my p40's so I never got that to run either. I created my own little openai compatible service using the transformers implementation. [https://github.com/kkaarrss/qwen2\\_service](https://github.com/kkaarrss/qwen2_service) If you want something else then Qwen VL you would need adapt this code a bit with the implementation for the specific model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5dty1c",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The best way I found is to just run the full Qwen VL 7B in transformers, it fits in 24 GB vram. Ofcourse this doesn&amp;#39;t help you if you want larger models and do not have much vram. The dynamically quantized unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit also works almost as good. Unsloth might have more of them.&lt;/p&gt;\n\n&lt;p&gt;The gguf&amp;#39;s that I tried performed terrible, although I did not try the final official implementation.&lt;/p&gt;\n\n&lt;p&gt;I think vllm can run full models, but they don&amp;#39;t support my p40&amp;#39;s so I never got that to run either. I created my own little openai compatible service using the transformers implementation. &lt;a href=\"https://github.com/kkaarrss/qwen2_service\"&gt;https://github.com/kkaarrss/qwen2_service&lt;/a&gt; If you want something else then Qwen VL you would need adapt this code a bit with the implementation for the specific model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/n5dty1c/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753592245,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mad6sy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5du4ey",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753592332,
            "send_replies": true,
            "parent_id": "t3_1mad6sy",
            "score": 1,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You're definitely not the first to report struggles with InternVL3 and getting it to run.\n\nI run gemma3-27b and find that it's quite alright, though YMMV.  In particular one test I did on a scanned memo was like perfect for the first half (~700 tok?) then it almost immediately hallucinated the rest.  But for tables and texts boxes and stuff, it's been solid.  (Just testing tagging a random image and seems alright.)\n\nLlama.cpp will run it just fine on CPU.  That's actually how I run it since I have a big CPU and don't need it often so didn't bother to quantize the mmproj and/or figure out offload ratio.  Just messing now it seems like Q4_K_M main model with fp16 mmproj and 12k context uses 22GB and works.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5du4ey",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;re definitely not the first to report struggles with InternVL3 and getting it to run.&lt;/p&gt;\n\n&lt;p&gt;I run gemma3-27b and find that it&amp;#39;s quite alright, though YMMV.  In particular one test I did on a scanned memo was like perfect for the first half (~700 tok?) then it almost immediately hallucinated the rest.  But for tables and texts boxes and stuff, it&amp;#39;s been solid.  (Just testing tagging a random image and seems alright.)&lt;/p&gt;\n\n&lt;p&gt;Llama.cpp will run it just fine on CPU.  That&amp;#39;s actually how I run it since I have a big CPU and don&amp;#39;t need it often so didn&amp;#39;t bother to quantize the mmproj and/or figure out offload ratio.  Just messing now it seems like Q4_K_M main model with fp16 mmproj and 12k context uses 22GB and works.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/n5du4ey/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753592332,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mad6sy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5eju0j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OutlandishnessIll466",
                      "can_mod_post": false,
                      "created_utc": 1753606710,
                      "send_replies": true,
                      "parent_id": "t1_n5e34dk",
                      "score": 1,
                      "author_fullname": "t2_e4ru5ouw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "256 GB is enough to run the unquantized 38B I guess. I have no experience with this on CPU only with vllm.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5eju0j",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;256 GB is enough to run the unquantized 38B I guess. I have no experience with this on CPU only with vllm.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mad6sy",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/n5eju0j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753606710,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5eo46y",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "kmouratidis",
                      "can_mod_post": false,
                      "created_utc": 1753609320,
                      "send_replies": true,
                      "parent_id": "t1_n5e34dk",
                      "score": 1,
                      "author_fullname": "t2_k6u7rfxb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; 4 3090s crammed into a single case crying for the sweet release of death as they slowly oven themselves\n\n\nHaving mine is a 4U case, I feel personally attacked 🥲\n\n\nI haven't tried it locally, but at work we used vLLM CPU-only inference for small models on AWS instances and it worked (after some pain).\n\n\nOne other option is to use transformers. Nearly all models are supported, and most can use GPU &amp; CPU by setting the device_map. Not sure how faster (if at all) it will be than CPU-only vLLM, but maybe worth a try.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5eo46y",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;4 3090s crammed into a single case crying for the sweet release of death as they slowly oven themselves&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Having mine is a 4U case, I feel personally attacked 🥲&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t tried it locally, but at work we used vLLM CPU-only inference for small models on AWS instances and it worked (after some pain).&lt;/p&gt;\n\n&lt;p&gt;One other option is to use transformers. Nearly all models are supported, and most can use GPU &amp;amp; CPU by setting the device_map. Not sure how faster (if at all) it will be than CPU-only vLLM, but maybe worth a try.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mad6sy",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/n5eo46y/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753609320,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5e34dk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Stickman561",
            "can_mod_post": false,
            "created_utc": 1753597057,
            "send_replies": true,
            "parent_id": "t3_1mad6sy",
            "score": 1,
            "author_fullname": "t2_crdgh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yeah seems like the common consensus is that Gemma is one of the only vision models llama.cpp runs perfectly, but I really do want to use one of the InternVL3 series models. Regarding compute, while I can’t say I’m a Titan of local models like some of the users on here - I don’t have 4 3090s crammed into a single case crying for the sweet release of death as they slowly oven themselves - my computer is no slouch. I have 32GB of VRAM (RTX 5090) and 256GB of DDR5 system memory at 6000 MHz paired with a 9950X, so even if splitting isn’t possible I’d be willing to wait the (painfully long) time for CPU inference. I just really don’t want to dip below the 38B class because then the projector model drops in scale a TON.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5e34dk",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah seems like the common consensus is that Gemma is one of the only vision models llama.cpp runs perfectly, but I really do want to use one of the InternVL3 series models. Regarding compute, while I can’t say I’m a Titan of local models like some of the users on here - I don’t have 4 3090s crammed into a single case crying for the sweet release of death as they slowly oven themselves - my computer is no slouch. I have 32GB of VRAM (RTX 5090) and 256GB of DDR5 system memory at 6000 MHz paired with a 9950X, so even if splitting isn’t possible I’d be willing to wait the (painfully long) time for CPU inference. I just really don’t want to dip below the 38B class because then the projector model drops in scale a TON.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/n5e34dk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753597057,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mad6sy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ejmxg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Edge2098",
            "can_mod_post": false,
            "created_utc": 1753606590,
            "send_replies": true,
            "parent_id": "t3_1mad6sy",
            "score": 1,
            "author_fullname": "t2_uaotuj04",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Running InternVL locally is like asking a racehorse to live in your garage possible, but chaotic. Try BLIP or IDEFICS if you want sanity with good tags.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ejmxg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Running InternVL locally is like asking a racehorse to live in your garage possible, but chaotic. Try BLIP or IDEFICS if you want sanity with good tags.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/n5ejmxg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753606590,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mad6sy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5erjnn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1753611331,
            "send_replies": true,
            "parent_id": "t3_1mad6sy",
            "score": 2,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have run koboldcpp, https://github.com/matatonic/openedai-vision, and tabbyAPI for vision models. \n\nIf you're using GGUF, quantize the vision part separate from the LLM part. It won't quant very well.\n\nMy use isn't image tagging, it's chat with images. For that I'd be going with something like florence or JoyCaption. \n\nI dunno what to do for Intern-S1 either, I thought it could contend with pixtral-large but nothing supports it... maybe exllama at like 3 bit if ppl ask.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5erjnn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have run koboldcpp, &lt;a href=\"https://github.com/matatonic/openedai-vision\"&gt;https://github.com/matatonic/openedai-vision&lt;/a&gt;, and tabbyAPI for vision models. &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re using GGUF, quantize the vision part separate from the LLM part. It won&amp;#39;t quant very well.&lt;/p&gt;\n\n&lt;p&gt;My use isn&amp;#39;t image tagging, it&amp;#39;s chat with images. For that I&amp;#39;d be going with something like florence or JoyCaption. &lt;/p&gt;\n\n&lt;p&gt;I dunno what to do for Intern-S1 either, I thought it could contend with pixtral-large but nothing supports it... maybe exllama at like 3 bit if ppl ask.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mad6sy/how_are_you_running_multimodal_textimage_models/n5erjnn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753611331,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mad6sy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]