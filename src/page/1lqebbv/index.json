[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Paper: [https://arxiv.org/abs/2507.01949](https://arxiv.org/abs/2507.01949)\n\nProject Page: [https://kwai-keye.github.io/](https://kwai-keye.github.io/)\n\nCode: [https://github.com/Kwai-Keye/Keye](https://github.com/Kwai-Keye/Keye)\n\n&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Kwai-Keye/Keye-VL-8B-Preview · Hugging Face",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "New Model"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 75,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lqebbv",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.94,
            "author_flair_background_color": "#93b1ba",
            "ups": 26,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
            "is_original_content": false,
            "author_fullname": "t2_qjpsv",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "New Model",
            "can_mod_post": false,
            "score": 26,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2114f43f8a483d864b74ab185248e7dc7f2acd59",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3.1"
              }
            ],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1751509766,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "huggingface.co",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2507.01949\"&gt;https://arxiv.org/abs/2507.01949&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Project Page: &lt;a href=\"https://kwai-keye.github.io/\"&gt;https://kwai-keye.github.io/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/Kwai-Keye/Keye\"&gt;https://github.com/Kwai-Keye/Keye&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?auto=webp&amp;s=5ac1fd6f606741f5c30142be649c50021a8588ec",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1bc04f5722002b089d9f495fa7cdaf7f3700c9e",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa0520732fdbe4ef3053c95ef226e0a6ee79c4f6",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76e575e0f61ad8ceb6ddc30f00b7be46f6ec4694",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a674ea1c399ba022e42f0633ac66250ac99a0f9e",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0eae6b8b89456d5ebcfbd6cc4c7678b03dd124f",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ba496f256b1dae2db014d8c779db7cafd30a82b",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "Llama 3.1",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ffb000",
            "id": "1lqebbv",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ninjasaid13",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "light",
            "permalink": "/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/",
            "stickied": false,
            "url": "https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview",
            "subreddit_subscribers": 494001,
            "created_utc": 1751509766,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n129y2v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Iory1998",
            "can_mod_post": false,
            "created_utc": 1751511958,
            "send_replies": true,
            "parent_id": "t3_1lqebbv",
            "score": 3,
            "author_fullname": "t2_byt5wa14",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Exciting, but again another vision model would not be GGUFied anytime soon, sadly!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n129y2v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Exciting, but again another vision model would not be GGUFied anytime soon, sadly!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/n129y2v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751511958,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1lqebbv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]