[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "A no non-sense, complete byte-pair encoding implementation, in python, completely from scratch.\n\n```py\n\"\"\"\n@file model.py\n@license cc-by-sa-nc-4.0\n@ref https://aclanthology.org/P16-1162/\n@ref https://huggingface.co/blog/catherinearnett/dangers-of-tokenizer-recycling\n\"\"\"\n\nimport argparse\nimport collections\nimport json\nimport math\n\n\nclass Corpus:\n    \"\"\"Load and initialize training data\"\"\"\n\n    @staticmethod\n    def default() -&gt; list[str]:\n        return [\"lo\", \"low\", \"lower\", \"newest\", \"wide\", \"wider\", \"widest\"]\n\n    @staticmethod\n    def read(path: str) -&gt; list[str]:\n        \"\"\"Load a flat list of words from a file, one per whitespace.\"\"\"\n        words = []\n        with open(path, \"r\") as file:\n            for line in file:\n                for word in line.split():\n                    words.append(word)\n        return words\n\n    @staticmethod\n    def words(path: str = None) -&gt; list[str]:\n        if path:\n            print(f\"Using corpus from file: {path}\")\n            return Corpus.read(path)\n        print(\"Using default corpus.\")\n        return Corpus.default()\n\n    @staticmethod\n    def vocab(path: str = None) -&gt; dict[str, int]:\n        \"\"\"Convert list of words into vocab dict: space-joined symbols -&gt; freq.\"\"\"\n        vocab = {}\n        for word in Corpus.words(path):\n            symbols = list(word)\n            vocab[\" \".join(symbols)] = 1\n        print(\"Initialized vocab:\")\n        print(json.dumps(vocab, indent=2))\n        return vocab\n\n\nclass Model:\n    \"\"\"Byte-pair Encoding\"\"\"\n\n    @staticmethod\n    def pairs(vocab: dict[str, int]) -&gt; dict[tuple[str, str], int]:\n        # print(\"Generating pairs:\")\n        pairs = collections.defaultdict(int)  # init freqs to 0\n        for word, freq in vocab.items():  # unpacks (\"l o w &lt;/w&gt;\", 5)\n            symbols = word.split()  # split word by char -&gt; [\"l\", \"o\", \"w\", ...]\n            for i in range(len(symbols) - 1):  # for each step in the set of symbols\n                cur = symbols[i]  # \"l\"\n                nxt = symbols[i + 1]  # \"o\"\n                pairs[cur, nxt] += freq  # p[(\"l\", \"o\")] += 1\n                # print(f\"i={i}, cur='{cur}', nxt='{nxt}', freq={freq}\")\n        return pairs  # {('l', 'o'): 1}\n\n    @staticmethod\n    def bigram(symbols: list[str], pair: tuple[str, str]) -&gt; list[str]:\n        bigram = []\n        i = 0\n        while i &lt; len(symbols):\n            # If this symbol and the next match the pair, merge them\n            if (\n                i &lt; len(symbols) - 1\n                and symbols[i] == pair[0]\n                and symbols[i + 1] == pair[1]\n            ):\n                bigram.append(symbols[i] + symbols[i + 1])\n                i += 2  # Skip the next symbol (it's merged)\n            else:\n                bigram.append(symbols[i])\n                i += 1\n        return bigram\n\n    @staticmethod\n    def merges(vocab: dict[str, int], pair: tuple[str, str]) -&gt; dict[str, int]:\n        # print(\"Updated pairs:\")\n        # print(json.dumps(vocab, indent=2))\n\n        new_vocab = {}  # new empty vocab\n        for word in vocab:  # for each pair in a given map\n            symbols = word.split()  # [\"l\", \"o\", \"w\", \"&lt;/w&gt;\"]\n            bigram = Model.bigram(symbols, pair)  # merge neighbors\n            new_word = \" \".join(bigram)  # new n-gram\n            # print(f\"word={word}, new_word={new_word}\")\n            new_vocab[new_word] = vocab[word]\n        return new_vocab\n\n\nclass Tokenizer:\n    def __init__(self, vocab: dict[str, int]):\n        self.model = {\n            \"type\": \"BPE\",\n            \"version\": \"0.1.0\",\n            \"vocab\": vocab,\n            \"merges\": [],\n        }\n\n    @property\n    def type(self) -&gt; str:\n        return self.model[\"type\"]\n\n    @property\n    def version(self) -&gt; str:\n        return self.model[\"version\"]\n\n    @property\n    def vocab(self) -&gt; dict[str, int]:\n        return self.model[\"vocab\"]\n\n    @vocab.setter\n    def vocab(self, value: dict[str, int]) -&gt; None:\n        self.model[\"vocab\"] = value\n\n    @property\n    def merges(self) -&gt; list[tuple[str, str]]:\n        return self.model[\"merges\"]\n\n    @merges.setter\n    def merges(self, value: list[tuple[str, str]]):\n        self.model[\"merges\"] = value\n\n    def train(self, num_merges: int) -&gt; None:\n        # Train vocab model (vocab is the set of all merges)\n        self.merges = []\n        for i in range(num_merges):\n            # pre-process merge pairs every cycle\n            pairs = Model.pairs(self.vocab)  # create pairs\n            if not pairs:  # bail if pairs is empty\n                print(f\"Exhausted all potential pairs! Halted at step {i}.\")\n                break\n            # use the highest ranked pair for the next merge cycle\n            best = max(pairs, key=pairs.get)  # get max rank\n            self.merges.append(best)\n            self.vocab = Model.merges(self.vocab, best)  # merge ranked pair\n\n    def save(self, path: str) -&gt; None:\n        with open(path, \"w\", encoding=\"utf-8\") as file:\n            json.dump(self.model, file, ensure_ascii=False, indent=2)\n\n    def load(self, path: str) -&gt; None:\n        with open(path, \"r\", encoding=\"utf-8\") as file:\n            self.model = json.load(file)\n\n    @property\n    def tokens(self) -&gt; list[str]:\n        # Collect All Unique Tokens\n        token_set = set()\n        for word in self.vocab:  # must be vocab!\n            for symbol in word.split():\n                token_set.add(symbol)\n        # Assign IDs in sorted order (order matters)\n        return sorted(list(token_set))\n\n    @property\n    def token_to_id(self) -&gt; dict[str, int]:\n        return {token: idx for idx, token in enumerate(self.tokens)}\n\n    @property\n    def id_to_token(self) -&gt; dict[int, str]:\n        return {idx: token for idx, token in enumerate(self.tokens)}\n\n    @property\n    def ranks(self) -&gt; dict[str, int]:\n        # Build the rank table (rank merges)\n        rank_table = {}\n        for i, pair in enumerate(self.merges):  # must be merges!\n            token = \"\".join(pair)\n            rank_table[token] = i\n        return rank_table\n\n    @property\n    def scores(self):\n        # Score the merges\n        scores = {}\n        for token in self.tokens:\n            rank = self.ranks.get(token)\n            scores[token] = -math.log(rank + 1) if rank else -1e6\n        return scores\n\n    def encode(self, token: str) -&gt; int:\n        return self.token_to_id[token]\n\n    def decode(self, id: int) -&gt; str:\n        return self.id_to_token[id]\n\n\ndef parse_args() -&gt; argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-m\",\n        \"--merges\",\n        required=False,\n        type=int,\n        default=10,\n        help=\"number of merges\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--corpus\",\n        required=False,\n        type=str,\n        default=None,\n        help=\"input plaintext file\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    # Get number of merges (training cycles)\n    num_merges = int(args.merges)\n\n    # Get words from corpus (training data)\n    vocab = Corpus.vocab(args.corpus)\n\n    # Train vocab model (vocab is the set of all merges)\n    tokenizer = Tokenizer(vocab)\n    tokenizer.train(args.merges)\n\n    # Print vocab training results (dump merges)\n    print(\"Merge Table:\")\n    print(json.dumps(tokenizer.merges, indent=2))\n\n    print(\"Final Vocab:\")\n    print(json.dumps(tokenizer.vocab, indent=2))\n\n    print(\"Tokenizer:\")\n    print(json.dumps(tokenizer.token_to_id, indent=2))\n\n    # Build the rank table (rank merges)\n    print(\"Rank Table:\")\n    print(json.dumps(tokenizer.ranks, indent=2))\n\n    # Score the merges\n    print(\"Token Scores:\")\n    print(json.dumps(tokenizer.scores, indent=2))\n```\n\n- Used the original NMT paper as a core reference.\n- Zero dependencies.\n- Accepts plain-text input.\n- Stateful memory and disk ops.\n- Single-threaded.\n- Extensible.\n\nIt's dead simple, to the point, and - most importantly - legible. Excellent for learning and comprehension.\n\nI genuinely don't understand why implementations are so convoluted when it's only 250 lines of code.\n\nThe is the models voice box. A model \"learns\" from human created data as its input. It then converges towards the most common patterns during back-propagation.\n\nWithout a solid tokenizer, it's garbage in and garbage out. This is, of course, a single piece of a much bigger puzzle.\n\nI'm very interested in doing this for graphemes. And of course, there's a paper and repository on this as well.\n\n- https://aclanthology.org/2025.coling-main.400\n\nI am not affiliated with any of these authors, papers, orgs, etc. I'm just a dude trying to figure this stuff out. I love tinkering and understanding how things work at a fundamental level.\n\nThe internet is becoming a scary place, so stay safe out there, and keep your personal data close to your vest. Things are just starting heat up.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Vox Populi",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mjlg5q",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_slcrtxpr",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754526096,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A no non-sense, complete byte-pair encoding implementation, in python, completely from scratch.&lt;/p&gt;\n\n&lt;p&gt;```py\n&amp;quot;&amp;quot;&amp;quot;\n@file model.py\n@license cc-by-sa-nc-4.0\n@ref &lt;a href=\"https://aclanthology.org/P16-1162/\"&gt;https://aclanthology.org/P16-1162/&lt;/a&gt;\n@ref &lt;a href=\"https://huggingface.co/blog/catherinearnett/dangers-of-tokenizer-recycling\"&gt;https://huggingface.co/blog/catherinearnett/dangers-of-tokenizer-recycling&lt;/a&gt;\n&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;import argparse\nimport collections\nimport json\nimport math&lt;/p&gt;\n\n&lt;p&gt;class Corpus:\n    &amp;quot;&amp;quot;&amp;quot;Load and initialize training data&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@staticmethod\ndef default() -&amp;gt; list[str]:\n    return [&amp;quot;lo&amp;quot;, &amp;quot;low&amp;quot;, &amp;quot;lower&amp;quot;, &amp;quot;newest&amp;quot;, &amp;quot;wide&amp;quot;, &amp;quot;wider&amp;quot;, &amp;quot;widest&amp;quot;]\n\n@staticmethod\ndef read(path: str) -&amp;gt; list[str]:\n    &amp;quot;&amp;quot;&amp;quot;Load a flat list of words from a file, one per whitespace.&amp;quot;&amp;quot;&amp;quot;\n    words = []\n    with open(path, &amp;quot;r&amp;quot;) as file:\n        for line in file:\n            for word in line.split():\n                words.append(word)\n    return words\n\n@staticmethod\ndef words(path: str = None) -&amp;gt; list[str]:\n    if path:\n        print(f&amp;quot;Using corpus from file: {path}&amp;quot;)\n        return Corpus.read(path)\n    print(&amp;quot;Using default corpus.&amp;quot;)\n    return Corpus.default()\n\n@staticmethod\ndef vocab(path: str = None) -&amp;gt; dict[str, int]:\n    &amp;quot;&amp;quot;&amp;quot;Convert list of words into vocab dict: space-joined symbols -&amp;gt; freq.&amp;quot;&amp;quot;&amp;quot;\n    vocab = {}\n    for word in Corpus.words(path):\n        symbols = list(word)\n        vocab[&amp;quot; &amp;quot;.join(symbols)] = 1\n    print(&amp;quot;Initialized vocab:&amp;quot;)\n    print(json.dumps(vocab, indent=2))\n    return vocab\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;class Model:\n    &amp;quot;&amp;quot;&amp;quot;Byte-pair Encoding&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@staticmethod\ndef pairs(vocab: dict[str, int]) -&amp;gt; dict[tuple[str, str], int]:\n    # print(&amp;quot;Generating pairs:&amp;quot;)\n    pairs = collections.defaultdict(int)  # init freqs to 0\n    for word, freq in vocab.items():  # unpacks (&amp;quot;l o w &amp;lt;/w&amp;gt;&amp;quot;, 5)\n        symbols = word.split()  # split word by char -&amp;gt; [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;, ...]\n        for i in range(len(symbols) - 1):  # for each step in the set of symbols\n            cur = symbols[i]  # &amp;quot;l&amp;quot;\n            nxt = symbols[i + 1]  # &amp;quot;o&amp;quot;\n            pairs[cur, nxt] += freq  # p[(&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;)] += 1\n            # print(f&amp;quot;i={i}, cur=&amp;#39;{cur}&amp;#39;, nxt=&amp;#39;{nxt}&amp;#39;, freq={freq}&amp;quot;)\n    return pairs  # {(&amp;#39;l&amp;#39;, &amp;#39;o&amp;#39;): 1}\n\n@staticmethod\ndef bigram(symbols: list[str], pair: tuple[str, str]) -&amp;gt; list[str]:\n    bigram = []\n    i = 0\n    while i &amp;lt; len(symbols):\n        # If this symbol and the next match the pair, merge them\n        if (\n            i &amp;lt; len(symbols) - 1\n            and symbols[i] == pair[0]\n            and symbols[i + 1] == pair[1]\n        ):\n            bigram.append(symbols[i] + symbols[i + 1])\n            i += 2  # Skip the next symbol (it&amp;#39;s merged)\n        else:\n            bigram.append(symbols[i])\n            i += 1\n    return bigram\n\n@staticmethod\ndef merges(vocab: dict[str, int], pair: tuple[str, str]) -&amp;gt; dict[str, int]:\n    # print(&amp;quot;Updated pairs:&amp;quot;)\n    # print(json.dumps(vocab, indent=2))\n\n    new_vocab = {}  # new empty vocab\n    for word in vocab:  # for each pair in a given map\n        symbols = word.split()  # [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;, &amp;quot;&amp;lt;/w&amp;gt;&amp;quot;]\n        bigram = Model.bigram(symbols, pair)  # merge neighbors\n        new_word = &amp;quot; &amp;quot;.join(bigram)  # new n-gram\n        # print(f&amp;quot;word={word}, new_word={new_word}&amp;quot;)\n        new_vocab[new_word] = vocab[word]\n    return new_vocab\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;class Tokenizer:\n    def &lt;strong&gt;init&lt;/strong&gt;(self, vocab: dict[str, int]):\n        self.model = {\n            &amp;quot;type&amp;quot;: &amp;quot;BPE&amp;quot;,\n            &amp;quot;version&amp;quot;: &amp;quot;0.1.0&amp;quot;,\n            &amp;quot;vocab&amp;quot;: vocab,\n            &amp;quot;merges&amp;quot;: [],\n        }&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@property\ndef type(self) -&amp;gt; str:\n    return self.model[&amp;quot;type&amp;quot;]\n\n@property\ndef version(self) -&amp;gt; str:\n    return self.model[&amp;quot;version&amp;quot;]\n\n@property\ndef vocab(self) -&amp;gt; dict[str, int]:\n    return self.model[&amp;quot;vocab&amp;quot;]\n\n@vocab.setter\ndef vocab(self, value: dict[str, int]) -&amp;gt; None:\n    self.model[&amp;quot;vocab&amp;quot;] = value\n\n@property\ndef merges(self) -&amp;gt; list[tuple[str, str]]:\n    return self.model[&amp;quot;merges&amp;quot;]\n\n@merges.setter\ndef merges(self, value: list[tuple[str, str]]):\n    self.model[&amp;quot;merges&amp;quot;] = value\n\ndef train(self, num_merges: int) -&amp;gt; None:\n    # Train vocab model (vocab is the set of all merges)\n    self.merges = []\n    for i in range(num_merges):\n        # pre-process merge pairs every cycle\n        pairs = Model.pairs(self.vocab)  # create pairs\n        if not pairs:  # bail if pairs is empty\n            print(f&amp;quot;Exhausted all potential pairs! Halted at step {i}.&amp;quot;)\n            break\n        # use the highest ranked pair for the next merge cycle\n        best = max(pairs, key=pairs.get)  # get max rank\n        self.merges.append(best)\n        self.vocab = Model.merges(self.vocab, best)  # merge ranked pair\n\ndef save(self, path: str) -&amp;gt; None:\n    with open(path, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as file:\n        json.dump(self.model, file, ensure_ascii=False, indent=2)\n\ndef load(self, path: str) -&amp;gt; None:\n    with open(path, &amp;quot;r&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as file:\n        self.model = json.load(file)\n\n@property\ndef tokens(self) -&amp;gt; list[str]:\n    # Collect All Unique Tokens\n    token_set = set()\n    for word in self.vocab:  # must be vocab!\n        for symbol in word.split():\n            token_set.add(symbol)\n    # Assign IDs in sorted order (order matters)\n    return sorted(list(token_set))\n\n@property\ndef token_to_id(self) -&amp;gt; dict[str, int]:\n    return {token: idx for idx, token in enumerate(self.tokens)}\n\n@property\ndef id_to_token(self) -&amp;gt; dict[int, str]:\n    return {idx: token for idx, token in enumerate(self.tokens)}\n\n@property\ndef ranks(self) -&amp;gt; dict[str, int]:\n    # Build the rank table (rank merges)\n    rank_table = {}\n    for i, pair in enumerate(self.merges):  # must be merges!\n        token = &amp;quot;&amp;quot;.join(pair)\n        rank_table[token] = i\n    return rank_table\n\n@property\ndef scores(self):\n    # Score the merges\n    scores = {}\n    for token in self.tokens:\n        rank = self.ranks.get(token)\n        scores[token] = -math.log(rank + 1) if rank else -1e6\n    return scores\n\ndef encode(self, token: str) -&amp;gt; int:\n    return self.token_to_id[token]\n\ndef decode(self, id: int) -&amp;gt; str:\n    return self.id_to_token[id]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def parse_args() -&amp;gt; argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;--merges&amp;quot;,\n        required=False,\n        type=int,\n        default=10,\n        help=&amp;quot;number of merges&amp;quot;,\n    )\n    parser.add_argument(\n        &amp;quot;-c&amp;quot;,\n        &amp;quot;--corpus&amp;quot;,\n        required=False,\n        type=str,\n        default=None,\n        help=&amp;quot;input plaintext file&amp;quot;,\n    )\n    return parser.parse_args()&lt;/p&gt;\n\n&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;:\n    args = parse_args()&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Get number of merges (training cycles)\nnum_merges = int(args.merges)\n\n# Get words from corpus (training data)\nvocab = Corpus.vocab(args.corpus)\n\n# Train vocab model (vocab is the set of all merges)\ntokenizer = Tokenizer(vocab)\ntokenizer.train(args.merges)\n\n# Print vocab training results (dump merges)\nprint(&amp;quot;Merge Table:&amp;quot;)\nprint(json.dumps(tokenizer.merges, indent=2))\n\nprint(&amp;quot;Final Vocab:&amp;quot;)\nprint(json.dumps(tokenizer.vocab, indent=2))\n\nprint(&amp;quot;Tokenizer:&amp;quot;)\nprint(json.dumps(tokenizer.token_to_id, indent=2))\n\n# Build the rank table (rank merges)\nprint(&amp;quot;Rank Table:&amp;quot;)\nprint(json.dumps(tokenizer.ranks, indent=2))\n\n# Score the merges\nprint(&amp;quot;Token Scores:&amp;quot;)\nprint(json.dumps(tokenizer.scores, indent=2))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Used the original NMT paper as a core reference.&lt;/li&gt;\n&lt;li&gt;Zero dependencies.&lt;/li&gt;\n&lt;li&gt;Accepts plain-text input.&lt;/li&gt;\n&lt;li&gt;Stateful memory and disk ops.&lt;/li&gt;\n&lt;li&gt;Single-threaded.&lt;/li&gt;\n&lt;li&gt;Extensible.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s dead simple, to the point, and - most importantly - legible. Excellent for learning and comprehension.&lt;/p&gt;\n\n&lt;p&gt;I genuinely don&amp;#39;t understand why implementations are so convoluted when it&amp;#39;s only 250 lines of code.&lt;/p&gt;\n\n&lt;p&gt;The is the models voice box. A model &amp;quot;learns&amp;quot; from human created data as its input. It then converges towards the most common patterns during back-propagation.&lt;/p&gt;\n\n&lt;p&gt;Without a solid tokenizer, it&amp;#39;s garbage in and garbage out. This is, of course, a single piece of a much bigger puzzle.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m very interested in doing this for graphemes. And of course, there&amp;#39;s a paper and repository on this as well.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://aclanthology.org/2025.coling-main.400\"&gt;https://aclanthology.org/2025.coling-main.400&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am not affiliated with any of these authors, papers, orgs, etc. I&amp;#39;m just a dude trying to figure this stuff out. I love tinkering and understanding how things work at a fundamental level.&lt;/p&gt;\n\n&lt;p&gt;The internet is becoming a scary place, so stay safe out there, and keep your personal data close to your vest. Things are just starting heat up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?auto=webp&amp;s=e6db33843099d99dba5c7ea4c05b57efab76d21a",
                    "width": 600,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1410c1e7e9cb4ef838128bfdf9421febd66849c6",
                      "width": 108,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdad77b771381495eba98ab1fd8af71cd545883b",
                      "width": 216,
                      "height": 216
                    },
                    {
                      "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99f54072c9e69c278627c03b6fad54b0e37a0d2b",
                      "width": 320,
                      "height": 320
                    }
                  ],
                  "variants": {},
                  "id": "L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mjlg5q",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "teleprint-me",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjlg5q/vox_populi/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjlg5q/vox_populi/",
            "subreddit_subscribers": 512425,
            "created_utc": 1754526096,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]