[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone,\n\nI'm planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via [runpod.io](http://runpod.io)), but I've hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. \n\nI'd appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!\n\n**The Goal**\n\n* **Primary:** Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.\n* **Secondary:** Use separate GPUs for other tasks\n\n**Core System Components**\n\n* **CPU:** AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (\\~300 CHF)\n* **Motherboard:** ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (\\~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )\n* **RAM:** 512 GB DDR4 ECC (Already have)\n* **Storage:** 2TB - 4TB PCIe 4.0 NVMe SSD (\\~300 CHF)\n* **CPU Cooler:** A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (\\~100 CHF)\n* **PSU:** Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (\\~500 CHF)\n\n**GPU Configuration**\n\n* **For the LLM:**\n   * **4x NVIDIA RTX 5090** (\\~8,000 CHF)\n   * I plan to **power-limit each RTX 5090 to 300-350W**. \n* **For other tasks**\n   * 2x NVIDIA RTX 5080 (\\~2,400 CHF total)\n\n**My Main Questions &amp; Problems**\n\n1. Case / Chassis\\*\\*:\\*\\* This is my biggest problem. What's the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?\n2. GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I'll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?\n3. General Sanity Check: Looking at the whole picture, does this setup look ok? \n4. Do you think using the first of the cases below would work with riser cables and the above listed hardware ? \n\nThanks in advance for your time!\n\nFor the cases so far i thought about: \n\n1. [https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr\\_1\\_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1](https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1)\n2. [ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr\\_1\\_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6](https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6)\n3. [https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr\\_1\\_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3](https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Help building an price efficient inference server (no fine tuning) + multi 5090 setup",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhu9tx",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_oaw1i0pr4",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754352667,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via &lt;a href=\"http://runpod.io\"&gt;runpod.io&lt;/a&gt;), but I&amp;#39;ve hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Goal&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Primary:&lt;/strong&gt; Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Secondary:&lt;/strong&gt; Use separate GPUs for other tasks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Core System Components&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 512 GB DDR4 ECC (Already have)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 2TB - 4TB PCIe 4.0 NVMe SSD (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (~100 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (~500 CHF)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU Configuration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;For the LLM:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;4x NVIDIA RTX 5090&lt;/strong&gt; (~8,000 CHF)&lt;/li&gt;\n&lt;li&gt;I plan to &lt;strong&gt;power-limit each RTX 5090 to 300-350W&lt;/strong&gt;. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For other tasks&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2x NVIDIA RTX 5080 (~2,400 CHF total)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My Main Questions &amp;amp; Problems&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Case / Chassis**:** This is my biggest problem. What&amp;#39;s the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?&lt;/li&gt;\n&lt;li&gt;GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I&amp;#39;ll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?&lt;/li&gt;\n&lt;li&gt;General Sanity Check: Looking at the whole picture, does this setup look ok? &lt;/li&gt;\n&lt;li&gt;Do you think using the first of the cases below would work with riser cables and the above listed hardware ? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for your time!&lt;/p&gt;\n\n&lt;p&gt;For the cases so far i thought about: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1\"&gt;https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6\"&gt;ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3\"&gt;https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?auto=webp&amp;s=9b598b3fe915c4e340a1d2be347d6ada11f361b7",
                    "width": 2400,
                    "height": 1260
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2ec29473ad9a43f57f6de38e719603168628711",
                      "width": 108,
                      "height": 56
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbe80d3cfbb71c2262379cd9b070f60a3559377",
                      "width": 216,
                      "height": 113
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=756815133007c791068d5b797184c842b074ab75",
                      "width": 320,
                      "height": 168
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4feb605e519121401324c6cc277e71f0c83948d",
                      "width": 640,
                      "height": 336
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad4525174d8ac6007a90f9bd6e65c6a7aa6a406c",
                      "width": 960,
                      "height": 504
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fece91f94e327af10d6bd9f2be0f89eeec62d4cb",
                      "width": 1080,
                      "height": 567
                    }
                  ],
                  "variants": {},
                  "id": "PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhu9tx",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Civil-Image5411",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754352667,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6zug0p",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Khipu28",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6z5oyt",
                                "score": 1,
                                "author_fullname": "t2_a0v2ol2u",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The MaxQ shows higher clocks than the workstation version at same TDP target. Up to 450W is needed on the workstation cards to catch the MaxQ. They seem to select better chips and both professional cards also have more CUs active.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6zug0p",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The MaxQ shows higher clocks than the workstation version at same TDP target. Up to 450W is needed on the workstation cards to catch the MaxQ. They seem to select better chips and both professional cards also have more CUs active.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhu9tx",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6zug0p/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754366290,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754366290,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z5oyt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Civil-Image5411",
                      "can_mod_post": false,
                      "created_utc": 1754356922,
                      "send_replies": true,
                      "parent_id": "t1_n6yxc0u",
                      "score": 1,
                      "author_fullname": "t2_oaw1i0pr4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks, \n\nYes i am serving multiple requests at once and the speed is important for me. \n\nThe max-q would likely be the easiest way to go, however the speed is significantly lower and the price similar.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z5oyt",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, &lt;/p&gt;\n\n&lt;p&gt;Yes i am serving multiple requests at once and the speed is important for me. &lt;/p&gt;\n\n&lt;p&gt;The max-q would likely be the easiest way to go, however the speed is significantly lower and the price similar.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhu9tx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6z5oyt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754356922,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yxc0u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1754353963,
            "send_replies": true,
            "parent_id": "t3_1mhu9tx",
            "score": 2,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you going to be serving multiple requests at once? If not there wont be an advantage to having 4 5090 vs 1 rtx pro 6000 + 1 5090.\n\nI think the hardest part of your build is managing the physical size and power consumption. Id recommend 1 rtx pro 6000 MAX Q. The maxq version is already limited to 300w and is about $8000 pair this with 1 5090 and you will have 128gb vram. If you drop the power profile on the 5090 too then at 600w things start to look very manageable.\n\nLooks like 11k in gpus and you would have 4k left over for everything else.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yxc0u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you going to be serving multiple requests at once? If not there wont be an advantage to having 4 5090 vs 1 rtx pro 6000 + 1 5090.&lt;/p&gt;\n\n&lt;p&gt;I think the hardest part of your build is managing the physical size and power consumption. Id recommend 1 rtx pro 6000 MAX Q. The maxq version is already limited to 300w and is about $8000 pair this with 1 5090 and you will have 128gb vram. If you drop the power profile on the 5090 too then at 600w things start to look very manageable.&lt;/p&gt;\n\n&lt;p&gt;Looks like 11k in gpus and you would have 4k left over for everything else.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6yxc0u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754353963,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhu9tx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6zc2on",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Civil-Image5411",
                      "can_mod_post": false,
                      "created_utc": 1754359179,
                      "send_replies": true,
                      "parent_id": "t1_n6z6v9n",
                      "score": 1,
                      "author_fullname": "t2_oaw1i0pr4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks, thats really helpful to know.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6zc2on",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, thats really helpful to know.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhu9tx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6zc2on/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754359179,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6z6v9n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BobbyL2k",
            "can_mod_post": false,
            "created_utc": 1754357349,
            "send_replies": true,
            "parent_id": "t3_1mhu9tx",
            "score": 2,
            "author_fullname": "t2_ghoyg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "- 5090s cannot be power limited to under 400w.\n- transient power usage spikes still happen with the power limit, the power limit is more like an average power consumption limiter rather than a hard limit.\n\nSo you will probably need the second power supply",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6z6v9n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;5090s cannot be power limited to under 400w.&lt;/li&gt;\n&lt;li&gt;transient power usage spikes still happen with the power limit, the power limit is more like an average power consumption limiter rather than a hard limit.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So you will probably need the second power supply&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6z6v9n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754357349,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhu9tx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n711jvk",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Dear-Argument7658",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6zbnb5",
                                                    "score": 1,
                                                    "author_fullname": "t2_y3hxemhj4",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I have been having decent performance with ik\\_llama.cpp for CPU offloading with the bigger MoE models, but it's not great for higher volumes since you have no benefit of batched requests. So maybe if you need high throughput it's better to go with the DDR4 setup like you planned.\n\nNot sure if it's relevant for your use case but I am currently testing nanonets/Nanonets-OCR-s for getting output from invoices and it seems to do a very decent job, even with Swedish documents. I use it to feed the text to GLM 4.5-Air to extract relevant bits in a structured output.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n711jvk",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have been having decent performance with ik_llama.cpp for CPU offloading with the bigger MoE models, but it&amp;#39;s not great for higher volumes since you have no benefit of batched requests. So maybe if you need high throughput it&amp;#39;s better to go with the DDR4 setup like you planned.&lt;/p&gt;\n\n&lt;p&gt;Not sure if it&amp;#39;s relevant for your use case but I am currently testing nanonets/Nanonets-OCR-s for getting output from invoices and it seems to do a very decent job, even with Swedish documents. I use it to feed the text to GLM 4.5-Air to extract relevant bits in a structured output.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mhu9tx",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n711jvk/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754389481,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754389481,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6zbnb5",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Civil-Image5411",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6z7lve",
                                          "score": 1,
                                          "author_fullname": "t2_oaw1i0pr4",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "That’s a good point about the new motherboard especially for a future MoE setup. Looks like I’ll need to stock up on even more DDR5 memory then 🫣.\n\nI haven’t tried LLaMA 4 scout yet, only LLaMA 3.3 72B instruct, and that didn’t perform well for my use case (image-to-structured output). Might be worth a try.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6zbnb5",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s a good point about the new motherboard especially for a future MoE setup. Looks like I’ll need to stock up on even more DDR5 memory then 🫣.&lt;/p&gt;\n\n&lt;p&gt;I haven’t tried LLaMA 4 scout yet, only LLaMA 3.3 72B instruct, and that didn’t perform well for my use case (image-to-structured output). Might be worth a try.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhu9tx",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6zbnb5/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754359031,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754359031,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6z7lve",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Dear-Argument7658",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6z422e",
                                "score": 1,
                                "author_fullname": "t2_y3hxemhj4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I am sorry I didn't consider your use case is not in English seeing you mentioned Swiss currency, in that case I am not surprised 72B performs better.\n\nThat's a substantial difference and I am a bit surprised it was that large, but thinking about it maybe \\~20 t/s is the best you are going to get with a single Blackwell. Been so spoiled with MoE models lately you tend to go blind for how slow the dense models runs.\n\nFor tensor parallel on 5090s I would think PCIe 5.0 can make a difference, but since I never tried myself I can't back that up with any true real facts - it probably wont merit the extra expense unless you will use the system for anything else besides GPU inference? If you ever want to do hybrid CPU+GPU inference a DDR5 platform will be a lot better, but that's not relevant for high performance inference.\n\nAnother thought, even though Llama 4 series was a disappointment, did you ever evaluate Scout for your use case? It's not a great model by any stretch but it seems to at least show pretty decent results for visual understanding.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6z7lve",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am sorry I didn&amp;#39;t consider your use case is not in English seeing you mentioned Swiss currency, in that case I am not surprised 72B performs better.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s a substantial difference and I am a bit surprised it was that large, but thinking about it maybe ~20 t/s is the best you are going to get with a single Blackwell. Been so spoiled with MoE models lately you tend to go blind for how slow the dense models runs.&lt;/p&gt;\n\n&lt;p&gt;For tensor parallel on 5090s I would think PCIe 5.0 can make a difference, but since I never tried myself I can&amp;#39;t back that up with any true real facts - it probably wont merit the extra expense unless you will use the system for anything else besides GPU inference? If you ever want to do hybrid CPU+GPU inference a DDR5 platform will be a lot better, but that&amp;#39;s not relevant for high performance inference.&lt;/p&gt;\n\n&lt;p&gt;Another thought, even though Llama 4 series was a disappointment, did you ever evaluate Scout for your use case? It&amp;#39;s not a great model by any stretch but it seems to at least show pretty decent results for visual understanding.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhu9tx",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6z7lve/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754357615,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754357615,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z422e",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Civil-Image5411",
                      "can_mod_post": false,
                      "created_utc": 1754356337,
                      "send_replies": true,
                      "parent_id": "t1_n6yz9ad",
                      "score": 1,
                      "author_fullname": "t2_oaw1i0pr4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the reply,\nI need a VLM i am processing images, unfortunately it’s not released yet for Qwen 3. Qwen 2.5 VL 32B performs worse for my use case than the 72B model. Maybe it’s language related the prompts are not in English or Chinese but in German, and I can measure the output accuracy precisely. Gemma performed significantly worse.\n\nFor single requests I got around 17 tokens per second with the fp8 quantized model using the RTX Pro 6000 WS and around 50 tokens per second using the 4x 5090s. I was getting around 1k/s total throughput for the RTX Pro 6000 didn’t check it for the 5090s. However, both are relevant for me.\n\nOk, so I will certainly need to add another PSU. What was your problem to keep it stable ? \n\nDo you think I will get a performance improvement using PCIe 5? The DDR5 prices are crazy at least $1.5k for 256GB for the ecc ram required for epyc. However, yes, motherboard and CPU are in an acceptable price range as long as I choose the EPYC instead of the Threadripper Pro.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z422e",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the reply,\nI need a VLM i am processing images, unfortunately it’s not released yet for Qwen 3. Qwen 2.5 VL 32B performs worse for my use case than the 72B model. Maybe it’s language related the prompts are not in English or Chinese but in German, and I can measure the output accuracy precisely. Gemma performed significantly worse.&lt;/p&gt;\n\n&lt;p&gt;For single requests I got around 17 tokens per second with the fp8 quantized model using the RTX Pro 6000 WS and around 50 tokens per second using the 4x 5090s. I was getting around 1k/s total throughput for the RTX Pro 6000 didn’t check it for the 5090s. However, both are relevant for me.&lt;/p&gt;\n\n&lt;p&gt;Ok, so I will certainly need to add another PSU. What was your problem to keep it stable ? &lt;/p&gt;\n\n&lt;p&gt;Do you think I will get a performance improvement using PCIe 5? The DDR5 prices are crazy at least $1.5k for 256GB for the ecc ram required for epyc. However, yes, motherboard and CPU are in an acceptable price range as long as I choose the EPYC instead of the Threadripper Pro.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhu9tx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6z422e/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754356337,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yz9ad",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dear-Argument7658",
            "can_mod_post": false,
            "created_utc": 1754354639,
            "send_replies": true,
            "parent_id": "t3_1mhu9tx",
            "score": 1,
            "author_fullname": "t2_y3hxemhj4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What's your main reason for choosing Qwen 2.5 VL 72B over the more recent 32B? Are you certain you saw 3x performance with 4x 5090 vs 1x RTX 6000 Pro? Was that for single requests or batched?\n\nIf the RTX 6000 Pro's performance is sufficient for your use case, I'd skip the quad GPU setup. It's not hard to get it running, but keeping it stable is the real challenge. You'll probably need dual PSUs for stability - I had a 4x 3090 rig with a quality 2000W PSU, and it still hit power limits when running LMDeploy with tensor parallel. GPU power limiting does little to reduce transients on NVIDIA hardware, unfortunately - but I can't say for the 5090.\n\nIf you do go with 4x 5090s, consider a more recent EPYC platform to utilize PCIe 5.0 (if you can find compatible risers). The CPU and motherboard costs are reasonable, and you'll have a much better platform, especially with Zen 5. Though DDR5 RDIMMs are still somewhat overpriced.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yz9ad",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your main reason for choosing Qwen 2.5 VL 72B over the more recent 32B? Are you certain you saw 3x performance with 4x 5090 vs 1x RTX 6000 Pro? Was that for single requests or batched?&lt;/p&gt;\n\n&lt;p&gt;If the RTX 6000 Pro&amp;#39;s performance is sufficient for your use case, I&amp;#39;d skip the quad GPU setup. It&amp;#39;s not hard to get it running, but keeping it stable is the real challenge. You&amp;#39;ll probably need dual PSUs for stability - I had a 4x 3090 rig with a quality 2000W PSU, and it still hit power limits when running LMDeploy with tensor parallel. GPU power limiting does little to reduce transients on NVIDIA hardware, unfortunately - but I can&amp;#39;t say for the 5090.&lt;/p&gt;\n\n&lt;p&gt;If you do go with 4x 5090s, consider a more recent EPYC platform to utilize PCIe 5.0 (if you can find compatible risers). The CPU and motherboard costs are reasonable, and you&amp;#39;ll have a much better platform, especially with Zen 5. Though DDR5 RDIMMs are still somewhat overpriced.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6yz9ad/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754354639,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhu9tx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70pwdz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754382895,
            "send_replies": true,
            "parent_id": "t3_1mhu9tx",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "there are many chinese \"AI\" server chassis for 8 GPUs, unfortunately I don't know any \"good\" brand to recommend. If ordering a noname brand from China is an option then you could search for \"8 GPU server\" on Taobao. Update: I've just found out that they also sell these servers through Ebay, more expensive than Taobao though but much easier to order.",
            "edited": 1754383084,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70pwdz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;there are many chinese &amp;quot;AI&amp;quot; server chassis for 8 GPUs, unfortunately I don&amp;#39;t know any &amp;quot;good&amp;quot; brand to recommend. If ordering a noname brand from China is an option then you could search for &amp;quot;8 GPU server&amp;quot; on Taobao. Update: I&amp;#39;ve just found out that they also sell these servers through Ebay, more expensive than Taobao though but much easier to order.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n70pwdz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754382895,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhu9tx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]