[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone,\n\nI'm planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via [runpod.io](http://runpod.io)), but I've hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. \n\nI'd appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!\n\n**The Goal**\n\n* **Primary:** Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.\n* **Secondary:** Use separate GPUs for other tasks\n\n**Core System Components**\n\n* **CPU:** AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (\\~300 CHF)\n* **Motherboard:** ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (\\~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )\n* **RAM:** 512 GB DDR4 ECC (Already have)\n* **Storage:** 2TB - 4TB PCIe 4.0 NVMe SSD (\\~300 CHF)\n* **CPU Cooler:** A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (\\~100 CHF)\n* **PSU:** Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (\\~500 CHF)\n\n**GPU Configuration**\n\n* **For the LLM:**\n   * **4x NVIDIA RTX 5090** (\\~8,000 CHF)\n   * I plan to **power-limit each RTX 5090 to 300-350W**. \n* **For other tasks**\n   * 2x NVIDIA RTX 5080 (\\~2,400 CHF total)\n\n**My Main Questions &amp; Problems**\n\n1. Case / Chassis\\*\\*:\\*\\* This is my biggest problem. What's the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?\n2. GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I'll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?\n3. General Sanity Check: Looking at the whole picture, does this setup look ok? \n4. Do you think using the first of the cases below would work with riser cables and the above listed hardware ? \n\nThanks in advance for your time!\n\nFor the cases so far i thought about: \n\n1. [https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr\\_1\\_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1](https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-1)\n2. [ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr\\_1\\_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6](https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754348992&amp;sprefix=mining+ri%2Caps%2C116&amp;sr=8-6)\n3. [https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr\\_1\\_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2\\_D32-G2a\\_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm\\_Dg-ZLK-GZpvwX1\\_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD\\_-7BaQ&amp;dib\\_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3](https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;dib_tag=se&amp;keywords=mining+rig&amp;qid=1754349402&amp;sprefix=mining+rig%2Caps%2C177&amp;sr=8-3)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Help building an price efficient inference server (no fine tuning) + multi 5090 setup",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mhu9tx",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_oaw1i0pr4",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754352667,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning a new build primarily for running a 72B model using vLLM (qwen 72b vl) with FP8.I think about  using four RTX 5090s for the highest throughput. I was also thinking about one rtx pro 6000 but the inference speed is much slower and the cost similar. I get about 3x the throughput with 4x 5090 (tested via &lt;a href=\"http://runpod.io\"&gt;runpod.io&lt;/a&gt;), but I&amp;#39;ve hit a wall concerning the case/chassis and the physical connectivity.  Also my budget is limited, I would like to keep it below 15k. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate a sanity check on the whole build, especially on the feasibility of housing and connecting four of these GPUs. Any comments, critiques, or suggestions are more than welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Goal&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Primary:&lt;/strong&gt; Run a 72B parameter model with FP8 quantisation using vLLM on a dedicated set of GPUs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Secondary:&lt;/strong&gt; Use separate GPUs for other tasks&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Core System Components&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD EPYC 7402P (24 Cores, 48 Threads, 180W TDP) (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Rack ROMED8-2T (Socket SP3, 8x DDR4, 7x PCIe 4.0 x16 slots) (~750 CHF) (  I chose a motherboard with pcie 4 because i will not use the setup for fine tuning and the pricing of the cpu + motherboard is much cheaper and i already have ram for it. )&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 512 GB DDR4 ECC (Already have)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 2TB - 4TB PCIe 4.0 NVMe SSD (~300 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; A high-performance air cooler compatible with AMD SP3, like a Noctua TR4-SP3. (~100 CHF)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Fortron Cannon Pro 2500W (80+ Platinum). Should be enough even with all GPUs. Could also add a second in case not or the ASUS Pro WS 3000W if its released soon (~500 CHF)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU Configuration&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;For the LLM:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;4x NVIDIA RTX 5090&lt;/strong&gt; (~8,000 CHF)&lt;/li&gt;\n&lt;li&gt;I plan to &lt;strong&gt;power-limit each RTX 5090 to 300-350W&lt;/strong&gt;. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;For other tasks&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2x NVIDIA RTX 5080 (~2,400 CHF total)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My Main Questions &amp;amp; Problems&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Case / Chassis**:** This is my biggest problem. What&amp;#39;s the best way to house 4x RTX 5090s (plus 2x 5080s)? An open-air frame seems likely, but which ones work well for this many cards? Is there a server chassis that could handle the spacing, where it would eventually also be possible to add more later ?&lt;/li&gt;\n&lt;li&gt;GPU Connectivity: The motherboard has 7x PCIe 4.0 x16 slots. I&amp;#39;ll need to use 6 of them, whats the best way to connect them, pcie riser cables, are there any problems fixing the gpu just in front or do i need  any kind of gpu support or is the pcie bracket strong enough?&lt;/li&gt;\n&lt;li&gt;General Sanity Check: Looking at the whole picture, does this setup look ok? &lt;/li&gt;\n&lt;li&gt;Do you think using the first of the cases below would work with riser cables and the above listed hardware ? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for your time!&lt;/p&gt;\n\n&lt;p&gt;For the cases so far i thought about: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1\"&gt;https://www.amazon.de/-/en/Mining-Support-Supply-Currency-Bitcoin-Black/dp/B094H1Z8RB/ref=sr_1_1?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-1&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6\"&gt;ttps://www.amazon.de/-/en/Mining-Currency-Ethereum-Bitcoin-Support-Black/dp/B09DGKLKY3/ref=sr_1_6?crid=7NH56ZDDBUQV&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754348992&amp;amp;sprefix=mining+ri%2Caps%2C116&amp;amp;sr=8-6&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3\"&gt;https://www.amazon.de/-/en/MININGEEK-Raised-Support-Cooling-Supplies-black/dp/B0D1G91MKC/ref=sr_1_3?crid=1W2DBFC5AP24X&amp;amp;dib=eyJ2IjoiMSJ9.Rjm2V5Wqm9dVGQCIK08wS6fZGBucCbqoY4Iz0kjl9lANDr5SgBbWclCJibrpyzj6tXm-WgfRhy1r2vYK-tMOIVNwE0vWUPONrN9WEpqJEPL75LmFsKkqd2roliTwhh0DhjLaviRusqziGpknffYQD2_D32-G2a_vWtZfgi96IZNKRNeT4NYf5KwFSjtTzfRFc9x-Th7XsLAHaNYFHJ5PEMStm_Dg-ZLK-GZpvwX1_io.ehk9A5dK-ACwnP-TPX45DxKR2nfOWkMpkK2iD_-7BaQ&amp;amp;dib_tag=se&amp;amp;keywords=mining+rig&amp;amp;qid=1754349402&amp;amp;sprefix=mining+rig%2Caps%2C177&amp;amp;sr=8-3&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?auto=webp&amp;s=9b598b3fe915c4e340a1d2be347d6ada11f361b7",
                    "width": 2400,
                    "height": 1260
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2ec29473ad9a43f57f6de38e719603168628711",
                      "width": 108,
                      "height": 56
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3fbe80d3cfbb71c2262379cd9b070f60a3559377",
                      "width": 216,
                      "height": 113
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=756815133007c791068d5b797184c842b074ab75",
                      "width": 320,
                      "height": 168
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4feb605e519121401324c6cc277e71f0c83948d",
                      "width": 640,
                      "height": 336
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad4525174d8ac6007a90f9bd6e65c6a7aa6a406c",
                      "width": 960,
                      "height": 504
                    },
                    {
                      "url": "https://external-preview.redd.it/PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fece91f94e327af10d6bd9f2be0f89eeec62d4cb",
                      "width": 1080,
                      "height": 567
                    }
                  ],
                  "variants": {},
                  "id": "PkgpYMq-5_74dXyko9Zh9FOppnVlxdQc6WIclap5mWY"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhu9tx",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Civil-Image5411",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754352667,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yxc0u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1754353963,
            "send_replies": true,
            "parent_id": "t3_1mhu9tx",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you going to be serving multiple requests at once? If not there wont be an advantage to having 4 5090 vs 1 rtx pro 6000 + 1 5090.\n\nI think the hardest part of your build is managing the physical size and power consumption. Id recommend 1 rtx pro 6000 MAX Q. The maxq version is already limited to 300w and is about $8000 pair this with 1 5090 and you will have 128gb vram. If you drop the power profile on the 5090 too then at 600w things start to look very manageable.\n\nLooks like 11k in gpus and you would have 4k left over for everything else.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yxc0u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you going to be serving multiple requests at once? If not there wont be an advantage to having 4 5090 vs 1 rtx pro 6000 + 1 5090.&lt;/p&gt;\n\n&lt;p&gt;I think the hardest part of your build is managing the physical size and power consumption. Id recommend 1 rtx pro 6000 MAX Q. The maxq version is already limited to 300w and is about $8000 pair this with 1 5090 and you will have 128gb vram. If you drop the power profile on the 5090 too then at 600w things start to look very manageable.&lt;/p&gt;\n\n&lt;p&gt;Looks like 11k in gpus and you would have 4k left over for everything else.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6yxc0u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754353963,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhu9tx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z422e",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Civil-Image5411",
                      "can_mod_post": false,
                      "created_utc": 1754356337,
                      "send_replies": true,
                      "parent_id": "t1_n6yz9ad",
                      "score": 1,
                      "author_fullname": "t2_oaw1i0pr4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the reply,\nI need a VLM i am processing images, unfortunately it’s not released yet for Qwen 3. Qwen 2.5 VL 32B performs worse for my use case than the 72B model. Maybe it’s language related the prompts are not in English or Chinese but in German, and I can measure the output accuracy precisely. Gemma performed significantly worse.\n\nFor single requests I got around 17 tokens per second with the fp8 quantized model using the RTX Pro 6000 WS and around 50 tokens per second using the 4x 5090s. I was getting around 1k/s total throughput for the RTX Pro 6000 didn’t check it for the 5090s. However, both are relevant for me.\n\nOk, so I will certainly need to add another PSU. What was your problem to keep it stable ? \n\nDo you think I will get a performance improvement using PCIe 5? The DDR5 prices are crazy at least $1.5k for 256GB for the ecc ram required for epyc. However, yes, motherboard and CPU are in an acceptable price range as long as I choose the EPYC instead of the Threadripper Pro.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z422e",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the reply,\nI need a VLM i am processing images, unfortunately it’s not released yet for Qwen 3. Qwen 2.5 VL 32B performs worse for my use case than the 72B model. Maybe it’s language related the prompts are not in English or Chinese but in German, and I can measure the output accuracy precisely. Gemma performed significantly worse.&lt;/p&gt;\n\n&lt;p&gt;For single requests I got around 17 tokens per second with the fp8 quantized model using the RTX Pro 6000 WS and around 50 tokens per second using the 4x 5090s. I was getting around 1k/s total throughput for the RTX Pro 6000 didn’t check it for the 5090s. However, both are relevant for me.&lt;/p&gt;\n\n&lt;p&gt;Ok, so I will certainly need to add another PSU. What was your problem to keep it stable ? &lt;/p&gt;\n\n&lt;p&gt;Do you think I will get a performance improvement using PCIe 5? The DDR5 prices are crazy at least $1.5k for 256GB for the ecc ram required for epyc. However, yes, motherboard and CPU are in an acceptable price range as long as I choose the EPYC instead of the Threadripper Pro.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhu9tx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6z422e/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754356337,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yz9ad",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dear-Argument7658",
            "can_mod_post": false,
            "created_utc": 1754354639,
            "send_replies": true,
            "parent_id": "t3_1mhu9tx",
            "score": 1,
            "author_fullname": "t2_y3hxemhj4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What's your main reason for choosing Qwen 2.5 VL 72B over the more recent 32B? Are you certain you saw 3x performance with 4x 5090 vs 1x RTX 6000 Pro? Was that for single requests or batched?\n\nIf the RTX 6000 Pro's performance is sufficient for your use case, I'd skip the quad GPU setup. It's not hard to get it running, but keeping it stable is the real challenge. You'll probably need dual PSUs for stability - I had a 4x 3090 rig with a quality 2000W PSU, and it still hit power limits when running LMDeploy with tensor parallel. GPU power limiting does little to reduce transients on NVIDIA hardware, unfortunately - but I can't say for the 5090.\n\nIf you do go with 4x 5090s, consider a more recent EPYC platform to utilize PCIe 5.0 (if you can find compatible risers). The CPU and motherboard costs are reasonable, and you'll have a much better platform, especially with Zen 5. Though DDR5 RDIMMs are still somewhat overpriced.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yz9ad",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your main reason for choosing Qwen 2.5 VL 72B over the more recent 32B? Are you certain you saw 3x performance with 4x 5090 vs 1x RTX 6000 Pro? Was that for single requests or batched?&lt;/p&gt;\n\n&lt;p&gt;If the RTX 6000 Pro&amp;#39;s performance is sufficient for your use case, I&amp;#39;d skip the quad GPU setup. It&amp;#39;s not hard to get it running, but keeping it stable is the real challenge. You&amp;#39;ll probably need dual PSUs for stability - I had a 4x 3090 rig with a quality 2000W PSU, and it still hit power limits when running LMDeploy with tensor parallel. GPU power limiting does little to reduce transients on NVIDIA hardware, unfortunately - but I can&amp;#39;t say for the 5090.&lt;/p&gt;\n\n&lt;p&gt;If you do go with 4x 5090s, consider a more recent EPYC platform to utilize PCIe 5.0 (if you can find compatible risers). The CPU and motherboard costs are reasonable, and you&amp;#39;ll have a much better platform, especially with Zen 5. Though DDR5 RDIMMs are still somewhat overpriced.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhu9tx/help_building_an_price_efficient_inference_server/n6yz9ad/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754354639,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhu9tx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]