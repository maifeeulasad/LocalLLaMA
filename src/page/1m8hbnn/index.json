[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "\n\nI’m experimenting with self-hosted LLM agents for software development tasks — think writing code, submitting PRs, etc. My current stack is OpenHands + LM Studio, which I’ve tested on an M4 Pro Mac Mini and a Windows machine with a 3080 Ti.\n\nThe Mac Mini actually held up better than expected for 7B/13B models (quantized), but anything larger is slow. The 3080 Ti felt underutilized — even at 100% GPU setting, performance wasn’t impressive.\n\nI’m now considering a dedicated GPU for my homelab server. The top candidates:\n\t•\tRTX 4000 Blackwell (24GB ECC) – £1400\n\t•\tRTX 4500 Blackwell (32GB ECC) – £2400\n\nUse case is primarily local coding agents, possibly running 13B–32B models, with a future goal of supporting multi-agent sessions. Power efficiency and stability matter — this will run 24/7.\n\nQuestions:\n\t•\tIs the 4000 Blackwell enough for local 32B models (quantized), or is 32GB VRAM realistically required?\n\t•\tAny caveats with Blackwell cards for LLMs (driver maturity, inference compatibility)?\n\t•\tWould a used 3090 or A6000 be more practical in terms of cost vs performance, despite higher power usage?\n\t•\tAnyone running OpenHands locally or in K8s — any advice around GPU utilization or deployment?\n\nLooking for input from people already running LLMs or agents locally. Thanks in advanced. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Considering RTX 4000 Blackwell for Local Agentic AI",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8hbnn",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_hnm8h",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753393605,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m experimenting with self-hosted LLM agents for software development tasks — think writing code, submitting PRs, etc. My current stack is OpenHands + LM Studio, which I’ve tested on an M4 Pro Mac Mini and a Windows machine with a 3080 Ti.&lt;/p&gt;\n\n&lt;p&gt;The Mac Mini actually held up better than expected for 7B/13B models (quantized), but anything larger is slow. The 3080 Ti felt underutilized — even at 100% GPU setting, performance wasn’t impressive.&lt;/p&gt;\n\n&lt;p&gt;I’m now considering a dedicated GPU for my homelab server. The top candidates:\n    • RTX 4000 Blackwell (24GB ECC) – £1400\n    • RTX 4500 Blackwell (32GB ECC) – £2400&lt;/p&gt;\n\n&lt;p&gt;Use case is primarily local coding agents, possibly running 13B–32B models, with a future goal of supporting multi-agent sessions. Power efficiency and stability matter — this will run 24/7.&lt;/p&gt;\n\n&lt;p&gt;Questions:\n    • Is the 4000 Blackwell enough for local 32B models (quantized), or is 32GB VRAM realistically required?\n    • Any caveats with Blackwell cards for LLMs (driver maturity, inference compatibility)?\n    • Would a used 3090 or A6000 be more practical in terms of cost vs performance, despite higher power usage?\n    • Anyone running OpenHands locally or in K8s — any advice around GPU utilization or deployment?&lt;/p&gt;\n\n&lt;p&gt;Looking for input from people already running LLMs or agents locally. Thanks in advanced. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m8hbnn",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "b1uedust",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/",
            "subreddit_subscribers": 504023,
            "created_utc": 1753393605,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4ztrup",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "-dysangel-",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4zbult",
                                "score": 1,
                                "author_fullname": "t2_12ggykute6",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yeah but if you want to run larger models, it would be a decent mix of price and performance. I splurged on the 512GB M3 Ultra. I still think that hardware will be enough to run very competent local agents, but current KV caching algorithms are so inefficient that you can't really use large contexts on anything over 100GB without waiting minutes for the first token.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4ztrup",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah but if you want to run larger models, it would be a decent mix of price and performance. I splurged on the 512GB M3 Ultra. I still think that hardware will be enough to run very competent local agents, but current KV caching algorithms are so inefficient that you can&amp;#39;t really use large contexts on anything over 100GB without waiting minutes for the first token.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8hbnn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/n4ztrup/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753400373,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753400373,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4zbult",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "b1uedust",
                      "can_mod_post": false,
                      "created_utc": 1753394510,
                      "send_replies": true,
                      "parent_id": "t1_n4zb3cv",
                      "score": 1,
                      "author_fullname": "t2_hnm8h",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I use it for work and coding. So I wouldn’t sell my baby :)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4zbult",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use it for work and coding. So I wouldn’t sell my baby :)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8hbnn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/n4zbult/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753394510,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4zb3cv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753394273,
            "send_replies": true,
            "parent_id": "t3_1m8hbnn",
            "score": 1,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A Mac Mini with 64GB of RAM is £1,999.00 - you could maybe sell the current one and switch?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4zb3cv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A Mac Mini with 64GB of RAM is £1,999.00 - you could maybe sell the current one and switch?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/n4zb3cv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753394273,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m8hbnn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4zg3if",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zipperlein",
            "can_mod_post": false,
            "created_utc": 1753395864,
            "send_replies": true,
            "parent_id": "t3_1m8hbnn",
            "score": 1,
            "author_fullname": "t2_x3duw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "U can load q4 quants of 32B models in 24GB of VRAM but context size and therefore concurrency will be pretty limited with just 24GB. U can get 2x3090s for the price of 1 A4000, at least in my area. Also, consider that either RTX 4000 Blackwell and RTX 4500 Blackwell have less VRAM bandwith than a 3090. I am runnning Qwen3-32B with vllm on 4x3090s. I get \\~45tk/s@200W even with large context sizes and there's lots of memory available for concurrency. Blackwell is good for future-proofing in terms of compability probabbly. If u can wait there's rumors for 5070 Super with 24GB. Maybe that would be an intersting option.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4zg3if",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;U can load q4 quants of 32B models in 24GB of VRAM but context size and therefore concurrency will be pretty limited with just 24GB. U can get 2x3090s for the price of 1 A4000, at least in my area. Also, consider that either RTX 4000 Blackwell and RTX 4500 Blackwell have less VRAM bandwith than a 3090. I am runnning Qwen3-32B with vllm on 4x3090s. I get ~45tk/s@200W even with large context sizes and there&amp;#39;s lots of memory available for concurrency. Blackwell is good for future-proofing in terms of compability probabbly. If u can wait there&amp;#39;s rumors for 5070 Super with 24GB. Maybe that would be an intersting option.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8hbnn/considering_rtx_4000_blackwell_for_local_agentic/n4zg3if/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753395864,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8hbnn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]