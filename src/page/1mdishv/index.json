[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi, I am a long time lurker, but I took a break after the rtx 5090 launch fail since I almost completely gave up on getting to run ai locally this year.\n\nWith everything that's going on in the world and the possibility of the ai being considered \"too dangerous\", apparently the music may already be, I want to ask which llm is \"good\" today (not in the way of SOTA, but by personal user experience). I am planning on using an intel b60 48gb vram or maybe 1-2 amd mi50 32gb. I am mostly interested in llm, vllm and probably one for coding, although it's not really needed since I know how to code, but it might come handy I don't know. I guess what I might need is probably 7-70b parameter ones, I also have 96gb ram so a larger moe might also be decent. The total storage for all ais is probably 2-3tb. If I am at this topic I suppose that the intel gpu might be better for image generation\n\nI am old enough to remember mixtral 7x8 but I have no idea if it's still relevant, I know some mistral small might be better, also I might be interested in the vllm one for ocr. I kinda have an idea of most of the llms including the new qwen moes, but I have no idea which of the old models are still relevant today. For example I know that lama 3, or even 3.3 is kinda \"outdated\" (since I have no better word, but you get what I mean), I am even aware of a new nemotron which is based on lama 70b but I am missing a lot of details.\n\nI know I should be able to find them on huggingface, and I might need to download vllm, ollama and intel playgrounds or idk how it is for it.\n\nI know exactly how to get the stable diffusion models, but while we are at it I might be interested in a few tts models (text to speech, preferably with voice cloning), I think I've heard of \"megatts 3\" and \"GPT-SoVITS\" but any tips here are helpful as well. Meanwhile I will to find the fastest whisper model for stt, I am certain I might have saved the link for it somewhere.\n\nSorry for creating trash posts that are probably lots and lots on weekly bases for this particular question (not that particular considering the title, but you get what I mean).",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best LLMs to preserve in case of internet apocalypse",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdishv",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1gwc678u",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753910249,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a long time lurker, but I took a break after the rtx 5090 launch fail since I almost completely gave up on getting to run ai locally this year.&lt;/p&gt;\n\n&lt;p&gt;With everything that&amp;#39;s going on in the world and the possibility of the ai being considered &amp;quot;too dangerous&amp;quot;, apparently the music may already be, I want to ask which llm is &amp;quot;good&amp;quot; today (not in the way of SOTA, but by personal user experience). I am planning on using an intel b60 48gb vram or maybe 1-2 amd mi50 32gb. I am mostly interested in llm, vllm and probably one for coding, although it&amp;#39;s not really needed since I know how to code, but it might come handy I don&amp;#39;t know. I guess what I might need is probably 7-70b parameter ones, I also have 96gb ram so a larger moe might also be decent. The total storage for all ais is probably 2-3tb. If I am at this topic I suppose that the intel gpu might be better for image generation&lt;/p&gt;\n\n&lt;p&gt;I am old enough to remember mixtral 7x8 but I have no idea if it&amp;#39;s still relevant, I know some mistral small might be better, also I might be interested in the vllm one for ocr. I kinda have an idea of most of the llms including the new qwen moes, but I have no idea which of the old models are still relevant today. For example I know that lama 3, or even 3.3 is kinda &amp;quot;outdated&amp;quot; (since I have no better word, but you get what I mean), I am even aware of a new nemotron which is based on lama 70b but I am missing a lot of details.&lt;/p&gt;\n\n&lt;p&gt;I know I should be able to find them on huggingface, and I might need to download vllm, ollama and intel playgrounds or idk how it is for it.&lt;/p&gt;\n\n&lt;p&gt;I know exactly how to get the stable diffusion models, but while we are at it I might be interested in a few tts models (text to speech, preferably with voice cloning), I think I&amp;#39;ve heard of &amp;quot;megatts 3&amp;quot; and &amp;quot;GPT-SoVITS&amp;quot; but any tips here are helpful as well. Meanwhile I will to find the fastest whisper model for stt, I am certain I might have saved the link for it somewhere.&lt;/p&gt;\n\n&lt;p&gt;Sorry for creating trash posts that are probably lots and lots on weekly bases for this particular question (not that particular considering the title, but you get what I mean).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdishv",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "nos_66",
            "discussion_type": null,
            "num_comments": 23,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/",
            "subreddit_subscribers": 507275,
            "created_utc": 1753910249,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n627xtp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "TheFrenchSavage",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n625cgh",
                                "score": 4,
                                "author_fullname": "t2_yoqx0",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "[you have 300 languages of wikipedia embeddings here](https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3)  \n\nYou would need 535GB to store it all, but I guess you could only download the English part, which could be 100GB maybe?  \n\nThen, you build your standard RAG:  \n\n- store embeddings database into an index, like FAISS (FAISS-gpu if you are rich).  \n- use the same embeddings model as the one used for the dataset, and same correlation metric (eg. If they used cosine similarity -&gt; use that\n- when the user asks a question, retrieve content using the embeddings if their query and the top N matches in your index.  \n- you could optionally use a reranker here, to go from 20 samples to only 5.  \n- then use the local LLM to answer the question using the chunks of wikipedia that were returned (or links to the articles, or quote relevant section with a source, etc...)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n627xtp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 3.1"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/datasets/Cohere/wikipedia-2023-11-embed-multilingual-v3\"&gt;you have 300 languages of wikipedia embeddings here&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;You would need 535GB to store it all, but I guess you could only download the English part, which could be 100GB maybe?  &lt;/p&gt;\n\n&lt;p&gt;Then, you build your standard RAG:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;store embeddings database into an index, like FAISS (FAISS-gpu if you are rich).&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;use the same embeddings model as the one used for the dataset, and same correlation metric (eg. If they used cosine similarity -&amp;gt; use that&lt;/li&gt;\n&lt;li&gt;when the user asks a question, retrieve content using the embeddings if their query and the top N matches in your index.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;you could optionally use a reranker here, to go from 20 samples to only 5.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;then use the local LLM to answer the question using the chunks of wikipedia that were returned (or links to the articles, or quote relevant section with a source, etc...)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdishv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n627xtp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753913903,
                                "author_flair_text": "Llama 3.1",
                                "treatment_tags": [],
                                "created_utc": 1753913903,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#93b1ba",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62fib3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ttkciar",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n625cgh",
                                "score": 3,
                                "author_fullname": "t2_cpegz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Wikipedia provides data dumps as a compressed xml files.\n\nIf you don't want revision history or images or other languages, the compressed file of english content is about 25GB.\n\nIf you want more, of course it will be more.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62fib3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wikipedia provides data dumps as a compressed xml files.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t want revision history or images or other languages, the compressed file of english content is about 25GB.&lt;/p&gt;\n\n&lt;p&gt;If you want more, of course it will be more.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdishv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n62fib3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753916343,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753916343,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n625cgh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Current-Stop7806",
                      "can_mod_post": false,
                      "created_utc": 1753913086,
                      "send_replies": true,
                      "parent_id": "t1_n61wvmm",
                      "score": 2,
                      "author_fullname": "t2_8c7clfk1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Please, how could I store all Wikipedia data, or is there any way to make it locally searchable through an LLM ? With complete texts and images ? Wow. üí•üí•üëç",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n625cgh",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Please, how could I store all Wikipedia data, or is there any way to make it locally searchable through an LLM ? With complete texts and images ? Wow. üí•üí•üëç&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdishv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n625cgh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753913086,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61wvmm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "silenceimpaired",
            "can_mod_post": false,
            "created_utc": 1753910538,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 8,
            "author_fullname": "t2_dissgzyl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen 3 30B A3B plus Wikipedia data. Should be able to run on various hardware including not 3d cards and intersects well with RAG and agentic type stuff.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61wvmm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen 3 30B A3B plus Wikipedia data. Should be able to run on various hardware including not 3d cards and intersects well with RAG and agentic type stuff.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n61wvmm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753910538,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n627aip",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ArsNeph",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n626hhm",
                                "score": 2,
                                "author_fullname": "t2_vt0xkv60d",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "No problem, but unless you are getting a new laptop relatively soon, or really fear a terrible scenario, it might not be worth your time. The pace of development of LLMs is insanely fast, most of these models will probably be obsolete within the next 3 months lol",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n627aip",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No problem, but unless you are getting a new laptop relatively soon, or really fear a terrible scenario, it might not be worth your time. The pace of development of LLMs is insanely fast, most of these models will probably be obsolete within the next 3 months lol&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdishv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n627aip/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753913698,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753913698,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n626hhm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Current-Stop7806",
                      "can_mod_post": false,
                      "created_utc": 1753913443,
                      "send_replies": true,
                      "parent_id": "t1_n622g4t",
                      "score": 1,
                      "author_fullname": "t2_8c7clfk1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's a wonderful comment. I'll save for keeping these models with me, although for now, my current laptop can't run most of them. üôèüëç",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n626hhm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s a wonderful comment. I&amp;#39;ll save for keeping these models with me, although for now, my current laptop can&amp;#39;t run most of them. üôèüëç&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdishv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n626hhm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753913443,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n622g4t",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ArsNeph",
            "can_mod_post": false,
            "created_utc": 1753912200,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 3,
            "author_fullname": "t2_vt0xkv60d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm not sure about any apocalypse scenarios, but if you're asking for models that are solid all-rounders with good world knowledge, there are a few. Mixtral 8x7B is ancient and doesn't hold up, Mistral Small 3.2 24B is far superior.\n\nConsider Qwen 3 30B MoE 2507 as a solid lightweight model with a STEM bias. Gemma 3 27B is a great model for world knowledge and languages. Llama 3.3 Nemotron 50B 1.5 is a pretty solid model overall. Llama 3.3 70B isn't the most intelligent, but still has vast world knowledge. GLM 4.5 Air 100B is a great all rounder. \n\nIf you want models you may not necessarily be able to run, Qwen 3 235B Non thinking 2507 and Deepseek V3 0324 are also great models",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n622g4t",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure about any apocalypse scenarios, but if you&amp;#39;re asking for models that are solid all-rounders with good world knowledge, there are a few. Mixtral 8x7B is ancient and doesn&amp;#39;t hold up, Mistral Small 3.2 24B is far superior.&lt;/p&gt;\n\n&lt;p&gt;Consider Qwen 3 30B MoE 2507 as a solid lightweight model with a STEM bias. Gemma 3 27B is a great model for world knowledge and languages. Llama 3.3 Nemotron 50B 1.5 is a pretty solid model overall. Llama 3.3 70B isn&amp;#39;t the most intelligent, but still has vast world knowledge. GLM 4.5 Air 100B is a great all rounder. &lt;/p&gt;\n\n&lt;p&gt;If you want models you may not necessarily be able to run, Qwen 3 235B Non thinking 2507 and Deepseek V3 0324 are also great models&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n622g4t/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753912200,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n628m30",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "zsydeepsky",
            "can_mod_post": false,
            "created_utc": 1753914117,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 3,
            "author_fullname": "t2_5utv25t2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "well, in case of \"internet apocalypse\", I would also predict that it will come with an electricity outage.  \nso I would always pick the one with minimal \"token per Joule\", so I will say Qwen 30B A3B is the best to go, since it can run on my gaming handheld with only 12W TDP budget.  \nI can power the model with just a USB power bank, what else can I complain about?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n628m30",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;well, in case of &amp;quot;internet apocalypse&amp;quot;, I would also predict that it will come with an electricity outage.&lt;br/&gt;\nso I would always pick the one with minimal &amp;quot;token per Joule&amp;quot;, so I will say Qwen 30B A3B is the best to go, since it can run on my gaming handheld with only 12W TDP budget.&lt;br/&gt;\nI can power the model with just a USB power bank, what else can I complain about?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n628m30/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753914117,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n629bg5",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Melodic_Guidance3767",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n625s1t",
                                "score": 1,
                                "author_fullname": "t2_d44fq0pw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "in that github there has the information you need if i recall correctly. all i have is an offline copy of the wiki to browse, i haven't gotten around to setting up my own RAG, but i know if i wanted a decent end of the world setup it'd be some sort of RAG setup, that's all!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n629bg5",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;in that github there has the information you need if i recall correctly. all i have is an offline copy of the wiki to browse, i haven&amp;#39;t gotten around to setting up my own RAG, but i know if i wanted a decent end of the world setup it&amp;#39;d be some sort of RAG setup, that&amp;#39;s all!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdishv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n629bg5/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753914340,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753914340,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n625s1t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Current-Stop7806",
                      "can_mod_post": false,
                      "created_utc": 1753913221,
                      "send_replies": true,
                      "parent_id": "t1_n61wx6c",
                      "score": 1,
                      "author_fullname": "t2_8c7clfk1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is wonderful information. What RAG do you suggest for doing local Wikipedia completely searchable ? üò≤",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n625s1t",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is wonderful information. What RAG do you suggest for doing local Wikipedia completely searchable ? üò≤&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdishv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n625s1t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753913221,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61wx6c",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Melodic_Guidance3767",
            "can_mod_post": false,
            "created_utc": 1753910551,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 3,
            "author_fullname": "t2_d44fq0pw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "frankly, get a RAG setup, d/l wikipedia as a database, and have it utilize that.  \n[https://wikichat.genie.stanford.edu/](https://wikichat.genie.stanford.edu/)\n\n[https://github.com/stanford-oval/WikiChat](https://github.com/stanford-oval/WikiChat)\n\nyou can also then start learning to add to that database, survival books, various types of information. i'd say RAG/RAG+LLM &gt; pure LLM regardless of the params.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61wx6c",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;frankly, get a RAG setup, d/l wikipedia as a database, and have it utilize that.&lt;br/&gt;\n&lt;a href=\"https://wikichat.genie.stanford.edu/\"&gt;https://wikichat.genie.stanford.edu/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/stanford-oval/WikiChat\"&gt;https://github.com/stanford-oval/WikiChat&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;you can also then start learning to add to that database, survival books, various types of information. i&amp;#39;d say RAG/RAG+LLM &amp;gt; pure LLM regardless of the params.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n61wx6c/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753910551,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n622aru",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "CryptoCryst828282",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n620c51",
                                          "score": 1,
                                          "author_fullname": "t2_b8e4vw6kg",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "For people who can't spend that much cash, I wouldnt be shocked to see this run 20+ on a set of 3 p40s.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n622aru",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For people who can&amp;#39;t spend that much cash, I wouldnt be shocked to see this run 20+ on a set of 3 p40s.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdishv",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n622aru/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753912155,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753912155,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n620c51",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "-dysangel-",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6204nw",
                                "score": 1,
                                "author_fullname": "t2_12ggykute6",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yeah it runs at 44 tok/sec on my M3 Ultra. It is going to be so crazy if/when they add the multi token support",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n620c51",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah it runs at 44 tok/sec on my M3 Ultra. It is going to be so crazy if/when they add the multi token support&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdishv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n620c51/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753911569,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753911569,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6204nw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CryptoCryst828282",
                      "can_mod_post": false,
                      "created_utc": 1753911508,
                      "send_replies": true,
                      "parent_id": "t1_n61xcpu",
                      "score": 1,
                      "author_fullname": "t2_b8e4vw6kg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Might add it runs amazingly on lower-end hardware, too.   I know someone with a 395 max that can punch out mid 20s on it and its not even optimized yet.   I wouldnt be shocked to se q4s running on macs at 35+",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6204nw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Might add it runs amazingly on lower-end hardware, too.   I know someone with a 395 max that can punch out mid 20s on it and its not even optimized yet.   I wouldnt be shocked to se q4s running on macs at 35+&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdishv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n6204nw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753911508,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61xcpu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753910680,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 2,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For coding, my current favourite is GLM 4.5 Air. It's smart, fast and should fit in your setup once a 4 bit GGUF comes out",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61xcpu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For coding, my current favourite is GLM 4.5 Air. It&amp;#39;s smart, fast and should fit in your setup once a 4 bit GGUF comes out&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n61xcpu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753910680,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61ybc8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CryptoCryst828282",
            "can_mod_post": false,
            "created_utc": 1753910966,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 2,
            "author_fullname": "t2_b8e4vw6kg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It would be very hard to destroy the internet completely if that's what you mean.   If you are playing around and just want something decent, get Mistral Small 24b it can run on lower end hardware.    I have MI50s and they are fun to play with but really not practical.   Wait for the 50 super series, they are supposedly doing 5070's with 18gb vram  i suspect will be in the 550 range.        The B60 is going to be damn near impossible to find, and even though I haven't tried it on that GPU but I have tried A770 and it was sad how slow that was on LLMs.     NVidia really is the best with ROCm a very distant second and Intel pretty much not even on the map.    As for what model i would keep in my dooms day... right now GLM 4.5 hands down is the best",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61ybc8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It would be very hard to destroy the internet completely if that&amp;#39;s what you mean.   If you are playing around and just want something decent, get Mistral Small 24b it can run on lower end hardware.    I have MI50s and they are fun to play with but really not practical.   Wait for the 50 super series, they are supposedly doing 5070&amp;#39;s with 18gb vram  i suspect will be in the 550 range.        The B60 is going to be damn near impossible to find, and even though I haven&amp;#39;t tried it on that GPU but I have tried A770 and it was sad how slow that was on LLMs.     NVidia really is the best with ROCm a very distant second and Intel pretty much not even on the map.    As for what model i would keep in my dooms day... right now GLM 4.5 hands down is the best&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n61ybc8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753910966,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62ltir",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Current-Stop7806",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62itmb",
                                "score": 1,
                                "author_fullname": "t2_8c7clfk1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What do you find funny ? Bank account means \"everything related to your money\", could be even your  \"billions\" in Crypto... :)))))",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62ltir",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What do you find funny ? Bank account means &amp;quot;everything related to your money&amp;quot;, could be even your  &amp;quot;billions&amp;quot; in Crypto... :)))))&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdishv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n62ltir/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753918370,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753918370,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62itmb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "peppernickel",
                      "can_mod_post": false,
                      "created_utc": 1753917398,
                      "send_replies": true,
                      "parent_id": "t1_n6287as",
                      "score": 1,
                      "author_fullname": "t2_bqvg5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Bank accounts... super funny!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62itmb",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Bank accounts... super funny!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdishv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n62itmb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753917398,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6287as",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1753913985,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 2,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "In case of the internet apocalypse, you will need first, to ensure food, shelter, and control of your bank accounts, your money and keeping a communication system with family and friends. Only after that you could think about all the other things. Whenever I had an internet fall, my main worry is to know what's going on in the world, the latest news, and ensure that everything is ok with my family and friends. In certain catastrophes, when the internet goes down, electricity goes the same path ( that's even more terrible ), and you need to keep water and food. üôèüëçüí•üí•üòé",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6287as",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In case of the internet apocalypse, you will need first, to ensure food, shelter, and control of your bank accounts, your money and keeping a communication system with family and friends. Only after that you could think about all the other things. Whenever I had an internet fall, my main worry is to know what&amp;#39;s going on in the world, the latest news, and ensure that everything is ok with my family and friends. In certain catastrophes, when the internet goes down, electricity goes the same path ( that&amp;#39;s even more terrible ), and you need to keep water and food. üôèüëçüí•üí•üòé&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n6287as/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753913985,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n62zkt7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Lurksome-Lurker",
                      "can_mod_post": false,
                      "created_utc": 1753923098,
                      "send_replies": true,
                      "parent_id": "t1_n62ggz1",
                      "score": 1,
                      "author_fullname": "t2_mxtlig20b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Your probably gonna want an abliterated or uncensored one. Don‚Äôt want it to nag me about seeing a doctor when I ask it how to amputate an infected foot.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62zkt7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your probably gonna want an abliterated or uncensored one. Don‚Äôt want it to nag me about seeing a doctor when I ask it how to amputate an infected foot.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": true,
                      "can_gild": false,
                      "link_id": "t3_1mdishv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n62zkt7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753923098,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62ggz1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1753916644,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "In case of an internet outage, you will probably want a RAG database populated from a Wikipedia dump, and a model with long context and good RAG skills.\n\nGemma3 checks both of those boxes, with 128K context limit and excellent RAG skills.  It comes in 12B and 27B sizes.  It probably makes sense to download both, so that you can use the 12B on low-power hardware, and switch up to the 27B when a question requires better competence.\n\nhttps://huggingface.co/google/gemma-3-12b-it\n\nhttps://huggingface.co/google/gemma-3-27b-it\n\nThere are also quantized versions of these available.  See the \"Model Tree\" in the right-hand sidebar on the above pages for quantized versions. I use Q4_K_M from Bartowski with llama.cpp.  Not sure what the best quant is for use with vLLM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62ggz1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In case of an internet outage, you will probably want a RAG database populated from a Wikipedia dump, and a model with long context and good RAG skills.&lt;/p&gt;\n\n&lt;p&gt;Gemma3 checks both of those boxes, with 128K context limit and excellent RAG skills.  It comes in 12B and 27B sizes.  It probably makes sense to download both, so that you can use the 12B on low-power hardware, and switch up to the 27B when a question requires better competence.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/google/gemma-3-12b-it\"&gt;https://huggingface.co/google/gemma-3-12b-it&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/google/gemma-3-27b-it\"&gt;https://huggingface.co/google/gemma-3-27b-it&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are also quantized versions of these available.  See the &amp;quot;Model Tree&amp;quot; in the right-hand sidebar on the above pages for quantized versions. I use Q4_K_M from Bartowski with llama.cpp.  Not sure what the best quant is for use with vLLM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n62ggz1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753916644,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n630zmg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Cerebral_Zero",
                      "can_mod_post": false,
                      "created_utc": 1753923598,
                      "send_replies": true,
                      "parent_id": "t1_n62wy40",
                      "score": 1,
                      "author_fullname": "t2_18qewv5ovg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'll have to download that one",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n630zmg",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll have to download that one&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdishv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n630zmg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753923598,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62wy40",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Merkaba_Crystal",
            "can_mod_post": false,
            "created_utc": 1753922155,
            "send_replies": true,
            "parent_id": "t3_1mdishv",
            "score": 2,
            "author_fullname": "t2_6qk6ym7n",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Medgemma 27b. It is a medical oriented LLM",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62wy40",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Medgemma 27b. It is a medical oriented LLM&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdishv/best_llms_to_preserve_in_case_of_internet/n62wy40/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753922155,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdishv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]