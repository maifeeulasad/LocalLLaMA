[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Edited for clarity: \n\nI’ve just built a new inference server and want to make sure my setup and questions are perfectly clear:\n\nHardware\n- GPUs: 2× NVIDIA RTX 6000 Pro Max‑Q (192 GB total VRAM)\n- CPU &amp; RAM: AMD EPYC 9255‑based motherboard with 768 GB DDR5‑6000 installed\n\nUse Case\n- inference very large models (ideally larger 70 B+ parameters like glm 4.5)\n- Extremely long context windows (100 K+ tokens)\n- 8‑bit weight quantization (GPTQ) or as high as feasible\n\nInference Engines Under Consideration\n- vLLM (sharded/paged memory support)\n- k‑Transformers (merged‑weights trick)\n\n-llama.cpp (no paged memory support)\n\nMy Questions\n\n1.\tOther Engines?\n\nBeyond vLLM, k‑Transformers, and llama.cpp, what inference engines excel at long‑context workloads with paged or sharded memory?\n\n2.\tMemory Sufficiency\nWith 192 GB VRAM and 768 GB system RAM, can I natively load and serve a larger  B‑parameter models quantized to 8‑bit weights (GPTQ) without weight‑merging tricks?\n\n3.\tVRAM Estimation\nHow can I calculate or benchmark peak GPU memory usage for “sparsely activated” models (e.g. GLM‑4.5 or r1 variants) where only ~37 B parameters are active per forward pass?\n\nThanks in advance for any pointers!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best Inference Server for Large Vram",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mczdxa",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_5l4zmzcw",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753861570,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753857429,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edited for clarity: &lt;/p&gt;\n\n&lt;p&gt;I’ve just built a new inference server and want to make sure my setup and questions are perfectly clear:&lt;/p&gt;\n\n&lt;p&gt;Hardware\n- GPUs: 2× NVIDIA RTX 6000 Pro Max‑Q (192 GB total VRAM)\n- CPU &amp;amp; RAM: AMD EPYC 9255‑based motherboard with 768 GB DDR5‑6000 installed&lt;/p&gt;\n\n&lt;p&gt;Use Case\n- inference very large models (ideally larger 70 B+ parameters like glm 4.5)\n- Extremely long context windows (100 K+ tokens)\n- 8‑bit weight quantization (GPTQ) or as high as feasible&lt;/p&gt;\n\n&lt;p&gt;Inference Engines Under Consideration\n- vLLM (sharded/paged memory support)\n- k‑Transformers (merged‑weights trick)&lt;/p&gt;\n\n&lt;p&gt;-llama.cpp (no paged memory support)&lt;/p&gt;\n\n&lt;p&gt;My Questions&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; Other Engines?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Beyond vLLM, k‑Transformers, and llama.cpp, what inference engines excel at long‑context workloads with paged or sharded memory?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Memory Sufficiency\nWith 192 GB VRAM and 768 GB system RAM, can I natively load and serve a larger  B‑parameter models quantized to 8‑bit weights (GPTQ) without weight‑merging tricks?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;VRAM Estimation\nHow can I calculate or benchmark peak GPU memory usage for “sparsely activated” models (e.g. GLM‑4.5 or r1 variants) where only ~37 B parameters are active per forward pass?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for any pointers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mczdxa",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Infamous_Jaguar_2151",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/",
            "subreddit_subscribers": 506973,
            "created_utc": 1753857429,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ycx8m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ggone20",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5y7h22",
                                "score": 1,
                                "author_fullname": "t2_nxled",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What was the trade off? Which framework gave you the best performance? Or do you mean 2x 6000s vs 4x 4090s? \n\nPaper specs do seem nice. Evaluation speed? You’re using glm only?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ycx8m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What was the trade off? Which framework gave you the best performance? Or do you mean 2x 6000s vs 4x 4090s? &lt;/p&gt;\n\n&lt;p&gt;Paper specs do seem nice. Evaluation speed? You’re using glm only?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mczdxa",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5ycx8m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753871179,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753871179,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5y7h22",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Infamous_Jaguar_2151",
                      "can_mod_post": false,
                      "created_utc": 1753868218,
                      "send_replies": true,
                      "parent_id": "t1_n5xp4zw",
                      "score": 3,
                      "author_fullname": "t2_5l4zmzcw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I traded 10-15 % peak TFLOPS for 50 % lower power, 4 × the VRAM of a 4090/card, and enough RAM to page 200 k-token contexts—so I can experiment with giant models on a single wall outlet instead of renting a rack. It’s unusual but I like that it fits in an atx case.",
                      "edited": 1753868994,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5y7h22",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I traded 10-15 % peak TFLOPS for 50 % lower power, 4 × the VRAM of a 4090/card, and enough RAM to page 200 k-token contexts—so I can experiment with giant models on a single wall outlet instead of renting a rack. It’s unusual but I like that it fits in an atx case.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mczdxa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5y7h22/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753868218,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5xufj4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "GPTrack_ai",
                      "can_mod_post": false,
                      "created_utc": 1753860599,
                      "send_replies": true,
                      "parent_id": "t1_n5xp4zw",
                      "score": 0,
                      "author_fullname": "t2_1tpuoj72sa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "indeed funny it is...",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5xufj4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;indeed funny it is...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mczdxa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xufj4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753860599,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5xp4zw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ggone20",
            "can_mod_post": false,
            "created_utc": 1753857603,
            "send_replies": true,
            "parent_id": "t3_1mczdxa",
            "score": 3,
            "author_fullname": "t2_nxled",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try them all. Hardware is funny.\n\nRun the same prompts/flows/whatever.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5xp4zw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try them all. Hardware is funny.&lt;/p&gt;\n\n&lt;p&gt;Run the same prompts/flows/whatever.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xp4zw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753857603,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mczdxa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5xxogc",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AntuaW",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5xtkes",
                                "score": 3,
                                "author_fullname": "t2_zwsbv3w",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/4\n\nWendell from Level1techs uses Intel's Memory Latency Checker (mlc) on Linux. Would that be sth you could run?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5xxogc",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/4\"&gt;https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Wendell from Level1techs uses Intel&amp;#39;s Memory Latency Checker (mlc) on Linux. Would that be sth you could run?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mczdxa",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xxogc/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753862486,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753862486,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5xw5uf",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5xtkes",
                                "score": 2,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If you don't mind me asking, I'd love to see the results of\n\n    CUDA_VISIBLE_DEVICES=-1 llama.cpp/build/bin/llama-bench -p 512 -n 128 -r 3 -t 24 --cpu-mask ffffff --cpu_strict 1 -m Qwen3-32B-Q4_K_M.gguf\n\nThe tg128 should be a more practical measure of the memory bandwidth, and the pp512 would also be very interesting since Zen5 is supposed to have a decent upgrade for its AVX512.  The model can be anything dense in the 32B-70B range.  Q4_K_M is a pretty common quant so would give a useful representation of performance, but I'd also like to see the results of it at bf16 if you'd be willing to try, since IME it's the most efficient format in terms of memory bandwidth utilization and prompt processing.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5xw5uf",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you don&amp;#39;t mind me asking, I&amp;#39;d love to see the results of&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=-1 llama.cpp/build/bin/llama-bench -p 512 -n 128 -r 3 -t 24 --cpu-mask ffffff --cpu_strict 1 -m Qwen3-32B-Q4_K_M.gguf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The tg128 should be a more practical measure of the memory bandwidth, and the pp512 would also be very interesting since Zen5 is supposed to have a decent upgrade for its AVX512.  The model can be anything dense in the 32B-70B range.  Q4_K_M is a pretty common quant so would give a useful representation of performance, but I&amp;#39;d also like to see the results of it at bf16 if you&amp;#39;d be willing to try, since IME it&amp;#39;s the most efficient format in terms of memory bandwidth utilization and prompt processing.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mczdxa",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xw5uf/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753861619,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753861619,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5xtkes",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Infamous_Jaguar_2151",
                      "can_mod_post": false,
                      "created_utc": 1753860099,
                      "send_replies": true,
                      "parent_id": "t1_n5xs1bf",
                      "score": 2,
                      "author_fullname": "t2_5l4zmzcw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Will do, which benchmark do you want?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5xtkes",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Will do, which benchmark do you want?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mczdxa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xtkes/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753860099,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5xs1bf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AntuaW",
            "can_mod_post": false,
            "created_utc": 1753859223,
            "send_replies": true,
            "parent_id": "t3_1mczdxa",
            "score": 2,
            "author_fullname": "t2_zwsbv3w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Could you benchmark your RAM bandwidth? Very interested of this CPU capabilities",
            "edited": 1753862502,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5xs1bf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Could you benchmark your RAM bandwidth? Very interested of this CPU capabilities&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xs1bf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753859223,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mczdxa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5xud95",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Infamous_Jaguar_2151",
                      "can_mod_post": false,
                      "created_utc": 1753860563,
                      "send_replies": true,
                      "parent_id": "t1_n5xtmw7",
                      "score": 4,
                      "author_fullname": "t2_5l4zmzcw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Sounds like k-Transformers might be the best option, thanks for the corrections.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5xud95",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sounds like k-Transformers might be the best option, thanks for the corrections.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mczdxa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xud95/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753860563,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5xtmw7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753860139,
            "send_replies": true,
            "parent_id": "t3_1mczdxa",
            "score": 3,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; which engines can most efficiently load and serve full FP8 DeepSeek R1-0528 sized models given that only the active experts (~37 B params) are used per request?\n\nNone- it doesn't work that way.  It's 37B parameters per-token, not request, and which parameters are active are mostly random (about 1/3 are consistent).  It's also worth mentioning that the experts are selected per-layer, so it's not like you even know early in a prediction which experts you load.  It's a separate decision every single layer.\n\n&gt; Will the inactive parameters sit in CPU/RAM and fit within my ram? If so how much context can I realistically expect?\n\nAgain, not how it works, but I do want to use this quote to point out that while PCIe5x16 fast, it's only 64GBps fast.  Your RAM is ~600GBps fast and your VRAM is 1700GBps fast.  Paging parameters over PCIe would basically give you consumer desktop speeds.  If the weights are on CPU, the CPU is better off just doing the calculation.\n\n&gt; Any recommended offload or memory-paging strategies in engines like vLLM for Mixture-of-Experts to hit fast tokens/s decode with large contexts?\n\nIDK about vllm, but IIRC it's not good at mixed inference though that may have/be changing.  The current best option is to use llama.cpp and offload the aforementioned common parameters from all layers to the GPU and then as many full layers as possible.  (Which isn't going to be many, but you might get like 20% or so on the two GPUs.)\n\n&gt; I’m hoping to stick with q8 and try out vllm for inference\n\nOne small callout here: \"Q8\" generally means \"Q8_0 quantization\" which is ~8.5 bits per weight.  fp8 is Deepseek's native format and is non-quantized (kind of) ~8 bit per weight floating point.  However, AFAIK there is still no fp8 engine for CPU so if you can't fit it all on GPU you must use Q8_0 which is technically (though not significantly) lossy and a little larger.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5xtmw7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;which engines can most efficiently load and serve full FP8 DeepSeek R1-0528 sized models given that only the active experts (~37 B params) are used per request?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;None- it doesn&amp;#39;t work that way.  It&amp;#39;s 37B parameters per-token, not request, and which parameters are active are mostly random (about 1/3 are consistent).  It&amp;#39;s also worth mentioning that the experts are selected per-layer, so it&amp;#39;s not like you even know early in a prediction which experts you load.  It&amp;#39;s a separate decision every single layer.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Will the inactive parameters sit in CPU/RAM and fit within my ram? If so how much context can I realistically expect?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Again, not how it works, but I do want to use this quote to point out that while PCIe5x16 fast, it&amp;#39;s only 64GBps fast.  Your RAM is ~600GBps fast and your VRAM is 1700GBps fast.  Paging parameters over PCIe would basically give you consumer desktop speeds.  If the weights are on CPU, the CPU is better off just doing the calculation.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Any recommended offload or memory-paging strategies in engines like vLLM for Mixture-of-Experts to hit fast tokens/s decode with large contexts?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;IDK about vllm, but IIRC it&amp;#39;s not good at mixed inference though that may have/be changing.  The current best option is to use llama.cpp and offload the aforementioned common parameters from all layers to the GPU and then as many full layers as possible.  (Which isn&amp;#39;t going to be many, but you might get like 20% or so on the two GPUs.)&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I’m hoping to stick with q8 and try out vllm for inference&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;One small callout here: &amp;quot;Q8&amp;quot; generally means &amp;quot;Q8_0 quantization&amp;quot; which is ~8.5 bits per weight.  fp8 is Deepseek&amp;#39;s native format and is non-quantized (kind of) ~8 bit per weight floating point.  However, AFAIK there is still no fp8 engine for CPU so if you can&amp;#39;t fit it all on GPU you must use Q8_0 which is technically (though not significantly) lossy and a little larger.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xtmw7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753860139,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mczdxa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5y1i2n",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "DorphinPack",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5y0nff",
                                                    "score": 1,
                                                    "author_fullname": "t2_zebuyjw9s",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Heh, you did what I would have done if I had more than a few hundred dollars and an aging workstation. I've been all in on \"context engineering\" as a result. But it's been very, very handy still in helping me with the bigger projects that are the reason I'm experiencing what the suits call a temporary liquidity problem so I'm very happy.\n\nJust to highlight the similarities my system is powercapped and underclocked to sit on my desk and trundle along, albeit on an eighth the VRAM and a third the RAM.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5y1i2n",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Heh, you did what I would have done if I had more than a few hundred dollars and an aging workstation. I&amp;#39;ve been all in on &amp;quot;context engineering&amp;quot; as a result. But it&amp;#39;s been very, very handy still in helping me with the bigger projects that are the reason I&amp;#39;m experiencing what the suits call a temporary liquidity problem so I&amp;#39;m very happy.&lt;/p&gt;\n\n&lt;p&gt;Just to highlight the similarities my system is powercapped and underclocked to sit on my desk and trundle along, albeit on an eighth the VRAM and a third the RAM.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mczdxa",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5y1i2n/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753864734,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753864734,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5y0nff",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Infamous_Jaguar_2151",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5xyorb",
                                          "score": 1,
                                          "author_fullname": "t2_5l4zmzcw",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Think of it as a long‑context LLM workbench: oversized RAM for paging, power‑capped Blackwell GPUs for desk‑friendly thermals, and just enough CPU to keep everything fed. It’s not built to win FLOPS benchmarks—it’s built to run jobs that overflow ordinary 24 GB cards without tripping your circuit breaker. It’s a weird setup but I do like the fact with the Supermicro H13SSL-NT it’s atx sized.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5y0nff",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Think of it as a long‑context LLM workbench: oversized RAM for paging, power‑capped Blackwell GPUs for desk‑friendly thermals, and just enough CPU to keep everything fed. It’s not built to win FLOPS benchmarks—it’s built to run jobs that overflow ordinary 24 GB cards without tripping your circuit breaker. It’s a weird setup but I do like the fact with the Supermicro H13SSL-NT it’s atx sized.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mczdxa",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5y0nff/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753864228,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753864228,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5xyorb",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DorphinPack",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5xwrih",
                                "score": 2,
                                "author_fullname": "t2_zebuyjw9s",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Ah, so you’ll be sampling different workloads by running different benchmarks?\n\nAnd it sounds like just you using it for experimentation?\n\nFor solo inference doing random tasks:\n\n- There are a lot of models smaller than Deepseek (and quants) that offer really compelling quality for complex tasks with long contexts AND fit on your VRAM.  \n\n- By contrast, I’m dealing with the hassle of getting good hybrid configs for each model and context permutation I want to run right now. 24GB VRAM problems.\n\n- I would definitely avoid the hybrid/selective-offload headache if you can. You can run quite a few fantastic everyday powerhouses at full precision or Q8 without CPU at all. With room to spare for  embeddings for vector based RAG and all sorts of fun things like that.\n\nFine tuning an itty bitty model I can actually do in 24GB is on my todo list but for now I mostly use my setup to churn through generating boilerplate code or PoCs as well as summarizing a backlog of documents I’ve been working through.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5xyorb",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah, so you’ll be sampling different workloads by running different benchmarks?&lt;/p&gt;\n\n&lt;p&gt;And it sounds like just you using it for experimentation?&lt;/p&gt;\n\n&lt;p&gt;For solo inference doing random tasks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;There are a lot of models smaller than Deepseek (and quants) that offer really compelling quality for complex tasks with long contexts AND fit on your VRAM.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;By contrast, I’m dealing with the hassle of getting good hybrid configs for each model and context permutation I want to run right now. 24GB VRAM problems.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I would definitely avoid the hybrid/selective-offload headache if you can. You can run quite a few fantastic everyday powerhouses at full precision or Q8 without CPU at all. With room to spare for  embeddings for vector based RAG and all sorts of fun things like that.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Fine tuning an itty bitty model I can actually do in 24GB is on my todo list but for now I mostly use my setup to churn through generating boilerplate code or PoCs as well as summarizing a backlog of documents I’ve been working through.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mczdxa",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xyorb/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753863075,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753863075,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5xwrih",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Infamous_Jaguar_2151",
                      "can_mod_post": false,
                      "created_utc": 1753861964,
                      "send_replies": true,
                      "parent_id": "t1_n5xvjm0",
                      "score": 1,
                      "author_fullname": "t2_5l4zmzcw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It’s mainly gonna be used for inference but also fine-tuning. Might even train a very small model.",
                      "edited": 1753873994,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5xwrih",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s mainly gonna be used for inference but also fine-tuning. Might even train a very small model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mczdxa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xwrih/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753861964,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5xvjm0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DorphinPack",
            "can_mod_post": false,
            "created_utc": 1753861259,
            "send_replies": true,
            "parent_id": "t3_1mczdxa",
            "score": 1,
            "author_fullname": "t2_zebuyjw9s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Workload workload workload! Whatcha doin with it?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5xvjm0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Workload workload workload! Whatcha doin with it?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xvjm0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753861259,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mczdxa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5xue9j",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTrack_ai",
            "can_mod_post": false,
            "created_utc": 1753860578,
            "send_replies": true,
            "parent_id": "t3_1mczdxa",
            "score": 0,
            "author_fullname": "t2_1tpuoj72sa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qmax....",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5xue9j",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qmax....&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mczdxa/best_inference_server_for_large_vram/n5xue9j/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753860578,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mczdxa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]