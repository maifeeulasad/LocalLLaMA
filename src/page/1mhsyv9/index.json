[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey r/LocalLLaMA! I'm struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.\n\n**Hardware:**\n\n* RTX 4090 (24GB VRAM)\n* 64GB DDR4 RAM\n* Using latest llama.cpp build (6088 with clang 19.1.5)\n\n**Model:**\n\n* DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4\\_K\\_M, 6 shards, \\~67GB total)\n* 110B parameters, MoE with 128 experts (8 active)\n\n**Current working config:**\n\n    llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot \".ffn_.*_exps.=CPU\" --port 8081\n\n**Performance issues:**\n\n* Only getting **3.37 tokens/sec** generation\n* **1.53 tokens/sec** prompt processing\n* GPU utilization only **12%** (should be much higher!)\n* GPU memory usage: 7.7GB/24GB (tons of headroom)\n* System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)\n\n**What I've tried:**\n\n* Without expert offloading (`-ot` flag) → OOMs trying to allocate 66GB on 24GB GPU\n* Higher `-ngl` values without expert offloading → System freezes/crashes after 30min loading\n* Different batch sizes → No improvement\n* For comparison, 12B dense models get 40-50 TPS on this setup",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Need help optimizing GLM-4.5-Air 110B (Q4_K_M) on RTX 4090 + 64GB RAM - Getting only 3.37 TPS",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhsyv9",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.87,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 11,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_uptissiz",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 11,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754349244,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! I&amp;#39;m struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;RTX 4090 (24GB VRAM)&lt;/li&gt;\n&lt;li&gt;64GB DDR4 RAM&lt;/li&gt;\n&lt;li&gt;Using latest llama.cpp build (6088 with clang 19.1.5)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4_K_M, 6 shards, ~67GB total)&lt;/li&gt;\n&lt;li&gt;110B parameters, MoE with 128 experts (8 active)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current working config:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; --port 8081\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance issues:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Only getting &lt;strong&gt;3.37 tokens/sec&lt;/strong&gt; generation&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;1.53 tokens/sec&lt;/strong&gt; prompt processing&lt;/li&gt;\n&lt;li&gt;GPU utilization only &lt;strong&gt;12%&lt;/strong&gt; (should be much higher!)&lt;/li&gt;\n&lt;li&gt;GPU memory usage: 7.7GB/24GB (tons of headroom)&lt;/li&gt;\n&lt;li&gt;System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Without expert offloading (&lt;code&gt;-ot&lt;/code&gt; flag) → OOMs trying to allocate 66GB on 24GB GPU&lt;/li&gt;\n&lt;li&gt;Higher &lt;code&gt;-ngl&lt;/code&gt; values without expert offloading → System freezes/crashes after 30min loading&lt;/li&gt;\n&lt;li&gt;Different batch sizes → No improvement&lt;/li&gt;\n&lt;li&gt;For comparison, 12B dense models get 40-50 TPS on this setup&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhsyv9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Pro-editor-1105",
            "discussion_type": null,
            "num_comments": 27,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754349244,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6zblv7",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "eloquentemu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6z5z9f",
                                          "score": 1,
                                          "author_fullname": "t2_lpdsy",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yeah, but most of your post was on optimizing layers, which is often a drop in the bucket performance-wise so I figured I'd point out why it would be particularly useful in OP's case.  Anyone running a MoE on CPU should offload non-experts regardless.\n\nAlso, just FWIW, `-ngl 99 -ot ffn_.*_exps.=CPU` mostly helps due to offloading the attention and output tensors which generally represent about 30% of the active parameters.  Shared experts specifically are usually only ~8% of active parameters if the model even has them (Qwen3 doesn't).  The ot",
                                          "edited": 1754360411,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6zblv7",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, but most of your post was on optimizing layers, which is often a drop in the bucket performance-wise so I figured I&amp;#39;d point out why it would be particularly useful in OP&amp;#39;s case.  Anyone running a MoE on CPU should offload non-experts regardless.&lt;/p&gt;\n\n&lt;p&gt;Also, just FWIW, &lt;code&gt;-ngl 99 -ot ffn_.*_exps.=CPU&lt;/code&gt; mostly helps due to offloading the attention and output tensors which generally represent about 30% of the active parameters.  Shared experts specifically are usually only ~8% of active parameters if the model even has them (Qwen3 doesn&amp;#39;t).  The ot&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhsyv9",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6zblv7/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754359017,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754359017,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6z5z9f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "segmond",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6yyi8g",
                                "score": 1,
                                "author_fullname": "t2_ah13x",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "offloading just the shared experts doubles the performance vs CPU only.  2x speed increase by using a 3060/1080/P40 is a big deal to run one of the best open weight models.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6z5z9f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;offloading just the shared experts doubles the performance vs CPU only.  2x speed increase by using a 3060/1080/P40 is a big deal to run one of the best open weight models.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhsyv9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6z5z9f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754357026,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754357026,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yyi8g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1754354379,
                      "send_replies": true,
                      "parent_id": "t1_n6ynvet",
                      "score": 3,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'll add that while offloading a couple layers of a large MoE doesn't usually improve performance by much, here it will likely be critical... Since OP is running a 67GB model on system with 64GB of RAM they need to maximize the use of VRAM offload in order to prevent swapping (and their performance would indicate they are swapping).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yyi8g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll add that while offloading a couple layers of a large MoE doesn&amp;#39;t usually improve performance by much, here it will likely be critical... Since OP is running a 67GB model on system with 64GB of RAM they need to maximize the use of VRAM offload in order to prevent swapping (and their performance would indicate they are swapping).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yyi8g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754354379,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6zgbvv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pro-editor-1105",
                      "can_mod_post": false,
                      "created_utc": 1754360701,
                      "send_replies": true,
                      "parent_id": "t1_n6ynvet",
                      "score": 1,
                      "author_fullname": "t2_uptissiz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "If I do all on the CPU then it just uses 6.8GB of vram, if i load even a single expert onto my GPU I get an OOM from apparently requiring 58GB. I have also changed to mradermachers iq4xs GGUF",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6zgbvv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If I do all on the CPU then it just uses 6.8GB of vram, if i load even a single expert onto my GPU I get an OOM from apparently requiring 58GB. I have also changed to mradermachers iq4xs GGUF&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6zgbvv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754360701,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ynvet",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "segmond",
            "can_mod_post": false,
            "created_utc": 1754350684,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 10,
            "author_fullname": "t2_ah13x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Begin by loading the shared experts on the GPU, that should be about 4gb, then set your context and see how much free memory you have on your GPU, then load just 1 expert tensor.  \nblk.(\\[0\\]).fnn\\_.\\*\\_exps.=CUDA0, see how much memory it takes, let's say you notice you have  17gb free, and you load 1 expert and it uses 3gb, then you can see that you can put 5 more on there, then change that 0 to 0-4, blk.(\\[0-5\\]).fnn\\_.\\*\\_exps=.CUDA0, and for anyone that has more GPU, go to the next GPU and keep loading\n\n    --override-tensor \"blk.([0-9]).ffn.*shexp.=CUDA0,.ffn.*shexp.*=CUDA0,ffn_.*_exps.=CPU\"",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ynvet",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Begin by loading the shared experts on the GPU, that should be about 4gb, then set your context and see how much free memory you have on your GPU, then load just 1 expert tensor.&lt;br/&gt;\nblk.([0]).fnn_.*_exps.=CUDA0, see how much memory it takes, let&amp;#39;s say you notice you have  17gb free, and you load 1 expert and it uses 3gb, then you can see that you can put 5 more on there, then change that 0 to 0-4, blk.([0-5]).fnn_.*_exps=.CUDA0, and for anyone that has more GPU, go to the next GPU and keep loading&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;--override-tensor &amp;quot;blk.([0-9]).ffn.*shexp.=CUDA0,.ffn.*shexp.*=CUDA0,ffn_.*_exps.=CPU&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ynvet/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754350684,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z42n3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "DeProgrammer99",
                      "can_mod_post": false,
                      "created_utc": 1754356342,
                      "send_replies": true,
                      "parent_id": "t1_n6yy9bi",
                      "score": 5,
                      "author_fullname": "t2_w4j8t",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'd just like to add that earlier \"-ot\" arguments take precedence over later \"-ot\" arguments. Didn't see that spelled out anywhere, but I've tested it out to make sure.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z42n3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d just like to add that earlier &amp;quot;-ot&amp;quot; arguments take precedence over later &amp;quot;-ot&amp;quot; arguments. Didn&amp;#39;t see that spelled out anywhere, but I&amp;#39;ve tested it out to make sure.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6z42n3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754356342,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n70p2c4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "RunPersonal6993",
                      "can_mod_post": false,
                      "created_utc": 1754382406,
                      "send_replies": true,
                      "parent_id": "t1_n6yy9bi",
                      "score": 2,
                      "author_fullname": "t2_5q2qgkzi",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks i tweaked your config a bit and my 2x RTX3090 + 9950X &amp; 96GB &amp; 6000Mhz CL36 and Using Unsloths Q4\\_KM im getting\n\nThis was like 1 or 2 tokens faster from your exact config. and loads the RTXes better to 23GB. Your config did 20 and 23 GB for me.\n\nprompt eval time =     531.69 ms /    18 tokens (   29.54 ms per token,    33.85 tokens per second)  \n      eval time =   73224.79 ms /  1383 tokens (   52.95 ms per token,    18.89 tokens per second)\n\n    podman run --rm -it \\\n          --gpus all \\\n          -v /home/lucyd/models/glm_4.5_air:/models:ro,z \\\n          -p 8080:8080 \\\n          --name glm4.5_air_server \\\n          localhost/my-latest-llama-server \\\n          --jinja \\\n          -m /models/GLM-4.5-Air-Q4_K_M-00001-of-00002.gguf \\\n          -c 90000 \\\n          --cache-type-k q8_0 \\\n          --cache-type-v q8_0 \\\n          --host 0.0.0.0 \\\n          --port 8080 \\\n          --flash-attn \\\n          -ngl 99 \\\n          -ot \"blk.([0-9]|1[01]).ffn_up_exps=CUDA0,blk.([0-9]|1[01]).ffn_gate_exps=CUDA0,blk.([0-9]|1[01]).ffn_down_exps=CUDA0,blk.(1[2-9]|2[0-3]).ffn_up_exps=CUDA1,blk.(1[2-9]|2[0-3]).ffn_gate_exps=CUDA1,blk.(1[2-9]|2[0-3]).\n\n Since official docker is not yet available i made do with this Dockerfile.\n\n    FROM nvidia/cuda:12.4.1-devel-ubuntu22.04\n    \n    RUN apt-get update &amp;&amp; apt-get install -y git cmake libcurl4-openssl-dev\n    \n    RUN git clone https://github.com/ggerganov/llama.cpp.git /src\n    \n    WORKDIR /src\n    \n    RUN mkdir build &amp;&amp; \\\n        cd build &amp;&amp; \\\n        cmake .. \\\n            -DGGML_CUDA=ON \\\n            -DCMAKE_CUDA_ARCHITECTURES=86 \\\n            -DCMAKE_EXE_LINKER_FLAGS='-L/usr/local/cuda/lib64 -lcuda' &amp;&amp; \\\n        cmake --build . --target llama-server -j\n    \n    ENTRYPOINT [\"/src/build/bin/llama-server\"]",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n70p2c4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks i tweaked your config a bit and my 2x RTX3090 + 9950X &amp;amp; 96GB &amp;amp; 6000Mhz CL36 and Using Unsloths Q4_KM im getting&lt;/p&gt;\n\n&lt;p&gt;This was like 1 or 2 tokens faster from your exact config. and loads the RTXes better to 23GB. Your config did 20 and 23 GB for me.&lt;/p&gt;\n\n&lt;p&gt;prompt eval time =     531.69 ms /    18 tokens (   29.54 ms per token,    33.85 tokens per second)&lt;br/&gt;\n      eval time =   73224.79 ms /  1383 tokens (   52.95 ms per token,    18.89 tokens per second)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;podman run --rm -it \\\n      --gpus all \\\n      -v /home/lucyd/models/glm_4.5_air:/models:ro,z \\\n      -p 8080:8080 \\\n      --name glm4.5_air_server \\\n      localhost/my-latest-llama-server \\\n      --jinja \\\n      -m /models/GLM-4.5-Air-Q4_K_M-00001-of-00002.gguf \\\n      -c 90000 \\\n      --cache-type-k q8_0 \\\n      --cache-type-v q8_0 \\\n      --host 0.0.0.0 \\\n      --port 8080 \\\n      --flash-attn \\\n      -ngl 99 \\\n      -ot &amp;quot;blk.([0-9]|1[01]).ffn_up_exps=CUDA0,blk.([0-9]|1[01]).ffn_gate_exps=CUDA0,blk.([0-9]|1[01]).ffn_down_exps=CUDA0,blk.(1[2-9]|2[0-3]).ffn_up_exps=CUDA1,blk.(1[2-9]|2[0-3]).ffn_gate_exps=CUDA1,blk.(1[2-9]|2[0-3]).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Since official docker is not yet available i made do with this Dockerfile.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;FROM nvidia/cuda:12.4.1-devel-ubuntu22.04\n\nRUN apt-get update &amp;amp;&amp;amp; apt-get install -y git cmake libcurl4-openssl-dev\n\nRUN git clone https://github.com/ggerganov/llama.cpp.git /src\n\nWORKDIR /src\n\nRUN mkdir build &amp;amp;&amp;amp; \\\n    cd build &amp;amp;&amp;amp; \\\n    cmake .. \\\n        -DGGML_CUDA=ON \\\n        -DCMAKE_CUDA_ARCHITECTURES=86 \\\n        -DCMAKE_EXE_LINKER_FLAGS=&amp;#39;-L/usr/local/cuda/lib64 -lcuda&amp;#39; &amp;amp;&amp;amp; \\\n    cmake --build . --target llama-server -j\n\nENTRYPOINT [&amp;quot;/src/build/bin/llama-server&amp;quot;]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n70p2c4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754382406,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yy9bi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "solidsnakeblue",
            "can_mod_post": false,
            "created_utc": 1754354291,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 5,
            "author_fullname": "t2_7zh6fslk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Now that I got it working I wanted to share what worked for me, slightly different system with 2 x 3090 and 128GB of ram.  This fills up the VRAM almost completely, looks like I am getting 107tk / sec PP and 4.5tk / sec generation\n\n    ./build/bin/llama-server \\\n        -m models/zai-org.GLM-4.5-Air-GGUF/zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf \\\n        --host 0.0.0.0 \\\n        --port 30000 \\\n        -c 100000 \\\n        --cache-type-k q8_0 \\\n        --cache-type-v q8_0 \\\n        -t 8 \\\n        -ngl 99  \\\n        -ot \"blk.[0-7].ffn_up_exps=CUDA0,blk.[0-7].ffn_gate_exps=CUDA0,blk.[0-7].ffn_down_exps=CUDA0\" \\\n        -ot \"blk.[8-9].ffn_up_exps=CUDA1,blk.[8-9].ffn_gate_exps=CUDA1,blk.[8-9].ffn_down_exps=CUDA1\" \\\n        -ot \"blk.1[0-9].ffn_up_exps=CUDA1,blk.1[0-9].ffn_gate_exps=CUDA1,blk.1[0-9].ffn_down_exps=CUDA1\" \\\n        -ot \".ffn_.*_exps.=CPU\" \\\n        --flash-attn\n\nYou could use a q4\\_0 cache and probably fit 32K context with your total amount of ram.  You can probably just use my first -ot exactly to offset the amount of blocks that will fit onto your GPU, if you OOM just lower the 7 in the \\[0-7\\] regex blocks.\n\nEdit:  I just pulled 2 sticks out so I could run 64GB of RAM at 4800MT instead of the 3600MT I get with 128GB, I wasn't able to get the model to run without spilling over into swap file for some reason despite having 48GB of VRAM.  Maybe it has something to do with the order things load in because the VRAM gets filled up at the very end, after all the RAM and Swap file has been used.  I ended up just going back to 128GB @ 3600MT.",
            "edited": 1754357527,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yy9bi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Now that I got it working I wanted to share what worked for me, slightly different system with 2 x 3090 and 128GB of ram.  This fills up the VRAM almost completely, looks like I am getting 107tk / sec PP and 4.5tk / sec generation&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./build/bin/llama-server \\\n    -m models/zai-org.GLM-4.5-Air-GGUF/zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf \\\n    --host 0.0.0.0 \\\n    --port 30000 \\\n    -c 100000 \\\n    --cache-type-k q8_0 \\\n    --cache-type-v q8_0 \\\n    -t 8 \\\n    -ngl 99  \\\n    -ot &amp;quot;blk.[0-7].ffn_up_exps=CUDA0,blk.[0-7].ffn_gate_exps=CUDA0,blk.[0-7].ffn_down_exps=CUDA0&amp;quot; \\\n    -ot &amp;quot;blk.[8-9].ffn_up_exps=CUDA1,blk.[8-9].ffn_gate_exps=CUDA1,blk.[8-9].ffn_down_exps=CUDA1&amp;quot; \\\n    -ot &amp;quot;blk.1[0-9].ffn_up_exps=CUDA1,blk.1[0-9].ffn_gate_exps=CUDA1,blk.1[0-9].ffn_down_exps=CUDA1&amp;quot; \\\n    -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; \\\n    --flash-attn\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You could use a q4_0 cache and probably fit 32K context with your total amount of ram.  You can probably just use my first -ot exactly to offset the amount of blocks that will fit onto your GPU, if you OOM just lower the 7 in the [0-7] regex blocks.&lt;/p&gt;\n\n&lt;p&gt;Edit:  I just pulled 2 sticks out so I could run 64GB of RAM at 4800MT instead of the 3600MT I get with 128GB, I wasn&amp;#39;t able to get the model to run without spilling over into swap file for some reason despite having 48GB of VRAM.  Maybe it has something to do with the order things load in because the VRAM gets filled up at the very end, after all the RAM and Swap file has been used.  I ended up just going back to 128GB @ 3600MT.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yy9bi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754354291,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n716kio",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Muted-Celebration-47",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n704dmf",
                                                    "score": 3,
                                                    "author_fullname": "t2_q2qi86l3f",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I just found PR from llamacpp that tried to solve complex regex override tensor =&gt; [https://github.com/ggml-org/llama.cpp/pull/15077](https://github.com/ggml-org/llama.cpp/pull/15077)  \nthis way, you just add e.g. --n-cpu-moe 2 in your command to easily optimize your speed.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n716kio",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just found PR from llamacpp that tried to solve complex regex override tensor =&amp;gt; &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15077\"&gt;https://github.com/ggml-org/llama.cpp/pull/15077&lt;/a&gt;&lt;br/&gt;\nthis way, you just add e.g. --n-cpu-moe 2 in your command to easily optimize your speed.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mhsyv9",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n716kio/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754391927,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754391927,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n704dmf",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "Muted-Celebration-47",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6zghx9",
                                          "score": 7,
                                          "author_fullname": "t2_q2qi86l3f",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I use this to generate automatically the best override tensor for my setup. It will not allow you to load downloaded model. If you want to load the downloaded model, you need to edit some code.  \n[https://github.com/k-koehler/gguf-tensor-overrider/tree/main](https://github.com/k-koehler/gguf-tensor-overrider/tree/main)\n\nFor testing, you can try my override tensor:  \nllama-server.exe -m zai-org.GLM-4.5-Air.gguf -ngl 50 -fa -c 32768 -ctk q8\\_0 -ctv q8\\_0 --no-mmap  -ot \".\\*attn.\\*=CUDA0\" -ot \".\\*norm.\\*=CUDA0\" -ot \"blk\\\\.\\[0-9\\]\\\\..\\*=CUDA0\" -ot \"output\\\\.weight=CUDA0\" -ot \".\\*ffn\\_gate.\\*=CUDA0\" -ot \".\\*exp\\_probs\\_b\\\\.bias=CUDA0\" -ot \"blk\\\\.10\\\\.ffn\\_up\\_exps\\\\.weight=CUDA0\" -ot \"blk\\\\.(1\\\\d|2\\[0-8\\])\\\\.ffn\\_.\\*\\_shexp\\\\.weight=CUDA0\" -ot \"token\\_embd\\\\.weight=CPU\" -ot \"blk\\\\.10\\\\.ffn\\_down\\_exps\\\\.weight=CPU\" -ot \"blk\\\\.(1\\[1-9\\]|\\[2-4\\]\\\\d)\\\\.ffn\\_.\\*\\_exps\\\\.weight=CPU\" -ot \"blk\\\\.(29|\\[3-4\\]\\\\d)\\\\.ffn\\_.\\*\\_shexp\\\\.weight=CPU\" -ot \"blk\\\\.46\\\\.nextn\\\\.(eh\\_proj|embed\\_tokens|shared\\_head\\_head)\\\\.weight=CPU\" -ts 0,0,1 --jinja",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n704dmf",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use this to generate automatically the best override tensor for my setup. It will not allow you to load downloaded model. If you want to load the downloaded model, you need to edit some code.&lt;br/&gt;\n&lt;a href=\"https://github.com/k-koehler/gguf-tensor-overrider/tree/main\"&gt;https://github.com/k-koehler/gguf-tensor-overrider/tree/main&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For testing, you can try my override tensor:&lt;br/&gt;\nllama-server.exe -m zai-org.GLM-4.5-Air.gguf -ngl 50 -fa -c 32768 -ctk q8_0 -ctv q8_0 --no-mmap  -ot &amp;quot;.*attn.*=CUDA0&amp;quot; -ot &amp;quot;.*norm.*=CUDA0&amp;quot; -ot &amp;quot;blk\\.[0-9]\\..*=CUDA0&amp;quot; -ot &amp;quot;output\\.weight=CUDA0&amp;quot; -ot &amp;quot;.*ffn_gate.*=CUDA0&amp;quot; -ot &amp;quot;.*exp_probs_b\\.bias=CUDA0&amp;quot; -ot &amp;quot;blk\\.10\\.ffn_up_exps\\.weight=CUDA0&amp;quot; -ot &amp;quot;blk\\.(1\\d|2[0-8])\\.ffn_.*_shexp\\.weight=CUDA0&amp;quot; -ot &amp;quot;token_embd\\.weight=CPU&amp;quot; -ot &amp;quot;blk\\.10\\.ffn_down_exps\\.weight=CPU&amp;quot; -ot &amp;quot;blk\\.(1[1-9]|[2-4]\\d)\\.ffn_.*_exps\\.weight=CPU&amp;quot; -ot &amp;quot;blk\\.(29|[3-4]\\d)\\.ffn_.*_shexp\\.weight=CPU&amp;quot; -ot &amp;quot;blk\\.46\\.nextn\\.(eh_proj|embed_tokens|shared_head_head)\\.weight=CPU&amp;quot; -ts 0,0,1 --jinja&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhsyv9",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n704dmf/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754370934,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754370934,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 7
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6zghx9",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pro-editor-1105",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6z5cv5",
                                "score": 3,
                                "author_fullname": "t2_uptissiz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "\\+1",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6zghx9",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;+1&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhsyv9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6zghx9/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754360763,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754360763,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z5cv5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "InfernalDread",
                      "can_mod_post": false,
                      "created_utc": 1754356801,
                      "send_replies": true,
                      "parent_id": "t1_n6yy9tx",
                      "score": 4,
                      "author_fullname": "t2_6tebyfr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Could you please share the command? I really have a similar setup to yours and want to make this model work. Thank you!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z5cv5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Could you please share the command? I really have a similar setup to yours and want to make this model work. Thank you!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6z5cv5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754356801,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yy9tx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Muted-Celebration-47",
            "can_mod_post": false,
            "created_utc": 1754354296,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 6,
            "author_fullname": "t2_q2qi86l3f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I got 10t/s with my 3090 + 64gb ram but I use a complex override tensor",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yy9tx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I got 10t/s with my 3090 + 64gb ram but I use a complex override tensor&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yy9tx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754354296,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6zgmaf",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Pro-editor-1105",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6ylviq",
                                          "score": 1,
                                          "author_fullname": "t2_uptissiz",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "That was for the q3 but i still decided to change to a better iq4xs one",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6zgmaf",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That was for the q3 but i still decided to change to a better iq4xs one&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhsyv9",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6zgmaf/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754360808,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754360808,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ylviq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Filo0104",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ykf07",
                                "score": 4,
                                "author_fullname": "t2_1qq10p5wjo",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "i was going to test that on my machine, but i see that hugging face marked it as unsafe\n\n```\nThe following viruses have been found: Pickle.Malware.SysAccess.sys.STACK\\_GLOBAL.UNOFFICIAL\n```",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ylviq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i was going to test that on my machine, but i see that hugging face marked it as unsafe&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nThe following viruses have been found: Pickle.Malware.SysAccess.sys.STACK\\_GLOBAL.UNOFFICIAL\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhsyv9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ylviq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754350024,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754350024,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ylt1o",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pro-editor-1105",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ykf07",
                                "score": 3,
                                "author_fullname": "t2_uptissiz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Now mrradermacher has uploaded some, he seems a lot more legit so i will try that.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ylt1o",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Now mrradermacher has uploaded some, he seems a lot more legit so i will try that.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhsyv9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ylt1o/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754350001,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754350001,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ykf07",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "solidsnakeblue",
                      "can_mod_post": false,
                      "created_utc": 1754349541,
                      "send_replies": true,
                      "parent_id": "t1_n6yjx7i",
                      "score": 1,
                      "author_fullname": "t2_7zh6fslk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Its in his current working config, \n\n    DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ykf07",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Its in his current working config, &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ykf07/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754349541,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yjx7i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Only_Situation_4713",
            "can_mod_post": false,
            "created_utc": 1754349376,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 2,
            "author_fullname": "t2_aafjsulg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Where did you get the gguf from? There's a lot of GGUFs floating around from when they were testing the PR.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yjx7i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Where did you get the gguf from? There&amp;#39;s a lot of GGUFs floating around from when they were testing the PR.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yjx7i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754349376,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yz884",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1754354629,
                      "send_replies": true,
                      "parent_id": "t1_n6yql46",
                      "score": 2,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Getting more tensors on the GPU I think should help prompt processing significantly, \n\nThis isn't particularly true on llama.cpp.  If you don't provide `--no-op-offload`  then it will stream the entire model to the GPU to process larger prompt batches (size &gt;=32 IIRC).  So you get the whole horsepower of the GPU but you're limited by PCIe speed vs prompt ubatch-size.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yz884",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Getting more tensors on the GPU I think should help prompt processing significantly, &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This isn&amp;#39;t particularly true on llama.cpp.  If you don&amp;#39;t provide &lt;code&gt;--no-op-offload&lt;/code&gt;  then it will stream the entire model to the GPU to process larger prompt batches (size &amp;gt;=32 IIRC).  So you get the whole horsepower of the GPU but you&amp;#39;re limited by PCIe speed vs prompt ubatch-size.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yz884/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754354629,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yql46",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cristoper",
            "can_mod_post": false,
            "created_utc": 1754351599,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 2,
            "author_fullname": "t2_38xkk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; GPU memory usage: 7.7GB/24GB\n\nYou can fit more on your GPU. You're currently keeping all tensors that match \".ffn_.*_exps.\" on your CPU... have you tried keeping more experts on your GPU (see u/segmond's comment)?\n\nGetting more tensors on the GPU I think should help prompt processing significantly, but I'm not sure how much it will help token generation. Do you know how many tokens/s can be expected with a 4090 + RAM? I'm definitely interested in what kind of performance you end up getting out of it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yql46",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;GPU memory usage: 7.7GB/24GB&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You can fit more on your GPU. You&amp;#39;re currently keeping all tensors that match &amp;quot;.ffn_.*_exps.&amp;quot; on your CPU... have you tried keeping more experts on your GPU (see &lt;a href=\"/u/segmond\"&gt;u/segmond&lt;/a&gt;&amp;#39;s comment)?&lt;/p&gt;\n\n&lt;p&gt;Getting more tensors on the GPU I think should help prompt processing significantly, but I&amp;#39;m not sure how much it will help token generation. Do you know how many tokens/s can be expected with a 4090 + RAM? I&amp;#39;m definitely interested in what kind of performance you end up getting out of it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yql46/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754351599,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n716lnl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Muted-Celebration-47",
            "can_mod_post": false,
            "created_utc": 1754391942,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 2,
            "author_fullname": "t2_q2qi86l3f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I just found PR from llamacpp that tried to solve complex regex override tensor =&gt; [https://github.com/ggml-org/llama.cpp/pull/15077](https://github.com/ggml-org/llama.cpp/pull/15077)  \nthis way, you just add e.g. --n-cpu-moe 2 in your command to easily optimize your speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n716lnl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just found PR from llamacpp that tried to solve complex regex override tensor =&amp;gt; &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15077\"&gt;https://github.com/ggml-org/llama.cpp/pull/15077&lt;/a&gt;&lt;br/&gt;\nthis way, you just add e.g. --n-cpu-moe 2 in your command to easily optimize your speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n716lnl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754391942,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6zxjle",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "solidsnakeblue",
                      "can_mod_post": false,
                      "created_utc": 1754367646,
                      "send_replies": true,
                      "parent_id": "t1_n6zir0x",
                      "score": 2,
                      "author_fullname": "t2_7zh6fslk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Can you share your launch parameters?  Curious how you're getting 8.5tk/sec, ty",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6zxjle",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can you share your launch parameters?  Curious how you&amp;#39;re getting 8.5tk/sec, ty&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6zxjle/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754367646,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6zir0x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RedditLLM",
            "can_mod_post": false,
            "created_utc": 1754361604,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 1,
            "author_fullname": "t2_1e8os02rp7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Performance shouldn't be that bad. Are you using llama.cpp?\n\nMy speed is 1 x 3090 + 1 x 4060 + ddr4 = 116.53 ms per token, 8.58 tokens per second.\n\nGLM-4.5-Air Q4\\_K\\_M, not downloaded, converted from GGUF myself.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6zir0x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Performance shouldn&amp;#39;t be that bad. Are you using llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;My speed is 1 x 3090 + 1 x 4060 + ddr4 = 116.53 ms per token, 8.58 tokens per second.&lt;/p&gt;\n\n&lt;p&gt;GLM-4.5-Air Q4_K_M, not downloaded, converted from GGUF myself.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6zir0x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754361604,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n71j37n",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "tomz17",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n70ygye",
                                "score": 1,
                                "author_fullname": "t2_1mhx5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Profoundly incorrect",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n71j37n",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Profoundly incorrect&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhsyv9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n71j37n/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754397071,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754397071,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n70ygye",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "mascool",
                      "can_mod_post": false,
                      "created_utc": 1754387846,
                      "send_replies": true,
                      "parent_id": "t1_n7085gi",
                      "score": 1,
                      "author_fullname": "t2_38juy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "PCIe speed will be the limiting factor more than DDR speeds",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n70ygye",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;PCIe speed will be the limiting factor more than DDR speeds&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n70ygye/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754387846,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7085gi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tomz17",
            "can_mod_post": false,
            "created_utc": 1754372901,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 1,
            "author_fullname": "t2_1mhx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "2 x 3090 @ 250w  + 1 x 9684x w/ 12x32GB DDR5 4800.   Initial tests showing a peak of 40t/s generation and 240t/s prompt processing and slowing down from there as the context fills up, but I haven't spent much time tuning these\n\nQ4\\_K\\_XL\n\n          -ngl 999 \n          -fa\n          -c 131072\n          --no-warmup\n          -ctk q8_0 \n          -ctv q8_0\n          --swa-full\n          --jinja\n          -ncmoe 27\n          -t 48\n          -ts 70,30\n\nGPU0 @ 23066MiB  \nGPU1 @ 22998MiB  \nCPU @ 38GB\n\n\n\nIMHO, your problem is you are using a consumer platform with 2 channels of DDR5.  That is NEVER going to go anywhere useful w.r.t. CPU offloading.  It's like trying to attach a fire hydrant to a straw.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7085gi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;2 x 3090 @ 250w  + 1 x 9684x w/ 12x32GB DDR5 4800.   Initial tests showing a peak of 40t/s generation and 240t/s prompt processing and slowing down from there as the context fills up, but I haven&amp;#39;t spent much time tuning these&lt;/p&gt;\n\n&lt;p&gt;Q4_K_XL&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;      -ngl 999 \n      -fa\n      -c 131072\n      --no-warmup\n      -ctk q8_0 \n      -ctv q8_0\n      --swa-full\n      --jinja\n      -ncmoe 27\n      -t 48\n      -ts 70,30\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;GPU0 @ 23066MiB&lt;br/&gt;\nGPU1 @ 22998MiB&lt;br/&gt;\nCPU @ 38GB&lt;/p&gt;\n\n&lt;p&gt;IMHO, your problem is you are using a consumer platform with 2 channels of DDR5.  That is NEVER going to go anywhere useful w.r.t. CPU offloading.  It&amp;#39;s like trying to attach a fire hydrant to a straw.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n7085gi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754372901,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]