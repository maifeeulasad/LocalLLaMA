[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Since the release of GLM 4.5, I've seen many contributors working hard to support at llama.cpp.\n\nHowever, as far as I remember, serise of quant model were registered on MLX community almost on the zero day in GLM case.\n\n1. Can the safetensor of usual MOE model be easily converted to quant using MLX? Or did Apple provides additional support for releasing of GLM model?\n\n2. Is it possible to perform fine-tuning using QLoRA with an already quantized MLX? GGUF cannot be used for fine-tuning once it is generated as I know.\n\n3. The most important question: Is it possible to fine-tune the GLM-4.5 Air model on Mac using the MLX framework right now?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Can I fine-tune GLM-4.5 Air via MLX?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mi3has",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1dhesoqqtu",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754382278,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754382014,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since the release of GLM 4.5, I&amp;#39;ve seen many contributors working hard to support at llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;However, as far as I remember, serise of quant model were registered on MLX community almost on the zero day in GLM case.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Can the safetensor of usual MOE model be easily converted to quant using MLX? Or did Apple provides additional support for releasing of GLM model?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is it possible to perform fine-tuning using QLoRA with an already quantized MLX? GGUF cannot be used for fine-tuning once it is generated as I know.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The most important question: Is it possible to fine-tune the GLM-4.5 Air model on Mac using the MLX framework right now?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mi3has",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Desperate-Sir-5088",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mi3has/can_i_finetune_glm45_air_via_mlx/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi3has/can_i_finetune_glm45_air_via_mlx/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754382014,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70pmnw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Tiny_Judge_2119",
            "can_mod_post": false,
            "created_utc": 1754382738,
            "send_replies": true,
            "parent_id": "t3_1mi3has",
            "score": 4,
            "author_fullname": "t2_aqcxxu50",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I don't know why GLM 4.5 took that long for llama cpp, it's quite standard deepseek v3 architecture with standard attention layer, it shouldn't take too much time to add support for it. For fine-tuning you can just check out the mlx-lm it has built in lora fine-tuning support the process is fair simply and straightforward",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70pmnw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know why GLM 4.5 took that long for llama cpp, it&amp;#39;s quite standard deepseek v3 architecture with standard attention layer, it shouldn&amp;#39;t take too much time to add support for it. For fine-tuning you can just check out the mlx-lm it has built in lora fine-tuning support the process is fair simply and straightforward&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi3has/can_i_finetune_glm45_air_via_mlx/n70pmnw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754382738,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi3has",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70pcpf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Careless_Garlic1438",
            "can_mod_post": false,
            "created_utc": 1754382575,
            "send_replies": true,
            "parent_id": "t3_1mi3has",
            "score": 2,
            "author_fullname": "t2_w3uuzkpbi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes you can Finetune MLX models, I had success with Mistral and Qwen though not all models can be fused afterwards and you need to use base model + adapter afterwards for inference … you need to install the python tools mlx-lm …",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70pcpf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes you can Finetune MLX models, I had success with Mistral and Qwen though not all models can be fused afterwards and you need to use base model + adapter afterwards for inference … you need to install the python tools mlx-lm …&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi3has/can_i_finetune_glm45_air_via_mlx/n70pcpf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754382575,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi3has",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]