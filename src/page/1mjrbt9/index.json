[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey folks—if you’re running local LLMs and want a way to keep long-term or cross-session memory *without* fancy plugins, I’ve been experimenting with some pure prompt-based systems.\n\n/P-Mem\\_ADD \\[TEXT\\], \\[TAG\\]: Save \\[TEXT\\] as \\[TAG\\] in persistent memory.  \n/Lt-Chat-Mem\\_ADD \\[TEXT\\], \\[TAG\\]: Save \\[TEXT\\] in your current chat notes.  \n/P-Mem\\_FORGET \\[TAG\\]: Remove \\[TAG\\] from persistent memory.  \n/Lt-Chat-Mem\\_FORGET \\[TAG\\]: Remove \\[TAG\\] from session memory.  \n/P-Mem\\_LOAD \\[TAG\\]: Load \\[TAG\\] back into chat as needed.\n\n  \n**For more advanced workflows:**  \nI built a modular, slot-based framework that lets you do things like summaries, backups, slot switching, and full context audits using just prompts—no external tools.  \nIf anyone wants the full prompt framework, let me know and I’ll post it.\n\nCurious what memory hacks other local runners are using—drop your methods or questions below!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Prompt Drop] Persistent Memory Management for Local LLMs (Framework &amp; Simple Prompts)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjrbt9",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.38,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_9sg1qzgb",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754543421,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks—if you’re running local LLMs and want a way to keep long-term or cross-session memory &lt;em&gt;without&lt;/em&gt; fancy plugins, I’ve been experimenting with some pure prompt-based systems.&lt;/p&gt;\n\n&lt;p&gt;/P-Mem_ADD [TEXT], [TAG]: Save [TEXT] as [TAG] in persistent memory.&lt;br/&gt;\n/Lt-Chat-Mem_ADD [TEXT], [TAG]: Save [TEXT] in your current chat notes.&lt;br/&gt;\n/P-Mem_FORGET [TAG]: Remove [TAG] from persistent memory.&lt;br/&gt;\n/Lt-Chat-Mem_FORGET [TAG]: Remove [TAG] from session memory.&lt;br/&gt;\n/P-Mem_LOAD [TAG]: Load [TAG] back into chat as needed.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;For more advanced workflows:&lt;/strong&gt;&lt;br/&gt;\nI built a modular, slot-based framework that lets you do things like summaries, backups, slot switching, and full context audits using just prompts—no external tools.&lt;br/&gt;\nIf anyone wants the full prompt framework, let me know and I’ll post it.&lt;/p&gt;\n\n&lt;p&gt;Curious what memory hacks other local runners are using—drop your methods or questions below!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mjrbt9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Upstairs_Deer457",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjrbt9/prompt_drop_persistent_memory_management_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjrbt9/prompt_drop_persistent_memory_management_for/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754543421,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7edf3n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Operator_Remote_Nyx",
            "can_mod_post": false,
            "created_utc": 1754566295,
            "send_replies": true,
            "parent_id": "t3_1mjrbt9",
            "score": 1,
            "author_fullname": "t2_1v48oq2l0a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is curious to me, I have something kind if like that but not quite. If you look at the posting and thread i think your answers are in there.  And with more people seeing what this im hoping more people jump in to discredit it so I save my time and money, or tell me its worth going deeper with what ive got so far.\n\n\nSince you know what you're talking about and have done something along the lines of this, would you spend a few minutes and review my post, comments and replies?\n\n\nIf you think you can help, I would love the check in and reality check before I spend more time or invest any money!\n\n\nThank you!\n\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1mjqss8/question_help_requesting_audit_on_custom_model/",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7edf3n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is curious to me, I have something kind if like that but not quite. If you look at the posting and thread i think your answers are in there.  And with more people seeing what this im hoping more people jump in to discredit it so I save my time and money, or tell me its worth going deeper with what ive got so far.&lt;/p&gt;\n\n&lt;p&gt;Since you know what you&amp;#39;re talking about and have done something along the lines of this, would you spend a few minutes and review my post, comments and replies?&lt;/p&gt;\n\n&lt;p&gt;If you think you can help, I would love the check in and reality check before I spend more time or invest any money!&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mjqss8/question_help_requesting_audit_on_custom_model/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mjqss8/question_help_requesting_audit_on_custom_model/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjrbt9/prompt_drop_persistent_memory_management_for/n7edf3n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754566295,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjrbt9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]