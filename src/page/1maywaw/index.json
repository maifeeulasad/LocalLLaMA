[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "[The initials of Devstral, Mistral, and Magistral as connected puzzle pieces](https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7)\n\n\n\ntl;dr: title. Here are the weights: [Devstral-Small-2507-Rebased-Vision](https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision) &amp; [Magistral-Small-2507-Rebased-Vision](https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision) &amp; [Devstral-Small-2507-Rebased-Vision-LoRA](https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA)\n\nI've been using Mistral-Small-3.2 for the past few weeks. It's pretty solid, and the combination of vision and speed make it a really good pick for me, but...\n\nI'm using sglang and it's really memory hungry which means it's hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I've tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at \\~Q6-Q8, but meh.\n\nThen I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for \\~1.1x the VRAM? Yes, please! I tried [mergekit](https://github.com/arcee-ai/mergekit#lora-extraction) but it requires the same architecture, so I'd either have to drop vision (which I also tried, and it seemed to work, but I don't like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it's actually pretty easy, you just have to copy the `model` weights over the `language_model` weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &gt;!Yes, I know 2+ other attempts were made (*one by unsloth, from whom I stole the weights, lol*) for the *exact* same thing, and could've saved me a whole day of pain, but I only remembered about it \\~5 mins ago, but this wasn't the core of what I wanted to do anyway so we'll conveniently call it a draw D:!&lt;\n\nWith the \"new\" models in place, the next step was to try creating LoRAs again. Well, mergekit didn't work. I almost quit, but decided to search the web for another method and I ended up finding [LoRD](https://github.com/thomasgauthier/LoRD), the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn't even tell me why, I just get a generic error, but it's probably the vision parts, or 1+ of the modules (linear\\_1 / linear\\_2 / merging\\_layer / lm\\_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in [vLLM](https://github.com/vllm-project/vllm/issues/18574)). In either case, it meant I couldn't run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.\n\nIf I'm not too lazy (which I'll likely be), I'll give this another go sometime, but now I'll just start my 761435 Karl Franz campaign.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Devstral &amp; Magistral as adapters of Mistral",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Other"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 75,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "tshdyj57ghff1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 108,
                    "x": 108,
                    "u": "https://preview.redd.it/tshdyj57ghff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9cc805736aa2013d7ca4bd816bdb649a9cd2d871"
                  },
                  {
                    "y": 216,
                    "x": 216,
                    "u": "https://preview.redd.it/tshdyj57ghff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41ed921c4e6a128b0fbd80d0921c46b2d6755243"
                  },
                  {
                    "y": 320,
                    "x": 320,
                    "u": "https://preview.redd.it/tshdyj57ghff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0ad4cd99e1863be63b3a4c08034865431c69624"
                  },
                  {
                    "y": 640,
                    "x": 640,
                    "u": "https://preview.redd.it/tshdyj57ghff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=897ba3761d52e61715c3eb1d34ba8e3708e3ee6f"
                  },
                  {
                    "y": 960,
                    "x": 960,
                    "u": "https://preview.redd.it/tshdyj57ghff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b2689357836116b0804c2c34e284749c615b663"
                  },
                  {
                    "y": 1080,
                    "x": 1080,
                    "u": "https://preview.redd.it/tshdyj57ghff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=932f2fe782da56be5e6bef05578c4e9edc202cda"
                  }
                ],
                "s": {
                  "y": 2048,
                  "x": 2048,
                  "u": "https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7"
                },
                "id": "tshdyj57ghff1"
              }
            },
            "name": "t3_1maywaw",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "ups": 29,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_k6u7rfxb",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Other",
            "can_mod_post": false,
            "score": 29,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=32c7761ccccf9d9cd3d58f04849c4249d01be54a",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "subreddit_type": "public",
            "created": 1753653691,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/tshdyj57ghff1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14e06a8a7213b113ef28becb5a61878fc952e8c7\"&gt;The initials of Devstral, Mistral, and Magistral as connected puzzle pieces&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;tl;dr: title. Here are the weights: &lt;a href=\"https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision\"&gt;Devstral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href=\"https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision\"&gt;Magistral-Small-2507-Rebased-Vision&lt;/a&gt; &amp;amp; &lt;a href=\"https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision-LoRA\"&gt;Devstral-Small-2507-Rebased-Vision-LoRA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using Mistral-Small-3.2 for the past few weeks. It&amp;#39;s pretty solid, and the combination of vision and speed make it a really good pick for me, but...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using sglang and it&amp;#39;s really memory hungry which means it&amp;#39;s hard to fit another model side-by-side without much extra VRAM or low quantization (GPTQ/AWQ). Instead, I&amp;#39;ve tuned the various parameters until I brought the VRAM usage low enough that I can also run Devstral with exllamav3 (Q6), but once in a while sglang throws an OOM when there are multiple queries with images, and I need to load the two servers in a specific order for it to work. It kinda sucks. Running exllama is much slower for any individual model, but would probably work fine for all the at ~Q6-Q8, but meh.&lt;/p&gt;\n\n&lt;p&gt;Then I got an idea: how about I treat retrofit Devstral/Magistral as LoRAs? 3 models for ~1.1x the VRAM? Yes, please! I tried &lt;a href=\"https://github.com/arcee-ai/mergekit#lora-extraction\"&gt;mergekit&lt;/a&gt; but it requires the same architecture, so I&amp;#39;d either have to drop vision (which I also tried, and it seemed to work, but I don&amp;#39;t like it!) or try to add vision to Devstral and Magistral. Since these two are trained on the same architecture, it&amp;#39;s actually pretty easy, you just have to copy the &lt;code&gt;model&lt;/code&gt; weights over the &lt;code&gt;language_model&lt;/code&gt; weights. I did this for both models, and spent a few hours running some benchmarks (in each repo README) to see if there was any significant issue, and it seems to be fine with most being well within the standard error range. I tested a few images and it seemed to work too. There is a significant difference between models, so I probably did that correct too. However, make sure to test on your own and tell me if you notice any issues! &lt;span class=\"md-spoiler-text\"&gt;Yes, I know 2+ other attempts were made (&lt;em&gt;one by unsloth, from whom I stole the weights, lol&lt;/em&gt;) for the &lt;em&gt;exact&lt;/em&gt; same thing, and could&amp;#39;ve saved me a whole day of pain, but I only remembered about it ~5 mins ago, but this wasn&amp;#39;t the core of what I wanted to do anyway so we&amp;#39;ll conveniently call it a draw D:&lt;/span&gt;&lt;/p&gt;\n\n&lt;p&gt;With the &amp;quot;new&amp;quot; models in place, the next step was to try creating LoRAs again. Well, mergekit didn&amp;#39;t work. I almost quit, but decided to search the web for another method and I ended up finding &lt;a href=\"https://github.com/thomasgauthier/LoRD\"&gt;LoRD&lt;/a&gt;, the original version of the mergekit code (and it has an Apache license!). It required quite a bit of tweaking to get it working for the Mistral model (and not OOM constantly), but after a few hours I think it succeeded in creating the adapter. I briefly tested with transformers in the same notebook, but sadly it cannot be loaded by sglang. It doesn&amp;#39;t even tell me why, I just get a generic error, but it&amp;#39;s probably the vision parts, or 1+ of the modules (linear_1 / linear_2 / merging_layer / lm_head). Or LoRA might not be support at all for Mistral 3.1 (e.g. like in &lt;a href=\"https://github.com/vllm-project/vllm/issues/18574\"&gt;vLLM&lt;/a&gt;). In either case, it meant I couldn&amp;#39;t run benchmarks to evaluate quality degration, so I uploaded that to huggingface as well if anyone wants to try.&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m not too lazy (which I&amp;#39;ll likely be), I&amp;#39;ll give this another go sometime, but now I&amp;#39;ll just start my 761435 Karl Franz campaign.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?auto=webp&amp;s=f33db570c3a8f16c2ac464fb9062565d9b50b904",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=336ca45300d9ad8f941487b0ce465efa53dd0e02",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fa2934bddfe176555bff114786099245f85abcf",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8cd13e62d30f7a7e58e32919b6d165c24a125225",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=22ed0b082947f94ee079c2a6004328efe6c66fc9",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcd80bad37121082c8b613252204288829d1c8be",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=11b16dfa1d4b69bdf4dc3a4f5e8a530bc6d72c2d",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "ExFuLA42V4peZpwQDsAgEzViFAWZpyUbQAHlGXRRxKQ"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#94e044",
            "id": "1maywaw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "kmouratidis",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/",
            "subreddit_subscribers": 506191,
            "created_utc": 1753653691,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5lrgne",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "a_beautiful_rhind",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5l3nu1",
                                                    "score": 2,
                                                    "author_fullname": "t2_h5utwre7",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I think so:\n\nhttps://github.com/turboderp-org/exllamav2/commit/9244003a40ee3e09762a5d9459e32af369621280",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5lrgne",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think so:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/turboderp-org/exllamav2/commit/9244003a40ee3e09762a5d9459e32af369621280\"&gt;https://github.com/turboderp-org/exllamav2/commit/9244003a40ee3e09762a5d9459e32af369621280&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1maywaw",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5lrgne/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753708847,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753708847,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5l3nu1",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "kmouratidis",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5ier3n",
                                          "score": 1,
                                          "author_fullname": "t2_k6u7rfxb",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Does Mistral 3.1+ with vision work on exllama? v2/v3?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5l3nu1",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does Mistral 3.1+ with vision work on exllama? v2/v3?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1maywaw",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5l3nu1/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753699191,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753699191,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ier3n",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "a_beautiful_rhind",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5idzst",
                                "score": 1,
                                "author_fullname": "t2_h5utwre7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "In my case I used exllamav2 to run HF weights without quantizing.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ier3n",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In my case I used exllamav2 to run HF weights without quantizing.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maywaw",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5ier3n/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753655284,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753655284,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5idzst",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "kmouratidis",
                      "can_mod_post": false,
                      "created_utc": 1753655024,
                      "send_replies": true,
                      "parent_id": "t1_n5ibmec",
                      "score": 1,
                      "author_fullname": "t2_k6u7rfxb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks, I'll give it a go! Although I think the issue will still be on the sglang side. Will have to see.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5idzst",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, I&amp;#39;ll give it a go! Although I think the issue will still be on the sglang side. Will have to see.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maywaw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5idzst/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753655024,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ibmec",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1753654228,
            "send_replies": true,
            "parent_id": "t3_1maywaw",
            "score": 10,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There is mergekit that works on other architectures. Someone had made a PR and then gave up on it: https://github.com/Ph0rk0z/mergekit\n\nI merged vison models with it before but don't have the internet bandwidth to handle the kind of models I want to work on. Partial merges were a bit iffy, my goal was to merge qwen-vl-72b with other finetunes.. pixtral-large pretty much serves this purpose for me tho. \n\nWas able to merge 7b vision qwen with the few RP tunes that existed. I think the VL was 2.5 and not qwen2, those didn't work.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ibmec",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is mergekit that works on other architectures. Someone had made a PR and then gave up on it: &lt;a href=\"https://github.com/Ph0rk0z/mergekit\"&gt;https://github.com/Ph0rk0z/mergekit&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I merged vison models with it before but don&amp;#39;t have the internet bandwidth to handle the kind of models I want to work on. Partial merges were a bit iffy, my goal was to merge qwen-vl-72b with other finetunes.. pixtral-large pretty much serves this purpose for me tho. &lt;/p&gt;\n\n&lt;p&gt;Was able to merge 7b vision qwen with the few RP tunes that existed. I think the VL was 2.5 and not qwen2, those didn&amp;#39;t work.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5ibmec/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753654228,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maywaw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5iqsws",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "JLeonsarmiento",
            "can_mod_post": false,
            "created_utc": 1753659472,
            "send_replies": true,
            "parent_id": "t3_1maywaw",
            "score": 4,
            "author_fullname": "t2_9b9s4a7g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Dude, Devstral with vision was all that I wanted for this Christmas. Thanks!\n\nNow I just need that someone mlxes this thing to 4 bit.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5iqsws",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Dude, Devstral with vision was all that I wanted for this Christmas. Thanks!&lt;/p&gt;\n\n&lt;p&gt;Now I just need that someone mlxes this thing to 4 bit.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5iqsws/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753659472,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maywaw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5iu016",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "kmouratidis",
                      "can_mod_post": false,
                      "created_utc": 1753660602,
                      "send_replies": true,
                      "parent_id": "t1_n5ir7q4",
                      "score": 2,
                      "author_fullname": "t2_k6u7rfxb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "If you have 2 models with the same architecture, you should be able to \"extract\" a LoRA (even if it was not trained as such) from one and then serve both efficiently with sglang:  https://docs.sglang.ai/backend/lora.html\n\n&gt; SGLang enables the use of LoRA adapters with a base model. By incorporating techniques from S-LoRA and Punica, SGLang can efficiently support multiple LoRA adapters for different sequences within a single batch of inputs.\n\nEdit: Or vLLM: https://docs.vllm.ai/en/latest/features/lora.html",
                      "edited": 1753660790,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5iu016",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you have 2 models with the same architecture, you should be able to &amp;quot;extract&amp;quot; a LoRA (even if it was not trained as such) from one and then serve both efficiently with sglang:  &lt;a href=\"https://docs.sglang.ai/backend/lora.html\"&gt;https://docs.sglang.ai/backend/lora.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;SGLang enables the use of LoRA adapters with a base model. By incorporating techniques from S-LoRA and Punica, SGLang can efficiently support multiple LoRA adapters for different sequences within a single batch of inputs.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Edit: Or vLLM: &lt;a href=\"https://docs.vllm.ai/en/latest/features/lora.html\"&gt;https://docs.vllm.ai/en/latest/features/lora.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maywaw",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5iu016/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753660602,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ir7q4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "lakySK",
            "can_mod_post": false,
            "created_utc": 1753659615,
            "send_replies": true,
            "parent_id": "t3_1maywaw",
            "score": 4,
            "author_fullname": "t2_y9y2q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Interesting idea! Could this be used for the thinking and instruct Qwens to have both available without needing 2x RAM or constant reloading?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ir7q4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting idea! Could this be used for the thinking and instruct Qwens to have both available without needing 2x RAM or constant reloading?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5ir7q4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753659615,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maywaw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5kpx7p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Blizado",
            "can_mod_post": false,
            "created_utc": 1753691417,
            "send_replies": true,
            "parent_id": "t3_1maywaw",
            "score": 0,
            "author_fullname": "t2_j0e2r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Interesting. I came also to this idea since MistralAI release more and more different models with different features like the new Voxtral. I wonder if they will ever bring one model which combines all their features. Maybe that is their main goal at the end, maybe they will use them only for their own Chatbot and never release it on Huggingface. I think it should be doable, only the right tools are missing.\n\nHow would it be with a MoE model? One expert for audio, another for vision etc.\n\nWe definitely need models that can do all stuff together.",
            "edited": 1753691698,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kpx7p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting. I came also to this idea since MistralAI release more and more different models with different features like the new Voxtral. I wonder if they will ever bring one model which combines all their features. Maybe that is their main goal at the end, maybe they will use them only for their own Chatbot and never release it on Huggingface. I think it should be doable, only the right tools are missing.&lt;/p&gt;\n\n&lt;p&gt;How would it be with a MoE model? One expert for audio, another for vision etc.&lt;/p&gt;\n\n&lt;p&gt;We definitely need models that can do all stuff together.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maywaw/devstral_magistral_as_adapters_of_mistral/n5kpx7p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753691417,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maywaw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]