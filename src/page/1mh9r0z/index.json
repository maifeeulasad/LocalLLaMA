[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’ve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I’ve shifted focus to the infra side, specifically LLM gateways.\n\nMost of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.\n\nSome quick observations from what I tried:\n\n* [Bifrost](https://getmax.im/2frost) (Go, self-hosted): Surprisingly fast even under high load. Saw around 11µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.\n* [Portkey](https://portkey.ai/features/ai-gateway): Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.\n* **Kong** and [Gloo](https://www.solo.io/products/gloo-ai-gateway): These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn’t feel natural. Not LLM-aware.\n* **Cloudflare’s AI Gateway**: Pretty good for lightweight routing if you're already using Cloudflare. But it’s a black box, not much visibility or customization.\n* **Aisera’s Gateway**: Geared toward enterprise support use cases. More of a vertical solution. Didn’t feel suitable for general-purpose LLM infra.\n* [LiteLLM](https://www.litellm.ai/): Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.\n\nWould love to hear what others are running in production, especially if you’re doing failover, traffic splitting, or anything more advanced.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best LLM gateway?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mh9r0z",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1tizhpru5u",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754303241,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I’ve shifted focus to the infra side, specifically LLM gateways.&lt;/p&gt;\n\n&lt;p&gt;Most of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.&lt;/p&gt;\n\n&lt;p&gt;Some quick observations from what I tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://getmax.im/2frost\"&gt;Bifrost&lt;/a&gt; (Go, self-hosted): Surprisingly fast even under high load. Saw around 11µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://portkey.ai/features/ai-gateway\"&gt;Portkey&lt;/a&gt;: Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kong&lt;/strong&gt; and &lt;a href=\"https://www.solo.io/products/gloo-ai-gateway\"&gt;Gloo&lt;/a&gt;: These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn’t feel natural. Not LLM-aware.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Cloudflare’s AI Gateway&lt;/strong&gt;: Pretty good for lightweight routing if you&amp;#39;re already using Cloudflare. But it’s a black box, not much visibility or customization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Aisera’s Gateway&lt;/strong&gt;: Geared toward enterprise support use cases. More of a vertical solution. Didn’t feel suitable for general-purpose LLM infra.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.litellm.ai/\"&gt;LiteLLM&lt;/a&gt;: Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear what others are running in production, especially if you’re doing failover, traffic splitting, or anything more advanced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mh9r0z",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Educational-Bison786",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/",
            "subreddit_subscribers": 509911,
            "created_utc": 1754303241,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uvasl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dismal_Ad4474",
            "can_mod_post": false,
            "created_utc": 1754309227,
            "send_replies": true,
            "parent_id": "t3_1mh9r0z",
            "score": 1,
            "author_fullname": "t2_1p2tjipcf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Litellm is easy to setup but difficult to scale when you are building for production. I have seen litellm fail around 250-300 RPS. It is also quite resource hungry leading to unnecessary infra complexity.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uvasl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Litellm is easy to setup but difficult to scale when you are building for production. I have seen litellm fail around 250-300 RPS. It is also quite resource hungry leading to unnecessary infra complexity.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/n6uvasl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754309227,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh9r0z",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uxq53",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sleepshiteat",
            "can_mod_post": false,
            "created_utc": 1754310156,
            "send_replies": true,
            "parent_id": "t3_1mh9r0z",
            "score": 1,
            "author_fullname": "t2_1mtenh9doj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "All your posts got maxim ai at the top. If you are doing promotion you should clearly state that.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uxq53",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;All your posts got maxim ai at the top. If you are doing promotion you should clearly state that.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/n6uxq53/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754310156,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh9r0z",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]