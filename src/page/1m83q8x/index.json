[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I need help with LLM-Document Processing.\n\nWhat would be the efficient and precise way to process long documents (avg. 100 pages / .docx, pdf). \n\nUse case:\n\nChecking a document for certain aspects and retrieving information for those certain aspects even if they are writting in chapters where they should not be.\n\nE.g. : information on how to install a software and safety information regarding the server.\n\nInstruction steps on the installation and the safety information should be seperated.\n\nInput: instructions for the installation with additional safety information (install the software and ensure to make a backup)\n\nOutput should be seperated information: \n\ninstall the software.\n\nBackup is necessary.\n\nIt is intended as a single-use workflow for each document and not to create a knowledgebase with text embedding.\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Document processing",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m83q8x",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_mfal4ndo",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753361925,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help with LLM-Document Processing.&lt;/p&gt;\n\n&lt;p&gt;What would be the efficient and precise way to process long documents (avg. 100 pages / .docx, pdf). &lt;/p&gt;\n\n&lt;p&gt;Use case:&lt;/p&gt;\n\n&lt;p&gt;Checking a document for certain aspects and retrieving information for those certain aspects even if they are writting in chapters where they should not be.&lt;/p&gt;\n\n&lt;p&gt;E.g. : information on how to install a software and safety information regarding the server.&lt;/p&gt;\n\n&lt;p&gt;Instruction steps on the installation and the safety information should be seperated.&lt;/p&gt;\n\n&lt;p&gt;Input: instructions for the installation with additional safety information (install the software and ensure to make a backup)&lt;/p&gt;\n\n&lt;p&gt;Output should be seperated information: &lt;/p&gt;\n\n&lt;p&gt;install the software.&lt;/p&gt;\n\n&lt;p&gt;Backup is necessary.&lt;/p&gt;\n\n&lt;p&gt;It is intended as a single-use workflow for each document and not to create a knowledgebase with text embedding.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m83q8x",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "zzrscbi",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m83q8x/document_processing/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m83q8x/document_processing/",
            "subreddit_subscribers": 504023,
            "created_utc": 1753361925,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4wc8vb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "daaain",
            "can_mod_post": false,
            "created_utc": 1753364215,
            "send_replies": true,
            "parent_id": "t3_1m83q8x",
            "score": 1,
            "author_fullname": "t2_47j85",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Even if you don't want to do embedding, you'd still need to somehow pre-process the documents. First of all, you'd of course need to extract the text from the docx / pdf documents, and then somehow chunk it up. If you're not embedding, you could keep the chunks more variable sized and semantic, say one block each chapter. One thing you could do is to summarise each chapter, so the LLM generating the output could do hierarchical queries through the summaries to find the right sections. You'll still need some backend to do the search through the text (can be Redis, Elasticsearch, Postgres, or just sed/grep, whatever you're familiar with) and you'd probably be best off wrapping it in an MCP, so the LLM can do incremental context building, otherwise you'll waiting a lot for prompt processing.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4wc8vb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Even if you don&amp;#39;t want to do embedding, you&amp;#39;d still need to somehow pre-process the documents. First of all, you&amp;#39;d of course need to extract the text from the docx / pdf documents, and then somehow chunk it up. If you&amp;#39;re not embedding, you could keep the chunks more variable sized and semantic, say one block each chapter. One thing you could do is to summarise each chapter, so the LLM generating the output could do hierarchical queries through the summaries to find the right sections. You&amp;#39;ll still need some backend to do the search through the text (can be Redis, Elasticsearch, Postgres, or just sed/grep, whatever you&amp;#39;re familiar with) and you&amp;#39;d probably be best off wrapping it in an MCP, so the LLM can do incremental context building, otherwise you&amp;#39;ll waiting a lot for prompt processing.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m83q8x/document_processing/n4wc8vb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753364215,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m83q8x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]