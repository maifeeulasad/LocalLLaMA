[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hello\n\nI am currently looking for the best launch parameters for CPU inference with llama.cpp. I am running the Qwen3-30B-A3B model on my laptop with the following specs:\n\nAMD Ryzen 7 PRO 7840u w/ Radeon 780M Graphics (16CPUs), 32GB Ram.\n\nSince the whole topic around the launch parameters is rather complex, I wanted to ask you about your experience and overall best practices regarding pure CPU inference.\n\nCurrently I am running llama.cpp with the following parameters:\n\nllama-server.exe -m models\\\\X -t 16 --n\\_predict 4096 --ctx-size 64000\n\nThanks in advance!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best Practice For CPU Inference",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgixw4",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.71,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1k95d35h",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754226810,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I am currently looking for the best launch parameters for CPU inference with llama.cpp. I am running the Qwen3-30B-A3B model on my laptop with the following specs:&lt;/p&gt;\n\n&lt;p&gt;AMD Ryzen 7 PRO 7840u w/ Radeon 780M Graphics (16CPUs), 32GB Ram.&lt;/p&gt;\n\n&lt;p&gt;Since the whole topic around the launch parameters is rather complex, I wanted to ask you about your experience and overall best practices regarding pure CPU inference.&lt;/p&gt;\n\n&lt;p&gt;Currently I am running llama.cpp with the following parameters:&lt;/p&gt;\n\n&lt;p&gt;llama-server.exe -m models\\X -t 16 --n_predict 4096 --ctx-size 64000&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgixw4",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "hudimudi",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/",
            "subreddit_subscribers": 509918,
            "created_utc": 1754226810,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6pb0yb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "emprahsFury",
            "can_mod_post": false,
            "created_utc": 1754232442,
            "send_replies": false,
            "parent_id": "t3_1mgixw4",
            "score": 3,
            "author_fullname": "t2_177r8n",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "you really need to play around with the number of threads. Use llamabench to test multiple thread counts and see where you start getting thread-level contention. And once you figure that out you will probably need to set --threads-batch so that you can use the full complement of threads on prompt-processing.\n\nAlso consider compiling your own version of llama.cpp with all the cpu instructions you have enabled. When you load a model look at what is enabled  \n`system_info: n_threads = 40 (n_threads_batch = 64) / 64 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VB`  \nMI = 1 | AVX512\\_VNNI = 1 | AVX512\\_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |\n\nThose should all match what your cpu is capable of. Also consider using \"obsolete\" quants like q4\\_0 as they offer online repacking which is great for avx performance.\n\nAnother benefit of compiling your own llama.cpp is that the gpu enabled builds always place something onto the gpu. Usually hundreds of MB at least. If you dont compile gpu support then you will free up that memory for useful things. Something like this (for your available instructions) inside the llama.cpp repo you cloned\n\n`cmake -S . -B build -DLLAMA_CURL=OFF -DGGML_HIP=OFF -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Release -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON  -DGGML_AVX512_VNNI=ON  -DGGM`  \n`L_AVX512_BF16=ON`\n\n`cmake --build build --target llama-server llama-bench llama-cli llama-quantize --config Release -- -j 16`",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pb0yb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you really need to play around with the number of threads. Use llamabench to test multiple thread counts and see where you start getting thread-level contention. And once you figure that out you will probably need to set --threads-batch so that you can use the full complement of threads on prompt-processing.&lt;/p&gt;\n\n&lt;p&gt;Also consider compiling your own version of llama.cpp with all the cpu instructions you have enabled. When you load a model look at what is enabled&lt;br/&gt;\n&lt;code&gt;system_info: n_threads = 40 (n_threads_batch = 64) / 64 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VB&lt;/code&gt;&lt;br/&gt;\nMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |&lt;/p&gt;\n\n&lt;p&gt;Those should all match what your cpu is capable of. Also consider using &amp;quot;obsolete&amp;quot; quants like q4_0 as they offer online repacking which is great for avx performance.&lt;/p&gt;\n\n&lt;p&gt;Another benefit of compiling your own llama.cpp is that the gpu enabled builds always place something onto the gpu. Usually hundreds of MB at least. If you dont compile gpu support then you will free up that memory for useful things. Something like this (for your available instructions) inside the llama.cpp repo you cloned&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;cmake -S . -B build -DLLAMA_CURL=OFF -DGGML_HIP=OFF -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Release -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON  -DGGML_AVX512_VNNI=ON  -DGGM&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;L_AVX512_BF16=ON&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;cmake --build build --target llama-server llama-bench llama-cli llama-quantize --config Release -- -j 16&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/n6pb0yb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754232442,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgixw4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6u77ij",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Malfun_Eddie",
                      "can_mod_post": false,
                      "created_utc": 1754297281,
                      "send_replies": true,
                      "parent_id": "t1_n6pduoi",
                      "score": 1,
                      "author_fullname": "t2_1auay9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is there an official docker image from?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6u77ij",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is there an official docker image from?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgixw4",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/n6u77ij/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754297281,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6pduoi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1754233304,
            "send_replies": true,
            "parent_id": "t3_1mgixw4",
            "score": 3,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Best practice is using ik_llama.cpp\n\n\nCf. https://huggingface.co/ubergarm/Qwen3-30B-A3B-GGUF/discussions/2#6862a6736935808d5a1cc96f",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pduoi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Best practice is using ik_llama.cpp&lt;/p&gt;\n\n&lt;p&gt;Cf. &lt;a href=\"https://huggingface.co/ubergarm/Qwen3-30B-A3B-GGUF/discussions/2#6862a6736935808d5a1cc96f\"&gt;https://huggingface.co/ubergarm/Qwen3-30B-A3B-GGUF/discussions/2#6862a6736935808d5a1cc96f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/n6pduoi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754233304,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgixw4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6qa56r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "My_Unbiased_Opinion",
            "can_mod_post": false,
            "created_utc": 1754243102,
            "send_replies": true,
            "parent_id": "t3_1mgixw4",
            "score": 2,
            "author_fullname": "t2_esiyl0yb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For CPU or CPU+GPU inference, ik_llama.cpp is the way to go. I'm getting almost 2x prompt processing speed vs llama.cpp. also use q4_0 since it's fastest on CPU. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qa56r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For CPU or CPU+GPU inference, ik_llama.cpp is the way to go. I&amp;#39;m getting almost 2x prompt processing speed vs llama.cpp. also use q4_0 since it&amp;#39;s fastest on CPU. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgixw4/best_practice_for_cpu_inference/n6qa56r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754243102,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgixw4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]