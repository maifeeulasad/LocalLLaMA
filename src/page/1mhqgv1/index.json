[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey r/LocalLLaMA, I am considering buying an M3 mac studio for local LLM server needs\n\nThe needs are as follows\n\n\\&gt;run LLM models LOCALLY (locality is non-negotiable)\n\n\\&gt;stream files, videos across multiple computers, emails and other basic server operations\n\nThe big limitation is, currently, we don't have the infrastructure to host larger servers, and for the time being, the LLM models the M3 studio can run are the main priorities.\n\nIf the mac studio can be sufficient as a server that we can safely, remotely log into, as well as download files, or stream files from, then it works great as we have an offer from a seller. If the M3 can work, under the current constraints, it would be perfect, but not sure how macOS would function as a small server for LLMs.\n\nIf not, we will focus on eliminating our current constraints and consider other options.\n\nThanks!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Maxed out M3 Mac studio as an LLM server for local employees?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhqgv1",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.79,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 11,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_s784can9o",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 11,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754343447,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754343152,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, I am considering buying an M3 mac studio for local LLM server needs&lt;/p&gt;\n\n&lt;p&gt;The needs are as follows&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;run LLM models LOCALLY (locality is non-negotiable)&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;stream files, videos across multiple computers, emails and other basic server operations&lt;/p&gt;\n\n&lt;p&gt;The big limitation is, currently, we don&amp;#39;t have the infrastructure to host larger servers, and for the time being, the LLM models the M3 studio can run are the main priorities.&lt;/p&gt;\n\n&lt;p&gt;If the mac studio can be sufficient as a server that we can safely, remotely log into, as well as download files, or stream files from, then it works great as we have an offer from a seller. If the M3 can work, under the current constraints, it would be perfect, but not sure how macOS would function as a small server for LLMs.&lt;/p&gt;\n\n&lt;p&gt;If not, we will focus on eliminating our current constraints and consider other options.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhqgv1",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Manderbillt2000",
            "discussion_type": null,
            "num_comments": 33,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754343152,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yagdd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Accomplished_Ad9530",
                      "can_mod_post": false,
                      "created_utc": 1754346216,
                      "send_replies": true,
                      "parent_id": "t1_n6y4udd",
                      "score": 2,
                      "author_fullname": "t2_88fma001",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "In principal batched inference should give a decent speedup on a Mac, however it's not currently implemented in any MLX/MPS inference engine that I know of. Also, there is a lot of low-hanging fruit for speedups on the horizon like multi-token prediction. All-in-all I wouldn't be surprised at a 10x increase in token throughput in the coming months.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yagdd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In principal batched inference should give a decent speedup on a Mac, however it&amp;#39;s not currently implemented in any MLX/MPS inference engine that I know of. Also, there is a lot of low-hanging fruit for speedups on the horizon like multi-token prediction. All-in-all I wouldn&amp;#39;t be surprised at a 10x increase in token throughput in the coming months.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhqgv1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6yagdd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754346216,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6y4udd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "false79",
            "can_mod_post": false,
            "created_utc": 1754344404,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 22,
            "author_fullname": "t2_wn888",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think Ultra M3 is fun toy for 1 person but if you are expecting 5+ people to compete for the 80 gpu cores, it's not enough.\n\nMac is one if not the best for VRAM capacity GB per dollar but GPU compute it's like a RTX 4070 GPU. It's not strong in this department unless you are using double digit billion param models.\n\nEdit: The benchmarks you see for Ultra M3 is because a User is testing it with all access to 80 cores.\n\nIf you have the same machine with n users, expect 1/n-th performance for the same hardware. Or worse, first one in gets all access while the others wait for their turn.",
            "edited": 1754345170,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y4udd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think Ultra M3 is fun toy for 1 person but if you are expecting 5+ people to compete for the 80 gpu cores, it&amp;#39;s not enough.&lt;/p&gt;\n\n&lt;p&gt;Mac is one if not the best for VRAM capacity GB per dollar but GPU compute it&amp;#39;s like a RTX 4070 GPU. It&amp;#39;s not strong in this department unless you are using double digit billion param models.&lt;/p&gt;\n\n&lt;p&gt;Edit: The benchmarks you see for Ultra M3 is because a User is testing it with all access to 80 cores.&lt;/p&gt;\n\n&lt;p&gt;If you have the same machine with n users, expect 1/n-th performance for the same hardware. Or worse, first one in gets all access while the others wait for their turn.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y4udd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754344404,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 22
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y3102",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Hasuto",
            "can_mod_post": false,
            "created_utc": 1754343823,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 4,
            "author_fullname": "t2_qta4h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You'll want to spend some time examining benchmarks for models you want to run first. And make sure you test those models first. Try faking data and using them over API to get a feel for what they can do.\n\nThen compare the quality and the performance to get an idea if it's useful or not for your use.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y3102",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;ll want to spend some time examining benchmarks for models you want to run first. And make sure you test those models first. Try faking data and using them over API to get a feel for what they can do.&lt;/p&gt;\n\n&lt;p&gt;Then compare the quality and the performance to get an idea if it&amp;#39;s useful or not for your use.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y3102/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754343823,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n711y8n",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "bfume",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6zvd5m",
                                          "score": 0,
                                          "author_fullname": "t2_8rj6u",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Know what else is important? Reading comprehension. ",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n711y8n",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Know what else is important? Reading comprehension. &lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhqgv1",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n711y8n/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754389686,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754389686,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 0
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6zvd5m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "henfiber",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6z5ffb",
                                "score": 2,
                                "author_fullname": "t2_lw9me25",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What about summarizing an article, a youtube transcript, RAG-based QA, sentiment analysis, image understanding, continuing a story, etc.? Most real-world use cases require feeding the model with data, so TTFT is really important.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6zvd5m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What about summarizing an article, a youtube transcript, RAG-based QA, sentiment analysis, image understanding, continuing a story, etc.? Most real-world use cases require feeding the model with data, so TTFT is really important.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhqgv1",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6zvd5m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754366689,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754366689,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z5ffb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bfume",
                      "can_mod_post": false,
                      "created_utc": 1754356827,
                      "send_replies": true,
                      "parent_id": "t1_n6yh7mm",
                      "score": 1,
                      "author_fullname": "t2_8rj6u",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "time to first token is awesome as long as you’re not (sigh) vibe coding.  not everyone wants an LLM for that.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z5ffb",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;time to first token is awesome as long as you’re not (sigh) vibe coding.  not everyone wants an LLM for that.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhqgv1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6z5ffb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754356827,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yh7mm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "CryptoCryst828282",
            "can_mod_post": false,
            "created_utc": 1754348473,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 8,
            "author_fullname": "t2_b8e4vw6kg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is a very bad idea.   People dont like to talk about the the time to first token on macs.   If you start getting 3/4 request in que it could take no joke 10m to first token on even a smaller model.   I have seen 30b models on high end m3/m4 take 4-5 min with a decent amount of context.     They are fun to play with, not sure i would want them in a production setting.\n\n  \nEdit: guess i should give my .02 on what i would use based on my experience if i wanted to have batching for multiple users i would likely get an x99 platform and 4-5 32gb mi50's   You could get that setup for 1000-1200 and easily run 30b models with several people asking at same time.    I dont even know how you would use it to server files, i guess you could get Thunderbolt drive arrays but in the past that has always ended badly for me.   Some things are better to do right and when its at a company, you really need it right.",
            "edited": 1754348690,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yh7mm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a very bad idea.   People dont like to talk about the the time to first token on macs.   If you start getting 3/4 request in que it could take no joke 10m to first token on even a smaller model.   I have seen 30b models on high end m3/m4 take 4-5 min with a decent amount of context.     They are fun to play with, not sure i would want them in a production setting.&lt;/p&gt;\n\n&lt;p&gt;Edit: guess i should give my .02 on what i would use based on my experience if i wanted to have batching for multiple users i would likely get an x99 platform and 4-5 32gb mi50&amp;#39;s   You could get that setup for 1000-1200 and easily run 30b models with several people asking at same time.    I dont even know how you would use it to server files, i guess you could get Thunderbolt drive arrays but in the past that has always ended badly for me.   Some things are better to do right and when its at a company, you really need it right.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6yh7mm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754348473,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6y6p5k",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": false,
                                                    "author": "Valuable-Run2129",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6y5dcr",
                                                    "score": 4,
                                                    "author_fullname": "t2_n1rqaeut",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "The prompt processing speed would be atrocious. I definitely wouldn’t recommend it. Especially because you’ll have to serve it statelessly for multiple users. \nIt could work well for something like Qwen3-30B-A3.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6y6p5k",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The prompt processing speed would be atrocious. I definitely wouldn’t recommend it. Especially because you’ll have to serve it statelessly for multiple users. \nIt could work well for something like Qwen3-30B-A3.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mhqgv1",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y6p5k/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754344982,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754344982,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 4
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6y5dcr",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Manderbillt2000",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6y3m8v",
                                          "score": 1,
                                          "author_fullname": "t2_s784can9o",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "we're looking at quantized non-reasoning models for the time being, deepseek v3's quantized at 2.71 bit is the main target (https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q2\\_K\\_XL), I believe, \\*should\\* work well on the m3 studio with 512gb of RAM. Any other LLM we can run on it is a bonus.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6y5dcr",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;we&amp;#39;re looking at quantized non-reasoning models for the time being, deepseek v3&amp;#39;s quantized at 2.71 bit is the main target (&lt;a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q2%5C_K%5C_XL\"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q2\\_K\\_XL&lt;/a&gt;), I believe, *should* work well on the m3 studio with 512gb of RAM. Any other LLM we can run on it is a bonus.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhqgv1",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y5dcr/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754344565,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754344565,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y3m8v",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Valuable-Run2129",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y1x7k",
                                "score": 1,
                                "author_fullname": "t2_n1rqaeut",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What size models do you plan to use and what do you mean by “streaming files”?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y3m8v",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What size models do you plan to use and what do you mean by “streaming files”?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhqgv1",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y3m8v/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754344011,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754344011,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6y3d34",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Manderbillt2000",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6y2upg",
                                          "score": 2,
                                          "author_fullname": "t2_s784can9o",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "512GB!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6y3d34",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;512GB!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhqgv1",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y3d34/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754343930,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754343930,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y2upg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Mr-Angry-Capybara",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y1x7k",
                                "score": 0,
                                "author_fullname": "t2_10cpjfd9gg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Total VRAM available?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y2upg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Total VRAM available?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhqgv1",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y2upg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754343768,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754343768,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y1x7k",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Manderbillt2000",
                      "can_mod_post": false,
                      "created_utc": 1754343472,
                      "send_replies": true,
                      "parent_id": "t1_n6y1o4m",
                      "score": 2,
                      "author_fullname": "t2_s784can9o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "At once, at most maybe 6 or 7 people for the time being",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y1x7k",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;At once, at most maybe 6 or 7 people for the time being&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhqgv1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y1x7k/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754343472,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6y1o4m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Valuable-Run2129",
            "can_mod_post": false,
            "created_utc": 1754343393,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 2,
            "author_fullname": "t2_n1rqaeut",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How many people would be using it?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y1o4m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How many people would be using it?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y1o4m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754343393,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y6jog",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "friedrichvonschiller",
            "can_mod_post": false,
            "created_utc": 1754344933,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 2,
            "author_fullname": "t2_37pb0w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The speed and quality will be more than sufficient for most tasks, and the price is right. I had been eyeing the M2 for org-wide inference when I worked for Johns Hopkins, but we had real data centers that didn't want a bunch of Studios in them, and the Mac Pro is bad.\n\nYou'll get enough throughput that it will feel commercial unless you use very large models. There may be queueing, as others mentioned, but you can communicate that.\n\nA well-trained smaller model can perform quite well. Bigger just means more absolute capacity to capture relationships and fine details in data. That often spills over into intelligence, but smaller models with good prompts will surprise you.\n\nAMD was another attractive price point, but they've hiked the cost of the MI350 series. It's still better than NVIDIA, but I wouldn't pay it if I weren't training.\n\nCloud services exist, but the cost is higher for similar performance. The ROI is much better on the Studio.\n\nJHU ended up just going full Copilot like everywhere else. Per-query services will always be both the least private and the best bang for the buck (very high GPU utilization is a *good* thing from an economic perspective).",
            "edited": 1754345858,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y6jog",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The speed and quality will be more than sufficient for most tasks, and the price is right. I had been eyeing the M2 for org-wide inference when I worked for Johns Hopkins, but we had real data centers that didn&amp;#39;t want a bunch of Studios in them, and the Mac Pro is bad.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ll get enough throughput that it will feel commercial unless you use very large models. There may be queueing, as others mentioned, but you can communicate that.&lt;/p&gt;\n\n&lt;p&gt;A well-trained smaller model can perform quite well. Bigger just means more absolute capacity to capture relationships and fine details in data. That often spills over into intelligence, but smaller models with good prompts will surprise you.&lt;/p&gt;\n\n&lt;p&gt;AMD was another attractive price point, but they&amp;#39;ve hiked the cost of the MI350 series. It&amp;#39;s still better than NVIDIA, but I wouldn&amp;#39;t pay it if I weren&amp;#39;t training.&lt;/p&gt;\n\n&lt;p&gt;Cloud services exist, but the cost is higher for similar performance. The ROI is much better on the Studio.&lt;/p&gt;\n\n&lt;p&gt;JHU ended up just going full Copilot like everywhere else. Per-query services will always be both the least private and the best bang for the buck (very high GPU utilization is a &lt;em&gt;good&lt;/em&gt; thing from an economic perspective).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y6jog/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754344933,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6z2xkk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "InterstellarReddit",
            "can_mod_post": false,
            "created_utc": 1754355936,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 2,
            "author_fullname": "t2_3aooiye4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hey fam, I have a  M4 max with 128 GB of RAM and it’s not as fast as my desktop running dual RTX 3090\n\nI think you’re better off getting a normal system and then building either an RTX server or connecting to hugging face because although the Mac’s are convenient, they’re not as powerful as actual desktop rigs or huggin face. \n\nThis has been my experience",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6z2xkk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey fam, I have a  M4 max with 128 GB of RAM and it’s not as fast as my desktop running dual RTX 3090&lt;/p&gt;\n\n&lt;p&gt;I think you’re better off getting a normal system and then building either an RTX server or connecting to hugging face because although the Mac’s are convenient, they’re not as powerful as actual desktop rigs or huggin face. &lt;/p&gt;\n\n&lt;p&gt;This has been my experience&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6z2xkk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754355936,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y5jnn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Accomplished_Ad9530",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y4zsa",
                                "score": 2,
                                "author_fullname": "t2_88fma001",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Oh, in that case, a maxed out M3 Studio Ultra should be able to run nearly anything you throw at it at reasonable speeds.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y5jnn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh, in that case, a maxed out M3 Studio Ultra should be able to run nearly anything you throw at it at reasonable speeds.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhqgv1",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y5jnn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754344618,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754344618,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y4zsa",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Manderbillt2000",
                      "can_mod_post": false,
                      "created_utc": 1754344450,
                      "send_replies": true,
                      "parent_id": "t1_n6y37g5",
                      "score": 1,
                      "author_fullname": "t2_s784can9o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "only one person using LLMs for the time being",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y4zsa",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;only one person using LLMs for the time being&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhqgv1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y4zsa/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754344450,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6y37g5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Accomplished_Ad9530",
            "can_mod_post": false,
            "created_utc": 1754343880,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_88fma001",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How many employees would be using the LLMs at once?\n\nRemote access and the rest is trivial on macOS, and most open source software you'd find on linux is available via homebrew. You might have a look at r/selfhosted to figure out what sort of services you want to run, and how to do so securely if any of them will be exposed to the internet.\n\nEdit: I see you answered 6-7 concurrent LLM users in another comment. In that case it really depends on the LLM models you have in mind, and I'd give the same advice as u/Hasuto: figure out what models work for you first. If you can get away with MoEs that are on the smaller side for active parameters, you'd be golden. Thankfully, MoEs are all the rage right now, so you have a plethora to choose from depending on your use-cases.",
            "edited": 1754344501,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y37g5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How many employees would be using the LLMs at once?&lt;/p&gt;\n\n&lt;p&gt;Remote access and the rest is trivial on macOS, and most open source software you&amp;#39;d find on linux is available via homebrew. You might have a look at &lt;a href=\"/r/selfhosted\"&gt;r/selfhosted&lt;/a&gt; to figure out what sort of services you want to run, and how to do so securely if any of them will be exposed to the internet.&lt;/p&gt;\n\n&lt;p&gt;Edit: I see you answered 6-7 concurrent LLM users in another comment. In that case it really depends on the LLM models you have in mind, and I&amp;#39;d give the same advice as &lt;a href=\"/u/Hasuto\"&gt;u/Hasuto&lt;/a&gt;: figure out what models work for you first. If you can get away with MoEs that are on the smaller side for active parameters, you&amp;#39;d be golden. Thankfully, MoEs are all the rage right now, so you have a plethora to choose from depending on your use-cases.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y37g5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754343880,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y6nqi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "chisleu",
            "can_mod_post": false,
            "created_utc": 1754344970,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_cbxyn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've got one of these. I think it could be up to the task for serving something like Qwen3 coder 30b a3b \n\nIt's definitely up to the task of remote management (macs have VNC built in) and file serving (again, samba is build in)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y6nqi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got one of these. I think it could be up to the task for serving something like Qwen3 coder 30b a3b &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s definitely up to the task of remote management (macs have VNC built in) and file serving (again, samba is build in)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y6nqi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754344970,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yqcz6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Indication4035",
            "can_mod_post": false,
            "created_utc": 1754351522,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_t350h0lz4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "you can enter your hardware spec on hugging face then go to a gguf you want to run. The page will tell you what size your specs can run.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yqcz6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you can enter your hardware spec on hugging face then go to a gguf you want to run. The page will tell you what size your specs can run.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6yqcz6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754351522,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yrrak",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1754352003,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Problem is if employees try to use it with cline or roo or some coding assistant that will try to use 80k tokens on every query and the mac will take 10 minutes to process each user. The only practical solution for coding are GPUs. For just chatting a mac is fine, though.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yrrak",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Problem is if employees try to use it with cline or roo or some coding assistant that will try to use 80k tokens on every query and the mac will take 10 minutes to process each user. The only practical solution for coding are GPUs. For just chatting a mac is fine, though.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6yrrak/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754352003,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6z5lkh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sealsBclubbin",
            "can_mod_post": false,
            "created_utc": 1754356888,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_1hyxu6yd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You should probs just get your employees Macbooks with at least 48gb ram so they can run the models themselves instead of through one machine",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6z5lkh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You should probs just get your employees Macbooks with at least 48gb ram so they can run the models themselves instead of through one machine&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6z5lkh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754356888,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6zy4wq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "decentralizedbee",
            "can_mod_post": false,
            "created_utc": 1754367913,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_hjqo50xu2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "how many employees do you have?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6zy4wq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;how many employees do you have?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6zy4wq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754367913,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n708ckg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GTHell",
            "can_mod_post": false,
            "created_utc": 1754373006,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_oqe4l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just buy a multiple used 3090. The problem with local inference is memory. Speed should be atleast acceptable on 3090.\n\nThen setup rate limit per minutes through Litellm or alternative to prevent everyone to crash the server.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n708ckg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just buy a multiple used 3090. The problem with local inference is memory. Speed should be atleast acceptable on 3090.&lt;/p&gt;\n\n&lt;p&gt;Then setup rate limit per minutes through Litellm or alternative to prevent everyone to crash the server.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n708ckg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754373006,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70q6bn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Cergorach",
            "can_mod_post": false,
            "created_utc": 1754383056,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_cs4w88d2l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Looks like a fully kitted out M3 Ultra becomes your only option then. Only local and no actual servers.\n\nM3 Ultra 512GB can run something like DS r1 671b quantized, it's not fast and you won't have a very large context window, but it works. But it's still not up to par to the unquantized model. Something like the DS r1 70b model gave me better results then ChatGPT 3.5 and possibly 4.0, but again, it's nothing compared to the current big models. You really need to set expectations.\n\nWhile you probably could setup a Mac Studio up as a file server, I wouldn't call it ideal. Just get a small NAS (Synology) and use that for file storage/distribution.\n\nAnd first determine your AI/LLM use cases before starting to think about buying hardware. Chances are good that it won't fit your needs within your constraints.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70q6bn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Looks like a fully kitted out M3 Ultra becomes your only option then. Only local and no actual servers.&lt;/p&gt;\n\n&lt;p&gt;M3 Ultra 512GB can run something like DS r1 671b quantized, it&amp;#39;s not fast and you won&amp;#39;t have a very large context window, but it works. But it&amp;#39;s still not up to par to the unquantized model. Something like the DS r1 70b model gave me better results then ChatGPT 3.5 and possibly 4.0, but again, it&amp;#39;s nothing compared to the current big models. You really need to set expectations.&lt;/p&gt;\n\n&lt;p&gt;While you probably could setup a Mac Studio up as a file server, I wouldn&amp;#39;t call it ideal. Just get a small NAS (Synology) and use that for file storage/distribution.&lt;/p&gt;\n\n&lt;p&gt;And first determine your AI/LLM use cases before starting to think about buying hardware. Chances are good that it won&amp;#39;t fit your needs within your constraints.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n70q6bn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754383056,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6zffd9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "rorowhat",
            "can_mod_post": false,
            "created_utc": 1754360371,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 1,
            "author_fullname": "t2_yq51a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Get a PC",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6zffd9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Get a PC&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6zffd9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754360371,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y3y0k",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "getmevodka",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y3bxq",
                                "score": 2,
                                "author_fullname": "t2_7uoa6r1b",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yeah i stay at what i said. the difference is just too big. almost as big as the power difference. the token generation speed goes down just way too quick on apple silicone with slightly larger models like the 235b and upwards. i own the m3 ultra 256gb and i wouldnt use more. sorry to say 🫥🫶",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y3y0k",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah i stay at what i said. the difference is just too big. almost as big as the power difference. the token generation speed goes down just way too quick on apple silicone with slightly larger models like the 235b and upwards. i own the m3 ultra 256gb and i wouldnt use more. sorry to say 🫥🫶&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhqgv1",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y3y0k/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754344115,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754344115,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6zkpd0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Bitter_Firefighter_1",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y3bxq",
                                "score": 1,
                                "author_fullname": "t2_939o3k0q",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Just off load the file serving.  Many options for that.  I feel your client does not have a great clue.  They insist on local but don't want to spend the $.  \n\nRight now it is basically one or the other",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6zkpd0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just off load the file serving.  Many options for that.  I feel your client does not have a great clue.  They insist on local but don&amp;#39;t want to spend the $.  &lt;/p&gt;\n\n&lt;p&gt;Right now it is basically one or the other&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhqgv1",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6zkpd0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754362337,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754362337,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y3bxq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Manderbillt2000",
                      "can_mod_post": false,
                      "created_utc": 1754343919,
                      "send_replies": true,
                      "parent_id": "t1_n6y34jm",
                      "score": 1,
                      "author_fullname": "t2_s784can9o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "inference would be  only one person at once, but others may hook in to work with files",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y3bxq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;inference would be  only one person at once, but others may hook in to work with files&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhqgv1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y3bxq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754343919,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6y34jm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "getmevodka",
            "can_mod_post": false,
            "created_utc": 1754343854,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 0,
            "author_fullname": "t2_7uoa6r1b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "as a single person inference machine yes, not for an office though... but just imho. i think youd need a good server with like 4 5090 cards to run a model fast and good enough for several employees to work with local ai at acceptable speeds, but maybe im wrong and someone comes around thats all for it and can tell us why and how.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y34jm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;as a single person inference machine yes, not for an office though... but just imho. i think youd need a good server with like 4 5090 cards to run a model fast and good enough for several employees to work with local ai at acceptable speeds, but maybe im wrong and someone comes around thats all for it and can tell us why and how.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6y34jm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754343854,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6z2ixp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dear-Argument7658",
            "can_mod_post": false,
            "created_utc": 1754355792,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 0,
            "author_fullname": "t2_y3hxemhj4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I see you mentioned 6-7 people at once in a comment - not sure if this means 6-7 parallel requests or people running inference randomly throughout an hour.\n\nA Mac Studio may be nice for a single user, and it's cool being able to run really large models, but to what end? With large models, you'll see very slow prompt processing (time to first token). If you scale down to smaller models, you could just as well run a PC with an NVIDIA GPU - you'd be able to process multiple requests simultaneously without slowdown, with 10-20x faster response times. Plus, you won't need to deal with macOS on a server.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6z2ixp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I see you mentioned 6-7 people at once in a comment - not sure if this means 6-7 parallel requests or people running inference randomly throughout an hour.&lt;/p&gt;\n\n&lt;p&gt;A Mac Studio may be nice for a single user, and it&amp;#39;s cool being able to run really large models, but to what end? With large models, you&amp;#39;ll see very slow prompt processing (time to first token). If you scale down to smaller models, you could just as well run a PC with an NVIDIA GPU - you&amp;#39;d be able to process multiple requests simultaneously without slowdown, with 10-20x faster response times. Plus, you won&amp;#39;t need to deal with macOS on a server.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6z2ixp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754355792,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6za2cc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Roland_Bodel_the_2nd",
            "can_mod_post": false,
            "created_utc": 1754358493,
            "send_replies": true,
            "parent_id": "t3_1mhqgv1",
            "score": 0,
            "author_fullname": "t2_i80v0xfb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For a single user on a larger model the performance is already like &lt;5tok/s.  Get m3 ultra with 512GB RAM for each user.  Alternatively you will need at least like a tinybox green as a shared multi-user system.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6za2cc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For a single user on a larger model the performance is already like &amp;lt;5tok/s.  Get m3 ultra with 512GB RAM for each user.  Alternatively you will need at least like a tinybox green as a shared multi-user system.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhqgv1/maxed_out_m3_mac_studio_as_an_llm_server_for/n6za2cc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754358493,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhqgv1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]