[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey r/LocalLLaMA,\nJust got an RTX 5090 with 32GB of VRAM and I'm looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.\n\n * What's the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?\n\n * Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?\n\n * Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?\n\nMy goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!\n\nThanks!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "RTX 5090 (32GB VRAM) - Full Fine-Tuning: What Can I Expect?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m5ro7s",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.82,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_dyvrh",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753124390,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,\nJust got an RTX 5090 with 32GB of VRAM and I&amp;#39;m looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;What&amp;#39;s the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m5ro7s",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "celsowm",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
            "subreddit_subscribers": 502721,
            "created_utc": 1753124390,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n4g2bg7",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Capable-Ad-7494",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n4f235b",
                                          "score": 1,
                                          "author_fullname": "t2_9so78ol2",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "The new one still counts fine tune activations at inference, which i find funny",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n4g2bg7",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The new one still counts fine tune activations at inference, which i find funny&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m5ro7s",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4g2bg7/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753146536,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753146536,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n4f235b",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ShengrenR",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4ezjl9",
                                "score": 3,
                                "author_fullname": "t2_ji4n4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Agreed, it's ancient, which hurts it in a bunch of ways - it's likely assuming all sorts of un-optimized, vanilla attention, no things like gqa, flash-attn etc etc - at full precision 64k context was a big deal 'way back when' lol\n\nA more recently updated calculator with lots of the modern changes: [https://apxml.com/tools/vram-calculator](https://apxml.com/tools/vram-calculator) \\- also has fine-tune calculations built in.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4f235b",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Agreed, it&amp;#39;s ancient, which hurts it in a bunch of ways - it&amp;#39;s likely assuming all sorts of un-optimized, vanilla attention, no things like gqa, flash-attn etc etc - at full precision 64k context was a big deal &amp;#39;way back when&amp;#39; lol&lt;/p&gt;\n\n&lt;p&gt;A more recently updated calculator with lots of the modern changes: &lt;a href=\"https://apxml.com/tools/vram-calculator\"&gt;https://apxml.com/tools/vram-calculator&lt;/a&gt; - also has fine-tune calculations built in.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5ro7s",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4f235b/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753134455,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753134455,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4ezjl9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MikeRoz",
                      "can_mod_post": false,
                      "created_utc": 1753133682,
                      "send_replies": true,
                      "parent_id": "t1_n4e7hgq",
                      "score": 1,
                      "author_fullname": "t2_ht2fg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Something is up with that calculator. I ran it for inference. It's telling me I'd need 305 GB for a 7B model to which I pass a 32k prompt in order to get back 32k tokens. This is with a Q4_K_M ggml quant on the model. These numbers seem absurd, and I don't generally use cache quantization either.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4ezjl9",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Something is up with that calculator. I ran it for inference. It&amp;#39;s telling me I&amp;#39;d need 305 GB for a 7B model to which I pass a 32k prompt in order to get back 32k tokens. This is with a Q4_K_M ggml quant on the model. These numbers seem absurd, and I don&amp;#39;t generally use cache quantization either.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5ro7s",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4ezjl9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753133682,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4e7hgq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ShengrenR",
            "can_mod_post": false,
            "created_utc": 1753125562,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 8,
            "author_fullname": "t2_ji4n4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Full fine-tune is not a game to be played by mere mortals.. and with a single 5090, I'm afraid to inform you, you are solidly in mere mortal realm.  The base model for a \\~7B model is \\~13GB, yes, but then you need gradients, optimizer, and activations. [https://rahulschand.github.io/gpu\\_poor/](https://rahulschand.github.io/gpu_poor/) can give you a (rough.. it's way out of date) flavor of what to expect.. e.g. a llama-2-7B, 1-batch, 1024+1024 context, with gradient checkpointing.. 66GB VRAM.  ouch.  \n[https://docs.unsloth.ai/](https://docs.unsloth.ai/) might help - but don't expect miracles; you may want to pivot to PEFT or simply rent GPU space, unless you want to target some much smaller models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4e7hgq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Full fine-tune is not a game to be played by mere mortals.. and with a single 5090, I&amp;#39;m afraid to inform you, you are solidly in mere mortal realm.  The base model for a ~7B model is ~13GB, yes, but then you need gradients, optimizer, and activations. &lt;a href=\"https://rahulschand.github.io/gpu_poor/\"&gt;https://rahulschand.github.io/gpu_poor/&lt;/a&gt; can give you a (rough.. it&amp;#39;s way out of date) flavor of what to expect.. e.g. a llama-2-7B, 1-batch, 1024+1024 context, with gradient checkpointing.. 66GB VRAM.  ouch.&lt;br/&gt;\n&lt;a href=\"https://docs.unsloth.ai/\"&gt;https://docs.unsloth.ai/&lt;/a&gt; might help - but don&amp;#39;t expect miracles; you may want to pivot to PEFT or simply rent GPU space, unless you want to target some much smaller models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4e7hgq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753125562,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4e7j0h",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "lolzinventor",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4e6trw",
                                "score": 8,
                                "author_fullname": "t2_3wc1i",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You can do it with 4 x 3090 but only about 2.5K context length.  Full fine tunes are worth it though.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4e7j0h",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can do it with 4 x 3090 but only about 2.5K context length.  Full fine tunes are worth it though.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5ro7s",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4e7j0h/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753125574,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753125574,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 8
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4f3sa3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "ShengrenR",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4e6trw",
                                "score": 8,
                                "author_fullname": "t2_ji4n4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yea.. I winced at \"I know 32GB is a lot\" knowing what info was about to come your way lol. Q/LoRA or rent compute imo.  The precision difference between 'full fine tune' and some of the more modern techniques are really not that horribly far off - somebody posted [https://www.reddit.com/r/LocalLLaMA/comments/1l9n911/abba\\_highly\\_expressive\\_hadamard\\_product/](https://www.reddit.com/r/LocalLLaMA/comments/1l9n911/abba_highly_expressive_hadamard_product/) recently and (I haven't run it) it looks pretty promising at first glance. Might check there, or their competing methods in benchmarks.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4f3sa3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yea.. I winced at &amp;quot;I know 32GB is a lot&amp;quot; knowing what info was about to come your way lol. Q/LoRA or rent compute imo.  The precision difference between &amp;#39;full fine tune&amp;#39; and some of the more modern techniques are really not that horribly far off - somebody posted &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1l9n911/abba_highly_expressive_hadamard_product/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1l9n911/abba_highly_expressive_hadamard_product/&lt;/a&gt; recently and (I haven&amp;#39;t run it) it looks pretty promising at first glance. Might check there, or their competing methods in benchmarks.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5ro7s",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4f3sa3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753134984,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753134984,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 8
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4e6trw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "celsowm",
                      "can_mod_post": false,
                      "created_utc": 1753125374,
                      "send_replies": true,
                      "parent_id": "t1_n4e65v6",
                      "score": 5,
                      "author_fullname": "t2_dyvrh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Eight 3090 (24gb) to full fine-tuning an 8b model? Oh my gosh",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4e6trw",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Eight 3090 (24gb) to full fine-tuning an 8b model? Oh my gosh&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5ro7s",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4e6trw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753125374,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4fgh9b",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Willing_Landscape_61",
                      "can_mod_post": false,
                      "created_utc": 1753139120,
                      "send_replies": true,
                      "parent_id": "t1_n4e65v6",
                      "score": 1,
                      "author_fullname": "t2_8lvrytgw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Nice!\nDo you have nvlinks? P2P enabled? What is the PCIe situation?\nI'm wondering if the bottleneck is compute or bandwidth for this task and hardware.\nI dream of such a server but with either 4090 (too pricey for now) with P2P enabled or MI100 with infinity fabric but I am unable to model the expected full fine tuning perf of either build.\nThx !",
                      "edited": 1753139384,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4fgh9b",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice!\nDo you have nvlinks? P2P enabled? What is the PCIe situation?\nI&amp;#39;m wondering if the bottleneck is compute or bandwidth for this task and hardware.\nI dream of such a server but with either 4090 (too pricey for now) with P2P enabled or MI100 with infinity fabric but I am unable to model the expected full fine tuning perf of either build.\nThx !&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5ro7s",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4fgh9b/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753139120,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4e65v6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "lolzinventor",
            "can_mod_post": false,
            "created_utc": 1753125187,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 6,
            "author_fullname": "t2_3wc1i",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "    Qwen/Qwen3-8B-Base\n    Context 4096\n    Deepspeed 3,  No offload, adamw_8bit,  \n    micro_batch_size_per_gpu: 1\n    gradient_accumulation_steps: 16\n    PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n    8x3090\n    0 GPU: 100% Memory: 98.70 % PCIe RX: 3022 MB/s, TX: 1888 MB/s \n    1 GPU: 100% Memory: 98.04 % PCIe RX: 2249 MB/s, TX: 1758 MB/s \n    2 GPU: 100% Memory: 98.61 % PCIe RX: 4749 MB/s, TX: 443 MB/s \n    3 GPU: 100% Memory: 98.21 % PCIe RX: 5818 MB/s, TX: 1991 MB/s \n    4 GPU: 100% Memory: 98.12 % PCIe RX: 4114 MB/s, TX: 1271 MB/s \n    5 GPU: 100% Memory: 93.40 % PCIe RX: 5832 MB/s, TX: 572 MB/s \n    6 GPU: 100% Memory: 98.61 % PCIe RX: 5328 MB/s, TX: 1074 MB/s \n    7 GPU: 100% Memory: 98.37 % PCIe RX: 1924 MB/s, TX: 2001 MB/s",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4e65v6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;pre&gt;&lt;code&gt;Qwen/Qwen3-8B-Base\nContext 4096\nDeepspeed 3,  No offload, adamw_8bit,  \nmicro_batch_size_per_gpu: 1\ngradient_accumulation_steps: 16\nPYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n8x3090\n0 GPU: 100% Memory: 98.70 % PCIe RX: 3022 MB/s, TX: 1888 MB/s \n1 GPU: 100% Memory: 98.04 % PCIe RX: 2249 MB/s, TX: 1758 MB/s \n2 GPU: 100% Memory: 98.61 % PCIe RX: 4749 MB/s, TX: 443 MB/s \n3 GPU: 100% Memory: 98.21 % PCIe RX: 5818 MB/s, TX: 1991 MB/s \n4 GPU: 100% Memory: 98.12 % PCIe RX: 4114 MB/s, TX: 1271 MB/s \n5 GPU: 100% Memory: 93.40 % PCIe RX: 5832 MB/s, TX: 572 MB/s \n6 GPU: 100% Memory: 98.61 % PCIe RX: 5328 MB/s, TX: 1074 MB/s \n7 GPU: 100% Memory: 98.37 % PCIe RX: 1924 MB/s, TX: 2001 MB/s\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4e65v6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753125187,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4ghc3g",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "bick_nyers",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4ffjfa",
                                "score": 2,
                                "author_fullname": "t2_6nwld4d3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Unfortunately that's a question you will need to answer yourself. It does not scale linearly and it is dependent on model arch.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4ghc3g",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately that&amp;#39;s a question you will need to answer yourself. It does not scale linearly and it is dependent on model arch.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5ro7s",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4ghc3g/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753151856,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753151856,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4ffjfa",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Willing_Landscape_61",
                      "can_mod_post": false,
                      "created_utc": 1753138811,
                      "send_replies": true,
                      "parent_id": "t1_n4ea8m0",
                      "score": 2,
                      "author_fullname": "t2_8lvrytgw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "How does context size impact VRAM requirements for pre training/full fine tuning?\nThx.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4ffjfa",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How does context size impact VRAM requirements for pre training/full fine tuning?\nThx.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5ro7s",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4ffjfa/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753138811,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4ea8m0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "bick_nyers",
            "can_mod_post": false,
            "created_utc": 1753126353,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 6,
            "author_fullname": "t2_6nwld4d3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Roughly 16x parameters is a good estimate for full fine-tune using AdamW and gradient checkpointing. Batch size 1 so will need to use gradient accumulation. You can work it down lower using low bit optimizers and stuff like that but roughly you are looking at fine-tuning 2B parameters per 5090.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4ea8m0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Roughly 16x parameters is a good estimate for full fine-tune using AdamW and gradient checkpointing. Batch size 1 so will need to use gradient accumulation. You can work it down lower using low bit optimizers and stuff like that but roughly you are looking at fine-tuning 2B parameters per 5090.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4ea8m0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753126353,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4e9ls4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753126172,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 3,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Do you \\*really\\* need a full fine tune at full precision? QLoRA at 4 bits should be far more practical to do! With this approach you can actually finetune models that properly use your hardware during inference later on. if you want to make models to run on your own pc, it makes little sense to train much smaller models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4e9ls4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you *really* need a full fine tune at full precision? QLoRA at 4 bits should be far more practical to do! With this approach you can actually finetune models that properly use your hardware during inference later on. if you want to make models to run on your own pc, it makes little sense to train much smaller models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4e9ls4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753126172,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4e6n83",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "lolzinventor",
                      "can_mod_post": false,
                      "created_utc": 1753125322,
                      "send_replies": true,
                      "parent_id": "t1_n4e4528",
                      "score": 2,
                      "author_fullname": "t2_3wc1i",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Possibly 3B",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4e6n83",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Possibly 3B&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5ro7s",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4e6n83/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753125322,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4e4528",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1753124618,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 2,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "1B params?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4e4528",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;1B params?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4e4528/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753124618,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4fxivi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1753144870,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 1,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think that without Lora and/or quantization you can maybe fine tune a 1-3B model with 32Gb of vram\n\nAlso, remember that you have to load the model, the context (training on longer context will be a pain in terms of vram... it really grow fast), gradient and optimizer.\n\nstill, check out Unsloth, it is the best \"memory saving\" training framework probably.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4fxivi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think that without Lora and/or quantization you can maybe fine tune a 1-3B model with 32Gb of vram&lt;/p&gt;\n\n&lt;p&gt;Also, remember that you have to load the model, the context (training on longer context will be a pain in terms of vram... it really grow fast), gradient and optimizer.&lt;/p&gt;\n\n&lt;p&gt;still, check out Unsloth, it is the best &amp;quot;memory saving&amp;quot; training framework probably.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4fxivi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753144870,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4fxj12",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Azuriteh",
            "can_mod_post": false,
            "created_utc": 1753144872,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 1,
            "author_fullname": "t2_wmv41",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For full fine-tune you can use Unsloth 4-bit quantization and be able to full fine-tune a 1b model easily! And that's it, anything bigger the VRAM will skyrocket (I don't remember but I think I was also to full fine-tune a 4B model with 4-bit quantization? You might try that).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4fxj12",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For full fine-tune you can use Unsloth 4-bit quantization and be able to full fine-tune a 1b model easily! And that&amp;#39;s it, anything bigger the VRAM will skyrocket (I don&amp;#39;t remember but I think I was also to full fine-tune a 4B model with 4-bit quantization? You might try that).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4fxj12/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753144872,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4g5jo6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ok_Warning2146",
            "can_mod_post": false,
            "created_utc": 1753147670,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 1,
            "author_fullname": "t2_s6sfw4yy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can full fine tune 200m embedding models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4g5jo6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can full fine tune 200m embedding models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4g5jo6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753147670,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4gl9kj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "QFGTrialByFire",
            "can_mod_post": false,
            "created_utc": 1753153347,
            "send_replies": true,
            "parent_id": "t3_1m5ro7s",
            "score": 2,
            "author_fullname": "t2_1h4o7f23eh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would suggest use a Lora on a quantised load of a model to fine tune your models it works great. Then when you've got a handle on your training data etc rent something decent on vastai or elsewhere to do the full model training. To be honest for most tasks just the lora is good enough. I've only got a 3080ti so only 12Gb of RAM but I can load Deepseek lite 16B prams at 4bit quant and train it using loara (slow but possible). So you should be able to do the same but with larger models - at around 40B quant.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4gl9kj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would suggest use a Lora on a quantised load of a model to fine tune your models it works great. Then when you&amp;#39;ve got a handle on your training data etc rent something decent on vastai or elsewhere to do the full model training. To be honest for most tasks just the lora is good enough. I&amp;#39;ve only got a 3080ti so only 12Gb of RAM but I can load Deepseek lite 16B prams at 4bit quant and train it using loara (slow but possible). So you should be able to do the same but with larger models - at around 40B quant.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/n4gl9kj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753153347,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5ro7s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]