[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "With all the new models coming out recently, I've been more and more curious about this. It seems like a few months ago we were all running Gemma 3, now everybody seems to be running Qwen 3, but with recent model releases, which is your go-to daily-driver and why, and if you have secondary model(s), what do you use them for?\n\nI've got a 7900 XTX 24GB, so all of my models are &lt;32B. But here are mine;\n\n* Mistral Small 3.2: A \"better\" version of Gemma 3, in a way. I really liked Gemma 3, but it hallucinated far too much on basic facts. Mistral doesn't on the other hand, it hallucinates far less ime. I'm mainly using it for general knowledge and image analysis and consistently does a better job at both than Gemma for me. Feels a bit cold or sterile compared to Gemma 3 though.\n\n* Qwen 3 30B-A3B-Thinking-2507: The \"Gemini 2.5\" at home model. I've compared it pretty extensively to 2.5 Flash Reasoning, and 2.5 Pro, and it's able to consistently beat Flash and more often than not come close to or match 2.5 Pro. I'm mainly using this model for complex queries, problem solving, and writing. It's a damn good writing model imo, but that's not a *major* use-case for me.\n\n* Qwen 3-Coder 30B-A3B-Instruct-2507: This model acts a lot like a mix of Gemini, Claude, and an openAI model to me in my eyes. It's a really, really capable coder. I'm a software engineer and it's a nice companion in that regard. A lot of people say it's like most like Claude, and from what I've seen from Claude outputs, I tend to agree. although I've never used Claude, admittedly.\n\nSo there we have it, those are the models I use and the use-case for each. I do occasionally use OpenRouter to serve GLM 4.5-Air and Kimi K2, but that's mostly just out of curiosity. So what's everybody else here running?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What's your 'primary' model and why? Do you run a secondary model?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhp2e5",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_oqajf",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754339961,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With all the new models coming out recently, I&amp;#39;ve been more and more curious about this. It seems like a few months ago we were all running Gemma 3, now everybody seems to be running Qwen 3, but with recent model releases, which is your go-to daily-driver and why, and if you have secondary model(s), what do you use them for?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a 7900 XTX 24GB, so all of my models are &amp;lt;32B. But here are mine;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Mistral Small 3.2: A &amp;quot;better&amp;quot; version of Gemma 3, in a way. I really liked Gemma 3, but it hallucinated far too much on basic facts. Mistral doesn&amp;#39;t on the other hand, it hallucinates far less ime. I&amp;#39;m mainly using it for general knowledge and image analysis and consistently does a better job at both than Gemma for me. Feels a bit cold or sterile compared to Gemma 3 though.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Qwen 3 30B-A3B-Thinking-2507: The &amp;quot;Gemini 2.5&amp;quot; at home model. I&amp;#39;ve compared it pretty extensively to 2.5 Flash Reasoning, and 2.5 Pro, and it&amp;#39;s able to consistently beat Flash and more often than not come close to or match 2.5 Pro. I&amp;#39;m mainly using this model for complex queries, problem solving, and writing. It&amp;#39;s a damn good writing model imo, but that&amp;#39;s not a &lt;em&gt;major&lt;/em&gt; use-case for me.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Qwen 3-Coder 30B-A3B-Instruct-2507: This model acts a lot like a mix of Gemini, Claude, and an openAI model to me in my eyes. It&amp;#39;s a really, really capable coder. I&amp;#39;m a software engineer and it&amp;#39;s a nice companion in that regard. A lot of people say it&amp;#39;s like most like Claude, and from what I&amp;#39;ve seen from Claude outputs, I tend to agree. although I&amp;#39;ve never used Claude, admittedly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So there we have it, those are the models I use and the use-case for each. I do occasionally use OpenRouter to serve GLM 4.5-Air and Kimi K2, but that&amp;#39;s mostly just out of curiosity. So what&amp;#39;s everybody else here running?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mhp2e5",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ayylmaonade",
            "discussion_type": null,
            "num_comments": 29,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754339961,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xtrcx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Patentsmatter",
            "can_mod_post": false,
            "created_utc": 1754341006,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 4,
            "author_fullname": "t2_cocl8roo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen 3 30B-A3B-Thinking-2507: This is my main model for text analysis. It is fast, and good prompts can take you far. The output is a bit heavy on markup, and it is tight lipped. Gemma 3 produces nicer text, but it introduces subtle errors and is not as capable understanding Non-English languages. Also, it is much slower (easily by factor 10).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xtrcx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen 3 30B-A3B-Thinking-2507: This is my main model for text analysis. It is fast, and good prompts can take you far. The output is a bit heavy on markup, and it is tight lipped. Gemma 3 produces nicer text, but it introduces subtle errors and is not as capable understanding Non-English languages. Also, it is much slower (easily by factor 10).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6xtrcx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754341006,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xtsse",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "figdish",
            "can_mod_post": false,
            "created_utc": 1754341018,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 4,
            "author_fullname": "t2_71h9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I’m running mostly Gemma3:latest. Had a project that i needed it to summarize articles and overall it was the best I had worked with - I tried deepseek-r1:latest as well as qwen3,:30b and gemma was seemingly the best at outputting what I wanted in the format i was requesting.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xtsse",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m running mostly Gemma3:latest. Had a project that i needed it to summarize articles and overall it was the best I had worked with - I tried deepseek-r1:latest as well as qwen3,:30b and gemma was seemingly the best at outputting what I wanted in the format i was requesting.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6xtsse/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754341018,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6yfq1v",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DeProgrammer99",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y19id",
                                "score": 1,
                                "author_fullname": "t2_w4j8t",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'd say 64 GB for Q3\\_K\\_L based on [https://www.reddit.com/r/LocalLLaMA/comments/1mhlkyx/comment/n6x36pn/](https://www.reddit.com/r/LocalLLaMA/comments/1mhlkyx/comment/n6x36pn/) . I just looked at its config.json, and it should be 184 KB/token of KV cache, so you *might* be able to fit 32k context alongside it with 64 GB of RAM and no KV cache quantization.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6yfq1v",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d say 64 GB for Q3_K_L based on &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mhlkyx/comment/n6x36pn/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mhlkyx/comment/n6x36pn/&lt;/a&gt; . I just looked at its config.json, and it should be 184 KB/token of KV cache, so you &lt;em&gt;might&lt;/em&gt; be able to fit 32k context alongside it with 64 GB of RAM and no KV cache quantization.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhp2e5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yfq1v/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754347982,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754347982,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y19id",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "NoobMLDude",
                      "can_mod_post": false,
                      "created_utc": 1754343268,
                      "send_replies": true,
                      "parent_id": "t1_n6xurjg",
                      "score": 1,
                      "author_fullname": "t2_t0syffr8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "How much VRAM is required for GLM 4.5 Air",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y19id",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How much VRAM is required for GLM 4.5 Air&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhp2e5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y19id/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754343268,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xurjg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1754341302,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 3,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Devstral with vision, gemma 3 27b or qwen 3 8b depending on my needs and how much VRAM I want to use. Occasionally I use an API model, like deepseek or GLM-4.5. When I have the hardware I will probably run GLM-4.5-Air or similar locally.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xurjg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Devstral with vision, gemma 3 27b or qwen 3 8b depending on my needs and how much VRAM I want to use. Occasionally I use an API model, like deepseek or GLM-4.5. When I have the hardware I will probably run GLM-4.5-Air or similar locally.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6xurjg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754341302,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y75yn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "chisleu",
            "can_mod_post": false,
            "created_utc": 1754345132,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 3,
            "author_fullname": "t2_cbxyn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "primary models are gemini 2.5 pro and anthropic claude sonnet 4.0\n\nSecondary models are models that can run Cline:  \n\\* Qwen 3 Coder 30b a3b  \n\\* GLM 4.5 Air  \n\\* devstral-small\n\nWhy do I have secondary models? Because good models are expensive. Sometimes, good enough is good enough.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y75yn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;primary models are gemini 2.5 pro and anthropic claude sonnet 4.0&lt;/p&gt;\n\n&lt;p&gt;Secondary models are models that can run Cline:&lt;br/&gt;\n* Qwen 3 Coder 30b a3b&lt;br/&gt;\n* GLM 4.5 Air&lt;br/&gt;\n* devstral-small&lt;/p&gt;\n\n&lt;p&gt;Why do I have secondary models? Because good models are expensive. Sometimes, good enough is good enough.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y75yn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754345132,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y2elz",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ayylmaonade",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y0o6c",
                                "score": 2,
                                "author_fullname": "t2_oqajf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Unsloth basically bolted the vision encoder from Mistral Small onto Devstral - https://huggingface.co/unsloth/Devstral-Small-2507-GGUF\n\nI'm not sure if they worked with Mistral directly, but it's a good option. They've got a multimodal Magistral too.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y2elz",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unsloth basically bolted the vision encoder from Mistral Small onto Devstral - &lt;a href=\"https://huggingface.co/unsloth/Devstral-Small-2507-GGUF\"&gt;https://huggingface.co/unsloth/Devstral-Small-2507-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if they worked with Mistral directly, but it&amp;#39;s a good option. They&amp;#39;ve got a multimodal Magistral too.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhp2e5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y2elz/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754343624,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754343624,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y9il9",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kmouratidis",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y0o6c",
                                "score": 1,
                                "author_fullname": "t2_k6u7rfxb",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You can copy the weights of Devstral / Magistral on top of the `language_model` part of Mistral:\n* https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision\n* https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision\n\nAnd it doesn't significantly affect quality.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y9il9",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can copy the weights of Devstral / Magistral on top of the &lt;code&gt;language_model&lt;/code&gt; part of Mistral:\n* &lt;a href=\"https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision\"&gt;https://huggingface.co/kmouratidis/Devstral-Small-2507-Rebased-Vision&lt;/a&gt;\n* &lt;a href=\"https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision\"&gt;https://huggingface.co/kmouratidis/Magistral-Small-2507-Rebased-Vision&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And it doesn&amp;#39;t significantly affect quality.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhp2e5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y9il9/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754345899,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754345899,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y0o6c",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "NoobMLDude",
                      "can_mod_post": false,
                      "created_utc": 1754343085,
                      "send_replies": true,
                      "parent_id": "t1_n6xt5kb",
                      "score": 1,
                      "author_fullname": "t2_t0syffr8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What do you mean “merging vision into Devstral”.\nI’m curious to understand how you use vision with Devstral. \nAlso doesn’t Devstral run on a Mac M1 ?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y0o6c",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What do you mean “merging vision into Devstral”.\nI’m curious to understand how you use vision with Devstral. \nAlso doesn’t Devstral run on a Mac M1 ?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhp2e5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y0o6c/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754343085,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xt5kb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kmouratidis",
            "can_mod_post": false,
            "created_utc": 1754340832,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 2,
            "author_fullname": "t2_k6u7rfxb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Devstral only. I used to have both it and Mistral, but since they're based on the same model, after merging vision into Devstral, I no longer need to swap, Devstral is good enough for general Q&amp;A too.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xt5kb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Devstral only. I used to have both it and Mistral, but since they&amp;#39;re based on the same model, after merging vision into Devstral, I no longer need to swap, Devstral is good enough for general Q&amp;amp;A too.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6xt5kb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754340832,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y14eu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jeffwadsworth",
            "can_mod_post": false,
            "created_utc": 1754343226,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 2,
            "author_fullname": "t2_11m4x2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Primary: GLM 4.5, which will soon be usable locally with llama.cpp using CPU, etc.  Its coding is phenomenal and inference is fast.  Secondary would be DS R1 0528 for analysis and writing.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y14eu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Primary: GLM 4.5, which will soon be usable locally with llama.cpp using CPU, etc.  Its coding is phenomenal and inference is fast.  Secondary would be DS R1 0528 for analysis and writing.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y14eu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754343226,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yaxik",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Spirited_Example_341",
            "can_mod_post": false,
            "created_utc": 1754346374,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 2,
            "author_fullname": "t2_122x8ksifg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "i like llama 3 stheno 3.2 8b 4-M for 1080 gtx ti :-) its pretty decent in my view for rp /chat",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yaxik",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i like llama 3 stheno 3.2 8b 4-M for 1080 gtx ti :-) its pretty decent in my view for rp /chat&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yaxik/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754346374,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y83o6",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "-dysangel-",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y3mdq",
                                "score": 1,
                                "author_fullname": "t2_12ggykute6",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'm sure the qwen model is smart and a good all rounder. It was decent at agentic tasks when I tried it, but it's for sure not as good at coding as GLM Air",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y83o6",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure the qwen model is smart and a good all rounder. It was decent at agentic tasks when I tried it, but it&amp;#39;s for sure not as good at coding as GLM Air&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhp2e5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y83o6/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754345434,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754345434,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6yflb3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ortegaalfredo",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y3mdq",
                                "score": 1,
                                "author_fullname": "t2_g177e",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I did some coding tests and couldn't tell the difference in quality between Air and Qwen3-235B-thinking. Perhaps I need more complex tests.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6yflb3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Alpaca"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I did some coding tests and couldn&amp;#39;t tell the difference in quality between Air and Qwen3-235B-thinking. Perhaps I need more complex tests.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhp2e5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yflb3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754347938,
                                "author_flair_text": "Alpaca",
                                "treatment_tags": [],
                                "created_utc": 1754347938,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bd9e9e",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y3mdq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ayylmaonade",
                      "can_mod_post": false,
                      "created_utc": 1754344012,
                      "send_replies": true,
                      "parent_id": "t1_n6xz5c9",
                      "score": 1,
                      "author_fullname": "t2_oqajf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "How do you find it compared to the new Thinking-2507 Qwen3 models? I've only used GLM 4.5-Air sparingly so far since I prefer to run stuff 100% locally and unfortunately don't have the hardware for GLM. But I've found 4.5 to be a really good coder, and have pretty good knowledge. Although I've also been *really* impressed with the new reasoning style of Qwen3 - is it noticeably different or stronger in any domains?",
                      "edited": 1754345046,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y3mdq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do you find it compared to the new Thinking-2507 Qwen3 models? I&amp;#39;ve only used GLM 4.5-Air sparingly so far since I prefer to run stuff 100% locally and unfortunately don&amp;#39;t have the hardware for GLM. But I&amp;#39;ve found 4.5 to be a really good coder, and have pretty good knowledge. Although I&amp;#39;ve also been &lt;em&gt;really&lt;/em&gt; impressed with the new reasoning style of Qwen3 - is it noticeably different or stronger in any domains?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhp2e5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y3mdq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754344012,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xz5c9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1754342620,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 2,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GLM 4.5 Air, because it's almost as smart as the big boys, but also fast enough to load large contexts on my machine, so I can finally run non-trivial local agentic tasks",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xz5c9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GLM 4.5 Air, because it&amp;#39;s almost as smart as the big boys, but also fast enough to load large contexts on my machine, so I can finally run non-trivial local agentic tasks&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6xz5c9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754342620,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yekha",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Jazzlike_Source_5983",
            "can_mod_post": false,
            "created_utc": 1754347594,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 1,
            "author_fullname": "t2_ap0ra8pe",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Locally, I primarily alternate between Command R7B and Command A on a Mac M4 Max with 128GB. Command A is a slayer, and the licensing absolutely kills me because I can't build with it. There are two other local LMs I love: Loki v4.3 8B 128K and Tesslate Synthia S1 27B (an absolutely killer Gemma 3 fine-tune). I'm a fan of the whole Gemma 3 line, and 3n 2B is shockingly rad. Haven't really bonded deeply with any other local models, but I've tried them all.\n\nI do most of my work on the cloud, and it's Sonnet 4, 2.5 Pro Deep Research, with DS R1 as a devil's advocate/harsh critic. Kimi K2 for some random inspiration sometimes. Grok 4 works for purely clerical purposes, ie. making faithful merges of a ton of files. As much as I despise Grok and xAI, for word processing (ie. taking the best elements of 4 different drafts, tweaking them to make the integrations flow correctly, and turning it into a document that uses my original writing without trying to rewrite it), Grok 4 is kind of the only model I trust to get it right. I use o3 for research when Gemini is acting bizarre which is way too often.\n\n(That said, I'm hoping to commission a serious fine tune within the next few months, and I think the results could be insanely cool - so I'm hoping to go all in on this and have one local model I use for just about everything)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yekha",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Locally, I primarily alternate between Command R7B and Command A on a Mac M4 Max with 128GB. Command A is a slayer, and the licensing absolutely kills me because I can&amp;#39;t build with it. There are two other local LMs I love: Loki v4.3 8B 128K and Tesslate Synthia S1 27B (an absolutely killer Gemma 3 fine-tune). I&amp;#39;m a fan of the whole Gemma 3 line, and 3n 2B is shockingly rad. Haven&amp;#39;t really bonded deeply with any other local models, but I&amp;#39;ve tried them all.&lt;/p&gt;\n\n&lt;p&gt;I do most of my work on the cloud, and it&amp;#39;s Sonnet 4, 2.5 Pro Deep Research, with DS R1 as a devil&amp;#39;s advocate/harsh critic. Kimi K2 for some random inspiration sometimes. Grok 4 works for purely clerical purposes, ie. making faithful merges of a ton of files. As much as I despise Grok and xAI, for word processing (ie. taking the best elements of 4 different drafts, tweaking them to make the integrations flow correctly, and turning it into a document that uses my original writing without trying to rewrite it), Grok 4 is kind of the only model I trust to get it right. I use o3 for research when Gemini is acting bizarre which is way too often.&lt;/p&gt;\n\n&lt;p&gt;(That said, I&amp;#39;m hoping to commission a serious fine tune within the next few months, and I think the results could be insanely cool - so I&amp;#39;m hoping to go all in on this and have one local model I use for just about everything)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yekha/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754347594,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yfooy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1754347969,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 1,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GLM 4.5-Air because I cannot tell the difference between it and Qwen-235B-Thinking, but GLM its much faster, and I can run it locally using 4x3090. Secondary model is Qwen-235B-Thinking, because its very good but slow.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yfooy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GLM 4.5-Air because I cannot tell the difference between it and Qwen-235B-Thinking, but GLM its much faster, and I can run it locally using 4x3090. Secondary model is Qwen-235B-Thinking, because its very good but slow.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yfooy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754347969,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yftze",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArchdukeofHyperbole",
            "can_mod_post": false,
            "created_utc": 1754348018,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 1,
            "author_fullname": "t2_1p41v97q5d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My primary model would be rwkv7-7.2B-g0 because it fits on my gpu and can do 1M context without generations slowing down. I don't really have a specific secondary model, but also use Gemini 4B, qwen coder 3AB, and a bunch of other ones I dont use so much.\n\nEdit: I meant gemma 4B lol",
            "edited": 1754348235,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yftze",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My primary model would be rwkv7-7.2B-g0 because it fits on my gpu and can do 1M context without generations slowing down. I don&amp;#39;t really have a specific secondary model, but also use Gemini 4B, qwen coder 3AB, and a bunch of other ones I dont use so much.&lt;/p&gt;\n\n&lt;p&gt;Edit: I meant gemma 4B lol&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yftze/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754348018,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yrnvb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cristoper",
            "can_mod_post": false,
            "created_utc": 1754351969,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 1,
            "author_fullname": "t2_38xkk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For daily research / q&amp;a / and help with writing critique and editing I'm still using gemma3-24b (q4_k_m) on my 3090. The qwen3 a3b models are much faster and almost as good (plus have more up-to-date knowledge), but I'm still used to the gemma3 output. Plus I sometimes use its image capability to write captions.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yrnvb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For daily research / q&amp;amp;a / and help with writing critique and editing I&amp;#39;m still using gemma3-24b (q4_k_m) on my 3090. The qwen3 a3b models are much faster and almost as good (plus have more up-to-date knowledge), but I&amp;#39;m still used to the gemma3 output. Plus I sometimes use its image capability to write captions.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yrnvb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754351969,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6z3b4w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "thebadslime",
            "can_mod_post": false,
            "created_utc": 1754356069,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 1,
            "author_fullname": "t2_i5os0v0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have weird repetition errors with Qwen3 models, so I prefer ERNIE 4.5 21BA3B. It runs a little faster than qwen 30BA3B and doesn't bug out nearly as often.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6z3b4w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have weird repetition errors with Qwen3 models, so I prefer ERNIE 4.5 21BA3B. It runs a little faster than qwen 30BA3B and doesn&amp;#39;t bug out nearly as often.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6z3b4w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754356069,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y1him",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "NoobMLDude",
                      "can_mod_post": false,
                      "created_utc": 1754343337,
                      "send_replies": true,
                      "parent_id": "t1_n6xwz97",
                      "score": 3,
                      "author_fullname": "t2_t0syffr8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Wow you have the luxury of using really Large models. I’m jealous",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y1him",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow you have the luxury of using really Large models. I’m jealous&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhp2e5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y1him/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754343337,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xwz97",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "InfiniteTrans69",
            "can_mod_post": false,
            "created_utc": 1754341966,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 1,
            "author_fullname": "t2_1j5dv6mrfz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Kimi K2 &gt; Qwen3-235B-A22B-2507 &gt; GLM-4.5",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xwz97",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 &amp;gt; Qwen3-235B-A22B-2507 &amp;gt; GLM-4.5&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6xwz97/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754341966,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yt229",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sxales",
            "can_mod_post": false,
            "created_utc": 1754352462,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 1,
            "author_fullname": "t2_5h1ye",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a potato server, so . . .\n\nMy default LLM is Phi-4, but I am thinking of switching to Qwen 3 30b a3b 2507. \n\nIf I need to specialize, I swap to GLM-4 0414 for coding, and Llama 3.x for natural language tasks (writing, summarizing, editing). \n\nGemma 3n e4b might be replacing Llama 3.x. Gemma 3 had some issues with hallucinations, but I've seen a marked decrease with e4b.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yt229",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a potato server, so . . .&lt;/p&gt;\n\n&lt;p&gt;My default LLM is Phi-4, but I am thinking of switching to Qwen 3 30b a3b 2507. &lt;/p&gt;\n\n&lt;p&gt;If I need to specialize, I swap to GLM-4 0414 for coding, and Llama 3.x for natural language tasks (writing, summarizing, editing). &lt;/p&gt;\n\n&lt;p&gt;Gemma 3n e4b might be replacing Llama 3.x. Gemma 3 had some issues with hallucinations, but I&amp;#39;ve seen a marked decrease with e4b.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6yt229/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754352462,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ycdie",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Jazzlike_Source_5983",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y7ttc",
                                "score": 3,
                                "author_fullname": "t2_ap0ra8pe",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Big Tiger doesn't have em-dashses (thank you god) and is an absolute nihilist.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ycdie",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Big Tiger doesn&amp;#39;t have em-dashses (thank you god) and is an absolute nihilist.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhp2e5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6ycdie/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754346850,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754346850,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6z291w",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "ttkciar",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6ygojy",
                                          "score": 1,
                                          "author_fullname": "t2_cpegz",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Just noticed someone downvoted without commenting, and looking around, there were a bunch of other good comments by other users which got downvoted to 0 as well.\n\nI upvoted those back up to 1.  Someone's got a bug up their butt, but until they deign to grace us with a comment explaining their position, we will never know why.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6z291w",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just noticed someone downvoted without commenting, and looking around, there were a bunch of other good comments by other users which got downvoted to 0 as well.&lt;/p&gt;\n\n&lt;p&gt;I upvoted those back up to 1.  Someone&amp;#39;s got a bug up their butt, but until they deign to grace us with a comment explaining their position, we will never know why.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhp2e5",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6z291w/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754355694,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1754355694,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ygojy",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ttkciar",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y7ttc",
                                "score": 0,
                                "author_fullname": "t2_cpegz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For creative writing, Big-Tiger-Gemma-27B-v3 is much more brutal, which is exactly what I need for my science-fiction writing side-project.  It is also very blunt about critiquing the user's prompt and calling them on any bullshit; it is an anti-sycophant.\n\nStock Gemma3 will try very hard to make \"nice\" content, even when given the description of a sci-fi combat scene which isn't nice at all.  Big-Tiger-Gemma-27B-v3 inferred combat scenes which actually made me physically wince.  I love it.\n\nIt is also more useful than Gemma3 for my persuasion research, in ways I would rather not describe, lest Google's legal team notice and decide TheDrummer is in violation of the (quite draconian) Gemma terms of service.\n\nThe Gemma license https://ai.google.dev/gemma/terms expressly prohibits derivative works which might be used to violate the Gemma \"prohibited use\" agreement https://ai.google.dev/gemma/prohibited_use_policy which is ridiculously broad.\n\nSo, yeah, I'm going to be vague about Big Tiger beyond what I've already said.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ygojy",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For creative writing, Big-Tiger-Gemma-27B-v3 is much more brutal, which is exactly what I need for my science-fiction writing side-project.  It is also very blunt about critiquing the user&amp;#39;s prompt and calling them on any bullshit; it is an anti-sycophant.&lt;/p&gt;\n\n&lt;p&gt;Stock Gemma3 will try very hard to make &amp;quot;nice&amp;quot; content, even when given the description of a sci-fi combat scene which isn&amp;#39;t nice at all.  Big-Tiger-Gemma-27B-v3 inferred combat scenes which actually made me physically wince.  I love it.&lt;/p&gt;\n\n&lt;p&gt;It is also more useful than Gemma3 for my persuasion research, in ways I would rather not describe, lest Google&amp;#39;s legal team notice and decide TheDrummer is in violation of the (quite draconian) Gemma terms of service.&lt;/p&gt;\n\n&lt;p&gt;The Gemma license &lt;a href=\"https://ai.google.dev/gemma/terms\"&gt;https://ai.google.dev/gemma/terms&lt;/a&gt; expressly prohibits derivative works which might be used to violate the Gemma &amp;quot;prohibited use&amp;quot; agreement &lt;a href=\"https://ai.google.dev/gemma/prohibited_use_policy\"&gt;https://ai.google.dev/gemma/prohibited_use_policy&lt;/a&gt; which is ridiculously broad.&lt;/p&gt;\n\n&lt;p&gt;So, yeah, I&amp;#39;m going to be vague about Big Tiger beyond what I&amp;#39;ve already said.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhp2e5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6ygojy/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754348297,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754348297,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y7ttc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "misterflyer",
                      "can_mod_post": false,
                      "created_utc": 1754345345,
                      "send_replies": true,
                      "parent_id": "t1_n6xvuy8",
                      "score": 1,
                      "author_fullname": "t2_maq0iwk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hey what are the biggest differences between Gemma3 vs BigTigerGemma?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y7ttc",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey what are the biggest differences between Gemma3 vs BigTigerGemma?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhp2e5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6y7ttc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754345345,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xvuy8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754341629,
            "send_replies": true,
            "parent_id": "t3_1mhp2e5",
            "score": 0,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It depends on what I'm doing.  When I can find time to do the R&amp;D I enjoy, my primary model is Phi-4-25B, with Tulu3-70B as an escalation (when Phi-4-25B is too stupid to answer well).  Phi-4-25B is also my go-to for Evol-Instruct, since it's *almost* as good at it as Gemma3-27B and has a much more permissive license.\n\nFor creative writing, RAG, and figuring out what my coworkers' code means, my go-to is Gemma3-27B (or increasingly Big-Tiger-Gemma-27B-v3).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xvuy8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends on what I&amp;#39;m doing.  When I can find time to do the R&amp;amp;D I enjoy, my primary model is Phi-4-25B, with Tulu3-70B as an escalation (when Phi-4-25B is too stupid to answer well).  Phi-4-25B is also my go-to for Evol-Instruct, since it&amp;#39;s &lt;em&gt;almost&lt;/em&gt; as good at it as Gemma3-27B and has a much more permissive license.&lt;/p&gt;\n\n&lt;p&gt;For creative writing, RAG, and figuring out what my coworkers&amp;#39; code means, my go-to is Gemma3-27B (or increasingly Big-Tiger-Gemma-27B-v3).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhp2e5/whats_your_primary_model_and_why_do_you_run_a/n6xvuy8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754341629,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhp2e5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]