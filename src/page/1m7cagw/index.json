[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "What I mean is instead of paying for claude code or junie, is it possible to buy hardware capable of running an equivalent model?\n\nClaude code is $20 per month\nJunie is cheaper at $18 per month for the best\n\nI know just renting is likely cheaper in the long term but this assumes no price increases, and locks you into whatever restrictions they have.\n\nIf i go to huggingface what hardware would i need to run their best model comfortably and with some future proofing?\n\nBudget is maybe up to £10k (any more feels unjustifiable vs renting).",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What is the best hardware for running the biggest models?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m7cagw",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_4fxgm",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753284449,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What I mean is instead of paying for claude code or junie, is it possible to buy hardware capable of running an equivalent model?&lt;/p&gt;\n\n&lt;p&gt;Claude code is $20 per month\nJunie is cheaper at $18 per month for the best&lt;/p&gt;\n\n&lt;p&gt;I know just renting is likely cheaper in the long term but this assumes no price increases, and locks you into whatever restrictions they have.&lt;/p&gt;\n\n&lt;p&gt;If i go to huggingface what hardware would i need to run their best model comfortably and with some future proofing?&lt;/p&gt;\n\n&lt;p&gt;Budget is maybe up to £10k (any more feels unjustifiable vs renting).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m7cagw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "sanitykey",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/",
            "subreddit_subscribers": 503757,
            "created_utc": 1753284449,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "body": "Sadly £10k won’t get you a server to run the big models at high speed. People are gonna recommend the Mac 512GB, but it’s too slow and doesn’t have enough RAM to run the big models with decent context.\n\nFor real performance you need big iron and real GPUs. Don’t even think about going DDR4, it’s just going to end in &lt; 1 token/second. Big iron is DDR5, PCIe 5.0, 12-channel CPUs, etc.\n\nThese costs are ballparked in dollars, but you get the point.\n\n- Motherboard (PCIe 5.0, DDR5, EPYC): $900\n- CPU epyc 12-channel: $2000\n- RAM: 1TB DDR5: $4500\n- Fast SSD: $500\n- PSU: $600\n\nThat’s about $8500 before you’ve bought GPUs, a case, etc etc. \n\nI use a similar system with a total of 192GB of VRAM from quad RTX A6000s. Between CPU/GPU with llama.cpp, Kimi Q4 runs at 20 tokens/second… but that’s with 192GB VRAM at a cost of base server + $20k.\n\nWithout any GPUs your rig is gonna run at what… 5-7 tokens/sec if you’re lucky?\n\nBuilding a rig to do fast inference of decent quants of big models is a $30k+ proposition with today’s hardware.",
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4qg3ls",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1753286513,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": 9,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "author_cakeday": true,
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4qg3ls",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sadly £10k won’t get you a server to run the big models at high speed. People are gonna recommend the Mac 512GB, but it’s too slow and doesn’t have enough RAM to run the big models with decent context.&lt;/p&gt;\n\n&lt;p&gt;For real performance you need big iron and real GPUs. Don’t even think about going DDR4, it’s just going to end in &amp;lt; 1 token/second. Big iron is DDR5, PCIe 5.0, 12-channel CPUs, etc.&lt;/p&gt;\n\n&lt;p&gt;These costs are ballparked in dollars, but you get the point.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Motherboard (PCIe 5.0, DDR5, EPYC): $900&lt;/li&gt;\n&lt;li&gt;CPU epyc 12-channel: $2000&lt;/li&gt;\n&lt;li&gt;RAM: 1TB DDR5: $4500&lt;/li&gt;\n&lt;li&gt;Fast SSD: $500&lt;/li&gt;\n&lt;li&gt;PSU: $600&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That’s about $8500 before you’ve bought GPUs, a case, etc etc. &lt;/p&gt;\n\n&lt;p&gt;I use a similar system with a total of 192GB of VRAM from quad RTX A6000s. Between CPU/GPU with llama.cpp, Kimi Q4 runs at 20 tokens/second… but that’s with 192GB VRAM at a cost of base server + $20k.&lt;/p&gt;\n\n&lt;p&gt;Without any GPUs your rig is gonna run at what… 5-7 tokens/sec if you’re lucky?&lt;/p&gt;\n\n&lt;p&gt;Building a rig to do fast inference of decent quants of big models is a $30k+ proposition with today’s hardware.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4qg3ls/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753286513,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4qaf8w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Turkino",
            "can_mod_post": false,
            "created_utc": 1753284955,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": 6,
            "author_fullname": "t2_ai06o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A couple of days ago people were chatting about building 8x A100 40gb systems for about $30,000.  \nYou'd also need to get more parts for them to run like an HGX backboard that sell for $9,000 used.\n\nSo your up to \\~$40,000 before getting to the rest of things.\n\nTo try to go cheaper, you could just buy up an old bitcoin mining setup and get as many 3090's as you can fit in. Likely won't be able to run the absolute largest models but you could easily get to the point of running 128b models if you get 4x+ cards, with some offloading depending on what your configuration ends up at.",
            "edited": 1753286347,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4qaf8w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A couple of days ago people were chatting about building 8x A100 40gb systems for about $30,000.&lt;br/&gt;\nYou&amp;#39;d also need to get more parts for them to run like an HGX backboard that sell for $9,000 used.&lt;/p&gt;\n\n&lt;p&gt;So your up to ~$40,000 before getting to the rest of things.&lt;/p&gt;\n\n&lt;p&gt;To try to go cheaper, you could just buy up an old bitcoin mining setup and get as many 3090&amp;#39;s as you can fit in. Likely won&amp;#39;t be able to run the absolute largest models but you could easily get to the point of running 128b models if you get 4x+ cards, with some offloading depending on what your configuration ends up at.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4qaf8w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753284955,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4qe14x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1753285944,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": 2,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; What is the best hardware for running the biggest models?\n\nhttps://www.exxactcorp.com/NVIDIA-DGXB-G1440-P2EDI36-E8103241\n\n&gt; Budget is maybe up to £10k\n\nthe choice of models will be very limited with such small budget.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4qe14x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;What is the best hardware for running the biggest models?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.exxactcorp.com/NVIDIA-DGXB-G1440-P2EDI36-E8103241\"&gt;https://www.exxactcorp.com/NVIDIA-DGXB-G1440-P2EDI36-E8103241&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Budget is maybe up to £10k&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;the choice of models will be very limited with such small budget.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4qe14x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753285944,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4qa1m8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "taylorwilsdon",
            "can_mod_post": false,
            "created_utc": 1753284850,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": 1,
            "author_fullname": "t2_32a7z",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Up until recently very few models qualified as “equivalent” but those in the running are large MoE like Kimi k2, deepseek and the new Qwen3-Coder-480B-A35B-Instruct.\n\nBecause they’re mixture of experts, they can run on different hardware than dense models which are basically GPU or bust. Your paved paths are basically buy a beefy Mac Studio and deal with slower prompt processing, build a giant GPU rig for a zillion dollars or run an Epyc build that uses fast multichannel RAM to do the needful.\n\n10k GBP is definitely enough to get you there with money left over, but how to spend that most wisely depends on your environment and use case. Are you a single person or a group of people? One call at a time or heavy concurrency?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4qa1m8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Up until recently very few models qualified as “equivalent” but those in the running are large MoE like Kimi k2, deepseek and the new Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt;\n\n&lt;p&gt;Because they’re mixture of experts, they can run on different hardware than dense models which are basically GPU or bust. Your paved paths are basically buy a beefy Mac Studio and deal with slower prompt processing, build a giant GPU rig for a zillion dollars or run an Epyc build that uses fast multichannel RAM to do the needful.&lt;/p&gt;\n\n&lt;p&gt;10k GBP is definitely enough to get you there with money left over, but how to spend that most wisely depends on your environment and use case. Are you a single person or a group of people? One call at a time or heavy concurrency?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4qa1m8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753284850,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4ql4gm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Prestigious_Thing797",
            "can_mod_post": false,
            "created_utc": 1753287919,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": 1,
            "author_fullname": "t2_1anh6qztwr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm running 2bit qwen 235B 2507 on a single RTX 6000 pro w/ 128k context  using q4 kvcache. \\~60 tokens/s generation and \\~850 tokens/s prompt processing with llama.cpp entirely on gpu.\n\nIt's expensive, and I don't have a quantitative read on the quality degradation for 2 bit, but anecdotally it's great.\n\nThere's cheaper franken systems you can setup with Mi50 or other cards (Recently I was looking at A16's which are essentially 4 16GB cards stapled together in a single card) but they all will have tradeoffs (FP8/FP4 support, memory bandwidth, etc).\n\nI'd really only recommend this setup if you care about training models or tinkering with other GPU stuff. Really most folks are better off consuming APIs from a cost perspective.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4ql4gm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running 2bit qwen 235B 2507 on a single RTX 6000 pro w/ 128k context  using q4 kvcache. ~60 tokens/s generation and ~850 tokens/s prompt processing with llama.cpp entirely on gpu.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s expensive, and I don&amp;#39;t have a quantitative read on the quality degradation for 2 bit, but anecdotally it&amp;#39;s great.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s cheaper franken systems you can setup with Mi50 or other cards (Recently I was looking at A16&amp;#39;s which are essentially 4 16GB cards stapled together in a single card) but they all will have tradeoffs (FP8/FP4 support, memory bandwidth, etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really only recommend this setup if you care about training models or tinkering with other GPU stuff. Really most folks are better off consuming APIs from a cost perspective.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4ql4gm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753287919,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4qnx24",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTrack_ai",
            "can_mod_post": false,
            "created_utc": 1753288704,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": 1,
            "author_fullname": "t2_1tpuoj72sa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "the food chain goes like this: rtx pro 6000, GH200 624GB, DGX Station, Mi325X 2TB, HGX B200 1.5TB, GB200 NVL36, GB200 NVL72.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4qnx24",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the food chain goes like this: rtx pro 6000, GH200 624GB, DGX Station, Mi325X 2TB, HGX B200 1.5TB, GB200 NVL36, GB200 NVL72.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4qnx24/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753288704,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4qbdpw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "GingerTapirs",
            "can_mod_post": false,
            "created_utc": 1753285219,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": 5,
            "author_fullname": "t2_10mb6w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It will almost never be cheaper to go the DIY route. £10,000 at today's exchange rate is $13,567. That equates to 678.35 months of subscription, or about 56 years and 6 months. 56 years ago was around the time when humans first landed on the moon. Today's computers are about 1.9 Trillion times faster than the computers available then (Apollo computer vs an RTX 4090). If you start with $13,567 invested at an annual return of 6% compounded yearly, and use it to pay for a subscription that initially costs $20 per month ($240 per year) but increases by 3% annually due to inflation, your investment would still grow significantly over 56 years. After covering the steadily rising subscription costs throughout, your remaining balance at the end of the 56 years would be approximately **$147,000**, thanks to the compounding effect outpacing the inflation-adjusted withdrawals.\n\nThat said, if you want to go ham on hardware because you can, the RTX 3090s currently are the best bang for buck. Go with a used server motherboard for the extra slots and lanes and load em up.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4qbdpw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It will almost never be cheaper to go the DIY route. £10,000 at today&amp;#39;s exchange rate is $13,567. That equates to 678.35 months of subscription, or about 56 years and 6 months. 56 years ago was around the time when humans first landed on the moon. Today&amp;#39;s computers are about 1.9 Trillion times faster than the computers available then (Apollo computer vs an RTX 4090). If you start with $13,567 invested at an annual return of 6% compounded yearly, and use it to pay for a subscription that initially costs $20 per month ($240 per year) but increases by 3% annually due to inflation, your investment would still grow significantly over 56 years. After covering the steadily rising subscription costs throughout, your remaining balance at the end of the 56 years would be approximately &lt;strong&gt;$147,000&lt;/strong&gt;, thanks to the compounding effect outpacing the inflation-adjusted withdrawals.&lt;/p&gt;\n\n&lt;p&gt;That said, if you want to go ham on hardware because you can, the RTX 3090s currently are the best bang for buck. Go with a used server motherboard for the extra slots and lanes and load em up.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4qbdpw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753285219,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4qceq1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753285499,
            "send_replies": true,
            "parent_id": "t3_1m7cagw",
            "score": -1,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "£10k can get you a Mac Studio with 512GB of RAM.\n\nIt can also get you a lot of Claude Code Max months - and I assume the price of quality inference will only continue to decrease in the coming years.\n\nI currently have both options, though I'm expecting over time that local inference on the Mac will become enough for me. It's already more than enough for replacing chat bots, but not for impressive results in large context agentic workflows yet (though I suspect the new Qwen 3 Coder variants may change that)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4qceq1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;£10k can get you a Mac Studio with 512GB of RAM.&lt;/p&gt;\n\n&lt;p&gt;It can also get you a lot of Claude Code Max months - and I assume the price of quality inference will only continue to decrease in the coming years.&lt;/p&gt;\n\n&lt;p&gt;I currently have both options, though I&amp;#39;m expecting over time that local inference on the Mac will become enough for me. It&amp;#39;s already more than enough for replacing chat bots, but not for impressive results in large context agentic workflows yet (though I suspect the new Qwen 3 Coder variants may change that)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7cagw/what_is_the_best_hardware_for_running_the_biggest/n4qceq1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753285499,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m7cagw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        }
      ],
      "before": null
    }
  }
]