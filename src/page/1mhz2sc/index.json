[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi everyone, I'm a student working on a project involving fine-tuning the **Gemma 3 4B Vision** model using **Unsloth** on a local WSL setup with a single NVIDIA RTX 4090. I'm running into a major issue where the save\\_pretrained\\_gguf function is taking **over 480 minutes** with no output, and I could really use some help troubleshooting this for my project deadline!  \n\n\n# Setup Details\n\n* **Environment**: Local WSL (tried on two different WSL machines)\n* **GPU**: Single NVIDIA RTX 4090 (confirmed with nvidia-smi)\n* **Model**: Gemma 3 4B Vision (using the vision notebook from Unsloth)\n* **Unsloth Version**: Latest (updated via pip install --upgrade unsloth unsloth\\_zoo)\n* **Other Versions**: Latest TRL, Transformers, and PyTorch\n* **Trainer**: SFTTrainer\n* **Code Snippet**:\n\n&amp;#8203;\n\n    model.save_pretrained_gguf(\"-eye-v1\", quantization_method=\"q4_k_m\")\n\nhttps://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;format=png&amp;auto=webp&amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f\n\nThe save\\_pretrained\\_gguf function for converting the fine-tuned model to GGUF format (with q4\\_k\\_m quantization) has been running for **over 8 hours** without completing or producing any output. I’ve tested this on two separate WSL machines, and the issue persists. No error messages are shown, but the process just hangs indefinitely.  \nthanks in advance !",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Student Unsloth Help] Save to GGUF Taking Forever with Gemma 3 4B Vision + Unsloth on WSL (Single 4090)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 46,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "1ptvi5n1h4hf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 36,
                    "x": 108,
                    "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bfb9ea527858591eb0b623579d8e9bfcd4d8ebc"
                  },
                  {
                    "y": 72,
                    "x": 216,
                    "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4bf37b3e767578e0622e6b784a6bb7dcfe9fbc97"
                  },
                  {
                    "y": 107,
                    "x": 320,
                    "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=78340c00de7b7b152f228d82d0f1791e0e73e27d"
                  },
                  {
                    "y": 214,
                    "x": 640,
                    "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7a246b9e5f76abbc705a645cccde20e016264aa"
                  },
                  {
                    "y": 321,
                    "x": 960,
                    "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bcc3554f93101caa3f2c40d43e1933f1d755b27"
                  },
                  {
                    "y": 361,
                    "x": 1080,
                    "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3354fd3b968faaca5e6cc36da6fa2f1ed0a75ba"
                  }
                ],
                "s": {
                  "y": 818,
                  "x": 2443,
                  "u": "https://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;format=png&amp;auto=webp&amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f"
                },
                "id": "1ptvi5n1h4hf1"
              }
            },
            "name": "t3_1mhz2sc",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_e4ojre534",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/7PXhEUB2aMKe-cmqZEGIO5saO8iivH8buydeI3lcRXk.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754366222,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m a student working on a project involving fine-tuning the &lt;strong&gt;Gemma 3 4B Vision&lt;/strong&gt; model using &lt;strong&gt;Unsloth&lt;/strong&gt; on a local WSL setup with a single NVIDIA RTX 4090. I&amp;#39;m running into a major issue where the save_pretrained_gguf function is taking &lt;strong&gt;over 480 minutes&lt;/strong&gt; with no output, and I could really use some help troubleshooting this for my project deadline!  &lt;/p&gt;\n\n&lt;h1&gt;Setup Details&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: Local WSL (tried on two different WSL machines)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Single NVIDIA RTX 4090 (confirmed with nvidia-smi)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model&lt;/strong&gt;: Gemma 3 4B Vision (using the vision notebook from Unsloth)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unsloth Version&lt;/strong&gt;: Latest (updated via pip install --upgrade unsloth unsloth_zoo)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Other Versions&lt;/strong&gt;: Latest TRL, Transformers, and PyTorch&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Trainer&lt;/strong&gt;: SFTTrainer&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code Snippet&lt;/strong&gt;:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;model.save_pretrained_gguf(&amp;quot;-eye-v1&amp;quot;, quantization_method=&amp;quot;q4_k_m&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f\"&gt;https://preview.redd.it/1ptvi5n1h4hf1.png?width=2443&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bebe14a8f0bcfa11c82c21459e17be76fcc5f1f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The save_pretrained_gguf function for converting the fine-tuned model to GGUF format (with q4_k_m quantization) has been running for &lt;strong&gt;over 8 hours&lt;/strong&gt; without completing or producing any output. I’ve tested this on two separate WSL machines, and the issue persists. No error messages are shown, but the process just hangs indefinitely.&lt;br/&gt;\nthanks in advance !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhz2sc",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "LeastExperience1579",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhz2sc/student_unsloth_help_save_to_gguf_taking_forever/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhz2sc/student_unsloth_help_save_to_gguf_taking_forever/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754366222,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n700963",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1754368905,
            "send_replies": true,
            "parent_id": "t3_1mhz2sc",
            "score": 1,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You have a 4090, you might want to consider doing FP8 QAT and then running inference in TensorRT-LLM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n700963",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You have a 4090, you might want to consider doing FP8 QAT and then running inference in TensorRT-LLM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhz2sc/student_unsloth_help_save_to_gguf_taking_forever/n700963/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754368905,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhz2sc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n705c5w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "duyntnet",
            "can_mod_post": false,
            "created_utc": 1754371425,
            "send_replies": true,
            "parent_id": "t3_1mhz2sc",
            "score": 1,
            "author_fullname": "t2_4d4pk3c4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just ask an LLM to write a script to merge Lora with the model (unloth can also do this). Then, use llama.cpp directly to quantize the model, which is what I did.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n705c5w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just ask an LLM to write a script to merge Lora with the model (unloth can also do this). Then, use llama.cpp directly to quantize the model, which is what I did.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhz2sc/student_unsloth_help_save_to_gguf_taking_forever/n705c5w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754371425,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhz2sc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70wf4r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Disposable110",
            "can_mod_post": false,
            "created_utc": 1754386699,
            "send_replies": true,
            "parent_id": "t3_1mhz2sc",
            "score": 1,
            "author_fullname": "t2_b8logo4s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Been going through this pain for the past week and what finally worked for me was Oogabooga for producing a LoRA from the base SafeTensors model, then merge the LoRA into the base model with Mergekit, and then quantizing the model into a GGUF with Llama.cpp, but I'm not sure how applicable that is to vision models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70wf4r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Been going through this pain for the past week and what finally worked for me was Oogabooga for producing a LoRA from the base SafeTensors model, then merge the LoRA into the base model with Mergekit, and then quantizing the model into a GGUF with Llama.cpp, but I&amp;#39;m not sure how applicable that is to vision models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhz2sc/student_unsloth_help_save_to_gguf_taking_forever/n70wf4r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754386699,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhz2sc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]