[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey all,\n\nI'm building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.\n\nI've shortlisted Meta's LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic's model requirements . I'm planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.\n\nI did look at [https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix](https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix) but it is somewhat out of date now.\n\nI have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)\n\nLooking for help with:\n\n* Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?\n* Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?\n* Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?\n\nI have some constraints:\n\n* Must be air-gapped\n* I can't use Chinese, Israeli or similar products. CISO doesn't allow it. I know some of the Chinese models would be a good fit, but its a no-go.\n* Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure\n\nWould love to hear from anyone who’s done this in production or lab.\n\nThanks in advance!\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Local LLM to back Elastic AI",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lyq22j",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.7,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_s8xklsb6",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752404403,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve shortlisted Meta&amp;#39;s LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic&amp;#39;s model requirements . I&amp;#39;m planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.&lt;/p&gt;\n\n&lt;p&gt;I did look at &lt;a href=\"https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix\"&gt;https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix&lt;/a&gt; but it is somewhat out of date now.&lt;/p&gt;\n\n&lt;p&gt;I have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)&lt;/p&gt;\n\n&lt;p&gt;Looking for help with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?&lt;/li&gt;\n&lt;li&gt;Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?&lt;/li&gt;\n&lt;li&gt;Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have some constraints:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be air-gapped&lt;/li&gt;\n&lt;li&gt;I can&amp;#39;t use Chinese, Israeli or similar products. CISO doesn&amp;#39;t allow it. I know some of the Chinese models would be a good fit, but its a no-go.&lt;/li&gt;\n&lt;li&gt;Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear from anyone who’s done this in production or lab.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?auto=webp&amp;s=9a1b4684102bb8c94296cbfa71ad3a31d0c0f257",
                    "width": 1920,
                    "height": 1080
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7dcc983e13bd8aed1654a54d718d49f54cdaae",
                      "width": 108,
                      "height": 60
                    },
                    {
                      "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6db7b95b72b6ba957c849b5433b50158cc281a2e",
                      "width": 216,
                      "height": 121
                    },
                    {
                      "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=54ac1a61a829459657d4fbc629848f4a7b86377b",
                      "width": 320,
                      "height": 180
                    },
                    {
                      "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19ae5d46a63ca2b66411c4548a6cff1870e42360",
                      "width": 640,
                      "height": 360
                    },
                    {
                      "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5dbba47c48a53ff43f7f01db90cc69cd595f0e8",
                      "width": 960,
                      "height": 540
                    },
                    {
                      "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=994bf868b62aa1188f41e6e7f154a8365d35c7fe",
                      "width": 1080,
                      "height": 607
                    }
                  ],
                  "variants": {},
                  "id": "G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lyq22j",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "OldManCyberNinja",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
            "subreddit_subscribers": 498343,
            "created_utc": 1752404403,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2wp1m0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ekaj",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2wh066",
                                "score": 1,
                                "author_fullname": "t2_3cajs",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That’s just a survey paper. Where’s an example from non academia/an actual occurrence?\n\nThis isn’t meant to be antagonistic but rather point out theoretical risks are just that, theoretical, until they’ve actually occurred.\n\nI’m not aware of any public models by any major lab being backdoored as that would be a big news event, let alone if one of the big Chinese labs did it.\n\nIt just sounds like this person doesn’t want to hire a consultant and has a paranoid/out of their depth CISO.",
                                "edited": 1752419159,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2wp1m0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s just a survey paper. Where’s an example from non academia/an actual occurrence?&lt;/p&gt;\n\n&lt;p&gt;This isn’t meant to be antagonistic but rather point out theoretical risks are just that, theoretical, until they’ve actually occurred.&lt;/p&gt;\n\n&lt;p&gt;I’m not aware of any public models by any major lab being backdoored as that would be a big news event, let alone if one of the big Chinese labs did it.&lt;/p&gt;\n\n&lt;p&gt;It just sounds like this person doesn’t want to hire a consultant and has a paranoid/out of their depth CISO.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lyq22j",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2wp1m0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752418870,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1752418870,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2wh066",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "cartogram",
                      "can_mod_post": false,
                      "created_utc": 1752416388,
                      "send_replies": true,
                      "parent_id": "t1_n2w4c9o",
                      "score": 2,
                      "author_fullname": "t2_3j5et",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "backdoors. see [A Survey on Backdoor Threats in LLMs](https://arxiv.org/abs/2502.05224)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2wh066",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;backdoors. see &lt;a href=\"https://arxiv.org/abs/2502.05224\"&gt;A Survey on Backdoor Threats in LLMs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": true,
                      "can_gild": false,
                      "link_id": "t3_1lyq22j",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2wh066/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752416388,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2w4c9o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "indicava",
            "can_mod_post": false,
            "created_utc": 1752411915,
            "send_replies": true,
            "parent_id": "t3_1lyq22j",
            "score": 2,
            "author_fullname": "t2_4dvff",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If it’s air gapped, what’s the risk of using a “foreign” open weights model?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2w4c9o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If it’s air gapped, what’s the risk of using a “foreign” open weights model?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2w4c9o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752411915,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lyq22j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2vvu1j",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ICanSeeYou7867",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2vvijc",
                                "score": 2,
                                "author_fullname": "t2_sqvpr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Nemotron 235B is based on Llama 405B Instruct.\n\n```\nLlama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct\n```\n\nOr the smaller models\nhttps://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct\n\nMistral Large is also an instruct\nhttps://huggingface.co/mistralai/Mistral-Large-Instruct-2411",
                                "edited": 1752408792,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2vvu1j",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nemotron 235B is based on Llama 405B Instruct.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nLlama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Or the smaller models\n&lt;a href=\"https://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct\"&gt;https://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Mistral Large is also an instruct\n&lt;a href=\"https://huggingface.co/mistralai/Mistral-Large-Instruct-2411\"&gt;https://huggingface.co/mistralai/Mistral-Large-Instruct-2411&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lyq22j",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2vvu1j/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752408435,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752408435,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2wffki",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "TheApadayo",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2vvijc",
                                "score": 2,
                                "author_fullname": "t2_8xrdj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "FYI a lot of newer model releases have dropped the “-instruct” part from the name and instead release the fine tuned variant as the main model and now have a “-base” model variant because 99% of people want the instruct model, not the base model.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2wffki",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;FYI a lot of newer model releases have dropped the “-instruct” part from the name and instead release the fine tuned variant as the main model and now have a “-base” model variant because 99% of people want the instruct model, not the base model.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lyq22j",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2wffki/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752415875,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1752415875,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2vvijc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OldManCyberNinja",
                      "can_mod_post": false,
                      "created_utc": 1752408294,
                      "send_replies": true,
                      "parent_id": "t1_n2vs9te",
                      "score": 1,
                      "author_fullname": "t2_s8xklsb6",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the reply. One constraint from Elastic is:\n\nSearch for an LLM (for example, `Mistral-Nemo-Instruct-2407`). Your chosen model must include `instruct` in its name in order to work with Elastic.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2vvijc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the reply. One constraint from Elastic is:&lt;/p&gt;\n\n&lt;p&gt;Search for an LLM (for example, &lt;code&gt;Mistral-Nemo-Instruct-2407&lt;/code&gt;). Your chosen model must include &lt;code&gt;instruct&lt;/code&gt; in its name in order to work with Elastic.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lyq22j",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2vvijc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752408294,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2vs9te",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ICanSeeYou7867",
            "can_mod_post": false,
            "created_utc": 1752406799,
            "send_replies": true,
            "parent_id": "t3_1lyq22j",
            "score": 0,
            "author_fullname": "t2_sqvpr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Im in a similar-ish scenario...\n\nI finally got my 4xH100 server setup as a gpu worker node in kubernetes... and im trying to find out which models to run.\n\nThe Qwen3 235B A22B would be a great fit,  but like you, im trying to (unfortunatrly) avoid Chinese models which is hard....\n\nThe Nvidia Nemotron Ultra 235B is probably the strongest, non-chinese model that I could fit on the 4 H100 cards using FP8.\n\nI have also considered using the smaller nemotron models (like the 70B or the 49B) and deploying 2-4 of those and loaded balancing them.\n\nLlama4's intelligence is pretty low compared to these other models, unfortunately.  But it would be consistent and fast.\n\nMistral/Pixtral large might be a good choice as well, but im not sure how well they perform compared to llama4.  Also sense they are dense models, they might be smarter but will definitely be slower.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2vs9te",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Im in a similar-ish scenario...&lt;/p&gt;\n\n&lt;p&gt;I finally got my 4xH100 server setup as a gpu worker node in kubernetes... and im trying to find out which models to run.&lt;/p&gt;\n\n&lt;p&gt;The Qwen3 235B A22B would be a great fit,  but like you, im trying to (unfortunatrly) avoid Chinese models which is hard....&lt;/p&gt;\n\n&lt;p&gt;The Nvidia Nemotron Ultra 235B is probably the strongest, non-chinese model that I could fit on the 4 H100 cards using FP8.&lt;/p&gt;\n\n&lt;p&gt;I have also considered using the smaller nemotron models (like the 70B or the 49B) and deploying 2-4 of those and loaded balancing them.&lt;/p&gt;\n\n&lt;p&gt;Llama4&amp;#39;s intelligence is pretty low compared to these other models, unfortunately.  But it would be consistent and fast.&lt;/p&gt;\n\n&lt;p&gt;Mistral/Pixtral large might be a good choice as well, but im not sure how well they perform compared to llama4.  Also sense they are dense models, they might be smarter but will definitely be slower.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2vs9te/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752406799,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lyq22j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]