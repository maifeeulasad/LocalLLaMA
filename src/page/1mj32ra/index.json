[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Qwen 30b vs. gpt-oss-20b architecture comparison",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "New Model"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 74,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mj32ra",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.92,
            "author_flair_background_color": null,
            "ups": 126,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_5cwsshv7",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "New Model",
            "can_mod_post": false,
            "score": 126,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/HsmvQ4GpEUBgE-eaPV02WX4c74y4-vpsGyF-bKYEyYY.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754482643,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/7v3m4xao5ehf1.jpeg",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/7v3m4xao5ehf1.jpeg?auto=webp&amp;s=476f4c45f2f32b5477e002fe70e39cf764b7b22d",
                    "width": 1477,
                    "height": 781
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/7v3m4xao5ehf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=088c64d88ac758a164401f3fc7ad5eb4cc81dc0f",
                      "width": 108,
                      "height": 57
                    },
                    {
                      "url": "https://preview.redd.it/7v3m4xao5ehf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e8bc1f9b80ced305e3a08d78d0678aa427f003d9",
                      "width": 216,
                      "height": 114
                    },
                    {
                      "url": "https://preview.redd.it/7v3m4xao5ehf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c33081ccf817e9e5d0a3429ab6b6cfb4519c5875",
                      "width": 320,
                      "height": 169
                    },
                    {
                      "url": "https://preview.redd.it/7v3m4xao5ehf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb7ee193c42fb34e9530b8b00e974a400665f39c",
                      "width": 640,
                      "height": 338
                    },
                    {
                      "url": "https://preview.redd.it/7v3m4xao5ehf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=939a13b45ca90b56b5c84509db0f7863a32c7b96",
                      "width": 960,
                      "height": 507
                    },
                    {
                      "url": "https://preview.redd.it/7v3m4xao5ehf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c58cc60dd0b960b9693f8741788c352c8bed1a8d",
                      "width": 1080,
                      "height": 571
                    }
                  ],
                  "variants": {},
                  "id": "1O2uxzbxUQGH7bYofOJi1kW9hBisSP4a8um35FI4JGs"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ffb000",
            "id": "1mj32ra",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "SunilKumarDash",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/",
            "stickied": false,
            "url": "https://i.redd.it/7v3m4xao5ehf1.jpeg",
            "subreddit_subscribers": 512875,
            "created_utc": 1754482643,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7eazhb",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "1998marcom",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7e35ph",
                                          "score": 1,
                                          "author_fullname": "t2_9d1bee6n",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Gemini has good long context coherence",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7eazhb",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gemini has good long context coherence&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj32ra",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n7eazhb/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754565234,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754565234,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7e35ph",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ninjasaid13",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7892i7",
                                "score": 2,
                                "author_fullname": "t2_qjpsv",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "&gt;New architectures like HRM are still being developed.\n\nLots of new architectures are often introduced but I still haven't heard back from the titans architecture from a year ago so they must not go anywhere.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7e35ph",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;New architectures like HRM are still being developed.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Lots of new architectures are often introduced but I still haven&amp;#39;t heard back from the titans architecture from a year ago so they must not go anywhere.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj32ra",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n7e35ph/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754561487,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754561487,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7892i7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "ClearApartment2627",
                      "can_mod_post": false,
                      "created_utc": 1754487073,
                      "send_replies": true,
                      "parent_id": "t1_n781cdo",
                      "score": 11,
                      "author_fullname": "t2_1p0o7y7278",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Your observations are correct, but...\n\n\nModel architecture is just a part of the equation.\nTraining data and training procedure are at least as important.\nGRPO and GSPO made a huge difference for Deepseek and Alibaba/Qwen.\n\n\nI am still optimistic wrt pushing the sota. New architectures like HRM are still being developed. \n\n\nThe entire AI game has just started.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7892i7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your observations are correct, but...&lt;/p&gt;\n\n&lt;p&gt;Model architecture is just a part of the equation.\nTraining data and training procedure are at least as important.\nGRPO and GSPO made a huge difference for Deepseek and Alibaba/Qwen.&lt;/p&gt;\n\n&lt;p&gt;I am still optimistic wrt pushing the sota. New architectures like HRM are still being developed. &lt;/p&gt;\n\n&lt;p&gt;The entire AI game has just started.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj32ra",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n7892i7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754487073,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 11
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n78r4g8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1754492463,
                      "send_replies": true,
                      "parent_id": "t1_n781cdo",
                      "score": 3,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I mean at the level the infographic goes to I'm honest not sure how different most models would be.  Like if Deepseek was on there with \"Multi-Head Latent Attention\" instead of \"Grouped Query Attention\" would you even notice?  All models are basically the same at this levels... A few tweaks here and there.  Even MoE just adds the little \"router\" cutout - everything else is the same.  \n\nThis is the first FP4 based model that's been released and that is _huge_.  Even if you might just quantize a bf16 model to Q4 anyway, having it natively FP4 cuts the requirements for fine-tuning by 4x!  We barely even have native FP8 models... just Deepseek but at 671B it's totally unmanageable anyways.\n\nThey also introduced a richer prompt format.  This is less impactful since the industry seems to love inventing new formats instead of using existing ones, but oh well.  It's still pretty interesting and would make implementing it in a production application easier.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78r4g8",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I mean at the level the infographic goes to I&amp;#39;m honest not sure how different most models would be.  Like if Deepseek was on there with &amp;quot;Multi-Head Latent Attention&amp;quot; instead of &amp;quot;Grouped Query Attention&amp;quot; would you even notice?  All models are basically the same at this levels... A few tweaks here and there.  Even MoE just adds the little &amp;quot;router&amp;quot; cutout - everything else is the same.  &lt;/p&gt;\n\n&lt;p&gt;This is the first FP4 based model that&amp;#39;s been released and that is &lt;em&gt;huge&lt;/em&gt;.  Even if you might just quantize a bf16 model to Q4 anyway, having it natively FP4 cuts the requirements for fine-tuning by 4x!  We barely even have native FP8 models... just Deepseek but at 671B it&amp;#39;s totally unmanageable anyways.&lt;/p&gt;\n\n&lt;p&gt;They also introduced a richer prompt format.  This is less impactful since the industry seems to love inventing new formats instead of using existing ones, but oh well.  It&amp;#39;s still pretty interesting and would make implementing it in a production application easier.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj32ra",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n78r4g8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754492463,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7bzgns",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "robertotomas",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7bpugq",
                                "score": 1,
                                "author_fullname": "t2_65zz9",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yes, well I agree they could do better maybe. it is speculation. but this architecture is definitely not it. the image in the OP is what makes me think it. I described very briefly why already",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7bzgns",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes, well I agree they could do better maybe. it is speculation. but this architecture is definitely not it. the image in the OP is what makes me think it. I described very briefly why already&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj32ra",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n7bzgns/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754526768,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754526768,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bpugq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "RabbitEater2",
                      "can_mod_post": false,
                      "created_utc": 1754523519,
                      "send_replies": true,
                      "parent_id": "t1_n781cdo",
                      "score": 1,
                      "author_fullname": "t2_w61xk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What makes you think they didn't release architecture they used for their top closed source models so people wouldn't copy them, and instead made an alright model based on what current open source SOTAs are using? O3 is very good and tops quite a few charts in varied areas, so they can release a good model if they wanted to.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bpugq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What makes you think they didn&amp;#39;t release architecture they used for their top closed source models so people wouldn&amp;#39;t copy them, and instead made an alright model based on what current open source SOTAs are using? O3 is very good and tops quite a few charts in varied areas, so they can release a good model if they wanted to.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj32ra",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n7bpugq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754523519,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n781cdo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "robertotomas",
            "can_mod_post": false,
            "created_utc": 1754484521,
            "send_replies": true,
            "parent_id": "t3_1mj32ra",
            "score": 25,
            "author_fullname": "t2_65zz9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "So no essential differences? just scaling factors (and apparently smaller training samples for oss). Honestly Im confused.\n\nThis whole saga seems similar to what is happening in Europe. mistral have been doing great things but essentially just can’t keep up. Well neither apparently can the US. Thinking worst case for a second: The only models there that compete appear less and less likely to be just models, they are gated behind an api, they may well be agentic. (There’s a good business case to do exactly that)\n\nWith the inability of meta and openai to push sota forward (if the is the case, it appears to be), it seems ever more likely that no one’s got an edge.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n781cdo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So no essential differences? just scaling factors (and apparently smaller training samples for oss). Honestly Im confused.&lt;/p&gt;\n\n&lt;p&gt;This whole saga seems similar to what is happening in Europe. mistral have been doing great things but essentially just can’t keep up. Well neither apparently can the US. Thinking worst case for a second: The only models there that compete appear less and less likely to be just models, they are gated behind an api, they may well be agentic. (There’s a good business case to do exactly that)&lt;/p&gt;\n\n&lt;p&gt;With the inability of meta and openai to push sota forward (if the is the case, it appears to be), it seems ever more likely that no one’s got an edge.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n781cdo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754484521,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj32ra",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 25
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n792fc5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "No_Afternoon_4260",
                      "can_mod_post": false,
                      "created_utc": 1754495646,
                      "send_replies": true,
                      "parent_id": "t1_n78758p",
                      "score": 4,
                      "author_fullname": "t2_cj9kap4bx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Or may be OAI used a open source architecture 🤷\nIt seems there goal is just a marketing stunt not to release something useful",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n792fc5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Or may be OAI used a open source architecture 🤷\nIt seems there goal is just a marketing stunt not to release something useful&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj32ra",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n792fc5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754495646,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n78758p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "iKy1e",
            "can_mod_post": false,
            "created_utc": 1754486456,
            "send_replies": true,
            "parent_id": "t3_1mj32ra",
            "score": 6,
            "author_fullname": "t2_aqewd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It’s interesting how there are actual improvements to be found, RoPE, group query attention, flash attention, MoE itself, but overall once an improvement is found everyone has it.\n\nIt really seems the datasets &amp; training techniques (&amp; access to compute) are the key differentiators between models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78758p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Ollama"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s interesting how there are actual improvements to be found, RoPE, group query attention, flash attention, MoE itself, but overall once an improvement is found everyone has it.&lt;/p&gt;\n\n&lt;p&gt;It really seems the datasets &amp;amp; training techniques (&amp;amp; access to compute) are the key differentiators between models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n78758p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754486456,
            "author_flair_text": "Ollama",
            "treatment_tags": [],
            "link_id": "t3_1mj32ra",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n788qbb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "dinerburgeryum",
            "can_mod_post": false,
            "created_utc": 1754486964,
            "send_replies": true,
            "parent_id": "t3_1mj32ra",
            "score": 3,
            "author_fullname": "t2_1j53p3yv3e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I know I keep beating this drum but why aren’t the attention sinks represented in this diagram?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n788qbb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I know I keep beating this drum but why aren’t the attention sinks represented in this diagram?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n788qbb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754486964,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj32ra",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n781d3h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Snoo_28140",
            "can_mod_post": false,
            "created_utc": 1754484528,
            "send_replies": true,
            "parent_id": "t3_1mj32ra",
            "score": 1,
            "author_fullname": "t2_6ms1kza7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That's pretty interesting, thanks!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n781d3h",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s pretty interesting, thanks!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n781d3h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754484528,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj32ra",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ad64v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Tusalo",
            "can_mod_post": false,
            "created_utc": 1754508747,
            "send_replies": true,
            "parent_id": "t3_1mj32ra",
            "score": 1,
            "author_fullname": "t2_11uxj39h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There is one novelty in the swiglu function used by oss, which seems a bit odd. They clamp the swish activated gate to values smaller or equal to 7. They also clamp the up projections to values between -7 and 7. Then, they add 1 to the clamped up projections giving values between -6 and 8 and only then multiply elementwise with the gate. This avoids single activations dominating in the MLP which is the case for Qwen.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ad64v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is one novelty in the swiglu function used by oss, which seems a bit odd. They clamp the swish activated gate to values smaller or equal to 7. They also clamp the up projections to values between -7 and 7. Then, they add 1 to the clamped up projections giving values between -6 and 8 and only then multiply elementwise with the gate. This avoids single activations dominating in the MLP which is the case for Qwen.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n7ad64v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754508747,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj32ra",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n787bff",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "QFGTrialByFire",
            "can_mod_post": false,
            "created_utc": 1754486511,
            "send_replies": true,
            "parent_id": "t3_1mj32ra",
            "score": 0,
            "author_fullname": "t2_1h4o7f23eh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "From an actual use point of view there is a lot of difference in actual output quality. Especially comparing code output on the coder instruct version of qwen. I wish it wasn't as the oss 20B runs on my gpu at 100tk/s while the qwen 30B overflows and runs 8tk/s. I mean its fair enough at least it flies on my 3080ti which is probably what they were aiming at, that it runs on local hardware but after tasting qwen 30B its hard to go backwards on output quality.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n787bff",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;From an actual use point of view there is a lot of difference in actual output quality. Especially comparing code output on the coder instruct version of qwen. I wish it wasn&amp;#39;t as the oss 20B runs on my gpu at 100tk/s while the qwen 30B overflows and runs 8tk/s. I mean its fair enough at least it flies on my 3080ti which is probably what they were aiming at, that it runs on local hardware but after tasting qwen 30B its hard to go backwards on output quality.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/n787bff/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754486511,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj32ra",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]