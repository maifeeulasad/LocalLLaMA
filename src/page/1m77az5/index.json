[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey,\n\nI wanted to check if I'm missing anything relevant in performance or quality with my quant strategy.\n\nMy setup is an EPYC Rome (no avx512 instruction set) with 512 GB RAM and a bunch of 3060 / 3090s. The inference engine is llama.cpp and I run almost everything large (r1, q3 235, q3 480) in UD-Q4\\_K\\_XL, while Kimi K2 uses UD-Q3\\_K\\_XL - CPU offload ofc. Smaller 30b/32b (Devstral, Magistral, Gemma-3, etc.) I run in UD-Q6\\_K\\_XL on the GPUs only.\n\nI settled on these quants after seeing tests on unrelated models some time ago that suggested diminishing returns after Q4\\_K\\_M. Another source I can't remember claimed Q8\\_0 for KV cache doesn't hurt quality and that even Q4\\_0 for the v cache is acceptable.\n\nAre my generalized assumptions still correct or where they ever correct?\n\n* larger models are more insensitive to quant\n* diminishing returns after \\~4.5bpw\n* Q8\\_0 KV is the way to go\n\nWould the ik\\_llama fork (with their special quants) provide a significant increase of quality/speed in my CPU-poor setup?\n\nEdit:\n\nI use it mainly for coding - sometimes obscure languages like OpenSCAD, reasoning in electrical engineering (which component could be the culprit if ..., what could be this component, it has .. color and ... marking) and some science related stuff like paper comprehension, generation of abstracts, keyword suggestion.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Which quantization approach is the way to go? (llama.cpp)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m77az5",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1tnm5zafaw",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753274570,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753271962,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I wanted to check if I&amp;#39;m missing anything relevant in performance or quality with my quant strategy.&lt;/p&gt;\n\n&lt;p&gt;My setup is an EPYC Rome (no avx512 instruction set) with 512 GB RAM and a bunch of 3060 / 3090s. The inference engine is llama.cpp and I run almost everything large (r1, q3 235, q3 480) in UD-Q4_K_XL, while Kimi K2 uses UD-Q3_K_XL - CPU offload ofc. Smaller 30b/32b (Devstral, Magistral, Gemma-3, etc.) I run in UD-Q6_K_XL on the GPUs only.&lt;/p&gt;\n\n&lt;p&gt;I settled on these quants after seeing tests on unrelated models some time ago that suggested diminishing returns after Q4_K_M. Another source I can&amp;#39;t remember claimed Q8_0 for KV cache doesn&amp;#39;t hurt quality and that even Q4_0 for the v cache is acceptable.&lt;/p&gt;\n\n&lt;p&gt;Are my generalized assumptions still correct or where they ever correct?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;larger models are more insensitive to quant&lt;/li&gt;\n&lt;li&gt;diminishing returns after ~4.5bpw&lt;/li&gt;\n&lt;li&gt;Q8_0 KV is the way to go&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would the ik_llama fork (with their special quants) provide a significant increase of quality/speed in my CPU-poor setup?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;I use it mainly for coding - sometimes obscure languages like OpenSCAD, reasoning in electrical engineering (which component could be the culprit if ..., what could be this component, it has .. color and ... marking) and some science related stuff like paper comprehension, generation of abstracts, keyword suggestion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m77az5",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "pixelterpy",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/",
            "subreddit_subscribers": 503518,
            "created_utc": 1753271962,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4persn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "pixelterpy",
                      "can_mod_post": false,
                      "created_utc": 1753275606,
                      "send_replies": true,
                      "parent_id": "t1_n4p7hbp",
                      "score": 4,
                      "author_fullname": "t2_1tnm5zafaw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh these are some nice gains for pp even with avx2 (1.9x). I was under the perception that avx512 (2.2x) leads to the major speedup. Thanks for clarification, definitely will check this out.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4persn",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh these are some nice gains for pp even with avx2 (1.9x). I was under the perception that avx512 (2.2x) leads to the major speedup. Thanks for clarification, definitely will check this out.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m77az5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4persn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753275606,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4p7hbp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "TyraVex",
            "can_mod_post": false,
            "created_utc": 1753273078,
            "send_replies": true,
            "parent_id": "t3_1m77az5",
            "score": 5,
            "author_fullname": "t2_5u6lp8ar",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you like tinkering and if you have the time, you should play with ik_llama.cpp. TG is the same or a bit better, but PP is way more efficient. The community is nice, mostly enthusiasts trying to push the Pareto frontier of consumer and prosumer inference efficiency and quality.\n\n\n\nhttps://github.com/ikawrakow/ik_llama.cpp/blob/main/README.md\n\n\nhttps://github.com/ikawrakow/ik_llama.cpp/wiki/Jan-2025:-prompt-processing-performance-comparison\n\nQuick-start Guide:\nhttps://github.com/ikawrakow/ik_llama.cpp/discussions/258",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4p7hbp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you like tinkering and if you have the time, you should play with ik_llama.cpp. TG is the same or a bit better, but PP is way more efficient. The community is nice, mostly enthusiasts trying to push the Pareto frontier of consumer and prosumer inference efficiency and quality.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp/blob/main/README.md\"&gt;https://github.com/ikawrakow/ik_llama.cpp/blob/main/README.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp/wiki/Jan-2025:-prompt-processing-performance-comparison\"&gt;https://github.com/ikawrakow/ik_llama.cpp/wiki/Jan-2025:-prompt-processing-performance-comparison&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quick-start Guide:\n&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp/discussions/258\"&gt;https://github.com/ikawrakow/ik_llama.cpp/discussions/258&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4p7hbp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753273078,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m77az5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4pfu55",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "un_passant",
            "can_mod_post": false,
            "created_utc": 1753275960,
            "send_replies": true,
            "parent_id": "t3_1m77az5",
            "score": 3,
            "author_fullname": "t2_7rqtc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Seconding the recommendations for ik\\_llama.cpp\n\nYou'll want to source your models from [https://huggingface.co/ubergarm/models](https://huggingface.co/ubergarm/models)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4pfu55",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Seconding the recommendations for ik_llama.cpp&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ll want to source your models from &lt;a href=\"https://huggingface.co/ubergarm/models\"&gt;https://huggingface.co/ubergarm/models&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4pfu55/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753275960,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m77az5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4qaxcu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FullstackSensei",
                      "can_mod_post": false,
                      "created_utc": 1753285095,
                      "send_replies": true,
                      "parent_id": "t1_n4pvs6i",
                      "score": 1,
                      "author_fullname": "t2_17n3nqtj56",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thing is, downloading several hundred GBs per model takes time. More so when the models themselves are updated every few days due to bug fixes. As you say, they're all tools, and I need to get things done at some point rather than testing several quants to find the smallest Ai can get away with. So, like OP, I've taken the shortcut of settling for Q4_K_XL for anything above 100B.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4qaxcu",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thing is, downloading several hundred GBs per model takes time. More so when the models themselves are updated every few days due to bug fixes. As you say, they&amp;#39;re all tools, and I need to get things done at some point rather than testing several quants to find the smallest Ai can get away with. So, like OP, I&amp;#39;ve taken the shortcut of settling for Q4_K_XL for anything above 100B.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m77az5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4qaxcu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753285095,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4qxm5x",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "segmond",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4qnh29",
                                "score": 1,
                                "author_fullname": "t2_ah13x",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yes, something like that, for chat and generic text generation, you won't notice much.  but for precise output such as generating structured format or utilizing agentic capabilities trained in the model, performance will degrade.   My first MP3 player was a CD mp3 player, no digital storage, I could burn hundreds of music at 128kbs on my CD.  I loved it, it was not super CD quality, but it was great enough and beat tape player.  I enjoyed it for years, I would take one CD and have equivalent of 10 CDs.   More so than having the best, what are we going to do with what we have?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4qxm5x",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes, something like that, for chat and generic text generation, you won&amp;#39;t notice much.  but for precise output such as generating structured format or utilizing agentic capabilities trained in the model, performance will degrade.   My first MP3 player was a CD mp3 player, no digital storage, I could burn hundreds of music at 128kbs on my CD.  I loved it, it was not super CD quality, but it was great enough and beat tape player.  I enjoyed it for years, I would take one CD and have equivalent of 10 CDs.   More so than having the best, what are we going to do with what we have?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m77az5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4qxm5x/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753291305,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753291305,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4rwmqa",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Former-Ad-5757",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4qnh29",
                                "score": 1,
                                "author_fullname": "t2_ihsdiwk6k",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The problem is that an llm is a next token-generator. An error is not a single local error which gets corrected later on, all the tokens after 1 hallucinated token are going down a different path and the quality of that respons will usually get worse the longer it goes on, and with the next reply it will see the previous (wrong path) reply as an in context text which will make it go further down the wrong road.\n\nBasically usually it is better to start a new chat (or delete/edit a wrong reply) after it starts hallucinating then to try to correct the model with further replies.\n\nThat is just the way an llm works. If you keep that in mind, then if you want chats that use 32k tokens or 128k tokens, do you understand what a difference between 99,9 and 99,7% means?  \n99,9% means that on 32k you will probably (statistically) have the model go down 3 wrong paths, and with 99,7 it means it will probably go down 9 wrong paths.  \nWith RP it doesn't really matter much if it goes down 9 \"wrong\" paths, but if you want to generate a json, then it gives you 9 chances for invalid json.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4rwmqa",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 3"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The problem is that an llm is a next token-generator. An error is not a single local error which gets corrected later on, all the tokens after 1 hallucinated token are going down a different path and the quality of that respons will usually get worse the longer it goes on, and with the next reply it will see the previous (wrong path) reply as an in context text which will make it go further down the wrong road.&lt;/p&gt;\n\n&lt;p&gt;Basically usually it is better to start a new chat (or delete/edit a wrong reply) after it starts hallucinating then to try to correct the model with further replies.&lt;/p&gt;\n\n&lt;p&gt;That is just the way an llm works. If you keep that in mind, then if you want chats that use 32k tokens or 128k tokens, do you understand what a difference between 99,9 and 99,7% means?&lt;br/&gt;\n99,9% means that on 32k you will probably (statistically) have the model go down 3 wrong paths, and with 99,7 it means it will probably go down 9 wrong paths.&lt;br/&gt;\nWith RP it doesn&amp;#39;t really matter much if it goes down 9 &amp;quot;wrong&amp;quot; paths, but if you want to generate a json, then it gives you 9 chances for invalid json.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m77az5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4rwmqa/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753301034,
                                "author_flair_text": "Llama 3",
                                "treatment_tags": [],
                                "created_utc": 1753301034,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#c7b594",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4qnh29",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pixelterpy",
                      "can_mod_post": false,
                      "created_utc": 1753288578,
                      "send_replies": true,
                      "parent_id": "t1_n4pvs6i",
                      "score": 1,
                      "author_fullname": "t2_1tnm5zafaw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm still struggling to get a grasp on the quality difference. The comparisons I come up with are always stupid so please forgive me but is q4 vs. q8 vs. f16 (weights / kv) the difference between 192 kbit, 256 kbit and 320 kbit MP3 (difference only perceptible if you have good hearing and proper speakers) or is it like phone speakers vs. studio equipment. \n\nMore quality beats less quality, I get it, but where is the sweet spot in 2025?\n\nDo you have more personal experiences of quality differences besides the vision models or were you always on Q8 weights and f16 kv?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4qnh29",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m still struggling to get a grasp on the quality difference. The comparisons I come up with are always stupid so please forgive me but is q4 vs. q8 vs. f16 (weights / kv) the difference between 192 kbit, 256 kbit and 320 kbit MP3 (difference only perceptible if you have good hearing and proper speakers) or is it like phone speakers vs. studio equipment. &lt;/p&gt;\n\n&lt;p&gt;More quality beats less quality, I get it, but where is the sweet spot in 2025?&lt;/p&gt;\n\n&lt;p&gt;Do you have more personal experiences of quality differences besides the vision models or were you always on Q8 weights and f16 kv?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m77az5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4qnh29/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753288578,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4pvs6i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "segmond",
            "can_mod_post": false,
            "created_utc": 1753280868,
            "send_replies": true,
            "parent_id": "t3_1m77az5",
            "score": 3,
            "author_fullname": "t2_ah13x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Always go for the biggest quant you can run comfortably.   If I can go for Q8 I do, then I work my way down.  I personally prefer quality over speed.   I run KV cache at full precision and never quantize.  I don't believe that it doesn't hurt quality, I'll only run it less if that's the only way I could run it.   I noticed this with some vision models, I tried running some in q8 and it could never complete some task no matter how much I tried, once I ran in fp16 it worked.  Since then I leave my KV alone at the expense of less context.   Quality over speed/quantity.\n\nAt the end of the day, these things are so good that one could gain value running a Q2 quant with kv at Q4.   It's all a tool, and everyone has to play the hand they have based on the tools/hardware available to them.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4pvs6i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Always go for the biggest quant you can run comfortably.   If I can go for Q8 I do, then I work my way down.  I personally prefer quality over speed.   I run KV cache at full precision and never quantize.  I don&amp;#39;t believe that it doesn&amp;#39;t hurt quality, I&amp;#39;ll only run it less if that&amp;#39;s the only way I could run it.   I noticed this with some vision models, I tried running some in q8 and it could never complete some task no matter how much I tried, once I ran in fp16 it worked.  Since then I leave my KV alone at the expense of less context.   Quality over speed/quantity.&lt;/p&gt;\n\n&lt;p&gt;At the end of the day, these things are so good that one could gain value running a Q2 quant with kv at Q4.   It&amp;#39;s all a tool, and everyone has to play the hand they have based on the tools/hardware available to them.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4pvs6i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753280868,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m77az5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4p91d7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1753273643,
            "send_replies": true,
            "parent_id": "t3_1m77az5",
            "score": 1,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It depends on how much effort and cost you want to put in. These methods in your post are in the low effort/cost category. If that is what you want then that is fine but there is a second category where you do things like further training (well-known example is QAT) or adding additional neural network blocks (well-known example is SVDQuant.)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4p91d7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends on how much effort and cost you want to put in. These methods in your post are in the low effort/cost category. If that is what you want then that is fine but there is a second category where you do things like further training (well-known example is QAT) or adding additional neural network blocks (well-known example is SVDQuant.)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4p91d7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753273643,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m77az5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4rxusd",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Former-Ad-5757",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4pd8tu",
                                "score": 1,
                                "author_fullname": "t2_ihsdiwk6k",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For coding I would say that q8\\_0kv is good enough. Coding is a nice deterministic process where a compiler or linter or other tools can detect errors and an agentic workflow will just detect the error and work around it.\n\nq8\\_0 introduces more errors, but coding can fix the errors so in the end it doesn't really matter. It is just 1 or 2 regenerations more and gets the same end-result.\n\nIf your use case was trying to solve problems with an unknown outcome then it would become more problematic and I would advice for better kv\\_cache.\n\nBut for coding etc q8 is idd good enough.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4rxusd",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 3"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For coding I would say that q8_0kv is good enough. Coding is a nice deterministic process where a compiler or linter or other tools can detect errors and an agentic workflow will just detect the error and work around it.&lt;/p&gt;\n\n&lt;p&gt;q8_0 introduces more errors, but coding can fix the errors so in the end it doesn&amp;#39;t really matter. It is just 1 or 2 regenerations more and gets the same end-result.&lt;/p&gt;\n\n&lt;p&gt;If your use case was trying to solve problems with an unknown outcome then it would become more problematic and I would advice for better kv_cache.&lt;/p&gt;\n\n&lt;p&gt;But for coding etc q8 is idd good enough.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m77az5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4rxusd/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753301379,
                                "author_flair_text": "Llama 3",
                                "treatment_tags": [],
                                "created_utc": 1753301379,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#c7b594",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4pd8tu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pixelterpy",
                      "can_mod_post": false,
                      "created_utc": 1753275105,
                      "send_replies": true,
                      "parent_id": "t1_n4p9r8k",
                      "score": 1,
                      "author_fullname": "t2_1tnm5zafaw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I added some of my use cases to the original post. Creative writing / RP is not in scope.\n\nYea, I could use bigger quants but they would be slower in tg and also demand beefier hardware in some cases. In my perception, the setup is now near best bang for the buck and I want to avoid investments with diminishing returns. But I would be more than happy to upgrade to 1 tb ram if for example ds-r1 q6 would be that much superior for my tasks.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4pd8tu",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I added some of my use cases to the original post. Creative writing / RP is not in scope.&lt;/p&gt;\n\n&lt;p&gt;Yea, I could use bigger quants but they would be slower in tg and also demand beefier hardware in some cases. In my perception, the setup is now near best bang for the buck and I want to avoid investments with diminishing returns. But I would be more than happy to upgrade to 1 tb ram if for example ds-r1 q6 would be that much superior for my tasks.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m77az5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4pd8tu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753275105,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4p9r8k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Former-Ad-5757",
            "can_mod_post": false,
            "created_utc": 1753273902,
            "send_replies": true,
            "parent_id": "t3_1m77az5",
            "score": 1,
            "author_fullname": "t2_ihsdiwk6k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What are you running on these setups?\n\nQ8\\_0KV still has a low error rate, and on a token-generating machine that means that if you generate 100k tokens you allmost (statistically) have an error somewhere, and everything following that error in that generation is probably also incorrect (as it generates based on previous tokens).\n\n  \nFor RP or something like that it is not a problem, but if you want the response to be as good as possible then why  not use best quality.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4p9r8k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What are you running on these setups?&lt;/p&gt;\n\n&lt;p&gt;Q8_0KV still has a low error rate, and on a token-generating machine that means that if you generate 100k tokens you allmost (statistically) have an error somewhere, and everything following that error in that generation is probably also incorrect (as it generates based on previous tokens).&lt;/p&gt;\n\n&lt;p&gt;For RP or something like that it is not a problem, but if you want the response to be as good as possible then why  not use best quality.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m77az5/which_quantization_approach_is_the_way_to_go/n4p9r8k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753273902,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1m77az5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]