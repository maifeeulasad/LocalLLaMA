[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "HRM model was trained on test?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 35,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgd3lh",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1nisx8ggay",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/OCP539vRz8IWhc6Q3DthwHNfIKX-3i-EHCMDWSNvU1s.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754205660,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/e4op2j02argf1.png",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?auto=webp&amp;s=e1cb3a7b4fab6a83b18ec019a437dd0066760a90",
                    "width": 1080,
                    "height": 271
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/e4op2j02argf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3719305d4458c68cccd4a2b97547cb6bc4be5251",
                      "width": 108,
                      "height": 27
                    },
                    {
                      "url": "https://preview.redd.it/e4op2j02argf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c08316fa1ffb2aae3cf07f96549867a06e61be34",
                      "width": 216,
                      "height": 54
                    },
                    {
                      "url": "https://preview.redd.it/e4op2j02argf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91eba80ed0ddcdeabe96e4902e4d87e705f19c16",
                      "width": 320,
                      "height": 80
                    },
                    {
                      "url": "https://preview.redd.it/e4op2j02argf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8eece1e33073e0e4fe5801c6917464a55d051355",
                      "width": 640,
                      "height": 160
                    },
                    {
                      "url": "https://preview.redd.it/e4op2j02argf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=912cb994049f506e225f2cabc4f96be28f0d1c2a",
                      "width": 960,
                      "height": 240
                    },
                    {
                      "url": "https://preview.redd.it/e4op2j02argf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a004430fb14ff34c77d8b66a8195c0b002006f5e",
                      "width": 1080,
                      "height": 271
                    }
                  ],
                  "variants": {},
                  "id": "nwQrGJrBDCDo-K7wxSfT8eAh3-H9AN45BOnvDCwUQPo"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mgd3lh",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ILoveMy2Balls",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/",
            "stickied": false,
            "url": "https://i.redd.it/e4op2j02argf1.png",
            "subreddit_subscribers": 509626,
            "created_utc": 1754205660,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6o669l",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "-dysangel-",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6o0b8m",
                                "score": 2,
                                "author_fullname": "t2_12ggykute6",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "no, it doesn't sound like an LLM. They just seem to be trying to get some hype by mentioning it's better than LLMs",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6o669l",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;no, it doesn&amp;#39;t sound like an LLM. They just seem to be trying to get some hype by mentioning it&amp;#39;s better than LLMs&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgd3lh",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6o669l/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754215705,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754215705,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6o5yck",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "No_Efficiency_1144",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6o0b8m",
                                "score": 1,
                                "author_fullname": "t2_1nkj9l14b0",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Its kinda unclear despite having read the paper",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6o5yck",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Its kinda unclear despite having read the paper&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgd3lh",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6o5yck/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754215576,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754215576,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6o0b8m",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Former-Ad-5757",
                      "can_mod_post": false,
                      "created_utc": 1754212204,
                      "send_replies": true,
                      "parent_id": "t1_n6nuq6l",
                      "score": 1,
                      "author_fullname": "t2_ihsdiwk6k",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is it meant as an llm? Or can you use a .6b llm as a router in front of 30 of these models and a big llm to enhance the llm output. Or perhaps offer these models as tools to a big llm.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6o0b8m",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "Llama 3"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is it meant as an llm? Or can you use a .6b llm as a router in front of 30 of these models and a big llm to enhance the llm output. Or perhaps offer these models as tools to a big llm.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgd3lh",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6o0b8m/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754212204,
                      "author_flair_text": "Llama 3",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#c7b594",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6nuq6l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Lumiphoton",
            "can_mod_post": false,
            "created_utc": 1754208866,
            "send_replies": true,
            "parent_id": "t3_1mgd3lh",
            "score": 10,
            "author_fullname": "t2_hle1y",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It seems that most people are missing why the paper is off. The simple fact is that the HRMs they trained are not generalist models like an LLM is, so they shouldn't even be making that comparison in the first place. It's very misleading of the authors.\n\nTheir HRMs are trained on one hyper-specific domain at a time (ARC-AGI / Sodoku / 30x30 Mazes). Note that they present three separate trained models. The Sodoku HRM can't solve ARC-AGI and vice versa! \n\nThey are fine as specialist models. Maybe the fast-slow architecture is even a useful innovation. But we won't know if it's a useful generalist architecture until they actually train it like an LLM.\n\nLLMs can do millions of different tasks. Their HRM (so far) can only do one!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6nuq6l",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It seems that most people are missing why the paper is off. The simple fact is that the HRMs they trained are not generalist models like an LLM is, so they shouldn&amp;#39;t even be making that comparison in the first place. It&amp;#39;s very misleading of the authors.&lt;/p&gt;\n\n&lt;p&gt;Their HRMs are trained on one hyper-specific domain at a time (ARC-AGI / Sodoku / 30x30 Mazes). Note that they present three separate trained models. The Sodoku HRM can&amp;#39;t solve ARC-AGI and vice versa! &lt;/p&gt;\n\n&lt;p&gt;They are fine as specialist models. Maybe the fast-slow architecture is even a useful innovation. But we won&amp;#39;t know if it&amp;#39;s a useful generalist architecture until they actually train it like an LLM.&lt;/p&gt;\n\n&lt;p&gt;LLMs can do millions of different tasks. Their HRM (so far) can only do one!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6nuq6l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754208866,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgd3lh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6nyikt",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "LagOps91",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6nwsek",
                                          "score": 1,
                                          "author_fullname": "t2_3wi6j7vwh",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "oh yes, training on the actual test set would have been much more egregious.  \n  \nI was mostly responding to \"So it's not as egregious as one might think.\", which i disagreed with as I think there is a serious problem of doing it they way they did.\n\nthe problem to me isn't that they did the training they way they did, but that they presented it as them beating major models/architectures. It's just not an apples to apples comparison and it shouldn't have been presented the way it was in the paper. It's like making a purpose-built chess engine and comparing it to langue models and language model architectures (well not quite, but you get where my issue lies).  \n  \n  \nthat's the part that is egregious to me. they must have known that there was a high chance for this to be misinterpreted and talked about by media/influencers.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6nyikt",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;oh yes, training on the actual test set would have been much more egregious.  &lt;/p&gt;\n\n&lt;p&gt;I was mostly responding to &amp;quot;So it&amp;#39;s not as egregious as one might think.&amp;quot;, which i disagreed with as I think there is a serious problem of doing it they way they did.&lt;/p&gt;\n\n&lt;p&gt;the problem to me isn&amp;#39;t that they did the training they way they did, but that they presented it as them beating major models/architectures. It&amp;#39;s just not an apples to apples comparison and it shouldn&amp;#39;t have been presented the way it was in the paper. It&amp;#39;s like making a purpose-built chess engine and comparing it to langue models and language model architectures (well not quite, but you get where my issue lies).  &lt;/p&gt;\n\n&lt;p&gt;that&amp;#39;s the part that is egregious to me. they must have known that there was a high chance for this to be misinterpreted and talked about by media/influencers.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mgd3lh",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6nyikt/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754211124,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754211124,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6nwsek",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "ResidentPositive4122",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6nvuvw",
                                "score": 3,
                                "author_fullname": "t2_10nxrjjgay",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'm saying it's not as egregious as literally training on the test set. They don't include the pairs they test on in their training. They only include the examples. I'm talking strictly from a ML perspective of training a model, choosing datasets, etc.\n\nNo comment on the paper at large, I found it overhyping and vague as well, and the hype comes in large measure from the uninitiated commentators on the side (influencers et all). I agree with you that the merits are TBD and further training on non-specific models needs to be done.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6nwsek",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m saying it&amp;#39;s not as egregious as literally training on the test set. They don&amp;#39;t include the pairs they test on in their training. They only include the examples. I&amp;#39;m talking strictly from a ML perspective of training a model, choosing datasets, etc.&lt;/p&gt;\n\n&lt;p&gt;No comment on the paper at large, I found it overhyping and vague as well, and the hype comes in large measure from the uninitiated commentators on the side (influencers et all). I agree with you that the merits are TBD and further training on non-specific models needs to be done.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgd3lh",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6nwsek/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754210092,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754210092,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6o603f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "-dysangel-",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6nvuvw",
                                "score": 1,
                                "author_fullname": "t2_12ggykute6",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yeah I've thought the same thing. Maybe I'm missing something interesting in there, but on the surface the idea sounds like \"did you know back propagation of a neural net can train the net to approximate a function?\"",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6o603f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah I&amp;#39;ve thought the same thing. Maybe I&amp;#39;m missing something interesting in there, but on the surface the idea sounds like &amp;quot;did you know back propagation of a neural net can train the net to approximate a function?&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgd3lh",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6o603f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754215605,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754215605,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6nvuvw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "LagOps91",
                      "can_mod_post": false,
                      "created_utc": 1754209540,
                      "send_replies": true,
                      "parent_id": "t1_n6npvd7",
                      "score": 2,
                      "author_fullname": "t2_3wi6j7vwh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I don't quite agree that it's not as egregious. They are comparing a purpose-built model to solve ARC-AGI puzzles with no knowledge in other domains (including language) to models, which are specifically built to handle language-related tasks in terms of architecture.\n\nI'm not saying their architecture sucks or anything, it is just entirely up in the air if it's any good at doing anything besides being a narrow AI that solves one specific type of puzzle task. Could be great, could be terrible. The paper doesn't really give me any indication in this regard.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6nvuvw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t quite agree that it&amp;#39;s not as egregious. They are comparing a purpose-built model to solve ARC-AGI puzzles with no knowledge in other domains (including language) to models, which are specifically built to handle language-related tasks in terms of architecture.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not saying their architecture sucks or anything, it is just entirely up in the air if it&amp;#39;s any good at doing anything besides being a narrow AI that solves one specific type of puzzle task. Could be great, could be terrible. The paper doesn&amp;#39;t really give me any indication in this regard.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgd3lh",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6nvuvw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754209540,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6npvd7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ResidentPositive4122",
            "can_mod_post": false,
            "created_utc": 1754206033,
            "send_replies": true,
            "parent_id": "t3_1mgd3lh",
            "score": 5,
            "author_fullname": "t2_10nxrjjgay",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The paper is all over the place in words, but after a brief look at the code, IIUC, they are training on the \"examples\" from the test set. Which is similar to \"test time training\" that some teams did for arc1. So it's not as egregious as one might think.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6npvd7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The paper is all over the place in words, but after a brief look at the code, IIUC, they are training on the &amp;quot;examples&amp;quot; from the test set. Which is similar to &amp;quot;test time training&amp;quot; that some teams did for arc1. So it&amp;#39;s not as egregious as one might think.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6npvd7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754206033,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgd3lh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6o8elg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1754216993,
            "send_replies": true,
            "parent_id": "t3_1mgd3lh",
            "score": 1,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "their code is avaible on github, just look at it...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6o8elg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;their code is avaible on github, just look at it...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6o8elg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754216993,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgd3lh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6of8i1",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "LagOps91",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6od1ji",
                                                    "score": 1,
                                                    "author_fullname": "t2_3wi6j7vwh",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "ARC-AGI style puzzles are a tiny part of the training data (if at all present) for language models. language models are general, they can do a ton of different things. so of course, a language model will not be nearly as good as a specialized model to solve ARC-AGI puzzles. it's like expecting an LLM to play chess at a superhuman level just because a purpose built ai chess engine can do it. it is not a fair apples to apples comparison.\n\nyou could absolutely take a language model and train it specifically to solve ARC-AGI puzzles, but you would ruin it's performance in other tasks in the process. for a narrow ai model, this isn't even a concern since there are no tasks for it to solve other than ARC-AGI puzzles.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6of8i1",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ARC-AGI style puzzles are a tiny part of the training data (if at all present) for language models. language models are general, they can do a ton of different things. so of course, a language model will not be nearly as good as a specialized model to solve ARC-AGI puzzles. it&amp;#39;s like expecting an LLM to play chess at a superhuman level just because a purpose built ai chess engine can do it. it is not a fair apples to apples comparison.&lt;/p&gt;\n\n&lt;p&gt;you could absolutely take a language model and train it specifically to solve ARC-AGI puzzles, but you would ruin it&amp;#39;s performance in other tasks in the process. for a narrow ai model, this isn&amp;#39;t even a concern since there are no tasks for it to solve other than ARC-AGI puzzles.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mgd3lh",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6of8i1/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754220579,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754220579,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6od1ji",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Substantial-Hippo165",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6o0sha",
                                          "score": 1,
                                          "author_fullname": "t2_kmsatmpt",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "The argument is that mathematics problem solving dont need natural language processing, if core ability is to solve problems then only that facet needs to be compared, that seems reasonable.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6od1ji",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The argument is that mathematics problem solving dont need natural language processing, if core ability is to solve problems then only that facet needs to be compared, that seems reasonable.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mgd3lh",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6od1ji/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754219485,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754219485,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6o0sha",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LagOps91",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6o0jhw",
                                "score": 2,
                                "author_fullname": "t2_3wi6j7vwh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "of course not. i never claimed that.  \n  \nbut if you compare yourself to language models, then you better be able to model language! otherwise, why compare yourself to those models in the first place?\n\nthere is nothing wrong with narrow ai. but to make narrow ai and benchmark it against general ai is just not a fair comparison to make.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6o0sha",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;of course not. i never claimed that.  &lt;/p&gt;\n\n&lt;p&gt;but if you compare yourself to language models, then you better be able to model language! otherwise, why compare yourself to those models in the first place?&lt;/p&gt;\n\n&lt;p&gt;there is nothing wrong with narrow ai. but to make narrow ai and benchmark it against general ai is just not a fair comparison to make.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgd3lh",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6o0sha/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754212492,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754212492,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6o0jhw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Former-Ad-5757",
                      "can_mod_post": false,
                      "created_utc": 1754212341,
                      "send_replies": true,
                      "parent_id": "t1_n6nvg43",
                      "score": 2,
                      "author_fullname": "t2_ihsdiwk6k",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Llms are not the end all of everything. A simple calculator tool is far less cheaper, if you can use this method to train all kinds of specialized tools then you can enhance the big llm output",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6o0jhw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "Llama 3"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Llms are not the end all of everything. A simple calculator tool is far less cheaper, if you can use this method to train all kinds of specialized tools then you can enhance the big llm output&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgd3lh",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6o0jhw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754212341,
                      "author_flair_text": "Llama 3",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#c7b594",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6nvg43",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1754209297,
            "send_replies": true,
            "parent_id": "t3_1mgd3lh",
            "score": 0,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's funny that suddenly everyone is surprised about this part now! I have pointed it out right when the paper has been posted here initially and it didn't get much attention.\n\nThe language is not 100% clear here, but even if there hasn't been any training done on the ARC-AGI dataset, it is clear that they have made a lot of slight variations of the kinds of visual puzzles that are part of ARC-AGI.  \n  \nI also want to point out that what they trained is \\*NOT\\* a language model, but an ARC-AGI puzzle solver! It doesn't understand language! It just take the puzzle example that demonstrates the rule (input + output example pair) as well as the input to generate a matching output applying the same same rule as in the example.\n\nWe do not know how well the architecture generalizes to other domains, including language, at all! I am quite puzzled (no pun intended) as to why they trained the model on puzzles only instead of training small language models to be able to make an apple to apple comparison between different model architectures. I would have loved to see some loss curve comparisons!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6nvg43",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s funny that suddenly everyone is surprised about this part now! I have pointed it out right when the paper has been posted here initially and it didn&amp;#39;t get much attention.&lt;/p&gt;\n\n&lt;p&gt;The language is not 100% clear here, but even if there hasn&amp;#39;t been any training done on the ARC-AGI dataset, it is clear that they have made a lot of slight variations of the kinds of visual puzzles that are part of ARC-AGI.  &lt;/p&gt;\n\n&lt;p&gt;I also want to point out that what they trained is *NOT* a language model, but an ARC-AGI puzzle solver! It doesn&amp;#39;t understand language! It just take the puzzle example that demonstrates the rule (input + output example pair) as well as the input to generate a matching output applying the same same rule as in the example.&lt;/p&gt;\n\n&lt;p&gt;We do not know how well the architecture generalizes to other domains, including language, at all! I am quite puzzled (no pun intended) as to why they trained the model on puzzles only instead of training small language models to be able to make an apple to apple comparison between different model architectures. I would have loved to see some loss curve comparisons!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6nvg43/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754209297,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgd3lh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6nt9ni",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SquashFront1303",
            "can_mod_post": false,
            "created_utc": 1754208015,
            "send_replies": true,
            "parent_id": "t3_1mgd3lh",
            "score": -2,
            "author_fullname": "t2_8jyw845p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If I am not wrong it sounds cheating",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6nt9ni",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If I am not wrong it sounds cheating&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/n6nt9ni/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754208015,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgd3lh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -2
          }
        }
      ],
      "before": null
    }
  }
]