[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?\n\nI have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?\n\nI don't mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.\n\nShould I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn't really matter to me too much. I'm happy to wait. \n\nThis is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. \n\nWhat llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. \n\nWhat quants?\n\nI want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? \n\nAny ideas, thoughts? Anyone tried to maximise long context processing?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How to maximise Prompt processing speed for long context usage?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhbp73",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_dmg3f1uq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754309485,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.&lt;/p&gt;\n\n&lt;p&gt;Should I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn&amp;#39;t really matter to me too much. I&amp;#39;m happy to wait. &lt;/p&gt;\n\n&lt;p&gt;This is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. &lt;/p&gt;\n\n&lt;p&gt;What llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. &lt;/p&gt;\n\n&lt;p&gt;What quants?&lt;/p&gt;\n\n&lt;p&gt;I want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? &lt;/p&gt;\n\n&lt;p&gt;Any ideas, thoughts? Anyone tried to maximise long context processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhbp73",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Ok-Kangaroo6055",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754309485,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6vmj7g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Efficiency_1144",
                      "can_mod_post": false,
                      "created_utc": 1754318442,
                      "send_replies": true,
                      "parent_id": "t1_n6v8cay",
                      "score": 1,
                      "author_fullname": "t2_1nkj9l14b0",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes the long context benchmarks are not that great.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6vmj7g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes the long context benchmarks are not that great.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhbp73",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6vmj7g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754318442,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6v8cay",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "LocoMod",
            "can_mod_post": false,
            "created_utc": 1754313926,
            "send_replies": true,
            "parent_id": "t3_1mhbp73",
            "score": 8,
            "author_fullname": "t2_6uuoq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you figure it out, let us know. This is the single biggest problem facing local LLM setups. It wont matter how smart these local models get if they fall apart after 8192 tokens or it takes 5 minutes to process 120000 tokens, and still forget half of it.\n\nOnce this is solved, then local LLM will really shine.\n\nDon't trust the benchmarks. Do your own long context testing and see for yourself how badly they will forget. This is why a robust RAG setup is ideal, but then you have to consider the time it takes to embed and retrieve the context, which will still be substantial for long texts.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v8cay",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you figure it out, let us know. This is the single biggest problem facing local LLM setups. It wont matter how smart these local models get if they fall apart after 8192 tokens or it takes 5 minutes to process 120000 tokens, and still forget half of it.&lt;/p&gt;\n\n&lt;p&gt;Once this is solved, then local LLM will really shine.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t trust the benchmarks. Do your own long context testing and see for yourself how badly they will forget. This is why a robust RAG setup is ideal, but then you have to consider the time it takes to embed and retrieve the context, which will still be substantial for long texts.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6v8cay/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754313926,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhbp73",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6uys98",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Ok-Kangaroo6055",
                      "can_mod_post": false,
                      "created_utc": 1754310554,
                      "send_replies": true,
                      "parent_id": "t1_n6uy9bi",
                      "score": 2,
                      "author_fullname": "t2_dmg3f1uq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm not expecting miracles on my pretty unsuitable rig. I'm just hoping for best possible results. 😕",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6uys98",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not expecting miracles on my pretty unsuitable rig. I&amp;#39;m just hoping for best possible results. 😕&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhbp73",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6uys98/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754310554,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6uy9bi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Alywan",
            "can_mod_post": false,
            "created_utc": 1754310357,
            "send_replies": true,
            "parent_id": "t3_1mhbp73",
            "score": 2,
            "author_fullname": "t2_q140j",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You could start with buying 30.000$ hardware",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uy9bi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You could start with buying 30.000$ hardware&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6uy9bi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754310357,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhbp73",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6v4qp4",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "anzzax",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6v3uyd",
                                          "score": 1,
                                          "author_fullname": "t2_zloia",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "In Option B you can reuse prefix with processed long context for multiple sessions, imagine enterprise rag on top of a stable knowledge base.   \nExplainer for Anthropic API but you can achieve similar with local LLMs: [https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6v4qp4",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In Option B you can reuse prefix with processed long context for multiple sessions, imagine enterprise rag on top of a stable knowledge base.&lt;br/&gt;\nExplainer for Anthropic API but you can achieve similar with local LLMs: &lt;a href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\"&gt;https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhbp73",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6v4qp4/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754312698,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754312698,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6v4zkn",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "knownboyofno",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6v3uyd",
                                          "score": 1,
                                          "author_fullname": "t2_5y9divj7",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "It does prompt caching of the unchanged part of the prompt. With the 1st one and it will have faster pp because it will hit the prompt caching more.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6v4zkn",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It does prompt caching of the unchanged part of the prompt. With the 1st one and it will have faster pp because it will hit the prompt caching more.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhbp73",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6v4zkn/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754312785,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754312785,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6v3uyd",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Ambitious_Tough7265",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6v1ccw",
                                "score": 1,
                                "author_fullname": "t2_ax0x38d9q",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "like what huge difference?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6v3uyd",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;like what huge difference?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhbp73",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6v3uyd/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754312386,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754312386,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6v1ccw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "anzzax",
                      "can_mod_post": false,
                      "created_utc": 1754311488,
                      "send_replies": true,
                      "parent_id": "t1_n6v000e",
                      "score": 2,
                      "author_fullname": "t2_zloia",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "And in regards to context management, you have to pay attention to prompt used in your tool of choice, there is a huge difference between:\n\n**Option A:**  \n\\`\\`\\`  \nAnswer my questions:\n\n1. q1\n2. q2\n3. ...\n\nWith a given context:  \n&lt;your-huge-context&gt;\n\n\\`\\`\\`\n\nand **Option B**:\n\n\\`\\`\\`  \nWith a given context:  \n&lt;your-huge-context&gt;\n\nAnswer my questions:\n\n1. q1\n2. q2\n3. ...\n\n\\`\\`\\`",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6v1ccw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And in regards to context management, you have to pay attention to prompt used in your tool of choice, there is a huge difference between:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Option A:&lt;/strong&gt;&lt;br/&gt;\n```&lt;br/&gt;\nAnswer my questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;q1&lt;/li&gt;\n&lt;li&gt;q2&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;With a given context:&lt;br/&gt;\n&amp;lt;your-huge-context&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;and &lt;strong&gt;Option B&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;```&lt;br/&gt;\nWith a given context:&lt;br/&gt;\n&amp;lt;your-huge-context&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;Answer my questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;q1&lt;/li&gt;\n&lt;li&gt;q2&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhbp73",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6v1ccw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754311488,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6v000e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "anzzax",
            "can_mod_post": false,
            "created_utc": 1754311004,
            "send_replies": true,
            "parent_id": "t3_1mhbp73",
            "score": 2,
            "author_fullname": "t2_zloia",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If the context is really long, it’s likely relatively stable. In that case, the main focus should probably be on smart context management and caching (e.g., https://github.com/LMCache/LMCache).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v000e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If the context is really long, it’s likely relatively stable. In that case, the main focus should probably be on smart context management and caching (e.g., &lt;a href=\"https://github.com/LMCache/LMCache\"&gt;https://github.com/LMCache/LMCache&lt;/a&gt;).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6v000e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754311004,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhbp73",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6v7fcq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kryptkpr",
            "can_mod_post": false,
            "created_utc": 1754313618,
            "send_replies": true,
            "parent_id": "t3_1mhbp73",
            "score": 1,
            "author_fullname": "t2_30i1a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Tweaking ubatch sizes up will help with long prompts, but generally this operation is compute bound.. better GPU, faster prompt progressing. I don't know anything about AMD, is that a \"good\" GPU? 3090 can pump 3-5 ktok/sec of prompt.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v7fcq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tweaking ubatch sizes up will help with long prompts, but generally this operation is compute bound.. better GPU, faster prompt progressing. I don&amp;#39;t know anything about AMD, is that a &amp;quot;good&amp;quot; GPU? 3090 can pump 3-5 ktok/sec of prompt.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6v7fcq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754313618,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1mhbp73",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6vmg20",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1754318415,
            "send_replies": true,
            "parent_id": "t3_1mhbp73",
            "score": 1,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I totally agree that putting whole text in context is better. Not much you can do in most cases about prompt processing speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6vmg20",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I totally agree that putting whole text in context is better. Not much you can do in most cases about prompt processing speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6vmg20/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754318415,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhbp73",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6w02f1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dr_Me_123",
            "can_mod_post": false,
            "created_utc": 1754322338,
            "send_replies": true,
            "parent_id": "t3_1mhbp73",
            "score": 1,
            "author_fullname": "t2_59yau29b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I’ll choose a high-performance NVIDIA GPU and a small-parameter dense model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6w02f1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’ll choose a high-performance NVIDIA GPU and a small-parameter dense model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6w02f1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754322338,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhbp73",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]