[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?\n\nI have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?\n\nI don't mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.\n\nShould I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn't really matter to me too much. I'm happy to wait. \n\nThis is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. \n\nWhat llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. \n\nWhat quants?\n\nI want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? \n\nAny ideas, thoughts? Anyone tried to maximise long context processing?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How to maximise Prompt processing speed for long context usage?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mhbp73",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_dmg3f1uq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754309485,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need a local llm  to be able to process 100-200k context in a reasonable timeframe. Does anyone know best llama.cpp flags to maximise Prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I have 16gb vram 9070 amd GPU and 32gb ram, which I may upgrade to 192gb ram at some point.  Ubuntu and windows. Had some driver problems with rocm but would that be significantly faster than Vulcan for prompt processing?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind super slow inference speed as long as its above 1t/s. If the prompt processing is at a reasonable speed.&lt;/p&gt;\n\n&lt;p&gt;Should I look towards moe models such as 30a3b qwen 3? Or better to go for 7b qwen? In any case prompt processing of such prompts is so slow I am not sure if either is viable. Anyone knows good tips or guide on how to maximise Prompt processing? Or benchmarks. Everything I Google seems to be about inference speed which doesn&amp;#39;t really matter to me too much. I&amp;#39;m happy to wait. &lt;/p&gt;\n\n&lt;p&gt;This is for batch processing long contexts. Mainly for full pdf ingestion because I find chunking and vectorisation unreliable at best. Putting the whole pdf text into context just performs better all the time essentially. &lt;/p&gt;\n\n&lt;p&gt;What llama cpp flags I should go for? Should I just offload kv cache to GPU while leaving the rest in ram? My local tests were very inconclusive at best. And very often would result in broken output anyway mostly with a3b qwen 3, where the llm would fall into repetition making the output mostly useless too. &lt;/p&gt;\n\n&lt;p&gt;What quants?&lt;/p&gt;\n\n&lt;p&gt;I want the llm to perform decently at that long context but maybe that is too much for these small LLMs. But there are 1m context qwen ggufs so one can hope? &lt;/p&gt;\n\n&lt;p&gt;Any ideas, thoughts? Anyone tried to maximise long context processing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhbp73",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Ok-Kangaroo6055",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/",
            "subreddit_subscribers": 509911,
            "created_utc": 1754309485,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6uys98",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Ok-Kangaroo6055",
                      "can_mod_post": false,
                      "created_utc": 1754310554,
                      "send_replies": true,
                      "parent_id": "t1_n6uy9bi",
                      "score": 1,
                      "author_fullname": "t2_dmg3f1uq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm not expecting miracles on my pretty unsuitable rig. I'm just hoping for best possible results. ðŸ˜•",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6uys98",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not expecting miracles on my pretty unsuitable rig. I&amp;#39;m just hoping for best possible results. ðŸ˜•&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhbp73",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6uys98/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754310554,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6uy9bi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Alywan",
            "can_mod_post": false,
            "created_utc": 1754310357,
            "send_replies": true,
            "parent_id": "t3_1mhbp73",
            "score": 1,
            "author_fullname": "t2_q140j",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You could start with buying 30.000$ hardware",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uy9bi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You could start with buying 30.000$ hardware&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhbp73/how_to_maximise_prompt_processing_speed_for_long/n6uy9bi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754310357,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhbp73",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]