[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone, I’m posting this on behalf of a student, who couldn’t post as he is new to reddit.\r\n\nOriginal post: \r\nI'm in the final stretch of my Master's thesis in computer science and wanted to share the simulation platform I've been building. I'm at the point where I'm designing my final experiments, and I would love to get some creative ideas from this community.\r\n\r\n**The Project: A Computer Simulation Platform with High-Fidelity Components**\r\n\r\nThe goal of my thesis is to study the dynamic interaction between **main memory and storage**. To do this, I've integrated three powerful simulation tools into a single, end-to-end framework:\r\n\r\n1. **The Host (gem5):** A full-system simulator that boots a real Linux kernel on a simulated ARM or x86 CPU. This runs the actual software stack.\r\n2. **The Main Memory (Ramulator):** A cycle-accurate DRAM simulator that models the detailed timings and internal state of a modern DDR memory subsystem. This lets me see the real effects of memory contention.\r\n3. **The Storage (SimpleSSD):** A high-fidelity NVMe SSD simulator that models the FTL, NAND channels, on-device cache, and different flash types.\r\n\r\nBasically, I've created a simulation platform where I can not only run real software but also swap out the hardware components at a very deep, architectural level. I can change the many things on the storage or the main memory side including but not limited to: SSD technology (MLC, TLC, ...), the flash timing parameters, or the memory from single-channel to dual-channel, and see the *true* system-level impact...\r\n\r\n**What I've Done So Far: I've Already Run** `llama.cpp`!\r\n\r\nTo prove the platform works, I've successfully run `llama.cpp` in the simulation to load the weights for a small model (\\~1B parameters) from the simulated SSD into the simulated RAM. It works! You can see the output:\r\n\r\n    root@aarch64-gem5:/home/root# ./llama/llama-cli -m ./fs/models/Llama-3.2-1B-Instruct-Q8_0.gguf --no-mmap -no-warmup --no-conversation -n 0\r\n    build: 5873 (f5e96b36) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for aarch64-linux-gnu\r\n    main: llama backend init\r\n    main: load the model and apply lora adapter, if any\r\n    llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from ./fs/models/Llama-3.2-1B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\r\n    llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\n    llama_model_loader: - kv Â  0: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.architecture str Â  Â  Â  Â  Â  Â  Â = llama\r\n    llama_model_loader: - kv Â  1: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.type str Â  Â  Â  Â  Â  Â  Â = model\r\n    llama_model_loader: - kv Â  2: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.name str Â  Â  Â  Â  Â  Â  Â = Llama 3.2 1B Instruct\r\n    llama_model_loader: - kv Â  3: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.organization str Â  Â  Â  Â  Â  Â  Â = Meta Llama\r\n    llama_model_loader: - kv Â  4: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.finetune str Â  Â  Â  Â  Â  Â  Â = Instruct\r\n    llama_model_loader: - kv Â  5: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.basename str Â  Â  Â  Â  Â  Â  Â = Llama-3.2\r\n    llama_model_loader: - kv Â  6: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.size_label str Â  Â  Â  Â  Â  Â  Â = 1B\r\n    llama_model_loader: - kv Â  7: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â llama.block_count u32 Â  Â  Â  Â  Â  Â  Â = 16\r\n    llama_model_loader: - kv Â  8: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.context_length u32 Â  Â  Â  Â  Â  Â  Â = 131072\r\n    llama_model_loader: - kv Â  9: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.embedding_length u32 Â  Â  Â  Â  Â  Â  Â = 2048\r\n    llama_model_loader: - kv Â 10: Â  Â  Â  Â  Â  Â  Â  Â  Â llama.feed_forward_length u32 Â  Â  Â  Â  Â  Â  Â = 8192\r\n    llama_model_loader: - kv Â 11: Â  Â  Â  Â  Â  Â  Â  Â  llama.attention.head_count u32 Â  Â  Â  Â  Â  Â  Â = 32\r\n    llama_model_loader: - kv Â 12: Â  Â  Â  Â  Â  Â  Â llama.attention.head_count_kv u32 Â  Â  Â  Â  Â  Â  Â = 8\r\n    llama_model_loader: - kv Â 13: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.rope.freq_base f32 Â  Â  Â  Â  Â  Â  Â = 500000.000000\r\n    llama_model_loader: - kv Â 14: Â  Â  llama.attention.layer_norm_rms_epsilon f32 Â  Â  Â  Â  Â  Â  Â = 0.000010\r\n    llama_model_loader: - kv Â 15: Â  Â  Â  Â  Â  Â  Â  Â  llama.attention.key_length u32 Â  Â  Â  Â  Â  Â  Â = 64\r\n    llama_model_loader: - kv Â 16: Â  Â  Â  Â  Â  Â  Â  llama.attention.value_length u32 Â  Â  Â  Â  Â  Â  Â = 64\r\n    llama_model_loader: - kv Â 17: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â general.file_type u32 Â  Â  Â  Â  Â  Â  Â = 7\r\n    llama_model_loader: - kv Â 18: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.vocab_size u32 Â  Â  Â  Â  Â  Â  Â = 128256\r\n    llama_model_loader: - kv Â 19: Â  Â  Â  Â  Â  Â  Â  Â  llama.rope.dimension_count u32 Â  Â  Â  Â  Â  Â  Â = 64\r\n    llama_model_loader: - kv Â 20: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tokenizer.ggml.model str Â  Â  Â  Â  Â  Â  Â = gpt2\r\n    llama_model_loader: - kv Â 21: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tokenizer.ggml.pre str Â  Â  Â  Â  Â  Â  Â = llama-bpe\r\n    llama_model_loader: - kv Â 22: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.tokens arr[str,128256] Â = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...\r\n    llama_model_loader: - kv Â 23: Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.token_type arr[i32,128256] Â = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n    llama_model_loader: - kv Â 24: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.merges arr[str,280147] Â = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\r\n    llama_model_loader: - kv Â 25: Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.bos_token_id u32 Â  Â  Â  Â  Â  Â  Â = 128000\r\n    llama_model_loader: - kv Â 26: Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.eos_token_id u32 Â  Â  Â  Â  Â  Â  Â = 128009\r\n    llama_model_loader: - kv Â 27: Â  Â  Â  Â  Â  Â tokenizer.ggml.padding_token_id u32 Â  Â  Â  Â  Â  Â  Â = 128004\r\n    llama_model_loader: - kv Â 28: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.chat_template str Â  Â  Â  Â  Â  Â  Â = {{- bos_token }}\\n{%- if custom_tools ...\r\n    llama_model_loader: - kv Â 29: Â  Â  Â  Â  Â  Â  Â  general.quantization_version u32 Â  Â  Â  Â  Â  Â  Â = 2\r\n    llama_model_loader: - type Â f32: Â  34 tensors\r\n    llama_model_loader: - type q8_0: Â 113 tensors\r\n    print_info: file format = GGUF V3 (latest)\r\n    print_info: file type Â  = Q8_0\r\n    print_info: file size Â  = 1.22 GiB (8.50 BPW) \r\n    load: special tokens cache size = 256\r\n    load: token to piece cache size = 0.7999 MB\r\n    print_info: arch Â  Â  Â  Â  Â  Â  = llama\r\n    print_info: vocab_only Â  Â  Â  = 0\r\n    print_info: n_ctx_train Â  Â  Â = 131072\r\n    print_info: n_embd Â  Â  Â  Â  Â  = 2048\r\n    print_info: n_layer Â  Â  Â  Â  Â = 16\r\n    print_info: n_head Â  Â  Â  Â  Â  = 32\r\n    print_info: n_head_kv Â  Â  Â  Â = 8\r\n    print_info: n_rot Â  Â  Â  Â  Â  Â = 64\r\n    print_info: n_swa Â  Â  Â  Â  Â  Â = 0\r\n    print_info: is_swa_any Â  Â  Â  = 0\r\n    print_info: n_embd_head_k Â  Â = 64\r\n    print_info: n_embd_head_v Â  Â = 64\r\n    print_info: n_gqa Â  Â  Â  Â  Â  Â = 4\r\n    print_info: n_embd_k_gqa Â  Â  = 512\r\n    print_info: n_embd_v_gqa Â  Â  = 512\r\n    print_info: f_norm_eps Â  Â  Â  = 0.0e+00\r\n    print_info: f_norm_rms_eps Â  = 1.0e-05\r\n    print_info: f_clamp_kqv Â  Â  Â = 0.0e+00\r\n    print_info: f_max_alibi_bias = 0.0e+00\r\n    print_info: f_logit_scale Â  Â = 0.0e+00\r\n    print_info: f_attn_scale Â  Â  = 0.0e+00\r\n    print_info: n_ff Â  Â  Â  Â  Â  Â  = 8192\r\n    print_info: n_expert Â  Â  Â  Â  = 0\r\n    print_info: n_expert_used Â  Â = 0\r\n    print_info: causal attn Â  Â  Â = 1\r\n    print_info: pooling type Â  Â  = 0\r\n    print_info: rope type Â  Â  Â  Â = 0\r\n    print_info: rope scaling Â  Â  = linear\r\n    print_info: freq_base_train Â = 500000.0\r\n    print_info: freq_scale_train = 1\r\n    print_info: n_ctx_orig_yarn Â = 131072\r\n    print_info: rope_finetuned Â  = unknown\r\n    print_info: model type Â  Â  Â  = 1B\r\n    print_info: model params Â  Â  = 1.24 B\r\n    print_info: general.name Â  Â  = Llama 3.2 1B Instruct\r\n    print_info: vocab type Â  Â  Â  = BPE\r\n    print_info: n_vocab Â  Â  Â  Â  Â = 128256\r\n    print_info: n_merges Â  Â  Â  Â  = 280147\r\n    print_info: BOS token Â  Â  Â  Â = 128000 '&lt;|begin_of_text|&gt;'\r\n    print_info: EOS token Â  Â  Â  Â = 128009 '&lt;|eot_id|&gt;'\r\n    print_info: EOT token Â  Â  Â  Â = 128009 '&lt;|eot_id|&gt;'\r\n    print_info: EOM token Â  Â  Â  Â = 128008 '&lt;|eom_id|&gt;'\r\n    print_info: PAD token Â  Â  Â  Â = 128004 '&lt;|finetune_right_pad_id|&gt;'\r\n    print_info: LF token Â  Â  Â  Â  = 198 'Ä'\r\n    print_info: EOG token Â  Â  Â  Â = 128001 '&lt;|end_of_text|&gt;'\r\n    print_info: EOG token Â  Â  Â  Â = 128008 '&lt;|eom_id|&gt;'\r\n    print_info: EOG token Â  Â  Â  Â = 128009 '&lt;|eot_id|&gt;'\r\n    print_info: max token length = 256\r\n    load_tensors: loading model tensors, this can take a while... (mmap = false)\r\n    load_tensors: Â  Â  Â  Â  Â CPU model buffer size = Â 1252.41 MiB\r\n    ..............................................................\r\n    llama_context: constructing llama_context\r\n    llama_context: n_seq_max Â  Â  = 1\r\n    llama_context: n_ctx Â  Â  Â  Â  = 4096\r\n    llama_context: n_ctx_per_seq = 4096\r\n    llama_context: n_batch Â  Â  Â  = 2048\r\n    llama_context: n_ubatch Â  Â  Â = 512\r\n    llama_context: causal_attn Â  = 1\r\n    llama_context: flash_attn Â  Â = 0\r\n    llama_context: freq_base Â  Â  = 500000.0\r\n    llama_context: freq_scale Â  Â = 1\r\n    llama_context: n_ctx_per_seq (4096) &lt; n_ctx_train (131072) -- the full capacity of the model will not be utilized\r\n    llama_context: Â  Â  Â  Â CPU Â output buffer size = Â  Â  0.49 MiB\r\n    llama_kv_cache_unified: Â  Â  Â  Â CPU KV buffer size = Â  128.00 MiB\r\n    llama_kv_cache_unified: size = Â 128.00 MiB ( Â 4096 cells, Â 16 layers, Â 1 seqs), K (f16): Â  64.00 MiB, V (f16): Â  64.00 MiB\r\n    llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\r\n    llama_context: Â  Â  Â  Â CPU compute buffer size = Â  280.01 MiB\r\n    llama_context: graph nodes Â = 582\r\n    llama_context: graph splits = 1\r\n    common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\r\n    main: llama threadpool init, n_threads = 2\r\n    \r\n    system_info: n_threads = 2 (n_threads_batch = 2) / 2 | CPU : NEON = 1 | ARM_FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \r\n    \r\n    sampler seed: 1968814452\r\n    sampler params: \r\n    Â  Â  repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\r\n    Â  Â  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\r\n    Â  Â  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\r\n    Â  Â  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\r\n    sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-n-sigma -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist \r\n    generate: n_ctx = 4096, n_batch = 2048, n_predict = 0, n_keep = 1\r\n    \r\n    \r\n    \r\n    llama_perf_sampler_print: Â  Â sampling time = Â  Â  Â  0.00 ms / Â  Â  0 runs Â  ( Â  Â  nan ms per token, Â  Â  Â nan tokens per second)\r\n    llama_perf_context_print: Â  Â  Â  Â load time = Â  Â 6928.00 ms\r\n    llama_perf_context_print: prompt eval time = Â  Â  Â  0.00 ms / Â  Â  1 tokens ( Â  Â 0.00 ms per token, Â  Â  Â inf tokens per second)\r\n    llama_perf_context_print: Â  Â  Â  Â eval time = Â  Â  Â  0.00 ms / Â  Â  1 runs Â  ( Â  Â 0.00 ms per token, Â  Â  Â inf tokens per second)\r\n    llama_perf_context_print: Â  Â  Â  total time = Â  Â 7144.00 ms / Â  Â  2 tokens\r\n\r\n**My Question for You: What Should I Explore Next?**\r\n\r\nNow that I have this platform, I want to run some interesting experiments focused on the impact of **storage and memory configurations** on LLM performance.\r\n\r\n**A quick note on scope:** My thesis is focused entirely on the memory and storage subsystems. While the CPU model is memory-latency aware, it's not a detailed out-of-order core, and simulating compute-intensive workloads like the full inference/training process takes a very long time. **Therefore, I'm primarily looking for experiments that stress the I/O and memory paths (like model loading), rather than the compute side of things.**\r\n\r\nHere are some of my initial thoughts:\r\n\r\n* **Time to first token:** How much does a super-fast (but expensive) SLC SSD improve the time to get the *first* token out, compared to a slower (but cheaper) QLC?\r\n* **Emerging Storage Technologies:** If there are any other storage technologies other than flash that are a strong candidate in the LLM era, feel free to discuss that as well.\r\n* **DRAM as the New Bottleneck:** If I simulate a futuristic PCIe Gen5 SSD, does the main memory speed (e.g., DDR5-4800 vs. DDR5-6000) become the actual bottleneck for loading?\r\n\r\nI'm really open to any ideas within this memory/storage scope. What performance mysteries about LLMs and system hardware have you always wanted to investigate?\r\n\r\nThank you for reading",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "I built a full-system computer simulation platform. What LLM experiments should I run?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mes3pu",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.56,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_9395938y",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754041251,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I’m posting this on behalf of a student, who couldn’t post as he is new to reddit.&lt;/p&gt;\n\n&lt;p&gt;Original post: \nI&amp;#39;m in the final stretch of my Master&amp;#39;s thesis in computer science and wanted to share the simulation platform I&amp;#39;ve been building. I&amp;#39;m at the point where I&amp;#39;m designing my final experiments, and I would love to get some creative ideas from this community.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Project: A Computer Simulation Platform with High-Fidelity Components&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The goal of my thesis is to study the dynamic interaction between &lt;strong&gt;main memory and storage&lt;/strong&gt;. To do this, I&amp;#39;ve integrated three powerful simulation tools into a single, end-to-end framework:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;The Host (gem5):&lt;/strong&gt; A full-system simulator that boots a real Linux kernel on a simulated ARM or x86 CPU. This runs the actual software stack.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Main Memory (Ramulator):&lt;/strong&gt; A cycle-accurate DRAM simulator that models the detailed timings and internal state of a modern DDR memory subsystem. This lets me see the real effects of memory contention.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Storage (SimpleSSD):&lt;/strong&gt; A high-fidelity NVMe SSD simulator that models the FTL, NAND channels, on-device cache, and different flash types.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Basically, I&amp;#39;ve created a simulation platform where I can not only run real software but also swap out the hardware components at a very deep, architectural level. I can change the many things on the storage or the main memory side including but not limited to: SSD technology (MLC, TLC, ...), the flash timing parameters, or the memory from single-channel to dual-channel, and see the &lt;em&gt;true&lt;/em&gt; system-level impact...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve Done So Far: I&amp;#39;ve Already Run&lt;/strong&gt; &lt;code&gt;llama.cpp&lt;/code&gt;!&lt;/p&gt;\n\n&lt;p&gt;To prove the platform works, I&amp;#39;ve successfully run &lt;code&gt;llama.cpp&lt;/code&gt; in the simulation to load the weights for a small model (~1B parameters) from the simulated SSD into the simulated RAM. It works! You can see the output:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root@aarch64-gem5:/home/root# ./llama/llama-cli -m ./fs/models/Llama-3.2-1B-Instruct-Q8_0.gguf --no-mmap -no-warmup --no-conversation -n 0\nbuild: 5873 (f5e96b36) with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for aarch64-linux-gnu\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from ./fs/models/Llama-3.2-1B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv Â  0: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.architecture str Â  Â  Â  Â  Â  Â  Â = llama\nllama_model_loader: - kv Â  1: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.type str Â  Â  Â  Â  Â  Â  Â = model\nllama_model_loader: - kv Â  2: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.name str Â  Â  Â  Â  Â  Â  Â = Llama 3.2 1B Instruct\nllama_model_loader: - kv Â  3: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.organization str Â  Â  Â  Â  Â  Â  Â = Meta Llama\nllama_model_loader: - kv Â  4: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.finetune str Â  Â  Â  Â  Â  Â  Â = Instruct\nllama_model_loader: - kv Â  5: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.basename str Â  Â  Â  Â  Â  Â  Â = Llama-3.2\nllama_model_loader: - kv Â  6: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  general.size_label str Â  Â  Â  Â  Â  Â  Â = 1B\nllama_model_loader: - kv Â  7: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â llama.block_count u32 Â  Â  Â  Â  Â  Â  Â = 16\nllama_model_loader: - kv Â  8: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.context_length u32 Â  Â  Â  Â  Â  Â  Â = 131072\nllama_model_loader: - kv Â  9: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.embedding_length u32 Â  Â  Â  Â  Â  Â  Â = 2048\nllama_model_loader: - kv Â 10: Â  Â  Â  Â  Â  Â  Â  Â  Â llama.feed_forward_length u32 Â  Â  Â  Â  Â  Â  Â = 8192\nllama_model_loader: - kv Â 11: Â  Â  Â  Â  Â  Â  Â  Â  llama.attention.head_count u32 Â  Â  Â  Â  Â  Â  Â = 32\nllama_model_loader: - kv Â 12: Â  Â  Â  Â  Â  Â  Â llama.attention.head_count_kv u32 Â  Â  Â  Â  Â  Â  Â = 8\nllama_model_loader: - kv Â 13: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.rope.freq_base f32 Â  Â  Â  Â  Â  Â  Â = 500000.000000\nllama_model_loader: - kv Â 14: Â  Â  llama.attention.layer_norm_rms_epsilon f32 Â  Â  Â  Â  Â  Â  Â = 0.000010\nllama_model_loader: - kv Â 15: Â  Â  Â  Â  Â  Â  Â  Â  llama.attention.key_length u32 Â  Â  Â  Â  Â  Â  Â = 64\nllama_model_loader: - kv Â 16: Â  Â  Â  Â  Â  Â  Â  llama.attention.value_length u32 Â  Â  Â  Â  Â  Â  Â = 64\nllama_model_loader: - kv Â 17: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â general.file_type u32 Â  Â  Â  Â  Â  Â  Â = 7\nllama_model_loader: - kv Â 18: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llama.vocab_size u32 Â  Â  Â  Â  Â  Â  Â = 128256\nllama_model_loader: - kv Â 19: Â  Â  Â  Â  Â  Â  Â  Â  llama.rope.dimension_count u32 Â  Â  Â  Â  Â  Â  Â = 64\nllama_model_loader: - kv Â 20: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tokenizer.ggml.model str Â  Â  Â  Â  Â  Â  Â = gpt2\nllama_model_loader: - kv Â 21: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tokenizer.ggml.pre str Â  Â  Â  Â  Â  Â  Â = llama-bpe\nllama_model_loader: - kv Â 22: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.tokens arr[str,128256] Â = [&amp;quot;!&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;&amp;#39;&amp;quot;, ...\nllama_model_loader: - kv Â 23: Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.token_type arr[i32,128256] Â = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv Â 24: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.merges arr[str,280147] Â = [&amp;quot;Ä  Ä &amp;quot;, &amp;quot;Ä  Ä Ä Ä &amp;quot;, &amp;quot;Ä Ä  Ä Ä &amp;quot;, &amp;quot;...\nllama_model_loader: - kv Â 25: Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.bos_token_id u32 Â  Â  Â  Â  Â  Â  Â = 128000\nllama_model_loader: - kv Â 26: Â  Â  Â  Â  Â  Â  Â  Â tokenizer.ggml.eos_token_id u32 Â  Â  Â  Â  Â  Â  Â = 128009\nllama_model_loader: - kv Â 27: Â  Â  Â  Â  Â  Â tokenizer.ggml.padding_token_id u32 Â  Â  Â  Â  Â  Â  Â = 128004\nllama_model_loader: - kv Â 28: Â  Â  Â  Â  Â  Â  Â  Â  Â  Â tokenizer.chat_template str Â  Â  Â  Â  Â  Â  Â = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv Â 29: Â  Â  Â  Â  Â  Â  Â  general.quantization_version u32 Â  Â  Â  Â  Â  Â  Â = 2\nllama_model_loader: - type Â f32: Â  34 tensors\nllama_model_loader: - type q8_0: Â 113 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type Â  = Q8_0\nprint_info: file size Â  = 1.22 GiB (8.50 BPW) \nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch Â  Â  Â  Â  Â  Â  = llama\nprint_info: vocab_only Â  Â  Â  = 0\nprint_info: n_ctx_train Â  Â  Â = 131072\nprint_info: n_embd Â  Â  Â  Â  Â  = 2048\nprint_info: n_layer Â  Â  Â  Â  Â = 16\nprint_info: n_head Â  Â  Â  Â  Â  = 32\nprint_info: n_head_kv Â  Â  Â  Â = 8\nprint_info: n_rot Â  Â  Â  Â  Â  Â = 64\nprint_info: n_swa Â  Â  Â  Â  Â  Â = 0\nprint_info: is_swa_any Â  Â  Â  = 0\nprint_info: n_embd_head_k Â  Â = 64\nprint_info: n_embd_head_v Â  Â = 64\nprint_info: n_gqa Â  Â  Â  Â  Â  Â = 4\nprint_info: n_embd_k_gqa Â  Â  = 512\nprint_info: n_embd_v_gqa Â  Â  = 512\nprint_info: f_norm_eps Â  Â  Â  = 0.0e+00\nprint_info: f_norm_rms_eps Â  = 1.0e-05\nprint_info: f_clamp_kqv Â  Â  Â = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale Â  Â = 0.0e+00\nprint_info: f_attn_scale Â  Â  = 0.0e+00\nprint_info: n_ff Â  Â  Â  Â  Â  Â  = 8192\nprint_info: n_expert Â  Â  Â  Â  = 0\nprint_info: n_expert_used Â  Â = 0\nprint_info: causal attn Â  Â  Â = 1\nprint_info: pooling type Â  Â  = 0\nprint_info: rope type Â  Â  Â  Â = 0\nprint_info: rope scaling Â  Â  = linear\nprint_info: freq_base_train Â = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn Â = 131072\nprint_info: rope_finetuned Â  = unknown\nprint_info: model type Â  Â  Â  = 1B\nprint_info: model params Â  Â  = 1.24 B\nprint_info: general.name Â  Â  = Llama 3.2 1B Instruct\nprint_info: vocab type Â  Â  Â  = BPE\nprint_info: n_vocab Â  Â  Â  Â  Â = 128256\nprint_info: n_merges Â  Â  Â  Â  = 280147\nprint_info: BOS token Â  Â  Â  Â = 128000 &amp;#39;&amp;lt;|begin_of_text|&amp;gt;&amp;#39;\nprint_info: EOS token Â  Â  Â  Â = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;\nprint_info: EOT token Â  Â  Â  Â = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;\nprint_info: EOM token Â  Â  Â  Â = 128008 &amp;#39;&amp;lt;|eom_id|&amp;gt;&amp;#39;\nprint_info: PAD token Â  Â  Â  Â = 128004 &amp;#39;&amp;lt;|finetune_right_pad_id|&amp;gt;&amp;#39;\nprint_info: LF token Â  Â  Â  Â  = 198 &amp;#39;Ä&amp;#39;\nprint_info: EOG token Â  Â  Â  Â = 128001 &amp;#39;&amp;lt;|end_of_text|&amp;gt;&amp;#39;\nprint_info: EOG token Â  Â  Â  Â = 128008 &amp;#39;&amp;lt;|eom_id|&amp;gt;&amp;#39;\nprint_info: EOG token Â  Â  Â  Â = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = false)\nload_tensors: Â  Â  Â  Â  Â CPU model buffer size = Â 1252.41 MiB\n..............................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max Â  Â  = 1\nllama_context: n_ctx Â  Â  Â  Â  = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch Â  Â  Â  = 2048\nllama_context: n_ubatch Â  Â  Â = 512\nllama_context: causal_attn Â  = 1\nllama_context: flash_attn Â  Â = 0\nllama_context: freq_base Â  Â  = 500000.0\nllama_context: freq_scale Â  Â = 1\nllama_context: n_ctx_per_seq (4096) &amp;lt; n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context: Â  Â  Â  Â CPU Â output buffer size = Â  Â  0.49 MiB\nllama_kv_cache_unified: Â  Â  Â  Â CPU KV buffer size = Â  128.00 MiB\nllama_kv_cache_unified: size = Â 128.00 MiB ( Â 4096 cells, Â 16 layers, Â 1 seqs), K (f16): Â  64.00 MiB, V (f16): Â  64.00 MiB\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\nllama_context: Â  Â  Â  Â CPU compute buffer size = Â  280.01 MiB\nllama_context: graph nodes Â = 582\nllama_context: graph splits = 1\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\nmain: llama threadpool init, n_threads = 2\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 2 | CPU : NEON = 1 | ARM_FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nsampler seed: 1968814452\nsampler params: \nÂ  Â  repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\nÂ  Â  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\nÂ  Â  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\nÂ  Â  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -&amp;gt; logit-bias -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 0, n_keep = 1\n\n\n\nllama_perf_sampler_print: Â  Â sampling time = Â  Â  Â  0.00 ms / Â  Â  0 runs Â  ( Â  Â  nan ms per token, Â  Â  Â nan tokens per second)\nllama_perf_context_print: Â  Â  Â  Â load time = Â  Â 6928.00 ms\nllama_perf_context_print: prompt eval time = Â  Â  Â  0.00 ms / Â  Â  1 tokens ( Â  Â 0.00 ms per token, Â  Â  Â inf tokens per second)\nllama_perf_context_print: Â  Â  Â  Â eval time = Â  Â  Â  0.00 ms / Â  Â  1 runs Â  ( Â  Â 0.00 ms per token, Â  Â  Â inf tokens per second)\nllama_perf_context_print: Â  Â  Â  total time = Â  Â 7144.00 ms / Â  Â  2 tokens\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;My Question for You: What Should I Explore Next?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now that I have this platform, I want to run some interesting experiments focused on the impact of &lt;strong&gt;storage and memory configurations&lt;/strong&gt; on LLM performance.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;A quick note on scope:&lt;/strong&gt; My thesis is focused entirely on the memory and storage subsystems. While the CPU model is memory-latency aware, it&amp;#39;s not a detailed out-of-order core, and simulating compute-intensive workloads like the full inference/training process takes a very long time. &lt;strong&gt;Therefore, I&amp;#39;m primarily looking for experiments that stress the I/O and memory paths (like model loading), rather than the compute side of things.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are some of my initial thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Time to first token:&lt;/strong&gt; How much does a super-fast (but expensive) SLC SSD improve the time to get the &lt;em&gt;first&lt;/em&gt; token out, compared to a slower (but cheaper) QLC?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Emerging Storage Technologies:&lt;/strong&gt; If there are any other storage technologies other than flash that are a strong candidate in the LLM era, feel free to discuss that as well.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DRAM as the New Bottleneck:&lt;/strong&gt; If I simulate a futuristic PCIe Gen5 SSD, does the main memory speed (e.g., DDR5-4800 vs. DDR5-6000) become the actual bottleneck for loading?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m really open to any ideas within this memory/storage scope. What performance mysteries about LLMs and system hardware have you always wanted to investigate?&lt;/p&gt;\n\n&lt;p&gt;Thank you for reading&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mes3pu",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Rachados22x2",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mes3pu/i_built_a_fullsystem_computer_simulation_platform/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mes3pu/i_built_a_fullsystem_computer_simulation_platform/",
            "subreddit_subscribers": 508541,
            "created_utc": 1754041251,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bt3jc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754045420,
            "send_replies": true,
            "parent_id": "t3_1mes3pu",
            "score": 2,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; DRAM as the New Bottleneck: If I simulate a futuristic PCIe Gen5 SSD, does the main memory speed (e.g., DDR5-4800 vs. DDR5-6000) become the actual bottleneck for loading?\n\nI'm not sure if I got this right, but the RAM could not be a bottleneck because it is much faster than even the fastest SSDs.\n\n&gt; What performance mysteries about LLMs and system hardware have you always wanted to investigate?\n\nI'm interested in memory ranks, everybody says that \"dual rank\" memory is faster but in my tests \"single rank\" modules with the same MT/s rating were faster than dual rank. A scientific proof would be nice.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bt3jc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;DRAM as the New Bottleneck: If I simulate a futuristic PCIe Gen5 SSD, does the main memory speed (e.g., DDR5-4800 vs. DDR5-6000) become the actual bottleneck for loading?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if I got this right, but the RAM could not be a bottleneck because it is much faster than even the fastest SSDs.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;What performance mysteries about LLMs and system hardware have you always wanted to investigate?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;m interested in memory ranks, everybody says that &amp;quot;dual rank&amp;quot; memory is faster but in my tests &amp;quot;single rank&amp;quot; modules with the same MT/s rating were faster than dual rank. A scientific proof would be nice.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mes3pu/i_built_a_fullsystem_computer_simulation_platform/n6bt3jc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754045420,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mes3pu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "f1ee0406-72f3-11ee-a31d-3a87eb85541f",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bltg5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "The_Duke_Of_Zill",
            "can_mod_post": false,
            "created_utc": 1754041665,
            "send_replies": true,
            "parent_id": "t3_1mes3pu",
            "score": 2,
            "author_fullname": "t2_bdbly5rd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Maybe you could test the difference in inference speed between RAM with different CAS latencies.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bltg5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Waiting for Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe you could test the difference in inference speed between RAM with different CAS latencies.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mes3pu/i_built_a_fullsystem_computer_simulation_platform/n6bltg5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754041665,
            "author_flair_text": "Waiting for Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1mes3pu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6d8imo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Red_Redditor_Reddit",
            "can_mod_post": false,
            "created_utc": 1754062693,
            "send_replies": true,
            "parent_id": "t3_1mes3pu",
            "score": 2,
            "author_fullname": "t2_8eelmfjg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm super confused as to what you're trying to do.  Once the model is loaded, overwhelmingly the bottleneck is the memory speed.  Everything else might as well not even exist at that point.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6d8imo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m super confused as to what you&amp;#39;re trying to do.  Once the model is loaded, overwhelmingly the bottleneck is the memory speed.  Everything else might as well not even exist at that point.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mes3pu/i_built_a_fullsystem_computer_simulation_platform/n6d8imo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754062693,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mes3pu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]