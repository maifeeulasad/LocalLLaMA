[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "It's a been a great 6 months to be using local AI as the performance delta has, on average, been very low for classic LLMs, with R1 typically being at or near SOTA, and smaller models consistently getting better and better benchmarks.  \n  \nHowever, the below are all things where there has been a surprising lag between closed systems' release dates and the availability of high quality local alternatives  \n  \n1. A voice mode that is on par with Chat Gpt. Most all the pieces are in place to have something akin to 4o with voice. Sesame, Kyutai, or Chatterbox for TTS, any local model for the LLM, decent STT is, I think, also a thing already. We just need the parts put together in a fairly user-friendly, fast streaming package.  \n  \n2. Local deep research on the level of o3's web search. o3 is quite amazing now in its ability to rapidly search several web pages to answer questions. There are some solutions for local llms but none that I've tried seem to be fulfilling the potential of web search agents with clever and easily customizable workflows. I would be fine with a much slower process if the answers were as good. Something like Qwen 235b I believe could do a great job of being the foundation of such an agent.  \n  \n3. A local visual llm that can reliably read any human-legible document. Maverick is quite good but not nearly as good as Gemini Pro or Chat GPT at this.\n\nWhat else am I forgetting about?\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Where local is lagging behind... Wish lists for the rest of 2025",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lxn8ry",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.88,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_syq52",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752280957,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a been a great 6 months to be using local AI as the performance delta has, on average, been very low for classic LLMs, with R1 typically being at or near SOTA, and smaller models consistently getting better and better benchmarks.  &lt;/p&gt;\n\n&lt;p&gt;However, the below are all things where there has been a surprising lag between closed systems&amp;#39; release dates and the availability of high quality local alternatives  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;A voice mode that is on par with Chat Gpt. Most all the pieces are in place to have something akin to 4o with voice. Sesame, Kyutai, or Chatterbox for TTS, any local model for the LLM, decent STT is, I think, also a thing already. We just need the parts put together in a fairly user-friendly, fast streaming package.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Local deep research on the level of o3&amp;#39;s web search. o3 is quite amazing now in its ability to rapidly search several web pages to answer questions. There are some solutions for local llms but none that I&amp;#39;ve tried seem to be fulfilling the potential of web search agents with clever and easily customizable workflows. I would be fine with a much slower process if the answers were as good. Something like Qwen 235b I believe could do a great job of being the foundation of such an agent.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A local visual llm that can reliably read any human-legible document. Maverick is quite good but not nearly as good as Gemini Pro or Chat GPT at this.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What else am I forgetting about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lxn8ry",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "nomorebuttsplz",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/",
            "subreddit_subscribers": 497825,
            "created_utc": 1752280957,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2nopqk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Secure_Reflection409",
            "can_mod_post": false,
            "created_utc": 1752284894,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 15,
            "author_fullname": "t2_by77ogdhr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hardware is our biggest issue.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2nopqk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hardware is our biggest issue.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2nopqk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752284894,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 15
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2nmxfx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "dinerburgeryum",
            "can_mod_post": false,
            "created_utc": 1752284238,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 3,
            "author_fullname": "t2_1j53p3yv3e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Seconding real-time voice mode. I kind of agree with your point that all the pieces are there; we have Qwen Omni, which absolutely supports streaming ingest and generation, but none of the code released demonstrates its usage. This is unfortunately a software problem, and not one that's easy to overcome.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2nmxfx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Seconding real-time voice mode. I kind of agree with your point that all the pieces are there; we have Qwen Omni, which absolutely supports streaming ingest and generation, but none of the code released demonstrates its usage. This is unfortunately a software problem, and not one that&amp;#39;s easy to overcome.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2nmxfx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752284238,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2nmhy0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SlowFail2433",
            "can_mod_post": false,
            "created_utc": 1752284082,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 1,
            "author_fullname": "t2_131eezppgs",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For LLMs vision abilities and long context abilities\nAlso image and audio generation by LLMs\n\n\nFor diffusion we are just not close to parity at all",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2nmhy0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For LLMs vision abilities and long context abilities\nAlso image and audio generation by LLMs&lt;/p&gt;\n\n&lt;p&gt;For diffusion we are just not close to parity at all&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2nmhy0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752284082,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2ozgod",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Mkengine",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2oqraj",
                                "score": 1,
                                "author_fullname": "t2_9p2xe",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I don't want to say I have a solution to your problems, but here are some links that may help for them:\n\n1. https://github.com/QwenLM/Qwen2.5-Omni\n\n2. https://github.com/DavidZWZ/Awesome-Deep-Research\n\n3. https://github.com/GiftMungmeeprued/document-parsers-list\n\nwhat are your thoughts?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2ozgod",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t want to say I have a solution to your problems, but here are some links that may help for them:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/QwenLM/Qwen2.5-Omni\"&gt;https://github.com/QwenLM/Qwen2.5-Omni&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/DavidZWZ/Awesome-Deep-Research\"&gt;https://github.com/DavidZWZ/Awesome-Deep-Research&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/GiftMungmeeprued/document-parsers-list\"&gt;https://github.com/GiftMungmeeprued/document-parsers-list&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;what are your thoughts?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lxn8ry",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2ozgod/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752306496,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752306496,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2oqraj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "nomorebuttsplz",
                      "can_mod_post": false,
                      "created_utc": 1752301534,
                      "send_replies": true,
                      "parent_id": "t1_n2oa9na",
                      "score": 2,
                      "author_fullname": "t2_syq52",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have a mac studio 512 gb",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2oqraj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a mac studio 512 gb&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxn8ry",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2oqraj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752301534,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2oa9na",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Square-Onion-1825",
            "can_mod_post": false,
            "created_utc": 1752293382,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 1,
            "author_fullname": "t2_1mkh7x2yxn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "i'm curious, what is your h/w set up? i'm buying a system that i will dedicate to AI dev and RAG work, but i'm kinda new at this...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2oa9na",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m curious, what is your h/w set up? i&amp;#39;m buying a system that i will dedicate to AI dev and RAG work, but i&amp;#39;m kinda new at this...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2oa9na/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752293382,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2ojz09",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "My_Unbiased_Opinion",
            "can_mod_post": false,
            "created_utc": 1752297930,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 1,
            "author_fullname": "t2_esiyl0yb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What I want is tool use web search inside the reasoning chain so the model can get information directly from the web while it reasons. Should massively increase quality without needing any more VRAM. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2ojz09",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What I want is tool use web search inside the reasoning chain so the model can get information directly from the web while it reasons. Should massively increase quality without needing any more VRAM. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2ojz09/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752297930,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2p6g88",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LevianMcBirdo",
            "can_mod_post": false,
            "created_utc": 1752310655,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 2,
            "author_fullname": "t2_cw9f6o0r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The deep research part is probably mostly a setup problem. Perplexity uses R1 for its own deep research and it's at least on par with o3 web search, probably better. Also their labs isn't bad either and probably uses mostly R1",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2p6g88",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The deep research part is probably mostly a setup problem. Perplexity uses R1 for its own deep research and it&amp;#39;s at least on par with o3 web search, probably better. Also their labs isn&amp;#39;t bad either and probably uses mostly R1&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2p6g88/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752310655,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2no7uz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "offlinesir",
            "can_mod_post": false,
            "created_utc": 1752284712,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 1,
            "author_fullname": "t2_jn5ft2le",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We are missing text to video at the level of Veo 2, 3, or even Sora. We are also missing personalized LLM's at the level of OpenAI's memory feature.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2no7uz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We are missing text to video at the level of Veo 2, 3, or even Sora. We are also missing personalized LLM&amp;#39;s at the level of OpenAI&amp;#39;s memory feature.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2no7uz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752284712,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2pyp5e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lorian0x7",
            "can_mod_post": false,
            "created_utc": 1752324987,
            "send_replies": true,
            "parent_id": "t3_1lxn8ry",
            "score": 1,
            "author_fullname": "t2_9ulpagz7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I wish for a local model that can run with 8 gb vram, or locally  on smartphone and it's as good a 4o",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2pyp5e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wish for a local model that can run with 8 gb vram, or locally  on smartphone and it&amp;#39;s as good a 4o&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2pyp5e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752324987,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxn8ry",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]