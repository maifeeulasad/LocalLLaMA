[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.\n\nFor example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. \n\nSo to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.\n\nIf I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers \\~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!\n\nGo as big as you can!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "N + N size GPU != 2N sized GPU, go big if you can",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8vu80",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.83,
            "author_flair_background_color": "#bbbdbf",
            "subreddit_type": "public",
            "ups": 34,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "is_original_content": false,
            "author_fullname": "t2_ah13x",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 34,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753440200,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Buy the largest GPU that you can really afford to.  Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc.   Multiple GPUs can be annoying.&lt;/p&gt;\n\n&lt;p&gt;For example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb.   If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. &lt;/p&gt;\n\n&lt;p&gt;So to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.&lt;/p&gt;\n\n&lt;p&gt;If I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers ~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context.  More context with same sized GPU, and it would be faster too!&lt;/p&gt;\n\n&lt;p&gt;Go as big as you can!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1m8vu80",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "segmond",
            "discussion_type": null,
            "num_comments": 22,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "light",
            "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753440200,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n55hif6",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DepthHour1669",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n55f2ra",
                                "score": 3,
                                "author_fullname": "t2_t6glzswk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What cpu? Core count and mhz matters.\n\nAlso what amount of RAM and which quant are you using? That changes the numbers.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n55hif6",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What cpu? Core count and mhz matters.&lt;/p&gt;\n\n&lt;p&gt;Also what amount of RAM and which quant are you using? That changes the numbers.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vu80",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n55hif6/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753475738,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753475738,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n55f2ra",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "segmond",
                      "can_mod_post": false,
                      "created_utc": 1753475029,
                      "send_replies": true,
                      "parent_id": "t1_n557a8i",
                      "score": 0,
                      "author_fullname": "t2_ah13x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "okay, you might be right.  it's a budget build, xeon v3 on x99, ddr 2440mhz, quad channel, pice3 x16 and x8.  show me how to make it better and to load it with llama.cpp, i have multi GPU that I'll like to utilize.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n55f2ra",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;okay, you might be right.  it&amp;#39;s a budget build, xeon v3 on x99, ddr 2440mhz, quad channel, pice3 x16 and x8.  show me how to make it better and to load it with llama.cpp, i have multi GPU that I&amp;#39;ll like to utilize.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vu80",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n55f2ra/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753475029,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n557a8i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DepthHour1669",
            "can_mod_post": false,
            "created_utc": 1753472746,
            "send_replies": true,
            "parent_id": "t3_1m8vu80",
            "score": 8,
            "author_fullname": "t2_t6glzswk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I’m pretty sure your setup is doing it wrong. \n\nI think it’ll be faster if you only use 1 MI50 16gb and leave the rest idle.\n\nContext (KV cache) is loaded into the first layer, and gets updated by the last layer. \n\nYou shouldn’t be splitting context into each GPU. That means you’re limited by PCIe bandwidth to read the context into the layer 1 GPU, not vram speed.\n\nKimi K2 layer 1 is the only dense layer, and it’s a lot smaller than the other 60 layers (about 2gb) so you should be putting it fully on the first GPU along with context.\n\nThe slowest part of your system are the 32GB/sec pcie links. Your system RAM should be substantially faster than that, DDR5 dual channel starts at 76GB/s and goes up to 614GB/s. \n\nSo that means (if your CPU is infinitely fast) if you are limited by memory bandwidth and not compute. You should keep KV cache on gpu 1, layer 1 on gpu 1, and offload the rest of the layers to RAM.  \n\nIf your cpu is slow, then all the math should be done on the GPU. In which case you’re limited by the 32GB/s bandwidth no matter what. \n\nTECHNICALLY kv cache is bigger max size than the MoE active weights (14gb at max context vs 11gb constant) so if you’re always at max context, you’d rather move the MoE weights around the pcie bus and keep the context on 1 GPU. That means you should keep kv cache and layer 1 on gpu 1, but also layer 2-61 attention and shared expert on gpu 1. And the rest of the weights (which are the MoE experts) on system RAM. \n\nMost likely, you have a slow cpu and you’re not always at max context and kv cache is closer to 0GB than 14GB. In that case you want to pass the latent representation (data between each layer) between the GPUs. In that case you put only kv cache and layer 1 on the GPU1, and then put the attention and shared expert for 3-4 layers on each of the rest of the GPUs, plus as much MoE as you can load. So GPU 2 holds the common weights for layer 2-4 plus as much MoE for layer 2-4 as it can fit, etc. This presumes the inference engine is using DMA to stream KV cache and not needing to load it into gpu vram, because this absolutely would not work if it needs to load it into gpu VRAM first.\n\n- what cpu(s) do you have?\n\n- what ram (DDR4, DDR5) and what speed (4800,\n5200, etc) is it?\n\n- how many channels of RAM? Dual channel? 1DPC or 2DPC?\n\n- are the GPUs on PCIE3 risers? PCIE4? X16 or x8?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n557a8i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m pretty sure your setup is doing it wrong. &lt;/p&gt;\n\n&lt;p&gt;I think it’ll be faster if you only use 1 MI50 16gb and leave the rest idle.&lt;/p&gt;\n\n&lt;p&gt;Context (KV cache) is loaded into the first layer, and gets updated by the last layer. &lt;/p&gt;\n\n&lt;p&gt;You shouldn’t be splitting context into each GPU. That means you’re limited by PCIe bandwidth to read the context into the layer 1 GPU, not vram speed.&lt;/p&gt;\n\n&lt;p&gt;Kimi K2 layer 1 is the only dense layer, and it’s a lot smaller than the other 60 layers (about 2gb) so you should be putting it fully on the first GPU along with context.&lt;/p&gt;\n\n&lt;p&gt;The slowest part of your system are the 32GB/sec pcie links. Your system RAM should be substantially faster than that, DDR5 dual channel starts at 76GB/s and goes up to 614GB/s. &lt;/p&gt;\n\n&lt;p&gt;So that means (if your CPU is infinitely fast) if you are limited by memory bandwidth and not compute. You should keep KV cache on gpu 1, layer 1 on gpu 1, and offload the rest of the layers to RAM.  &lt;/p&gt;\n\n&lt;p&gt;If your cpu is slow, then all the math should be done on the GPU. In which case you’re limited by the 32GB/s bandwidth no matter what. &lt;/p&gt;\n\n&lt;p&gt;TECHNICALLY kv cache is bigger max size than the MoE active weights (14gb at max context vs 11gb constant) so if you’re always at max context, you’d rather move the MoE weights around the pcie bus and keep the context on 1 GPU. That means you should keep kv cache and layer 1 on gpu 1, but also layer 2-61 attention and shared expert on gpu 1. And the rest of the weights (which are the MoE experts) on system RAM. &lt;/p&gt;\n\n&lt;p&gt;Most likely, you have a slow cpu and you’re not always at max context and kv cache is closer to 0GB than 14GB. In that case you want to pass the latent representation (data between each layer) between the GPUs. In that case you put only kv cache and layer 1 on the GPU1, and then put the attention and shared expert for 3-4 layers on each of the rest of the GPUs, plus as much MoE as you can load. So GPU 2 holds the common weights for layer 2-4 plus as much MoE for layer 2-4 as it can fit, etc. This presumes the inference engine is using DMA to stream KV cache and not needing to load it into gpu vram, because this absolutely would not work if it needs to load it into gpu VRAM first.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;what cpu(s) do you have?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;what ram (DDR4, DDR5) and what speed (4800,\n5200, etc) is it?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;how many channels of RAM? Dual channel? 1DPC or 2DPC?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;are the GPUs on PCIE3 risers? PCIE4? X16 or x8?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n557a8i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753472746,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vu80",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n55hv59",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "holchansg",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n52ra2m",
                                "score": 1,
                                "author_fullname": "t2_ppu9v",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "My experience is similar, the less GPUs the better.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n55hv59",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My experience is similar, the less GPUs the better.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vu80",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n55hv59/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753475841,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753475841,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n52ra2m",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "segmond",
                      "can_mod_post": false,
                      "created_utc": 1753447119,
                      "send_replies": true,
                      "parent_id": "t1_n52codd",
                      "score": -1,
                      "author_fullname": "t2_ah13x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I know how to override tensor, I'm loading the entire model in VRAM, my point is that I get less context.   If you load different weights from different layers in different GPU, performance goes down, since data has to be moved across GPUs.   I have over 400gb in VRAM, I think I know what I'm doing.   My point is if you can buy 32gb, get it instead of multiple 16gb, go as big as you can especially if you plan to run huge models.  If I had 3 48gb models.  I would be able to load 6 layers into each for a total of 18 layers, and have enough free ram to get to 20k context easy.\n\n3x48gb = 144gb vram and I can get 18layers at 20k context\n\n10x16gb = 160gb vram, I can get 20layers at 4k context or 10layers to get more context.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52ra2m",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I know how to override tensor, I&amp;#39;m loading the entire model in VRAM, my point is that I get less context.   If you load different weights from different layers in different GPU, performance goes down, since data has to be moved across GPUs.   I have over 400gb in VRAM, I think I know what I&amp;#39;m doing.   My point is if you can buy 32gb, get it instead of multiple 16gb, go as big as you can especially if you plan to run huge models.  If I had 3 48gb models.  I would be able to load 6 layers into each for a total of 18 layers, and have enough free ram to get to 20k context easy.&lt;/p&gt;\n\n&lt;p&gt;3x48gb = 144gb vram and I can get 18layers at 20k context&lt;/p&gt;\n\n&lt;p&gt;10x16gb = 160gb vram, I can get 20layers at 4k context or 10layers to get more context.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vu80",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n52ra2m/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753447119,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52codd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1753441246,
            "send_replies": true,
            "parent_id": "t3_1m8vu80",
            "score": 5,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you're using llama.cpp or similar, have you tried using `--override-tensor` to only put attention tensors in GPUs? I.e. putting all `ffn` tensors on CPU. As a bonus, the CPU is faster with `ffn` than with `attn`, so the more attention tensors you offload to GPU the faster it goes.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52codd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re using llama.cpp or similar, have you tried using &lt;code&gt;--override-tensor&lt;/code&gt; to only put attention tensors in GPUs? I.e. putting all &lt;code&gt;ffn&lt;/code&gt; tensors on CPU. As a bonus, the CPU is faster with &lt;code&gt;ffn&lt;/code&gt; than with &lt;code&gt;attn&lt;/code&gt;, so the more attention tensors you offload to GPU the faster it goes.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n52codd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753441246,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vu80",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n54j0mf",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "BobbyL2k",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n54ibrt",
                                                    "score": 1,
                                                    "author_fullname": "t2_ghoyg",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Like I’ve said, as large as you can afford. It’s just an example. \n\nSurely most people here can save up to a 5060 Ti 16GB. It’s not that expensive.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n54j0mf",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Like I’ve said, as large as you can afford. It’s just an example. &lt;/p&gt;\n\n&lt;p&gt;Surely most people here can save up to a 5060 Ti 16GB. It’s not that expensive.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m8vu80",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n54j0mf/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753465689,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753465689,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n54ibrt",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "segmond",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n54574t",
                                          "score": 1,
                                          "author_fullname": "t2_ah13x",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I would like 6 rtrx pro 6000, but how many of us can afford $55,000 for a computer?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n54ibrt",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would like 6 rtrx pro 6000, but how many of us can afford $55,000 for a computer?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m8vu80",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n54ibrt/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753465501,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753465501,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n54574t",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "BobbyL2k",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5423uj",
                                "score": 1,
                                "author_fullname": "t2_ghoyg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "VRAM capacity and bandwidth scale pretty much linearity with NVIDIA’s pricing. So I wouldn’t say that there’s a definitive sweet spot. I generally recommend getting the biggest machine you can afford. It could be a single 5060 Ti 16GB, an EPYC system with 6x RTX PRO 6000, or a DGX system.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n54574t",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;VRAM capacity and bandwidth scale pretty much linearity with NVIDIA’s pricing. So I wouldn’t say that there’s a definitive sweet spot. I generally recommend getting the biggest machine you can afford. It could be a single 5060 Ti 16GB, an EPYC system with 6x RTX PRO 6000, or a DGX system.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vu80",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n54574t/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753461892,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753461892,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n54zqrp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DepthHour1669",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5423uj",
                                "score": 1,
                                "author_fullname": "t2_t6glzswk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The 2nd RTX 6000 is just a vram sink for MoE models",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n54zqrp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The 2nd RTX 6000 is just a vram sink for MoE models&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vu80",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n54zqrp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753470489,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753470489,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5423uj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Mobile_Tart_1016",
                      "can_mod_post": false,
                      "created_utc": 1753461008,
                      "send_replies": true,
                      "parent_id": "t1_n52u8r3",
                      "score": 0,
                      "author_fullname": "t2_9pnhbbia3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Two RTX PRO 6000 is the sweet spot. \nYou get a system that can run pretty much everything, even MoE ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5423uj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Two RTX PRO 6000 is the sweet spot. \nYou get a system that can run pretty much everything, even MoE &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vu80",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n5423uj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753461008,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52u8r3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BobbyL2k",
            "can_mod_post": false,
            "created_utc": 1753448135,
            "send_replies": true,
            "parent_id": "t3_1m8vu80",
            "score": 1,
            "author_fullname": "t2_ghoyg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes, absolutely. And GDDR bandwidth is significantly faster than PCI-E. So the RTX PRO 6000 is a well positioned product, NVIDIA knows what they are doing.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52u8r3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, absolutely. And GDDR bandwidth is significantly faster than PCI-E. So the RTX PRO 6000 is a well positioned product, NVIDIA knows what they are doing.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n52u8r3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753448135,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vu80",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n530juj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CheatCodesOfLife",
            "can_mod_post": false,
            "created_utc": 1753450216,
            "send_replies": true,
            "parent_id": "t3_1m8vu80",
            "score": 1,
            "author_fullname": "t2_32el727b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Agreed. And you can train bigger models, run weird models like Bagel,  etc more easily with 1 GPU. The left over vram on each GPU is annoying / feels wasteful when running MoEs across a bunch of cards.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n530juj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Agreed. And you can train bigger models, run weird models like Bagel,  etc more easily with 1 GPU. The left over vram on each GPU is annoying / feels wasteful when running MoEs across a bunch of cards.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n530juj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753450216,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vu80",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n54frxo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "segmond",
                      "can_mod_post": false,
                      "created_utc": 1753464804,
                      "send_replies": true,
                      "parent_id": "t1_n53neme",
                      "score": 3,
                      "author_fullname": "t2_ah13x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Lack of $$$ is the problem.  When I bought my 16gb MI50s for $90, the 32gb MI50s were $500 at ebay.  I have no regrets about my decision.   But I'm just giving the PSA, if the price point is close enough, then just go for the bigger GPU, the payoff is worth it down the line.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n54frxo",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Lack of $$$ is the problem.  When I bought my 16gb MI50s for $90, the 32gb MI50s were $500 at ebay.  I have no regrets about my decision.   But I&amp;#39;m just giving the PSA, if the price point is close enough, then just go for the bigger GPU, the payoff is worth it down the line.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vu80",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n54frxo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753464804,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n54gn6j",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "GPTrack_ai",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n53ytrj",
                                "score": 1,
                                "author_fullname": "t2_1tpuoj72sa",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "nothing is good enough for LLMs right now. we are at 1T parameter open source models now. the sky is the limit. PS: compared  to real stuff RTX Pro 6000 is exceptionally cheap.",
                                "edited": 1753465249,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n54gn6j",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;nothing is good enough for LLMs right now. we are at 1T parameter open source models now. the sky is the limit. PS: compared  to real stuff RTX Pro 6000 is exceptionally cheap.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vu80",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n54gn6j/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753465041,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753465041,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n53ytrj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "loki-midgard",
                      "can_mod_post": false,
                      "created_utc": 1753460079,
                      "send_replies": true,
                      "parent_id": "t1_n53neme",
                      "score": 5,
                      "author_fullname": "t2_cpg2nk6ct",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Maybe price? If older/smaller cards are good enough for the task, why buy a more expensive cards?",
                      "edited": 1753463932,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n53ytrj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe price? If older/smaller cards are good enough for the task, why buy a more expensive cards?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vu80",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n53ytrj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753460079,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n58ffau",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "GPTrack_ai",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n55nknp",
                                                    "score": 1,
                                                    "author_fullname": "t2_1tpuoj72sa",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "rtx pro goes for 7.5k. but arguring about price is pointless. for big/good models you need bleeding edge hardware. qwen3-235B fits in GH200 624GB. but thats more expensiv than 2 rtx pros. these are your choices.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n58ffau",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;rtx pro goes for 7.5k. but arguring about price is pointless. for big/good models you need bleeding edge hardware. qwen3-235B fits in GH200 624GB. but thats more expensiv than 2 rtx pros. these are your choices.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m8vu80",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n58ffau/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753518656,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753518656,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n55nknp",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "eloquentemu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n55c2d2",
                                          "score": 0,
                                          "author_fullname": "t2_lpdsy",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "&gt; inferencing is done in fp4 nowadays ... all models below 100B are IMHO crap\n\nOkay, so exactly what model am I running on my Rtx6000 then?  Qwen3-235B won't fit at fp4. Am I supposed to buy two of them now to get any value?\n\nAlso the 5090 is $2000 while the Pro 6000 is ~$8500.  Scalped prices are $2500 and $8800 (std) - $12500 (server).  It's definitely more 1/4 than 1/3.  Also, smaller models do plenty of tasks just fine especially considering their speed on a 5090.",
                                          "edited": 1753487543,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n55nknp",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;inferencing is done in fp4 nowadays ... all models below 100B are IMHO crap&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Okay, so exactly what model am I running on my Rtx6000 then?  Qwen3-235B won&amp;#39;t fit at fp4. Am I supposed to buy two of them now to get any value?&lt;/p&gt;\n\n&lt;p&gt;Also the 5090 is $2000 while the Pro 6000 is ~$8500.  Scalped prices are $2500 and $8800 (std) - $12500 (server).  It&amp;#39;s definitely more 1/4 than 1/3.  Also, smaller models do plenty of tasks just fine especially considering their speed on a 5090.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m8vu80",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n55nknp/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753477508,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753477508,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 0
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n55c2d2",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "GPTrack_ai",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n54svqt",
                                "score": -1,
                                "author_fullname": "t2_1tpuoj72sa",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "inferencing is done in fp4 nowadays + the bigger model, the better is it. one if the simple scaling laws. all models below 100B are IMHO crap and for academic research only. 5090 is for gaming and has 1/3 of the price but has also only 1/3 of the VRAM...",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n55c2d2",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;inferencing is done in fp4 nowadays + the bigger model, the better is it. one if the simple scaling laws. all models below 100B are IMHO crap and for academic research only. 5090 is for gaming and has 1/3 of the price but has also only 1/3 of the VRAM...&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vu80",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n55c2d2/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753474154,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753474154,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": -1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n54svqt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1753468496,
                      "send_replies": true,
                      "parent_id": "t1_n53neme",
                      "score": 0,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "In addition to price alone, the value proposition is actually not that great.  What does 96GB get you?  Old 70B models at Q8?  Qwen-235B at Q2? Full bf6 32B?  A 32GB card can run small dense models with good quants and context.\n\nI don't think the the Pro 6000 is a bad buy, mind you, but for most people a 5090 will do most of what they want at ~1/4 the price.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n54svqt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In addition to price alone, the value proposition is actually not that great.  What does 96GB get you?  Old 70B models at Q8?  Qwen-235B at Q2? Full bf6 32B?  A 32GB card can run small dense models with good quants and context.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think the the Pro 6000 is a bad buy, mind you, but for most people a 5090 will do most of what they want at ~1/4 the price.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vu80",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n54svqt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753468496,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n53neme",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "LOW_SCORE",
            "no_follow": true,
            "author": "GPTrack_ai",
            "can_mod_post": false,
            "created_utc": 1753456910,
            "send_replies": true,
            "parent_id": "t3_1m8vu80",
            "score": -5,
            "author_fullname": "t2_1tpuoj72sa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": true,
            "body": "As Jensen always says, scale up before you scale out.... I will never understand why people use GPUs below the RTX Pro 6000...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n53neme",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As Jensen always says, scale up before you scale out.... I will never understand why people use GPUs below the RTX Pro 6000...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": "comment score below threshold",
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/n53neme/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753456910,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vu80",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -5
          }
        }
      ],
      "before": null
    }
  }
]