[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I tested 500 math problems from HuggingFaceH4/MATH-500 and discovered something surprising: Qwen3's Chinese Chain-of-Thought achieves 97% accuracy using only 61% of the tokens its English CoT needs. The efficiency gap grows with problem complexity - for the hardest problems (Level 5), Chinese needs just 65% of English tokens.\n\nThis contradicts most research showing English as the more efficient reasoning language in LLMs. The difference appears to stem from training data: English CoT is exploratory and self-doubting (\"Wait, let me check...\"), while Chinese CoT is direct and confident. Same problem, completely different reasoning styles.\n\nKey findings:\n- Overall: Chinese uses 40% fewer tokens for same accuracy\n- Efficiency scales: 7% advantage on easy problems → 35% on hardest\n- English hit token limits on 15.4% of problems; Chinese only 0.6%\n- When given more tokens, English can match accuracy but still uses 40% more\n\nFull analysis with methodology, case studies, and reproducible code: https://github.com/PastaPastaPasta/llm-chinese-english\n\nTested on: qwen3-30b-a3b-thinking-2507-mlx@6bit via LM Studio",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Qwen3 Uses 40% Fewer Tokens When Reasoning in Chinese vs English",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 104,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1min2c3",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "ups": 14,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ynfaa",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 14,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/3yM1YJj_ZpYksP-TJ0pe0cKsv0FxUcxVWGobejapRrc.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754431944,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tested 500 math problems from HuggingFaceH4/MATH-500 and discovered something surprising: Qwen3&amp;#39;s Chinese Chain-of-Thought achieves 97% accuracy using only 61% of the tokens its English CoT needs. The efficiency gap grows with problem complexity - for the hardest problems (Level 5), Chinese needs just 65% of English tokens.&lt;/p&gt;\n\n&lt;p&gt;This contradicts most research showing English as the more efficient reasoning language in LLMs. The difference appears to stem from training data: English CoT is exploratory and self-doubting (&amp;quot;Wait, let me check...&amp;quot;), while Chinese CoT is direct and confident. Same problem, completely different reasoning styles.&lt;/p&gt;\n\n&lt;p&gt;Key findings:\n- Overall: Chinese uses 40% fewer tokens for same accuracy\n- Efficiency scales: 7% advantage on easy problems → 35% on hardest\n- English hit token limits on 15.4% of problems; Chinese only 0.6%\n- When given more tokens, English can match accuracy but still uses 40% more&lt;/p&gt;\n\n&lt;p&gt;Full analysis with methodology, case studies, and reproducible code: &lt;a href=\"https://github.com/PastaPastaPasta/llm-chinese-english\"&gt;https://github.com/PastaPastaPasta/llm-chinese-english&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tested on: qwen3-30b-a3b-thinking-2507-mlx@6bit via LM Studio&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/y6r4oreky9hf1.png",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/y6r4oreky9hf1.png?auto=webp&amp;s=71fd8247b4a71b5bb0f025593e91ae20102b4ded",
                    "width": 1050,
                    "height": 784
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9411b4817b468e3653bbb24545bb86567f9aff4e",
                      "width": 108,
                      "height": 80
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e313d8547c3684bce868ef32001e0a0497675930",
                      "width": 216,
                      "height": 161
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96b46effef3077a8bd28468cc14a3ae3c9d9b9ba",
                      "width": 320,
                      "height": 238
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55636863c8127aeded3b9275c0615ec7fa28ce29",
                      "width": 640,
                      "height": 477
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c5d0e8e53cdf6fb3450d968f45dd71a96670748",
                      "width": 960,
                      "height": 716
                    }
                  ],
                  "variants": {},
                  "id": "NGrFEOFC_35j6Uz_6zEFlEZETBI5wWBfnbcLP3PZU4I"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1min2c3",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "PastaBlizzard",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/",
            "stickied": false,
            "url": "https://i.redd.it/y6r4oreky9hf1.png",
            "subreddit_subscribers": 511364,
            "created_utc": 1754431944,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n74n955",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "PastaBlizzard",
                      "can_mod_post": false,
                      "created_utc": 1754432721,
                      "send_replies": true,
                      "parent_id": "t1_n74m63z",
                      "score": 1,
                      "author_fullname": "t2_ynfaa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, this is why the primary focus is on the difference in the substance of their Chain of Thought as well as the increasing efficiency with harder problems. If we were simply looking at token differences due to encoding differences and how tokens are counted we would expect a constant efficiency ratio, not one that decreases with harder problems.\n\nWhat we do see is that while easy problems are solved in Chinese with 90% of the token usage as English, hard problems solved in Chinese use only 60% of the tokens as English.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74n955",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, this is why the primary focus is on the difference in the substance of their Chain of Thought as well as the increasing efficiency with harder problems. If we were simply looking at token differences due to encoding differences and how tokens are counted we would expect a constant efficiency ratio, not one that decreases with harder problems.&lt;/p&gt;\n\n&lt;p&gt;What we do see is that while easy problems are solved in Chinese with 90% of the token usage as English, hard problems solved in Chinese use only 60% of the tokens as English.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1min2c3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n74n955/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754432721,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n74m63z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "shokuninstudio",
            "can_mod_post": false,
            "created_utc": 1754432360,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 6,
            "author_fullname": "t2_4xzh04rz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Tokens add up differently in both languages because their writing systems are very different.\n\nHanzi isn't like the Latin alphabet. It can take 4 characters to say the same thing that requires 15 characters in English. Sometimes a single character can express what takes two or three words in English.\n\nAnd then there is verb conjugation in English that doesn't have equivalents in Chinese.",
            "edited": 1754432583,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74m63z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tokens add up differently in both languages because their writing systems are very different.&lt;/p&gt;\n\n&lt;p&gt;Hanzi isn&amp;#39;t like the Latin alphabet. It can take 4 characters to say the same thing that requires 15 characters in English. Sometimes a single character can express what takes two or three words in English.&lt;/p&gt;\n\n&lt;p&gt;And then there is verb conjugation in English that doesn&amp;#39;t have equivalents in Chinese.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n74m63z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754432360,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "body": "Now to figure out how to make it reason in Chinese, but give final answer in English...",
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n75b91g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Cool-Chemical-5629",
            "can_mod_post": false,
            "created_utc": 1754440772,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 1,
            "author_fullname": "t2_qz1qjc86",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "author_cakeday": true,
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75b91g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Now to figure out how to make it reason in Chinese, but give final answer in English...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n75b91g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754440772,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n75e4vp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "gamblingapocalypse",
            "can_mod_post": false,
            "created_utc": 1754441761,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 1,
            "author_fullname": "t2_fz3utn30",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "So you’re saying I have to learn Chinese huh? ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75e4vp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So you’re saying I have to learn Chinese huh? &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n75e4vp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754441761,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]