[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? \n\n- Apple m4 pro chip\n- 14 core CPU, 20 core GPU\n- 48GB unified memory. \n- One TB SSD storage \n\nEventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Is this set up sufficient?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mg40u1",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1ook1izxzg",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754176144,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Apple m4 pro chip&lt;/li&gt;\n&lt;li&gt;14 core CPU, 20 core GPU&lt;/li&gt;\n&lt;li&gt;48GB unified memory. &lt;/li&gt;\n&lt;li&gt;One TB SSD storage &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Eventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mg40u1",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "Wild-Muffin9190",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
            "subreddit_subscribers": 509052,
            "created_utc": 1754176144,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6lwemx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Federal-Effective879",
            "can_mod_post": false,
            "created_utc": 1754177473,
            "send_replies": true,
            "parent_id": "t3_1mg40u1",
            "score": 4,
            "author_fullname": "t2_702zh1r2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This would be a good setup for running models like Qwen 3 Coder 30B-A3B. If you want to learn Python, read books or tutorials, and write code yourself, rather than letting the LLM do everything for you. Ask the LLM questions about how to do things, rather than letting the LLM do everything for you, particularly when you're learning. When you get to more complex programs, human guidance and corrections will be essential.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lwemx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This would be a good setup for running models like Qwen 3 Coder 30B-A3B. If you want to learn Python, read books or tutorials, and write code yourself, rather than letting the LLM do everything for you. Ask the LLM questions about how to do things, rather than letting the LLM do everything for you, particularly when you&amp;#39;re learning. When you get to more complex programs, human guidance and corrections will be essential.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/n6lwemx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754177473,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg40u1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6lwprn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Toooooool",
            "can_mod_post": false,
            "created_utc": 1754177579,
            "send_replies": true,
            "parent_id": "t3_1mg40u1",
            "score": 2,
            "author_fullname": "t2_8llornh4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The Qwen3 30B LLM that was just released is 18.6GB for the Q4\\_K\\_M and 36GB for the Q8:  \n[https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)  \nso 48GB is plenty.\n\nAs for training your own LLM, maybe start by learning merging / finetuning.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lwprn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The Qwen3 30B LLM that was just released is 18.6GB for the Q4_K_M and 36GB for the Q8:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF&lt;/a&gt;&lt;br/&gt;\nso 48GB is plenty.&lt;/p&gt;\n\n&lt;p&gt;As for training your own LLM, maybe start by learning merging / finetuning.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/n6lwprn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754177579,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg40u1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ly4n2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "rpiguy9907",
            "can_mod_post": false,
            "created_utc": 1754178077,
            "send_replies": true,
            "parent_id": "t3_1mg40u1",
            "score": 2,
            "author_fullname": "t2_7l87g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes plenty of very good smaller models will run just fine!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ly4n2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes plenty of very good smaller models will run just fine!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/n6ly4n2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754178077,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg40u1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6mb7z0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MrPecunius",
            "can_mod_post": false,
            "created_utc": 1754182973,
            "send_replies": true,
            "parent_id": "t3_1mg40u1",
            "score": 1,
            "author_fullname": "t2_or0ok9hd7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have the binned (12/16 core) M4 Pro in a Macbook Pro (48GB/1TB), and I love it for models up to a little over 30GB in actual size. The machine itself is excellent in general.\n\nSpeed-wise, I don't feel that I missed anything by not getting the 14/20 core M4 Pro, and benchmarks back that up. Memory bandwidth is the main factor in inference speed with these machines.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6mb7z0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have the binned (12/16 core) M4 Pro in a Macbook Pro (48GB/1TB), and I love it for models up to a little over 30GB in actual size. The machine itself is excellent in general.&lt;/p&gt;\n\n&lt;p&gt;Speed-wise, I don&amp;#39;t feel that I missed anything by not getting the 14/20 core M4 Pro, and benchmarks back that up. Memory bandwidth is the main factor in inference speed with these machines.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/n6mb7z0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754182973,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg40u1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]