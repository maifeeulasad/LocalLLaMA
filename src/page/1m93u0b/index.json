[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm thinking of building a desktop app that helps you:\n\n\n\n\\- Detect your hardware (GPU, RAM, CPU)\n\n\\- Benchmark local AI models (GGUF/ONNX) automatically\n\n\\- Tell you which quant config runs best (Q4, Q5, etc.)\n\n\\- Show ratings like \"This model is great for coding, 12 tok/s on 8GB RAM\"\n\n\\- Launch models directly in one click\n\n\n\nLike HuggingFace meets Steam meets LM Studio — but optimized for \\*you\\*.\n\n\n\nWould you use this? What would you want it to do?\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Would you use this? Desktop app for auto-benchmarking GGUF/ONNX models locally",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m93u0b",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1uanajb3sh",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753460989,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of building a desktop app that helps you:&lt;/p&gt;\n\n&lt;p&gt;- Detect your hardware (GPU, RAM, CPU)&lt;/p&gt;\n\n&lt;p&gt;- Benchmark local AI models (GGUF/ONNX) automatically&lt;/p&gt;\n\n&lt;p&gt;- Tell you which quant config runs best (Q4, Q5, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Show ratings like &amp;quot;This model is great for coding, 12 tok/s on 8GB RAM&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;- Launch models directly in one click&lt;/p&gt;\n\n&lt;p&gt;Like HuggingFace meets Steam meets LM Studio — but optimized for *you*.&lt;/p&gt;\n\n&lt;p&gt;Would you use this? What would you want it to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m93u0b",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Conscious-Drive-1448",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753460989,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n54b3eq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Cool-Chemical-5629",
                      "can_mod_post": false,
                      "created_utc": 1753463521,
                      "send_replies": true,
                      "parent_id": "t1_n543mok",
                      "score": 1,
                      "author_fullname": "t2_qz1qjc86",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "The problem is that it says the info provided by the app would be optimized for \"you\", so that means optimized for your hardware and use cases. There isn't going to be such thing as \"doing one run of these benchmarks on all of the most popular weights on HuggingFace\", the benchmarks would still run on your own hardware.\n\nBut I believe a good approach would be an app that would recommend models based on user's hardware and use cases (perhaps even based on their prompts that would be then matched with models which are good at handling similar prompts). The app would allow benchmarking models on user's computer (optional) and sharing the anonymized results into a public database which would be then used to refine model recommendations to all users based on their own hardware and needs / use cases.\n\nI know I could use that kind of app myself. Would certainly beat downloading all of the models and testing them myself.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n54b3eq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The problem is that it says the info provided by the app would be optimized for &amp;quot;you&amp;quot;, so that means optimized for your hardware and use cases. There isn&amp;#39;t going to be such thing as &amp;quot;doing one run of these benchmarks on all of the most popular weights on HuggingFace&amp;quot;, the benchmarks would still run on your own hardware.&lt;/p&gt;\n\n&lt;p&gt;But I believe a good approach would be an app that would recommend models based on user&amp;#39;s hardware and use cases (perhaps even based on their prompts that would be then matched with models which are good at handling similar prompts). The app would allow benchmarking models on user&amp;#39;s computer (optional) and sharing the anonymized results into a public database which would be then used to refine model recommendations to all users based on their own hardware and needs / use cases.&lt;/p&gt;\n\n&lt;p&gt;I know I could use that kind of app myself. Would certainly beat downloading all of the models and testing them myself.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m93u0b",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/n54b3eq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753463521,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n543mok",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ForsookComparison",
            "can_mod_post": false,
            "created_utc": 1753461447,
            "send_replies": true,
            "parent_id": "t3_1m93u0b",
            "score": 1,
            "author_fullname": "t2_on5es7pe3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think it would make more sense to have a lightweight tool that scans hardware, maybe runs one or two benchmarks on a small model to get an idea for throughput, and *then* refers to a list of models/quants that you've compiled to determine what will likely run best.\n\nIf you do one run of these benchmarks on all of the most popular weights on HuggingFace there's no real need for me to download all of them and rerun the same tests. The unique factor my machine offers is the hardware config.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n543mok",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think it would make more sense to have a lightweight tool that scans hardware, maybe runs one or two benchmarks on a small model to get an idea for throughput, and &lt;em&gt;then&lt;/em&gt; refers to a list of models/quants that you&amp;#39;ve compiled to determine what will likely run best.&lt;/p&gt;\n\n&lt;p&gt;If you do one run of these benchmarks on all of the most popular weights on HuggingFace there&amp;#39;s no real need for me to download all of them and rerun the same tests. The unique factor my machine offers is the hardware config.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m93u0b/would_you_use_this_desktop_app_for/n543mok/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753461447,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m93u0b",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]