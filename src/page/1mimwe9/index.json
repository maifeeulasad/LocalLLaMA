[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Seems pretty standard stuff",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "OpenAI released Fine-tuning guide for GPT-OSS",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 73,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mimwe9",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "ups": 31,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_5gg8p4q4",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 31,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=731141103b56a844b825bab2d05e1ce36c0f8dfe",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754431536,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "cookbook.openai.com",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems pretty standard stuff&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?auto=webp&amp;s=6358f7da610cb4eda31a2a9c1d4a8493bd1a94c3",
                    "width": 1200,
                    "height": 628
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e21b918a6bd47ae52601f8bbd51d5018895a7666",
                      "width": 108,
                      "height": 56
                    },
                    {
                      "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=090f92abf1592b127e1ff7a9ff1ffcba1e77635b",
                      "width": 216,
                      "height": 113
                    },
                    {
                      "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7758dffb5743f1126d5bc62fd9d7dd1019ce18e3",
                      "width": 320,
                      "height": 167
                    },
                    {
                      "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=11ab391878f109e16178aaa55bd6d3f3b344fed6",
                      "width": 640,
                      "height": 334
                    },
                    {
                      "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e2938682341d6b004d612bbea72d6b275f9b7af",
                      "width": 960,
                      "height": 502
                    },
                    {
                      "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37d0ba9b7515c806f00722d7fd8c14e8ab5c6b5b",
                      "width": 1080,
                      "height": 565
                    }
                  ],
                  "variants": {},
                  "id": "1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mimwe9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Snoo_64233",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/",
            "stickied": false,
            "url": "https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers",
            "subreddit_subscribers": 511885,
            "created_utc": 1754431536,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "7dba5c08-72f1-11ee-9b6f-ca195bc297d4",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n75q5to",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "lizerome",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n75i8wq",
                                          "score": 1,
                                          "author_fullname": "t2_14wyb7",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "The model card explicitly mentions that they left out \"biorisk\" and other materials from the pretraining stage on purpose. If they did the same with NSFW content, the gooners are out of luck. However, it might still be possible to train the \"I must check whether this conflicts with the policies 20 times\" behavior out of it with finetuning.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n75q5to",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The model card explicitly mentions that they left out &amp;quot;biorisk&amp;quot; and other materials from the pretraining stage on purpose. If they did the same with NSFW content, the gooners are out of luck. However, it might still be possible to train the &amp;quot;I must check whether this conflicts with the policies 20 times&amp;quot; behavior out of it with finetuning.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mimwe9",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n75q5to/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754445920,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754445920,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n75i8wq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "JiminP",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n74nicg",
                                "score": 1,
                                "author_fullname": "t2_81us3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I bet it's the former case, based on my interaction with horizon alpha, but take my words with care because I mistakenly assumed that it was not from OpenAI based on its poor multilingual performance....",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n75i8wq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 70B"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I bet it&amp;#39;s the former case, based on my interaction with horizon alpha, but take my words with care because I mistakenly assumed that it was not from OpenAI based on its poor multilingual performance....&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mimwe9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n75i8wq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754443167,
                                "author_flair_text": "Llama 70B",
                                "treatment_tags": [],
                                "created_utc": 1754443167,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n74nicg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Snoo_64233",
                      "can_mod_post": false,
                      "created_utc": 1754432806,
                      "send_replies": true,
                      "parent_id": "t1_n74k47b",
                      "score": 11,
                      "author_fullname": "t2_5gg8p4q4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have done fine-tuning/dreamboothing (if you remember it) for Stable Diffusion 1.5 back in the good ole day. From my experience, it depends on how it was lobotomized. Was a model trained with all necessary data, but just that there is additional  layered fine-tuning to steer clear of the harmful content to be spit out? Or was the model decapitated from the get-go with \"harmful\" content left out from the data?  If former, I would say yes. If latter, it is rather complicated. You probably can feed new info to it via fine-tuning to an extent, but you can't really use fine-tuning as a primary mean of (harmful) knowledge acquisition and still expect the model to work like a charm in its newest state.\n\nThat is for image model. Probably the same thing with LLMs.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74nicg",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have done fine-tuning/dreamboothing (if you remember it) for Stable Diffusion 1.5 back in the good ole day. From my experience, it depends on how it was lobotomized. Was a model trained with all necessary data, but just that there is additional  layered fine-tuning to steer clear of the harmful content to be spit out? Or was the model decapitated from the get-go with &amp;quot;harmful&amp;quot; content left out from the data?  If former, I would say yes. If latter, it is rather complicated. You probably can feed new info to it via fine-tuning to an extent, but you can&amp;#39;t really use fine-tuning as a primary mean of (harmful) knowledge acquisition and still expect the model to work like a charm in its newest state.&lt;/p&gt;\n\n&lt;p&gt;That is for image model. Probably the same thing with LLMs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mimwe9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n74nicg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754432806,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 11
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n74mbit",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "entsnack",
                      "can_mod_post": false,
                      "created_utc": 1754432410,
                      "send_replies": true,
                      "parent_id": "t1_n74k47b",
                      "score": 3,
                      "author_fullname": "t2_1a48h7vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "ez",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74mbit",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "a": ":X:",
                          "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                          "e": "emoji"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ez&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mimwe9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n74mbit/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754432410,
                      "author_flair_text": ":X:",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "transparent",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n75412u",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DeProgrammer99",
                      "can_mod_post": false,
                      "created_utc": 1754438301,
                      "send_replies": true,
                      "parent_id": "t1_n74k47b",
                      "score": 3,
                      "author_fullname": "t2_w4j8t",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I just had it generate &gt;700 lines of code in llama.cpp, and it didn't do any worse than the last several models I tried the same test with and posted about. I don't think it's any dumber than a 120B-A5B should be, and it's probably better than the \"square root of total times active\" dense equivalent estimation.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n75412u",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just had it generate &amp;gt;700 lines of code in llama.cpp, and it didn&amp;#39;t do any worse than the last several models I tried the same test with and posted about. I don&amp;#39;t think it&amp;#39;s any dumber than a 120B-A5B should be, and it&amp;#39;s probably better than the &amp;quot;square root of total times active&amp;quot; dense equivalent estimation.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mimwe9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n75412u/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754438301,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n74k47b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "RandumbRedditor1000",
            "can_mod_post": false,
            "created_utc": 1754431683,
            "send_replies": true,
            "parent_id": "t3_1mimwe9",
            "score": 18,
            "author_fullname": "t2_s7n3irsrx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I wonder if we'll be able to un-lobotomize it",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74k47b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wonder if we&amp;#39;ll be able to un-lobotomize it&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n74k47b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754431683,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mimwe9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 18
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n74kllo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ForsookComparison",
            "can_mod_post": false,
            "created_utc": 1754431842,
            "send_replies": true,
            "parent_id": "t3_1mimwe9",
            "score": 2,
            "author_fullname": "t2_on5es7pe3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; *Note: This notebook is designed to be run on a single H100 GPU with 80GB of memory. If you have access to a smaller GPU, you can reduce the batch size and sequence length in the hyperparameters below.*\n\nRIP - I was just having some good luck claiming on-demand lambda instances",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74kllo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;Note: This notebook is designed to be run on a single H100 GPU with 80GB of memory. If you have access to a smaller GPU, you can reduce the batch size and sequence length in the hyperparameters below.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;RIP - I was just having some good luck claiming on-demand lambda instances&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n74kllo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754431842,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mimwe9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n74ohhx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Cool-Chemical-5629",
            "can_mod_post": false,
            "created_utc": 1754433134,
            "send_replies": true,
            "parent_id": "t3_1mimwe9",
            "score": 2,
            "author_fullname": "t2_qz1qjc86",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "So, somebody said GPT-OSS is DOA, so I was thinking whoever un-lobotomizes it should call their newly created model GPT-OSS CPR... 🤣",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74ohhx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So, somebody said GPT-OSS is DOA, so I was thinking whoever un-lobotomizes it should call their newly created model GPT-OSS CPR... 🤣&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n74ohhx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754433134,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mimwe9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76lteo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1754459035,
            "send_replies": true,
            "parent_id": "t3_1mimwe9",
            "score": 1,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I am waiting for abliteration guys like huihui or u/mlabonne :)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76lteo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am waiting for abliteration guys like huihui or &lt;a href=\"/u/mlabonne\"&gt;u/mlabonne&lt;/a&gt; :)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mimwe9/openai_released_finetuning_guide_for_gptoss/n76lteo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754459035,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mimwe9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]