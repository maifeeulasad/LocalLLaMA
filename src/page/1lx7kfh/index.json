[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have only been utilizing LLMs for a little more than a year and only started down the local LLM path a few months ago, so bare with me if there are already papers about my following question:\n\nAre there any models/agents that can cache previous context windows, encode the cached context into weight scalers, and then apply the new weights to the models, so they essentially hardcode all conversations into their own training data?\n\nI understand why you wouldn't want to do this on a public LLM as you are relying on the integrity of the entire userbase not to intentionally break the model with adversarial prompts, but is this potentially possible within the limitations of the current llama.cpp toolset?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Potentially Noob Question Regarding Live Weight Adjustments",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lx7kfh",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_7ltgyzy0",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752242208,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have only been utilizing LLMs for a little more than a year and only started down the local LLM path a few months ago, so bare with me if there are already papers about my following question:&lt;/p&gt;\n\n&lt;p&gt;Are there any models/agents that can cache previous context windows, encode the cached context into weight scalers, and then apply the new weights to the models, so they essentially hardcode all conversations into their own training data?&lt;/p&gt;\n\n&lt;p&gt;I understand why you wouldn&amp;#39;t want to do this on a public LLM as you are relying on the integrity of the entire userbase not to intentionally break the model with adversarial prompts, but is this potentially possible within the limitations of the current llama.cpp toolset?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lx7kfh",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Pyromancer777",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lx7kfh/potentially_noob_question_regarding_live_weight/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lx7kfh/potentially_noob_question_regarding_live_weight/",
            "subreddit_subscribers": 497502,
            "created_utc": 1752242208,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2k6sa0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SlowFail2433",
            "can_mod_post": false,
            "created_utc": 1752245969,
            "send_replies": true,
            "parent_id": "t3_1lx7kfh",
            "score": 1,
            "author_fullname": "t2_131eezppgs",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "No we simply do not have the ability to induce memorisation of a specific conversation using weight scalers",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2k6sa0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No we simply do not have the ability to induce memorisation of a specific conversation using weight scalers&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lx7kfh/potentially_noob_question_regarding_live_weight/n2k6sa0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752245969,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lx7kfh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2k78pe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1752246098,
            "send_replies": true,
            "parent_id": "t3_1lx7kfh",
            "score": 1,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That's kinda the holy grail of LLMs. It doesn't work like that, unfortunately.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2k78pe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s kinda the holy grail of LLMs. It doesn&amp;#39;t work like that, unfortunately.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lx7kfh/potentially_noob_question_regarding_live_weight/n2k78pe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752246098,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lx7kfh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "03eba0e8-72f2-11ee-96eb-9a14648159ce",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2l2pmo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MixtureOfAmateurs",
            "can_mod_post": false,
            "created_utc": 1752254907,
            "send_replies": true,
            "parent_id": "t3_1lx7kfh",
            "score": 1,
            "author_fullname": "t2_u8rt5j6ej",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Has not been done.\n\nWhen I read your post I imagine the model training a small set of weights, or more practically a set of embedded vectors, after a conversation finishes. So the next chat has a set of token like objects the user can't see and the model can't decode to tokens because they're some new point in embedding space. They would be attended to like normal, and then the next chat would update (not overwrite) them.\n\nThe problem with this is you need back propagation to come to these weights, which means you need a loss function. Like a correct and incorrect answer for it to train on. Oh wait never mind you could train a small embedding model to encode the past conversation.\n\nLike during instruction tuning the embedding model is given a conversation, and the big model is asked questions about it but the only thing in it's context is the questions and the output of the embedding model. So the loss function (I think it's a cost function actually) of the embedding model is 'feed shit to big model, if correct answer re-enforce, else negative re-enforcement'. The large models weights could be locked for efficiency as well. \n\nI have no idea how this would work for more than one conversation, and it would be more useful for compression of very long contexts than long term memory, but it's still a cool idea.\n\nProbably not at all what you had in mind but thanks, I'll for sure never make it but will consider making it over the next few weeks.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2l2pmo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "koboldcpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Has not been done.&lt;/p&gt;\n\n&lt;p&gt;When I read your post I imagine the model training a small set of weights, or more practically a set of embedded vectors, after a conversation finishes. So the next chat has a set of token like objects the user can&amp;#39;t see and the model can&amp;#39;t decode to tokens because they&amp;#39;re some new point in embedding space. They would be attended to like normal, and then the next chat would update (not overwrite) them.&lt;/p&gt;\n\n&lt;p&gt;The problem with this is you need back propagation to come to these weights, which means you need a loss function. Like a correct and incorrect answer for it to train on. Oh wait never mind you could train a small embedding model to encode the past conversation.&lt;/p&gt;\n\n&lt;p&gt;Like during instruction tuning the embedding model is given a conversation, and the big model is asked questions about it but the only thing in it&amp;#39;s context is the questions and the output of the embedding model. So the loss function (I think it&amp;#39;s a cost function actually) of the embedding model is &amp;#39;feed shit to big model, if correct answer re-enforce, else negative re-enforcement&amp;#39;. The large models weights could be locked for efficiency as well. &lt;/p&gt;\n\n&lt;p&gt;I have no idea how this would work for more than one conversation, and it would be more useful for compression of very long contexts than long term memory, but it&amp;#39;s still a cool idea.&lt;/p&gt;\n\n&lt;p&gt;Probably not at all what you had in mind but thanks, I&amp;#39;ll for sure never make it but will consider making it over the next few weeks.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lx7kfh/potentially_noob_question_regarding_live_weight/n2l2pmo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752254907,
            "author_flair_text": "koboldcpp",
            "treatment_tags": [],
            "link_id": "t3_1lx7kfh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]