[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.\n\nAs per the title, I have attempted to run Qwen3-235B-A22B using \\`llama-server\\` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?\n\nMachine specification are:\n\n    CPU: i3-12100F (12th Gen Intel)\n    RAM: 128GB (4\\*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\n    Motherboard: MSI PRO B660M-A WIFI DDR4\n    GPU: GeForce RTX 3090 24GB VRAM\n\n(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)\n\n`llama-server` launch options:\n\n    llama-server \\\n      --host 0.0.0.0 \\\n      --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n      --ctx-size 16384 \\\n      --n-gpu-layers 99 \\\n      --flash-attn \\\n      --threads 3 \\\n      -ot \"exps=CPU\" \\\n      --seed 3407 \\\n      --prio 3 \\\n      --temp 0.6 \\\n      --min-p 0.0 \\\n      --top-p 0.95 \\\n      --top-k 20 \\\n      --no-mmap \\\n      --no-warmup \\\n      --mlock\n\nAny advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Qwen3-235B-A22B @ 0.7t/s. Hardware or configuration bottleneck?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lysmo9",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_tfa1mcp1",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752412727,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.&lt;/p&gt;\n\n&lt;p&gt;As per the title, I have attempted to run Qwen3-235B-A22B using `llama-server` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?&lt;/p&gt;\n\n&lt;p&gt;Machine specification are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CPU: i3-12100F (12th Gen Intel)\nRAM: 128GB (4\\*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\nMotherboard: MSI PRO B660M-A WIFI DDR4\nGPU: GeForce RTX 3090 24GB VRAM\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; launch options:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server \\\n  --host 0.0.0.0 \\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n  --ctx-size 16384 \\\n  --n-gpu-layers 99 \\\n  --flash-attn \\\n  --threads 3 \\\n  -ot &amp;quot;exps=CPU&amp;quot; \\\n  --seed 3407 \\\n  --prio 3 \\\n  --temp 0.6 \\\n  --min-p 0.0 \\\n  --top-p 0.95 \\\n  --top-k 20 \\\n  --no-mmap \\\n  --no-warmup \\\n  --mlock\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Any advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lysmo9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ConnectionOutside485",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
            "subreddit_subscribers": 498343,
            "created_utc": 1752412727,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2wng7y",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ConnectionOutside485",
                      "can_mod_post": false,
                      "created_utc": 1752418389,
                      "send_replies": true,
                      "parent_id": "t1_n2w7pt9",
                      "score": 1,
                      "author_fullname": "t2_tfa1mcp1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks. I will check the XMP settings when the machine is next restarted. I see how I might already be losing some performance here with 2166 vs. 2666!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2wng7y",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks. I will check the XMP settings when the machine is next restarted. I see how I might already be losing some performance here with 2166 vs. 2666!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lysmo9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wng7y/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752418389,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2w7pt9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BumbleSlob",
            "can_mod_post": false,
            "created_utc": 1752413184,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 2,
            "author_fullname": "t2_1j7fhlcqkp",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The thing that jumped out to me as the obvious bottleneck was your memory speeds. 2166Mtps is dog slow. You might be able to juice some extra performance by turning on XMP in your bios and ensuring your sticks are running at 2666Mtps.\n\nBut otherwise, I’d say you should target a ram bandwidth upgrade. Your mobo should support up to DDR4-4800 or so. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2w7pt9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The thing that jumped out to me as the obvious bottleneck was your memory speeds. 2166Mtps is dog slow. You might be able to juice some extra performance by turning on XMP in your bios and ensuring your sticks are running at 2666Mtps.&lt;/p&gt;\n\n&lt;p&gt;But otherwise, I’d say you should target a ram bandwidth upgrade. Your mobo should support up to DDR4-4800 or so. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2w7pt9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752413184,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2wg4oo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "YouDontSeemRight",
            "can_mod_post": false,
            "created_utc": 1752416104,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 1,
            "author_fullname": "t2_1b7gjxtue9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The issue is qwen 235B is an MOE. You can override the expert layers and send them to CPU while keeping the static layers in GPU. Unfortunately though your CPU is garbage and definitely the bottle neck. I also don't think your -ot regex is correct",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wg4oo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The issue is qwen 235B is an MOE. You can override the expert layers and send them to CPU while keeping the static layers in GPU. Unfortunately though your CPU is garbage and definitely the bottle neck. I also don&amp;#39;t think your -ot regex is correct&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wg4oo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752416104,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2w8pge",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ICanSeeYou7867",
            "can_mod_post": false,
            "created_utc": 1752413546,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 1,
            "author_fullname": "t2_sqvpr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How much of your gpu is being used?\n\nSince you have 99 layers supposedly being sent to the GPU, but its not crashing....\n\nI imagine your override tensor line might be pushing all the experts to your RAM and CPU?\n\nLooking at this post:\nhttps://www.reddit.com/r/LocalLLaMA/s/bLOx23oaNN\n\nMaybe you should push specific layers to the GPU instead?\n\nOr remove the OT line, and adjust your gpu layers to something your GPU can handle.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2w8pge",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How much of your gpu is being used?&lt;/p&gt;\n\n&lt;p&gt;Since you have 99 layers supposedly being sent to the GPU, but its not crashing....&lt;/p&gt;\n\n&lt;p&gt;I imagine your override tensor line might be pushing all the experts to your RAM and CPU?&lt;/p&gt;\n\n&lt;p&gt;Looking at this post:\n&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/bLOx23oaNN\"&gt;https://www.reddit.com/r/LocalLLaMA/s/bLOx23oaNN&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Maybe you should push specific layers to the GPU instead?&lt;/p&gt;\n\n&lt;p&gt;Or remove the OT line, and adjust your gpu layers to something your GPU can handle.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2w8pge/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752413546,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2w9hw3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1752413832,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 1,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How full is your GPU? You put all the largest layers to CPU.\n\nWhat is your memory b/w with the ram? Download mlc and run a test. Probably a few more layers on the 3090 will raise it but don't expect a miracle judging by that memory.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2w9hw3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How full is your GPU? You put all the largest layers to CPU.&lt;/p&gt;\n\n&lt;p&gt;What is your memory b/w with the ram? Download mlc and run a test. Probably a few more layers on the 3090 will raise it but don&amp;#39;t expect a miracle judging by that memory.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2w9hw3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752413832,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2wh5l6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Common_Heron2171",
            "can_mod_post": false,
            "created_utc": 1752416436,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 1,
            "author_fullname": "t2_1ornc3yr5v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "buy faster DRAMs...\n\nmaybe try:  -ctk q8\\_0 -ctv q8\\_0\n\nAlso I guess your 3090 is near idle, so you could utilize it for speculative decoding\n\n\\-md (unsloth's)Qwen3-8B-UD-Q8\\_K\\_XL.gguf or something --draft 12 --draft-p-min 0.9 -ngld 99 (maybe  -ctkd q8\\_0 -ctvd q8\\_0)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wh5l6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;buy faster DRAMs...&lt;/p&gt;\n\n&lt;p&gt;maybe try:  -ctk q8_0 -ctv q8_0&lt;/p&gt;\n\n&lt;p&gt;Also I guess your 3090 is near idle, so you could utilize it for speculative decoding&lt;/p&gt;\n\n&lt;p&gt;-md (unsloth&amp;#39;s)Qwen3-8B-UD-Q8_K_XL.gguf or something --draft 12 --draft-p-min 0.9 -ngld 99 (maybe  -ctkd q8_0 -ctvd q8_0)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wh5l6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752416436,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2wldya",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MDT-49",
            "can_mod_post": false,
            "created_utc": 1752417758,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 1,
            "author_fullname": "t2_h8yrica5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think the -ot regex might not be right. Maybe try `-ot \".ffn_.*_exps.=CPU\"`. I think your CPU has 4 cores, so try increasing `--threads` to 4. \n\nAs other people have already mentioned, the RAM speeds are going to be the major limitation in this setup. I feel like you might be better off switching to the best dense LLM you can find and run that fully in VRAM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wldya",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think the -ot regex might not be right. Maybe try &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt;. I think your CPU has 4 cores, so try increasing &lt;code&gt;--threads&lt;/code&gt; to 4. &lt;/p&gt;\n\n&lt;p&gt;As other people have already mentioned, the RAM speeds are going to be the major limitation in this setup. I feel like you might be better off switching to the best dense LLM you can find and run that fully in VRAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wldya/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752417758,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2wo2zf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "makistsa",
            "can_mod_post": false,
            "created_utc": 1752418580,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 1,
            "author_fullname": "t2_3l1o090d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Your ram speed is slow, but even then you should have higher t/s. With 3 p-cores, 50% faster ram, and 16gb gpu i get &gt;5t/s using a q3 quant.\n\nYour ram seems to be able to run at least at 2666. Try to fix that.\n\nCheck how much vram you are using. With 24gb vram and all exps on CPU you probably have a lot of it unused\n\nBut the main problem should be something else. Check if the gpu is even working at all during inference\n\nEdit: Is the llama server compiled with cuda?\n\nEdit2: I did a quick test with your settings. Your gpu is definitely not doing anything.",
            "edited": 1752420099,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wo2zf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your ram speed is slow, but even then you should have higher t/s. With 3 p-cores, 50% faster ram, and 16gb gpu i get &amp;gt;5t/s using a q3 quant.&lt;/p&gt;\n\n&lt;p&gt;Your ram seems to be able to run at least at 2666. Try to fix that.&lt;/p&gt;\n\n&lt;p&gt;Check how much vram you are using. With 24gb vram and all exps on CPU you probably have a lot of it unused&lt;/p&gt;\n\n&lt;p&gt;But the main problem should be something else. Check if the gpu is even working at all during inference&lt;/p&gt;\n\n&lt;p&gt;Edit: Is the llama server compiled with cuda?&lt;/p&gt;\n\n&lt;p&gt;Edit2: I did a quick test with your settings. Your gpu is definitely not doing anything.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wo2zf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752418580,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2wryum",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Secure_Reflection409",
            "can_mod_post": false,
            "created_utc": 1752419750,
            "send_replies": true,
            "parent_id": "t3_1lysmo9",
            "score": 1,
            "author_fullname": "t2_by77ogdhr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Use a draft model as per the thread I made the other day.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2wryum",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Use a draft model as per the thread I made the other day.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wryum/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752419750,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lysmo9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]