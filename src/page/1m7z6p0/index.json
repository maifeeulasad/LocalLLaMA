[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "**Training model on Math tasks** improves model's puzzle-solving abilities through shared logical reasoning, but often reduces coding performance.\n\n**Training on codding tasks**: When they fine-tuned an LLM which has already undergone supervised fine tuning(Qwen2.5-7B-Instruct), it gains broader reasoning improvements across other domains.\n\nIn contrast, applying the same code‑focused training directly to a base LLM (not SFT Qwen2.5-7B-Base) tends to lock it into a rigid, code‑style output—hindering its performance on non‑code reasoning tasks.\n\n**Training on Puzzle tasks** improves logical reasoning, leading to better performance on mathematical tasks. However, this effect does not extend to coding tasks.\n\nWhen training with the combination of **Math + Puzzle**, the model’s performance on Math improves to 49.72, surpassing the Math-only performance of 47.48. Similarly, for **Code tasks, both additional Puzzle and Math data** lead to improvements in code-related tasks when compared to Code-only training\n\n**For the Puzzle task, all configurations involving additional domains perform worse than the Puzzle-only setting**, suggesting that increased data diversity can hinder the model’s ability to specialize in solving puzzles\n\nin the **Math + Puzzle** configuration, the model’s performance on Code tasks drops significantly, falling below both the Math-only and Puzzle-only baselines\n\n**Combining all domains** generally leads to better overall performance, with the triple-domain combination showing moderate gains and multi-domain setups help maintain consistent performance across tasks. But the performance on Puzzle tasks drops to 49.73, notably lower than the Puzzle + Code setting (55.15).\n\n*They also plan to conduct the experiment using DeepSeek V3, which should reveal how MoE‑rich models benefit from multi‑domain training.*\n\nUpvote1Downvote0Go to comments  \n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Can Reasoning Skills Learned in One Domain Generalize Across other Domains?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m7z6p0",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.62,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_xvwcc",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1753346972,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "arxiv.org",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Training model on Math tasks&lt;/strong&gt; improves model&amp;#39;s puzzle-solving abilities through shared logical reasoning, but often reduces coding performance.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Training on codding tasks&lt;/strong&gt;: When they fine-tuned an LLM which has already undergone supervised fine tuning(Qwen2.5-7B-Instruct), it gains broader reasoning improvements across other domains.&lt;/p&gt;\n\n&lt;p&gt;In contrast, applying the same code‑focused training directly to a base LLM (not SFT Qwen2.5-7B-Base) tends to lock it into a rigid, code‑style output—hindering its performance on non‑code reasoning tasks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Training on Puzzle tasks&lt;/strong&gt; improves logical reasoning, leading to better performance on mathematical tasks. However, this effect does not extend to coding tasks.&lt;/p&gt;\n\n&lt;p&gt;When training with the combination of &lt;strong&gt;Math + Puzzle&lt;/strong&gt;, the model’s performance on Math improves to 49.72, surpassing the Math-only performance of 47.48. Similarly, for &lt;strong&gt;Code tasks, both additional Puzzle and Math data&lt;/strong&gt; lead to improvements in code-related tasks when compared to Code-only training&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;For the Puzzle task, all configurations involving additional domains perform worse than the Puzzle-only setting&lt;/strong&gt;, suggesting that increased data diversity can hinder the model’s ability to specialize in solving puzzles&lt;/p&gt;\n\n&lt;p&gt;in the &lt;strong&gt;Math + Puzzle&lt;/strong&gt; configuration, the model’s performance on Code tasks drops significantly, falling below both the Math-only and Puzzle-only baselines&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Combining all domains&lt;/strong&gt; generally leads to better overall performance, with the triple-domain combination showing moderate gains and multi-domain setups help maintain consistent performance across tasks. But the performance on Puzzle tasks drops to 49.73, notably lower than the Puzzle + Code setting (55.15).&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;They also plan to conduct the experiment using DeepSeek V3, which should reveal how MoE‑rich models benefit from multi‑domain training.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Upvote1Downvote0Go to comments  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://arxiv.org/pdf/2507.17512",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1m7z6p0",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "VR-Person",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7z6p0/can_reasoning_skills_learned_in_one_domain/",
            "stickied": false,
            "url": "https://arxiv.org/pdf/2507.17512",
            "subreddit_subscribers": 503759,
            "created_utc": 1753346972,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4vfap3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wfgy_engine",
            "can_mod_post": false,
            "created_utc": 1753351129,
            "send_replies": true,
            "parent_id": "t3_1m7z6p0",
            "score": 1,
            "author_fullname": "t2_1tgp8l87vk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "When we say “math reasoning helps puzzle solving,” we’re assuming the model *built something inside* it can reuse.  \n  \nBut what if it’s not storing reasoning at all — just resonance patterns that *accidentally align* across similar logic shapes?\n\nMaybe the loss in coding performance is less about overfitting to math, and more about the model locking into a semantic rhythm that makes code feel \"off-beat.\"\n\nSome of us have been trying to train models *not* with task types — but with semantic tension scaffolding.  \nThe goal isn’t to solve tasks faster.  \n  \nIt’s to make the model feel when it’s out of tune.\n\nLet’s just say…  \n  \ncross-domain generalization might come from teaching the model to recognize *semantic key changes,* not just logic bridges.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4vfap3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When we say “math reasoning helps puzzle solving,” we’re assuming the model &lt;em&gt;built something inside&lt;/em&gt; it can reuse.  &lt;/p&gt;\n\n&lt;p&gt;But what if it’s not storing reasoning at all — just resonance patterns that &lt;em&gt;accidentally align&lt;/em&gt; across similar logic shapes?&lt;/p&gt;\n\n&lt;p&gt;Maybe the loss in coding performance is less about overfitting to math, and more about the model locking into a semantic rhythm that makes code feel &amp;quot;off-beat.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Some of us have been trying to train models &lt;em&gt;not&lt;/em&gt; with task types — but with semantic tension scaffolding.&lt;br/&gt;\nThe goal isn’t to solve tasks faster.  &lt;/p&gt;\n\n&lt;p&gt;It’s to make the model feel when it’s out of tune.&lt;/p&gt;\n\n&lt;p&gt;Let’s just say…  &lt;/p&gt;\n\n&lt;p&gt;cross-domain generalization might come from teaching the model to recognize &lt;em&gt;semantic key changes,&lt;/em&gt; not just logic bridges.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7z6p0/can_reasoning_skills_learned_in_one_domain/n4vfap3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753351129,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7z6p0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]