[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Here is the thing, the expert layers run amazing on CPU  (~~\\~17T/s~~ 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .\n\nYou can offload just the attention layers to GPU  (requiring about 5 to 8GB of VRAM) for fast prefill.\n\n* KV cache for the sequence\n* Attention weights &amp; activations\n* Routing tables\n* LayerNorms and other “non-expert” parameters\n\nNo giant MLP weights are resident on the GPU, so memory use stays low.\n\nThis yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.\n\n64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)\n\n&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)\n\n&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)\n\nwith 5GB of vram usage!\n\nHonestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.\n\nedit: with this latest PR: [https://github.com/ggml-org/llama.cpp/pull/15157](https://github.com/ggml-org/llama.cpp/pull/15157)\n\n    ~/build/llama.cpp/build-cuda/bin/llama-server \\\n        -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n        --n-cpu-moe 36 \\    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.\n        --n-gpu-layers 999 \\   #everything else on the GPU, about 8GB\n        -c 0 -fa \\   #max context (128k), flash attention\n        --jinja --reasoning-format none \\\n        --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n    \n    \n    \n    prompt eval time =   94593.62 ms / 12717 tokens (    7.44 ms per token,   134.44 tokens per second)\n           eval time =   76741.17 ms /  1966 tokens (   39.03 ms per token,    25.62 tokens per second)\n\nHitting above 25T/s with only 8GB VRAM use!\n\nCompared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :\n\n    ~/build/llama.cpp/build-cuda/bin/llama-server \\\n        -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n        --n-cpu-moe 28 \\\n        --n-gpu-layers 999 \\\n        -c 0 -fa \\\n        --jinja --reasoning-format none \\\n        --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n    \n    prompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)\n           eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)\n\nHonestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "120B runs awesome on just 8GB VRAM!",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mke7ef",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 440,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_69r67vj3",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 440,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754642259,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754605924,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU  (&lt;del&gt;~17T/s&lt;/del&gt; 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt;\n\n&lt;p&gt;You can offload just the attention layers to GPU  (requiring about 5 to 8GB of VRAM) for fast prefill.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;KV cache for the sequence&lt;/li&gt;\n&lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt;\n&lt;li&gt;Routing tables&lt;/li&gt;\n&lt;li&gt;LayerNorms and other “non-expert” parameters&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt;\n\n&lt;p&gt;This yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt;\n\n&lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the &amp;#39;hot&amp;#39; experts in memory even if the whole model doesn&amp;#39;t fit in memory)&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;with 5GB of vram usage!&lt;/p&gt;\n\n&lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt;\n\n&lt;p&gt;edit: with this latest PR: &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15157\"&gt;https://github.com/ggml-org/llama.cpp/pull/15157&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \\\n    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n    --n-cpu-moe 36 \\    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn&amp;#39;t even make things that much faster.\n    --n-gpu-layers 999 \\   #everything else on the GPU, about 8GB\n    -c 0 -fa \\   #max context (128k), flash attention\n    --jinja --reasoning-format none \\\n    --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \\\n\n\n\nprompt eval time =   94593.62 ms / 12717 tokens (    7.44 ms per token,   134.44 tokens per second)\n       eval time =   76741.17 ms /  1966 tokens (   39.03 ms per token,    25.62 tokens per second)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Hitting above 25T/s with only 8GB VRAM use!&lt;/p&gt;\n\n&lt;p&gt;Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \\\n    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n    --n-cpu-moe 28 \\\n    --n-gpu-layers 999 \\\n    -c 0 -fa \\\n    --jinja --reasoning-format none \\\n    --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \\\n\nprompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)\n       eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8.png?auto=webp&amp;s=c39db485b57bb485da2e2686c60420f0ec476d39",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6d24f943ce19d12db1601ab8005b8f6b78cb4b8",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06b993fce286e9e134f83911bb2a2e991910df83",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d05c83e0ea980d9c460d2acea2d8590febdf22d3",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24ca82925cc73821aca58d59b6fcacc772b0d70c",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4c2bbd05f0090b924e9b3b73467d3d45862b8471",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fc58ded3f823aeb23b65df306dc1f08d65cb78cf",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "aoTIOGp4IeiDA4o2BmmYi251dex2VNN97dvqHfT33_8"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mke7ef",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Wrong-Historian",
            "discussion_type": null,
            "num_comments": 56,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/",
            "subreddit_subscribers": 513813,
            "created_utc": 1754605924,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7icewz",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "fp4guru",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 27,
                                "author_fullname": "t2_1tp8zldw5g",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I get 12 with unsloth gguf and 4090. Which one is your gguf from?\n\nI changed the layer to 37 , getting 23.\nNew finding: unsloth's gguf loading speed is much faster than ggml version, not sure why.",
                                "edited": 1754615503,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7icewz",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I get 12 with unsloth gguf and 4090. Which one is your gguf from?&lt;/p&gt;\n\n&lt;p&gt;I changed the layer to 37 , getting 23.\nNew finding: unsloth&amp;#39;s gguf loading speed is much faster than ggml version, not sure why.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7icewz/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754609496,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754609496,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 27
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7khcej",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Awwtifishal",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7i9xiw",
                                                    "score": 3,
                                                    "author_fullname": "t2_1d96a8k10t",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "\\--n-cpu-moe 28 means the weights of all experts of the first 28 layers, not 28 experts",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7khcej",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;--n-cpu-moe 28 means the weights of all experts of the first 28 layers, not 28 experts&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mke7ef",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7khcej/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754643124,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754643124,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                },
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7jen1s",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Glittering-Call8746",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7i9xiw",
                                                    "score": 2,
                                                    "author_fullname": "t2_tqwl6sawb",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "That's nice.. anyone try with 3070 8gb ? Or 3080 10gb ? I have both. No idea how to get started with ubuntu with git compiled cuda",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7jen1s",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s nice.. anyone try with 3070 8gb ? Or 3080 10gb ? I have both. No idea how to get started with ubuntu with git compiled cuda&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mke7ef",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jen1s/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754623461,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754623461,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                },
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7jyu1j",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Paradigmind",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7i9xiw",
                                                    "score": 1,
                                                    "author_fullname": "t2_6ste18zta",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Hello sir. You seem very knowledgable. Pretty impressive stuff you come up with. Do you have a similar hint, or setup for GLM-4.5 Air on a 3090 and 96GB Ram?\n\nAlso, I'm a noob. Is your approach similar to [this one](https://www.reddit.com/r/LocalLLaMA/s/SF7PR65MfX)?",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7jyu1j",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hello sir. You seem very knowledgable. Pretty impressive stuff you come up with. Do you have a similar hint, or setup for GLM-4.5 Air on a 3090 and 96GB Ram?&lt;/p&gt;\n\n&lt;p&gt;Also, I&amp;#39;m a noob. Is your approach similar to &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/SF7PR65MfX\"&gt;this one&lt;/a&gt;?&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mke7ef",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jyu1j/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754632816,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754632816,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7i9xiw",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "Wrong-Historian",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7i9968",
                                          "score": 26,
                                          "author_fullname": "t2_69r67vj3",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "yes, that was me. But that was --n-cpu-moe 28 (28 experts on CPU, and pretty much maxing out VRAM of 3090) vs --cpu-moe (all experts on CPU) using just 5GB of VRAM.\n\nThe result is decrease in generation speed from 25T/s to 17T/s because obviously the GPU is faster even when it runs just some of the experts.\n\nThe more VRAM you have, the more expert layers can run on the GPU, and that will make things faster. But the *biggest* win is keeping all the other stuff on the GPU (and that will just take \\~5GB).",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7i9xiw",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes, that was me. But that was --n-cpu-moe 28 (28 experts on CPU, and pretty much maxing out VRAM of 3090) vs --cpu-moe (all experts on CPU) using just 5GB of VRAM.&lt;/p&gt;\n\n&lt;p&gt;The result is decrease in generation speed from 25T/s to 17T/s because obviously the GPU is faster even when it runs just some of the experts.&lt;/p&gt;\n\n&lt;p&gt;The more VRAM you have, the more expert layers can run on the GPU, and that will make things faster. But the &lt;em&gt;biggest&lt;/em&gt; win is keeping all the other stuff on the GPU (and that will just take ~5GB).&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mke7ef",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i9xiw/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754608651,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754608651,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 26
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7kiyqg",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "sussus_amogus69420",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7i9968",
                                          "score": 1,
                                          "author_fullname": "t2_uw2mo87y",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "getting 45 T/s with an M4 Max with the V-Ram limit override command (8Bit, MLX)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7kiyqg",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;getting 45 T/s with an M4 Max with the V-Ram limit override command (8Bit, MLX)&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mke7ef",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kiyqg/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754644053,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754644053,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7i9968",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "AdamDhahabi",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 15,
                                "author_fullname": "t2_x5lnbc2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yesterday some member here reported 25 t/s with a single RTX 3090.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7i9968",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday some member here reported 25 t/s with a single RTX 3090.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i9968/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754608424,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754608424,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 15
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7idl98",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Admirable-Star7088",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 10,
                                "author_fullname": "t2_qhlcbiy3k",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah 11 t/s is perfectly fine, I just thought if I can get even more speed, why not? :P  \nApparently, it appears I can't get higher speeds after some more trying now. I think my RAM may be a limit factor here as it's currently running at \\~half the MHz speed compared to your RAM.\n\nI also tried Qwen3-235B-A22B as I thought perhaps I will see more massive speed gains because it has much more active parameters that can be offloaded to VRAM, but nope. Without `--cpu-moe` I get \\~2.5 t/s, and with `--cpu-moe` I get \\~3 t/s. Better than nothing of course, but I'm a bit surprised that it was not more.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7idl98",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah 11 t/s is perfectly fine, I just thought if I can get even more speed, why not? :P&lt;br/&gt;\nApparently, it appears I can&amp;#39;t get higher speeds after some more trying now. I think my RAM may be a limit factor here as it&amp;#39;s currently running at ~half the MHz speed compared to your RAM.&lt;/p&gt;\n\n&lt;p&gt;I also tried Qwen3-235B-A22B as I thought perhaps I will see more massive speed gains because it has much more active parameters that can be offloaded to VRAM, but nope. Without &lt;code&gt;--cpu-moe&lt;/code&gt; I get ~2.5 t/s, and with &lt;code&gt;--cpu-moe&lt;/code&gt; I get ~3 t/s. Better than nothing of course, but I&amp;#39;m a bit surprised that it was not more.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7idl98/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754609901,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754609901,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 10
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7kdmjo",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Wrong-Historian",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7io586",
                                          "score": 2,
                                          "author_fullname": "t2_69r67vj3",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Reasoning is still on. I use reasoning medium (I set it in OpenWebUI which connects to llama-cpp-server)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7kdmjo",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Reasoning is still on. I use reasoning medium (I set it in OpenWebUI which connects to llama-cpp-server)&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mke7ef",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kdmjo/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754640974,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754640974,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7io586",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Psychological_Ad8426",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 1,
                                "author_fullname": "t2_81n44sue",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Do you feel like the accuracy is still good with reasoning off?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7io586",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you feel like the accuracy is still good with reasoning off?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": true,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7io586/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754613615,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754613615,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7i7dx0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754607806,
                      "send_replies": true,
                      "parent_id": "t1_n7i5rda",
                      "score": 81,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "    CUDA_VISIBLE_DEVICES=0  ~/build/llama.cpp/build-cuda/bin/llama-server \\\n       -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n       --cpu-moe \\\n       --n-gpu-layers 20 \\\n       -c 0 -fa --jinja --reasoning-format none \\\n       --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n\nThis is on Linux (Ubuntu 24.04). The very latest llama-cpp from git compiled for cuda. I have DDR5 96GB 6800 and GPU is 3090 (but only using the 5GB VRAM) though. I'd think 11-12T/s is still decent for a 120B, right?\n\n  \nEdit: I've updated the command in the main post. Increasing -n-gpu-layers will make things even faster. Then with --cpu-moe  it will still run experts on CPU. About 8GB VRAM for 25T/s token generation and 100T/s prefill.",
                      "edited": 1754639801,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7i7dx0",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0  ~/build/llama.cpp/build-cuda/bin/llama-server \\\n   -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n   --cpu-moe \\\n   --n-gpu-layers 20 \\\n   -c 0 -fa --jinja --reasoning-format none \\\n   --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \\\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This is on Linux (Ubuntu 24.04). The very latest llama-cpp from git compiled for cuda. I have DDR5 96GB 6800 and GPU is 3090 (but only using the 5GB VRAM) though. I&amp;#39;d think 11-12T/s is still decent for a 120B, right?&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;ve updated the command in the main post. Increasing -n-gpu-layers will make things even faster. Then with --cpu-moe  it will still run experts on CPU. About 8GB VRAM for 25T/s token generation and 100T/s prefill.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i7dx0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754607806,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 81
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7i5rda",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Admirable-Star7088",
            "can_mod_post": false,
            "created_utc": 1754607253,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 87,
            "author_fullname": "t2_qhlcbiy3k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have 16GB VRAM and 128GB RAM but \"only\" get \\~11-12 t/s. Can you show the full set of commands you use to gain this sort of speed? I apparently do something wrong.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7i5rda",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have 16GB VRAM and 128GB RAM but &amp;quot;only&amp;quot; get ~11-12 t/s. Can you show the full set of commands you use to gain this sort of speed? I apparently do something wrong.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i5rda/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754607253,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 87
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7l6d71",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "InsideYork",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7jx1qk",
                                          "score": 1,
                                          "author_fullname": "t2_12s3hn4y0b",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks. Any way to use tools that don't cripple it?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7l6d71",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks. Any way to use tools that don&amp;#39;t cripple it?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mke7ef",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7l6d71/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754655310,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754655310,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7jx1qk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Wrong-Historian",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7ivetn",
                                "score": 6,
                                "author_fullname": "t2_69r67vj3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It's mainly the prefill that kills it. That's about 100T/s.... So 1000 token of context is 10 seconds etc\n\nA setup of 4x3090 was shown to be over 1000T/s for this model",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7jx1qk",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s mainly the prefill that kills it. That&amp;#39;s about 100T/s.... So 1000 token of context is 10 seconds etc&lt;/p&gt;\n\n&lt;p&gt;A setup of 4x3090 was shown to be over 1000T/s for this model&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jx1qk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754631869,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754631869,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ivetn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "InsideYork",
                      "can_mod_post": false,
                      "created_utc": 1754616199,
                      "send_replies": true,
                      "parent_id": "t1_n7i770q",
                      "score": 7,
                      "author_fullname": "t2_12s3hn4y0b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah giving it more context tokens kill speed. MCP servers in the tools do too",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ivetn",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah giving it more context tokens kill speed. MCP servers in the tools do too&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7ivetn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754616199,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7i93he",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754608372,
                      "send_replies": true,
                      "parent_id": "t1_n7i770q",
                      "score": 7,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ill test tomorrow. I was testing with 3090 maxed out VRAM (so not just --cpu-moe but more on the GPU, --n-cpu-moe 28, but still far from all experts on GPU) and it did slow down somewhat (from 25T/s to 18T/s) for very long context, not that dramatic.\n\nSo the difference is --n-cpu-moe 28  (28 experts on CPU) vs --cpu-moe  (all experts on CPU). I just wouldn't expect a difference in 'slowdown with long context'\n\nI'll see what happens with --cpu-moe.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7i93he",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ill test tomorrow. I was testing with 3090 maxed out VRAM (so not just --cpu-moe but more on the GPU, --n-cpu-moe 28, but still far from all experts on GPU) and it did slow down somewhat (from 25T/s to 18T/s) for very long context, not that dramatic.&lt;/p&gt;\n\n&lt;p&gt;So the difference is --n-cpu-moe 28  (28 experts on CPU) vs --cpu-moe  (all experts on CPU). I just wouldn&amp;#39;t expect a difference in &amp;#39;slowdown with long context&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll see what happens with --cpu-moe.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i93he/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754608372,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7jsycy",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Mushoz",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7ity13",
                                "score": 7,
                                "author_fullname": "t2_gwpq7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "vLLM does support CPU offloading: https://docs.vllm.ai/en/v0.8.1/getting_started/examples/basic.html\n\nSee the `--cpu-offload-gb` switch",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7jsycy",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vLLM does support CPU offloading: &lt;a href=\"https://docs.vllm.ai/en/v0.8.1/getting_started/examples/basic.html\"&gt;https://docs.vllm.ai/en/v0.8.1/getting_started/examples/basic.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;See the &lt;code&gt;--cpu-offload-gb&lt;/code&gt; switch&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jsycy/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754629814,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754629814,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 7
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ity13",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "No-Refrigerator-1672",
                      "can_mod_post": false,
                      "created_utc": 1754615666,
                      "send_replies": true,
                      "parent_id": "t1_n7i770q",
                      "score": 3,
                      "author_fullname": "t2_baavelp5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "The decay of prompt processing speed is normal behaviour for all LLMs; hewever, in llama.cpp this devay is really bad. On dense models, you can expect the speed to half when going from 4k to 16k long prompt; sometimes even worse. Industrial grade solutions (i.e. vLLM) handle this decay much better and falloff is significantly less pronounced for them; but they never support CPU offloading.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ity13",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The decay of prompt processing speed is normal behaviour for all LLMs; hewever, in llama.cpp this devay is really bad. On dense models, you can expect the speed to half when going from 4k to 16k long prompt; sometimes even worse. Industrial grade solutions (i.e. vLLM) handle this decay much better and falloff is significantly less pronounced for them; but they never support CPU offloading.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7ity13/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754615666,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7i770q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Clipbeam",
            "can_mod_post": false,
            "created_utc": 1754607739,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 27,
            "author_fullname": "t2_1uxihq0vev",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "And have you tested with longer prompts? I noticed that as I increase context required, it exponentially slows down on my system",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7i770q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And have you tested with longer prompts? I noticed that as I increase context required, it exponentially slows down on my system&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i770q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754607739,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 27
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7l1pm1",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Infantryman1977",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7jz8fz",
                                "score": 1,
                                "author_fullname": "t2_g3lrqs8p",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That is very good outputs!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7l1pm1",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That is very good outputs!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7l1pm1/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754653452,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754653452,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7jz8fz",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754633025,
                      "send_replies": true,
                      "parent_id": "t1_n7iw2ub",
                      "score": 2,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's cool. What's your prefill speed for longer context?\n\nEdit: Yeah, I'm now also hitting &gt; 30T/s on my 3090.\n\n    ~/build/llama.cpp/build-cuda/bin/llama-server \n    -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \n    --n-cpu-moe 28 \n    --n-gpu-layers 999 \n    -c 0 -fa \n    --jinja --reasoning-format none \n    --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n\n    prompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)\n    eval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)",
                      "edited": 1754642332,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7jz8fz",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s cool. What&amp;#39;s your prefill speed for longer context?&lt;/p&gt;\n\n&lt;p&gt;Edit: Yeah, I&amp;#39;m now also hitting &amp;gt; 30T/s on my 3090.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \n-m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \n--n-cpu-moe 28 \n--n-gpu-layers 999 \n-c 0 -fa \n--jinja --reasoning-format none \n--host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \\\n\nprompt eval time =   78003.66 ms / 12715 tokens (    6.13 ms per token,   163.01 tokens per second)\neval time =   70376.61 ms /  2169 tokens (   32.45 ms per token,    30.82 tokens per second)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jz8fz/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754633025,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7jj876",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Infantryman1977",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7jh5ou",
                                "score": 11,
                                "author_fullname": "t2_g3lrqs8p",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "2 of 3 and 3 of 3 are in the same directory. llama.cpp is smart enough to load them all.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7jj876",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;2 of 3 and 3 of 3 are in the same directory. llama.cpp is smart enough to load them all.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jj876/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754625386,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754625386,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 11
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7jn4rj",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "BalorNG",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7jh5ou",
                                "score": 2,
                                "author_fullname": "t2_b6gw9q",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "New pruning techniques unlocked, take that MIT! :))",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7jn4rj",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;New pruning techniques unlocked, take that MIT! :))&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jn4rj/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754627096,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754627096,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7jh5ou",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Vivid-Anywhere2075",
                      "can_mod_post": false,
                      "created_utc": 1754624507,
                      "send_replies": true,
                      "parent_id": "t1_n7iw2ub",
                      "score": 3,
                      "author_fullname": "t2_rkof7swv",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is it proper when you use just the 1/3 weights?\n\n    /models/gpt-oss-120b-mxfp4-00001-of-00003.gguf",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7jh5ou",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is it proper when you use just the 1/3 weights?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jh5ou/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754624507,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7l1jf3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Infantryman1977",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7ki9b0",
                                "score": 1,
                                "author_fullname": "t2_g3lrqs8p",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It is the recommended parameter from either unsloth, ollama or openai. I thought the same when I first saw that! lol",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7l1jf3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is the recommended parameter from either unsloth, ollama or openai. I thought the same when I first saw that! lol&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7l1jf3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754653382,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754653382,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ki9b0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "__Maximum__",
                      "can_mod_post": false,
                      "created_utc": 1754643645,
                      "send_replies": true,
                      "parent_id": "t1_n7iw2ub",
                      "score": 1,
                      "author_fullname": "t2_fzqff6k3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Why with temp of 1.0?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ki9b0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why with temp of 1.0?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7ki9b0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754643645,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7iw2ub",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Infantryman1977",
            "can_mod_post": false,
            "created_utc": 1754616443,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 23,
            "author_fullname": "t2_g3lrqs8p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Getting roughly 35 t/s (5090, 9950X, 192GB DDR5): \n\n    docker run -d --gpus all \\\n      --name llamacpp-chatgpt120 \\\n      --restart unless-stopped \\\n      -p 8080:8080 \\\n      -v /home/infantryman/llamacpp:/models \\\n      llamacpp-server-cuda:latest \\\n      --model /models/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n      --alias chatgpt \\\n      --host 0.0.0.0 \\\n      --port 8080 \\\n      --jinja \\\n      --ctx-size 32768 \\\n      --n-cpu-moe 19 \\\n      --flash-attn \\\n      --temp 1.0 \\\n      --top-p 1.0 \\\n      --top-k 0 \\\n      --n-gpu-layers 999",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7iw2ub",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Getting roughly 35 t/s (5090, 9950X, 192GB DDR5): &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker run -d --gpus all \\\n  --name llamacpp-chatgpt120 \\\n  --restart unless-stopped \\\n  -p 8080:8080 \\\n  -v /home/infantryman/llamacpp:/models \\\n  llamacpp-server-cuda:latest \\\n  --model /models/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n  --alias chatgpt \\\n  --host 0.0.0.0 \\\n  --port 8080 \\\n  --jinja \\\n  --ctx-size 32768 \\\n  --n-cpu-moe 19 \\\n  --flash-attn \\\n  --temp 1.0 \\\n  --top-p 1.0 \\\n  --top-k 0 \\\n  --n-gpu-layers 999\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7iw2ub/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754616443,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 23
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7j5vi3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Gary5Host9",
            "can_mod_post": false,
            "created_utc": 1754620026,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 9,
            "author_fullname": "t2_ad2x8irq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a 3060ti and 64gb of ram so this is very interesting to me",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7j5vi3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3060ti and 64gb of ram so this is very interesting to me&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7j5vi3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754620026,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7jt9j1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "dreamai87",
                      "can_mod_post": false,
                      "created_utc": 1754629964,
                      "send_replies": true,
                      "parent_id": "t1_n7jiwf7",
                      "score": 11,
                      "author_fullname": "t2_q1q7nlxa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It’s possible when they will add option in ui as of now not.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7jt9j1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s possible when they will add option in ui as of now not.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jt9j1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754629964,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 11
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7jiwf7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Dentuam",
            "can_mod_post": false,
            "created_utc": 1754625249,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 7,
            "author_fullname": "t2_znctr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "is --cpu-moe possible on LMStudio?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7jiwf7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;is --cpu-moe possible on LMStudio?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jiwf7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754625249,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7jr8p5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "nightowlflaps",
            "can_mod_post": false,
            "created_utc": 1754628990,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 5,
            "author_fullname": "t2_1fhmpgckfd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Any way for this to work on koboldcpp?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7jr8p5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Any way for this to work on koboldcpp?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jr8p5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754628990,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7it20o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "cristoper",
            "can_mod_post": false,
            "created_utc": 1754615347,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 7,
            "author_fullname": "t2_38xkk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Does anyone know how this compares (tokens/s) with glm-4.5-air on the same hardware?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7it20o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how this compares (tokens/s) with glm-4.5-air on the same hardware?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7it20o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754615347,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7kcjr8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754640350,
                      "send_replies": true,
                      "parent_id": "t1_n7k71mc",
                      "score": 2,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'd say nice job OpenAI. Whole world is bitching on this model but they've designed the perfect architecture for running-at-home on consumer hardware.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7kcjr8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d say nice job OpenAI. Whole world is bitching on this model but they&amp;#39;ve designed the perfect architecture for running-at-home on consumer hardware.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kcjr8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754640350,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k71mc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "c-rious",
            "can_mod_post": false,
            "created_utc": 1754637213,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 4,
            "author_fullname": "t2_mjqap",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Feels like MoE is saving NVIDIA - out of VRAM scarcity this new architecture arrived, you still need big and lots of compute to train large models, but can keep consumer VRAM fairly below datacenter cards. Nice job Jensen!\n\nAlso, thanks for mentioning --cpu-moe flag TIL!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k71mc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Feels like MoE is saving NVIDIA - out of VRAM scarcity this new architecture arrived, you still need big and lots of compute to train large models, but can keep consumer VRAM fairly below datacenter cards. Nice job Jensen!&lt;/p&gt;\n\n&lt;p&gt;Also, thanks for mentioning --cpu-moe flag TIL!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k71mc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754637213,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7j8021",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ok-Farm4498",
            "can_mod_post": false,
            "created_utc": 1754620832,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 2,
            "author_fullname": "t2_cchvc8e7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a 3090, 5060 ti and 128gb of ddr5 ram.  I didn’t think there would be a way to get anything more than a crawl with a 120b model",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7j8021",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3090, 5060 ti and 128gb of ddr5 ram.  I didn’t think there would be a way to get anything more than a crawl with a 120b model&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7j8021/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754620832,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7jwk0h",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754631611,
                      "send_replies": true,
                      "parent_id": "t1_n7jq6uq",
                      "score": 3,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah prefill is about 100T/s....\n\nIf you want that to be faster you really need 4x 3090. That was shown to have prefill of ~1000T/s",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7jwk0h",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah prefill is about 100T/s....&lt;/p&gt;\n\n&lt;p&gt;If you want that to be faster you really need 4x 3090. That was shown to have prefill of ~1000T/s&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jwk0h/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754631611,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7jq6uq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "one-wandering-mind",
            "can_mod_post": false,
            "created_utc": 1754628502,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 2,
            "author_fullname": "t2_1nb4dvvcpa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Am I reading this right there it is 28 seconds to the first token for a context or 3440 tokens? That is really slow. Is it significantly faster than CPU only ?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7jq6uq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Am I reading this right there it is 28 seconds to the first token for a context or 3440 tokens? That is really slow. Is it significantly faster than CPU only ?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jq6uq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754628502,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7jueqc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tomByrer",
            "can_mod_post": false,
            "created_utc": 1754630522,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 2,
            "author_fullname": "t2_ff4zz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I assume you're talking about GPT-OSS-120B?\n\nI guess there's hope for my RTX3080 to be used for AI.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7jueqc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I assume you&amp;#39;re talking about GPT-OSS-120B?&lt;/p&gt;\n\n&lt;p&gt;I guess there&amp;#39;s hope for my RTX3080 to be used for AI.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7jueqc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754630522,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7l30x4",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "floppypancakes4u",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7krs39",
                                "score": 1,
                                "author_fullname": "t2_uimda",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Sorry, im just now getting into llm at home so I'm trying to be a sponge and learn as much as I can. Why does having context length high hurt the quality so much? How does chatgpt and other services still provide quality answers with 10k+ context length?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7l30x4",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry, im just now getting into llm at home so I&amp;#39;m trying to be a sponge and learn as much as I can. Why does having context length high hurt the quality so much? How does chatgpt and other services still provide quality answers with 10k+ context length?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7l30x4/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754653995,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754653995,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7krs39",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754648891,
                      "send_replies": true,
                      "parent_id": "t1_n7k3bw5",
                      "score": 1,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "128k but the prefill speed is just 120T/s so uhmmm with 120k context it will take 1000 seconds to first token.....   (maybe you can use some context caching or something). You'll far sooner run into actual practical speed limits than that you fill up the context of the model. You'll get much further with some intelligent compression/RAG of context and trying to limit context to &lt;4000 tokens etc, instead of trying to stuff 100k tokens into the context (which also really hurt the quality of responses of any model, so it's bad practice anyway).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7krs39",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;128k but the prefill speed is just 120T/s so uhmmm with 120k context it will take 1000 seconds to first token.....   (maybe you can use some context caching or something). You&amp;#39;ll far sooner run into actual practical speed limits than that you fill up the context of the model. You&amp;#39;ll get much further with some intelligent compression/RAG of context and trying to limit context to &amp;lt;4000 tokens etc, instead of trying to stuff 100k tokens into the context (which also really hurt the quality of responses of any model, so it&amp;#39;s bad practice anyway).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7krs39/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754648891,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k3bw5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Michaeli_Starky",
            "can_mod_post": false,
            "created_utc": 1754635211,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 2,
            "author_fullname": "t2_11kc8cz9rh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How large is the context?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k3bw5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How large is the context?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k3bw5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754635211,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ktvnw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "OrdinaryAdditional91",
            "can_mod_post": false,
            "created_utc": 1754649961,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 2,
            "author_fullname": "t2_8jxxxmh1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How to use llama.cpp serve with kilo code or cline? The response format seems to have some issues, including tags like &lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;, which cannot be proper parsed by the tools.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ktvnw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How to use llama.cpp serve with kilo code or cline? The response format seems to have some issues, including tags like &amp;lt;|start|&amp;gt;assistant&amp;lt;|channel|&amp;gt;final&amp;lt;|message|&amp;gt;, which cannot be proper parsed by the tools.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7ktvnw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754649961,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7k3gzk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "RemindMeBot",
                      "can_mod_post": false,
                      "created_utc": 1754635289,
                      "send_replies": true,
                      "parent_id": "t1_n7k3d55",
                      "score": 1,
                      "author_fullname": "t2_gbm4p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I will be messaging you in 2 days on [**2025-08-10 06:40:30 UTC**](http://www.wolframalpha.com/input/?i=2025-08-10%2006:40:30%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k3d55/?context=3)\n\n[**2 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1mke7ef%2F120b_runs_awesome_on_just_8gb_vram%2Fn7k3d55%2F%5D%0A%0ARemindMe%21%202025-08-10%2006%3A40%3A30%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%201mke7ef)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
                      "edited": 1754650524,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7k3gzk",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I will be messaging you in 2 days on &lt;a href=\"http://www.wolframalpha.com/input/?i=2025-08-10%2006:40:30%20UTC%20To%20Local%20Time\"&gt;&lt;strong&gt;2025-08-10 06:40:30 UTC&lt;/strong&gt;&lt;/a&gt; to remind you of &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k3d55/?context=3\"&gt;&lt;strong&gt;this link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1mke7ef%2F120b_runs_awesome_on_just_8gb_vram%2Fn7k3d55%2F%5D%0A%0ARemindMe%21%202025-08-10%2006%3A40%3A30%20UTC\"&gt;&lt;strong&gt;2 OTHERS CLICKED THIS LINK&lt;/strong&gt;&lt;/a&gt; to send a PM to also be reminded and to reduce spam.&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Parent commenter can &lt;/sup&gt; &lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Delete%20Comment&amp;amp;message=Delete%21%201mke7ef\"&gt;&lt;sup&gt;delete this message to hide from others.&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/\"&gt;&lt;sup&gt;Info&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here\"&gt;&lt;sup&gt;Custom&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=List%20Of%20Reminders&amp;amp;message=MyReminders%21\"&gt;&lt;sup&gt;Your Reminders&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=Watchful1&amp;amp;subject=RemindMeBot%20Feedback\"&gt;&lt;sup&gt;Feedback&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k3gzk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754635289,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k3d55",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Infamous_Land_1220",
            "can_mod_post": false,
            "created_utc": 1754635230,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 1,
            "author_fullname": "t2_9y3v6ur0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "!remindme 2 days",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k3d55",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;!remindme 2 days&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k3d55/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754635230,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7k8bpl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Atyzzze",
                      "can_mod_post": false,
                      "created_utc": 1754637923,
                      "send_replies": true,
                      "parent_id": "t1_n7k4drp",
                      "score": 2,
                      "author_fullname": "t2_zld9t26uj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "No, without enough total memory you can forget it. Swapping to disk for something like this just isn't feasible. At least double your ram, then you should be able to.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7k8bpl",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, without enough total memory you can forget it. Swapping to disk for something like this just isn&amp;#39;t feasible. At least double your ram, then you should be able to.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k8bpl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754637923,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k4drp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lumos675",
            "can_mod_post": false,
            "created_utc": 1754635776,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 1,
            "author_fullname": "t2_7ivs4x3r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Guys i have only a 4060ti with 16gb vram and 32gb ram. Do i have any hope to run this model?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k4drp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Guys i have only a 4060ti with 16gb vram and 32gb ram. Do i have any hope to run this model?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k4drp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754635776,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kli3h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DawarAzhar",
            "can_mod_post": false,
            "created_utc": 1754645498,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 1,
            "author_fullname": "t2_x4bhz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "64 GB RAM, RTX 3060, Ryzen 5950x - going to try it today!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kli3h",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;64 GB RAM, RTX 3060, Ryzen 5950x - going to try it today!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kli3h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754645498,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7kuh4a",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754650244,
                      "send_replies": true,
                      "parent_id": "t1_n7kmy1m",
                      "score": 1,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "gpt-oss 20B",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7kuh4a",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;gpt-oss 20B&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kuh4a/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754650244,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7kmy1m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "OXKSA1",
            "can_mod_post": false,
            "created_utc": 1754646304,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 1,
            "author_fullname": "t2_4220ryow",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "i want to do this but i only have 12gb vram and 32gb ram, is there model which can fit for my specs?  \n(win11 btw)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kmy1m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i want to do this but i only have 12gb vram and 32gb ram, is there model which can fit for my specs?&lt;br/&gt;\n(win11 btw)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kmy1m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754646304,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kumok",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Key_Extension_6003",
            "can_mod_post": false,
            "created_utc": 1754650317,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 1,
            "author_fullname": "t2_8zyug5sg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "!remindme 6 days",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kumok",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;!remindme 6 days&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kumok/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754650317,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7l3v04",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wrxld",
            "can_mod_post": false,
            "created_utc": 1754654340,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 1,
            "author_fullname": "t2_7u4lxwnw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Chat, is this real?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7l3v04",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Chat, is this real?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7l3v04/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754654340,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7kwz5k",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "SunTrainAi",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7kqq2s",
                                "score": 1,
                                "author_fullname": "t2_16dxelngi2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Just compare a Maverick to 14b Models and you will be surprised too",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7kwz5k",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just compare a Maverick to 14b Models and you will be surprised too&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kwz5k/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754651394,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754651394,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7kqq2s",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754648345,
                      "send_replies": true,
                      "parent_id": "t1_n7ko4oy",
                      "score": 3,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Its **by far** the best model you can run locally at actual practical speeds without going to a full 4x 3090 setup or something. You need to compare it to like 14B models which will give similar speeds as this. You get the performance/speed of a 14B but at the intelligence of o4-mini. On low-end consumer hardware. INSANE. People bitch about it because they compare it to 671B, but that's not the point of this model. It's still an order-of-magnitude improvement of speed-intelligence.\n\nOh wait, you need the erotic-AI-girlfriend thing, and this model doesn't do that. Yeah ok. Sucks to sucks.",
                      "edited": 1754648597,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7kqq2s",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Its &lt;strong&gt;by far&lt;/strong&gt; the best model you can run locally at actual practical speeds without going to a full 4x 3090 setup or something. You need to compare it to like 14B models which will give similar speeds as this. You get the performance/speed of a 14B but at the intelligence of o4-mini. On low-end consumer hardware. INSANE. People bitch about it because they compare it to 671B, but that&amp;#39;s not the point of this model. It&amp;#39;s still an order-of-magnitude improvement of speed-intelligence.&lt;/p&gt;\n\n&lt;p&gt;Oh wait, you need the erotic-AI-girlfriend thing, and this model doesn&amp;#39;t do that. Yeah ok. Sucks to sucks.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kqq2s/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754648345,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7kyqzt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "tarruda",
                      "can_mod_post": false,
                      "created_utc": 1754652185,
                      "send_replies": true,
                      "parent_id": "t1_n7ko4oy",
                      "score": 1,
                      "author_fullname": "t2_dphk4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I wouldn't be so quick too judge GPT-OSS. Lots of inference engines still have bugs and don't support its full capabilities.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7kyqzt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wouldn&amp;#39;t be so quick too judge GPT-OSS. Lots of inference engines still have bugs and don&amp;#39;t support its full capabilities.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7kyqzt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754652185,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7ko4oy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DrummerPrevious",
            "can_mod_post": false,
            "created_utc": 1754646954,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": -1,
            "author_fullname": "t2_8w0wbm6d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why would i run a stupid model ?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ko4oy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why would i run a stupid model ?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7ko4oy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754646954,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7k1b5g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "thetaFAANG",
            "can_mod_post": false,
            "created_utc": 1754634115,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 0,
            "author_fullname": "t2_da5i8ajs",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That’s fascinating",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k1b5g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s fascinating&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7k1b5g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754634115,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]