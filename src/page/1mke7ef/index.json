[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Here is the thing, the expert layers run amazing on CPU  (\\~17T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .\n\nYou can offload just the attention layers to GPU  (requiring about 5GB of VRAM) for fast prefill.\n\n* KV cache for the sequence\n* Attention weights &amp; activations\n* Routing tables\n* LayerNorms and other “non-expert” parameters\n\nNo giant MLP weights are resident on the GPU, so memory use stays low.\n\nThis yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.\n\n64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)\n\n&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)\n\n&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)\n\nwith 5GB of vram usage!\n\nHonestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "120B runs awesome on just 8GB VRAM!",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mke7ef",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 40,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_69r67vj3",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 40,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754606345,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754605924,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU  (~17T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt;\n\n&lt;p&gt;You can offload just the attention layers to GPU  (requiring about 5GB of VRAM) for fast prefill.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;KV cache for the sequence&lt;/li&gt;\n&lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt;\n&lt;li&gt;Routing tables&lt;/li&gt;\n&lt;li&gt;LayerNorms and other “non-expert” parameters&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt;\n\n&lt;p&gt;This yields an amazing snappy system for a 120B model!  Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt;\n\n&lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the &amp;#39;hot&amp;#39; experts in memory even if the whole model doesn&amp;#39;t fit in memory)&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;with 5GB of vram usage!&lt;/p&gt;\n\n&lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mke7ef",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Wrong-Historian",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/",
            "subreddit_subscribers": 513416,
            "created_utc": 1754605924,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7icewz",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "fp4guru",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 4,
                                "author_fullname": "t2_1tp8zldw5g",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I get 12 with unsloth gguf and 4090. Which one is your gguf from?\n\nI changed the layer to 37 , getting 23.\nNew finding: unsloth's gguf loading speed is much faster than ggml version, not sure why.",
                                "edited": 1754615503,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7icewz",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I get 12 with unsloth gguf and 4090. Which one is your gguf from?&lt;/p&gt;\n\n&lt;p&gt;I changed the layer to 37 , getting 23.\nNew finding: unsloth&amp;#39;s gguf loading speed is much faster than ggml version, not sure why.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7icewz/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754609496,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754609496,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7i9xiw",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Wrong-Historian",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7i9968",
                                          "score": 3,
                                          "author_fullname": "t2_69r67vj3",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "yes, that was me. But that was --n-cpu-moe 28 (28 experts on CPU, and pretty much maxing out VRAM of 3090) vs --cpu-moe (all experts on CPU) using just 5GB of VRAM.\n\nThe result is decrease in generation speed from 25T/s to 17T/s because obviously the GPU is faster even when it runs just some of the experts.\n\nThe more VRAM you have, the more expert layers can run on the GPU, and that will make things faster. But the *biggest* win is keeping all the other stuff on the GPU (and that will just take \\~5GB).",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7i9xiw",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes, that was me. But that was --n-cpu-moe 28 (28 experts on CPU, and pretty much maxing out VRAM of 3090) vs --cpu-moe (all experts on CPU) using just 5GB of VRAM.&lt;/p&gt;\n\n&lt;p&gt;The result is decrease in generation speed from 25T/s to 17T/s because obviously the GPU is faster even when it runs just some of the experts.&lt;/p&gt;\n\n&lt;p&gt;The more VRAM you have, the more expert layers can run on the GPU, and that will make things faster. But the &lt;em&gt;biggest&lt;/em&gt; win is keeping all the other stuff on the GPU (and that will just take ~5GB).&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mke7ef",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i9xiw/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754608651,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754608651,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7i9968",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AdamDhahabi",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 1,
                                "author_fullname": "t2_x5lnbc2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yesterday some member here reported 25 t/s with a single RTX 3090.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7i9968",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday some member here reported 25 t/s with a single RTX 3090.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i9968/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754608424,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754608424,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7idl98",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Admirable-Star7088",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 1,
                                "author_fullname": "t2_qhlcbiy3k",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah 11 t/s is perfectly fine, I just thought if I can get even more speed, why not? :P  \nApparently, it appears I can't get higher speeds after some more trying now. I think my RAM may be a limit factor here as it's currently running at \\~half the MHz speed compared to your RAM.\n\nI also tried Qwen3-235B-A22B as I thought perhaps I will see more massive speed gains because it has much more active parameters that can be offloaded to VRAM, but nope. Without `--cpu-moe` I get \\~2.5 t/s, and with `--cpu-moe` I get \\~3 t/s. Better than nothing of course, but I'm a bit surprised that it was not more.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7idl98",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah 11 t/s is perfectly fine, I just thought if I can get even more speed, why not? :P&lt;br/&gt;\nApparently, it appears I can&amp;#39;t get higher speeds after some more trying now. I think my RAM may be a limit factor here as it&amp;#39;s currently running at ~half the MHz speed compared to your RAM.&lt;/p&gt;\n\n&lt;p&gt;I also tried Qwen3-235B-A22B as I thought perhaps I will see more massive speed gains because it has much more active parameters that can be offloaded to VRAM, but nope. Without &lt;code&gt;--cpu-moe&lt;/code&gt; I get ~2.5 t/s, and with &lt;code&gt;--cpu-moe&lt;/code&gt; I get ~3 t/s. Better than nothing of course, but I&amp;#39;m a bit surprised that it was not more.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7idl98/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754609901,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754609901,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7io586",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Psychological_Ad8426",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7i7dx0",
                                "score": 1,
                                "author_fullname": "t2_81n44sue",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Do you feel like the accuracy is still good with reasoning off?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7io586",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you feel like the accuracy is still good with reasoning off?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": true,
                                "can_gild": false,
                                "link_id": "t3_1mke7ef",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7io586/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754613615,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754613615,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7i7dx0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754607806,
                      "send_replies": true,
                      "parent_id": "t1_n7i5rda",
                      "score": 10,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "    CUDA_VISIBLE_DEVICES=0  ~/build/llama.cpp/build-cuda/bin/llama-server \\\n       -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n       --cpu-moe \\\n       --n-gpu-layers 20 \\\n       -c 0 -fa --jinja --reasoning-format none \\\n       --host 0.0.0.0 --port 8502 --api-key \"dummy\" \\\n\nThis is on Linux (Ubuntu 24.04). The very latest llama-cpp from git compiled for cuda. I have DDR5 96GB 6800 and GPU is 3090 (but only using the 5GB VRAM) though. I'd think 11-12T/s is still decent for a 120B, right?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7i7dx0",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0  ~/build/llama.cpp/build-cuda/bin/llama-server \\\n   -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \\\n   --cpu-moe \\\n   --n-gpu-layers 20 \\\n   -c 0 -fa --jinja --reasoning-format none \\\n   --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \\\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This is on Linux (Ubuntu 24.04). The very latest llama-cpp from git compiled for cuda. I have DDR5 96GB 6800 and GPU is 3090 (but only using the 5GB VRAM) though. I&amp;#39;d think 11-12T/s is still decent for a 120B, right?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i7dx0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754607806,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 10
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7i5rda",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Admirable-Star7088",
            "can_mod_post": false,
            "created_utc": 1754607253,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 6,
            "author_fullname": "t2_qhlcbiy3k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have 16GB VRAM and 128GB RAM but \"only\" get \\~11-12 t/s. Can you show the full set of commands you use to gain this sort of speed? I apparently do something wrong.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7i5rda",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have 16GB VRAM and 128GB RAM but &amp;quot;only&amp;quot; get ~11-12 t/s. Can you show the full set of commands you use to gain this sort of speed? I apparently do something wrong.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i5rda/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754607253,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7i93he",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Wrong-Historian",
                      "can_mod_post": false,
                      "created_utc": 1754608372,
                      "send_replies": true,
                      "parent_id": "t1_n7i770q",
                      "score": 2,
                      "author_fullname": "t2_69r67vj3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ill test tomorrow. I was testing with 3090 maxed out VRAM (so not just --cpu-moe but more on the GPU, --n-cpu-moe 28, but still far from all experts on GPU) and it did slow down somewhat (from 25T/s to 18T/s) for very long context, not that dramatic.\n\nSo the difference is --n-cpu-moe 28  (28 experts on CPU) vs --cpu-moe  (all experts on CPU). I just wouldn't expect a difference in 'slowdown with long context'\n\nI'll see what happens with --cpu-moe.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7i93he",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ill test tomorrow. I was testing with 3090 maxed out VRAM (so not just --cpu-moe but more on the GPU, --n-cpu-moe 28, but still far from all experts on GPU) and it did slow down somewhat (from 25T/s to 18T/s) for very long context, not that dramatic.&lt;/p&gt;\n\n&lt;p&gt;So the difference is --n-cpu-moe 28  (28 experts on CPU) vs --cpu-moe  (all experts on CPU). I just wouldn&amp;#39;t expect a difference in &amp;#39;slowdown with long context&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll see what happens with --cpu-moe.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mke7ef",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i93he/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754608372,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7i770q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Clipbeam",
            "can_mod_post": false,
            "created_utc": 1754607739,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 3,
            "author_fullname": "t2_1uxihq0vev",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "And have you tested with longer prompts? I noticed that as I increase context required, it exponentially slows down on my system",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7i770q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And have you tested with longer prompts? I noticed that as I increase context required, it exponentially slows down on my system&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7i770q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754607739,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7it20o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cristoper",
            "can_mod_post": false,
            "created_utc": 1754615347,
            "send_replies": true,
            "parent_id": "t3_1mke7ef",
            "score": 1,
            "author_fullname": "t2_38xkk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Does anyone know how this compares (tokens/s) with glm-4.5-air on the same hardware?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7it20o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how this compares (tokens/s) with glm-4.5-air on the same hardware?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/n7it20o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754615347,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mke7ef",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]