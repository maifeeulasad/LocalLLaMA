[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "It seems to me that these 2 seem to be roughly comparable in performance for running LLM? Does anyone else have experience or thoughts on this? \n\nComparing 64gb M4 Max (16 core) with 128gb Max+ for 3 reasons:\n\n* Costs are roughly similar (mac costs more naturally but close enough to be competitive)\n* Max+ has to split memory between sys and vram, it's not unified like M4, and it needs to load models into sys ram first before offloading to vram, so realistically it's best to leave it at 64gb/64gb split.\n* Neither has enough GPU power to run models larger models than would fit in 64gb at a fast enough speed so there's little to no benefit to more vram\n* The M4 Max really isn't 64gb, since it can only use 75% of shared memory in vram, so it's really 48gb, which is a significant trade-off\n\n\nLLM use: \n\nThere's not a ton of information out there, but from what I've seen they seem roughly similar in speed\n\nImage Generation: \n\nI struggle to find any information on this but I'm really interested in this\n\n* M4 - I know you can generate images on M4 using tools like Draw Things, but it's pretty slow compared to a mid-tier nvidia card. I don't know if you can use comfyui \n* Max+ - I can't find information, but I imagine it's slightly better performance than M4, but performance on AMD is not well supported with stable diffusion compared with nvidia... but honestly just guessing here.\n\nGaming: Obviously M4 loses on this one, but this is far from a priority to me.\n\nExternal Support: M4 wins massively, TB5 everywhere\n\nBuild Quality/Support: Again M4 wins massively.\n\nEfficiency: Not enough information, but M4 and apple generally are known to be extremely power efficient.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Understanding trade-offs between: m4 max studio vs AI Max+ 395",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1ltv847",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.58,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_8cqm44sa9",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1751897557,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems to me that these 2 seem to be roughly comparable in performance for running LLM? Does anyone else have experience or thoughts on this? &lt;/p&gt;\n\n&lt;p&gt;Comparing 64gb M4 Max (16 core) with 128gb Max+ for 3 reasons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Costs are roughly similar (mac costs more naturally but close enough to be competitive)&lt;/li&gt;\n&lt;li&gt;Max+ has to split memory between sys and vram, it&amp;#39;s not unified like M4, and it needs to load models into sys ram first before offloading to vram, so realistically it&amp;#39;s best to leave it at 64gb/64gb split.&lt;/li&gt;\n&lt;li&gt;Neither has enough GPU power to run models larger models than would fit in 64gb at a fast enough speed so there&amp;#39;s little to no benefit to more vram&lt;/li&gt;\n&lt;li&gt;The M4 Max really isn&amp;#39;t 64gb, since it can only use 75% of shared memory in vram, so it&amp;#39;s really 48gb, which is a significant trade-off&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;LLM use: &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s not a ton of information out there, but from what I&amp;#39;ve seen they seem roughly similar in speed&lt;/p&gt;\n\n&lt;p&gt;Image Generation: &lt;/p&gt;\n\n&lt;p&gt;I struggle to find any information on this but I&amp;#39;m really interested in this&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;M4 - I know you can generate images on M4 using tools like Draw Things, but it&amp;#39;s pretty slow compared to a mid-tier nvidia card. I don&amp;#39;t know if you can use comfyui &lt;/li&gt;\n&lt;li&gt;Max+ - I can&amp;#39;t find information, but I imagine it&amp;#39;s slightly better performance than M4, but performance on AMD is not well supported with stable diffusion compared with nvidia... but honestly just guessing here.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Gaming: Obviously M4 loses on this one, but this is far from a priority to me.&lt;/p&gt;\n\n&lt;p&gt;External Support: M4 wins massively, TB5 everywhere&lt;/p&gt;\n\n&lt;p&gt;Build Quality/Support: Again M4 wins massively.&lt;/p&gt;\n\n&lt;p&gt;Efficiency: Not enough information, but M4 and apple generally are known to be extremely power efficient.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1ltv847",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "siegekeebsofficial",
            "discussion_type": null,
            "num_comments": 35,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/",
            "subreddit_subscribers": 495645,
            "created_utc": 1751897557,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1ubnag",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "siegekeebsofficial",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1u6max",
                                          "score": 1,
                                          "author_fullname": "t2_8cqm44sa9",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks a lot! That's definitely a usable speed",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1ubnag",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks a lot! That&amp;#39;s definitely a usable speed&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1ubnag/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751909452,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751909452,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1u6max",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "thomthehound",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1tqhto",
                                "score": 2,
                                "author_fullname": "t2_vxbs7cf4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "On my Evo X-2 (Strix Halo, 128 GB) I get \n\nImage (1024x1024 batch size 1): \n\nSDXL (Illustrious) \\~ 1.5 it/s \n\nFlux.1 dev (GGUF Q8) \\~ 4.7 s/it (notice this is seconds/per and not per second) \n\nChroma (GGUF Q8) \\~ 8.8 s/it\n\nVideo (832x480 33 frames):\n\nWan 2.1 t2v 1.3B FP16 \\~ 12.5 s/it\n\nThis required installing custom, and somewhat buggy, Python wheels with PyTorch compiled against ROCm. AMD estimates full ROCm Windows support sometime in August.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1u6max",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;On my Evo X-2 (Strix Halo, 128 GB) I get &lt;/p&gt;\n\n&lt;p&gt;Image (1024x1024 batch size 1): &lt;/p&gt;\n\n&lt;p&gt;SDXL (Illustrious) ~ 1.5 it/s &lt;/p&gt;\n\n&lt;p&gt;Flux.1 dev (GGUF Q8) ~ 4.7 s/it (notice this is seconds/per and not per second) &lt;/p&gt;\n\n&lt;p&gt;Chroma (GGUF Q8) ~ 8.8 s/it&lt;/p&gt;\n\n&lt;p&gt;Video (832x480 33 frames):&lt;/p&gt;\n\n&lt;p&gt;Wan 2.1 t2v 1.3B FP16 ~ 12.5 s/it&lt;/p&gt;\n\n&lt;p&gt;This required installing custom, and somewhat buggy, Python wheels with PyTorch compiled against ROCm. AMD estimates full ROCm Windows support sometime in August.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1u6max/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751907987,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751907987,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1ts9cd",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "siegekeebsofficial",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1tqvli",
                                          "score": 2,
                                          "author_fullname": "t2_8cqm44sa9",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "My reading comprehension skills failed me, thanks for the detailed original response",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1ts9cd",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My reading comprehension skills failed me, thanks for the detailed original response&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1ts9cd/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751903864,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751903864,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1tqvli",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "randomfoo2",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1tqhto",
                                "score": 1,
                                "author_fullname": "t2_eztox",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "&gt;Strix Halo has more raw TFLOPS, but it's hard to say how that translates to real-world image performance, I haven't tested that at all.\n\nNo.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1tqvli",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Strix Halo has more raw TFLOPS, but it&amp;#39;s hard to say how that translates to real-world image performance, I haven&amp;#39;t tested that at all.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;No.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tqvli/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751903476,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751903476,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1tqhto",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "siegekeebsofficial",
                      "can_mod_post": false,
                      "created_utc": 1751903368,
                      "send_replies": true,
                      "parent_id": "t1_n1tjqoi",
                      "score": 2,
                      "author_fullname": "t2_8cqm44sa9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for this! I was hoping to get someone who actually has some experience with this hardware to clarify some details! \n\nYou've definitely made it clear that the max+ has some significant advantages\n\nHave you tried doing any image generation with it?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1tqhto",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for this! I was hoping to get someone who actually has some experience with this hardware to clarify some details! &lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ve definitely made it clear that the max+ has some significant advantages&lt;/p&gt;\n\n&lt;p&gt;Have you tried doing any image generation with it?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tqhto/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751903368,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": {
                                                                "kind": "Listing",
                                                                "data": {
                                                                  "after": null,
                                                                  "dist": null,
                                                                  "modhash": "",
                                                                  "geo_filter": "",
                                                                  "children": [
                                                                    {
                                                                      "kind": "t1",
                                                                      "data": {
                                                                        "subreddit_id": "t5_81eyvm",
                                                                        "approved_at_utc": null,
                                                                        "author_is_blocked": false,
                                                                        "comment_type": null,
                                                                        "awarders": [],
                                                                        "mod_reason_by": null,
                                                                        "banned_by": null,
                                                                        "author_flair_type": "text",
                                                                        "total_awards_received": 0,
                                                                        "subreddit": "LocalLLaMA",
                                                                        "author_flair_template_id": null,
                                                                        "distinguished": null,
                                                                        "likes": null,
                                                                        "replies": "",
                                                                        "user_reports": [],
                                                                        "saved": false,
                                                                        "id": "n1u1tfh",
                                                                        "banned_at_utc": null,
                                                                        "mod_reason_title": null,
                                                                        "gilded": 0,
                                                                        "archived": false,
                                                                        "collapsed_reason_code": null,
                                                                        "no_follow": true,
                                                                        "author": "randomfoo2",
                                                                        "can_mod_post": false,
                                                                        "send_replies": true,
                                                                        "parent_id": "t1_n1tzxzt",
                                                                        "score": 1,
                                                                        "author_fullname": "t2_eztox",
                                                                        "approved_by": null,
                                                                        "mod_note": null,
                                                                        "all_awardings": [],
                                                                        "collapsed": false,
                                                                        "body": "I use Likwid Bench and STREAM for my CPU testing. If they're not representative for testing ¯\\\\\\_(ツ)\\_/¯ - although I will say that if you use the llama.cpp CPU backend you get a significantly lower tg128 that seems to reflect a lower CPU mbw.\n\nSemianalysis' results are no surprise. When I did my testing on MI300X last year it was easy to get 2X+ differences in inference performance with minor changes to the default settings: [https://shisa.ai/posts/tuning-vllm-mi300x/](https://shisa.ai/posts/tuning-vllm-mi300x/)\n\nTraining was subtly broken (causing GPU resets) and took &gt;2 months (longer than I had the hardware I was testing) to fix.\n\nIt is better now, however this is for CDNA. RDNA3 support is a whole different ball of wax (if you don't have gfx1100, you ... should probably pretend you do). While most things work, there are rough spots and you'd still be much better off with a 3090: [https://llm-tracker.info/howto/AMD-GPUs](https://llm-tracker.info/howto/AMD-GPUs)",
                                                                        "edited": false,
                                                                        "gildings": {},
                                                                        "author_flair_css_class": null,
                                                                        "name": "t1_n1u1tfh",
                                                                        "is_submitter": false,
                                                                        "downs": 0,
                                                                        "author_flair_richtext": [],
                                                                        "author_patreon_flair": false,
                                                                        "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use Likwid Bench and STREAM for my CPU testing. If they&amp;#39;re not representative for testing ¯\\_(ツ)_/¯ - although I will say that if you use the llama.cpp CPU backend you get a significantly lower tg128 that seems to reflect a lower CPU mbw.&lt;/p&gt;\n\n&lt;p&gt;Semianalysis&amp;#39; results are no surprise. When I did my testing on MI300X last year it was easy to get 2X+ differences in inference performance with minor changes to the default settings: &lt;a href=\"https://shisa.ai/posts/tuning-vllm-mi300x/\"&gt;https://shisa.ai/posts/tuning-vllm-mi300x/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Training was subtly broken (causing GPU resets) and took &amp;gt;2 months (longer than I had the hardware I was testing) to fix.&lt;/p&gt;\n\n&lt;p&gt;It is better now, however this is for CDNA. RDNA3 support is a whole different ball of wax (if you don&amp;#39;t have gfx1100, you ... should probably pretend you do). While most things work, there are rough spots and you&amp;#39;d still be much better off with a 3090: &lt;a href=\"https://llm-tracker.info/howto/AMD-GPUs\"&gt;https://llm-tracker.info/howto/AMD-GPUs&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                                                        "removal_reason": null,
                                                                        "collapsed_reason": null,
                                                                        "link_id": "t3_1ltv847",
                                                                        "associated_award": null,
                                                                        "stickied": false,
                                                                        "author_premium": false,
                                                                        "can_gild": false,
                                                                        "top_awarded_type": null,
                                                                        "unrepliable_reason": null,
                                                                        "author_flair_text_color": null,
                                                                        "score_hidden": false,
                                                                        "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1u1tfh/",
                                                                        "subreddit_type": "public",
                                                                        "locked": false,
                                                                        "report_reasons": null,
                                                                        "created": 1751906625,
                                                                        "author_flair_text": null,
                                                                        "treatment_tags": [],
                                                                        "created_utc": 1751906625,
                                                                        "subreddit_name_prefixed": "r/LocalLLaMA",
                                                                        "controversiality": 0,
                                                                        "depth": 6,
                                                                        "author_flair_background_color": null,
                                                                        "collapsed_because_crowd_control": null,
                                                                        "mod_reports": [],
                                                                        "num_reports": null,
                                                                        "ups": 1
                                                                      }
                                                                    }
                                                                  ],
                                                                  "before": null
                                                                }
                                                              },
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n1tzxzt",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "FullstackSensei",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n1tx8da",
                                                              "score": 1,
                                                              "author_fullname": "t2_17n3nqtj56",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Thanks for all the details!\n\nThe memory copy test is not indicative of real world performance here, since it forces copying buffers in memory. This thrashes both cache and the memory controller, hence why you see less than half peak performance (86 vs 212GB/s). In real compute workloads, buffers won't be copied and only pointers will be passed around. The driver stack will take care of moving ownership of the buffer from the CPU process side to the GPU side without copying any data.\n\nDespite knowing this since forever, I'm still very disappointed every time I read how bad the software stack still is on the AMD side. I'll never understand why they have HIPBlas and rocBLAS. Semianalysis did an amazing investigation last December and found that often times  you'll get very different performance for the same operation depending on which one you use, because AMD hasn't been able to keep both in sync with whatever latest optimization they did for any Blas operation...",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n1tzxzt",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for all the details!&lt;/p&gt;\n\n&lt;p&gt;The memory copy test is not indicative of real world performance here, since it forces copying buffers in memory. This thrashes both cache and the memory controller, hence why you see less than half peak performance (86 vs 212GB/s). In real compute workloads, buffers won&amp;#39;t be copied and only pointers will be passed around. The driver stack will take care of moving ownership of the buffer from the CPU process side to the GPU side without copying any data.&lt;/p&gt;\n\n&lt;p&gt;Despite knowing this since forever, I&amp;#39;m still very disappointed every time I read how bad the software stack still is on the AMD side. I&amp;#39;ll never understand why they have HIPBlas and rocBLAS. Semianalysis did an amazing investigation last December and found that often times  you&amp;#39;ll get very different performance for the same operation depending on which one you use, because AMD hasn&amp;#39;t been able to keep both in sync with whatever latest optimization they did for any Blas operation...&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1ltv847",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tzxzt/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1751906084,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1751906084,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n1tx8da",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "randomfoo2",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n1tvld8",
                                                    "score": 2,
                                                    "author_fullname": "t2_eztox",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "As you may be able to see from the output table the backend is `ROCm`.\n\nI try to use hipBLASLt as it is almost always faster than rocBLAS, but in this case it doesn't work because it can't find the right shapes. Note, that I have filed a bug as the gfx1151 kernels are often-times significantly slower than gfx1100 kernels (the latter causes GPU crashes after a period of time however).\n\nVulkan often has faster performance on Strix Halo because even 5 months after release, the software is terribly underbaked. If you want to see more of my testing: [https://llm-tracker.info/\\_TOORG/Strix-Halo](https://llm-tracker.info/_TOORG/Strix-Halo)",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n1tx8da",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As you may be able to see from the output table the backend is &lt;code&gt;ROCm&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I try to use hipBLASLt as it is almost always faster than rocBLAS, but in this case it doesn&amp;#39;t work because it can&amp;#39;t find the right shapes. Note, that I have filed a bug as the gfx1151 kernels are often-times significantly slower than gfx1100 kernels (the latter causes GPU crashes after a period of time however).&lt;/p&gt;\n\n&lt;p&gt;Vulkan often has faster performance on Strix Halo because even 5 months after release, the software is terribly underbaked. If you want to see more of my testing: &lt;a href=\"https://llm-tracker.info/_TOORG/Strix-Halo\"&gt;https://llm-tracker.info/_TOORG/Strix-Halo&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1ltv847",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tx8da/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1751905304,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1751905304,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1tvld8",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "FullstackSensei",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1tucr1",
                                          "score": 1,
                                          "author_fullname": "t2_17n3nqtj56",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks! Not bad if you ask me.\nThat's using Vulkan? Why no rocBLAS? Or is that intentional because the vulkan backend is faster? How do you find performance with 16k or 32k context?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1tvld8",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks! Not bad if you ask me.\nThat&amp;#39;s using Vulkan? Why no rocBLAS? Or is that intentional because the vulkan backend is faster? How do you find performance with 16k or 32k context?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tvld8/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751904830,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751904830,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1tucr1",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "randomfoo2",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1tt6sl",
                                "score": 3,
                                "author_fullname": "t2_eztox",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "    # ROCBLAS_USE_HIPBLASLT=1 llama.cpp-hip/build/bin/llama-bench -m /home/lhl/models/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf\n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    ggml_cuda_init: found 1 ROCm devices:\n      Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    rocBLAS error: No hipBLASLt solution found\n    This message will be only be displayed once, unless the ROCBLAS_VERBOSE_HIPBLASLT_ERROR environment variable is set.\n    \n    rocBLAS warning: hipBlasLT failed, falling back to tensile.\n    This message will be only be displayed once, unless the ROCBLAS_VERBOSE_TENSILE_ERROR environment variable is set.\n    | qwen3moe 235B.A22B Q3_K - Medium |  96.59 GiB |   235.09 B | ROCm,RPC   |  99 |           pp512 |        122.68 ± 0.58 |\n    | qwen3moe 235B.A22B Q3_K - Medium |  96.59 GiB |   235.09 B | ROCm,RPC   |  99 |           tg128 |         12.30 ± 0.02 |",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1tucr1",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;pre&gt;&lt;code&gt;# ROCBLAS_USE_HIPBLASLT=1 llama.cpp-hip/build/bin/llama-bench -m /home/lhl/models/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\nrocBLAS error: No hipBLASLt solution found\nThis message will be only be displayed once, unless the ROCBLAS_VERBOSE_HIPBLASLT_ERROR environment variable is set.\n\nrocBLAS warning: hipBlasLT failed, falling back to tensile.\nThis message will be only be displayed once, unless the ROCBLAS_VERBOSE_TENSILE_ERROR environment variable is set.\n| qwen3moe 235B.A22B Q3_K - Medium |  96.59 GiB |   235.09 B | ROCm,RPC   |  99 |           pp512 |        122.68 ± 0.58 |\n| qwen3moe 235B.A22B Q3_K - Medium |  96.59 GiB |   235.09 B | ROCm,RPC   |  99 |           tg128 |         12.30 ± 0.02 |\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tucr1/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751904470,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751904470,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1tt6sl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FullstackSensei",
                      "can_mod_post": false,
                      "created_utc": 1751904132,
                      "send_replies": true,
                      "parent_id": "t1_n1tjqoi",
                      "score": 1,
                      "author_fullname": "t2_17n3nqtj56",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "You should also be able to run Qwen 3 235B Q3_K_M or XL. Would be interesting to see the pp and tg for that on Strix Halo.\n\nI find the unified memory argument in general very uninformed. People regurgitate it with zero understanding of what it means.\n\nIntel has been doing \"unified memory\" for like 20 years, but never bothered marketing it. In general, unified memory doesn't matter at all in integrated GPUs. Like zero difference. The reason is that all compute frameworks that work on iGPUs know they're working with integrated hardware and the hardware itself doesn't care where the compute buffer is allocated in physical RAM.\n\nIf anyone bothered to read the documentation for programming any iGPU for compute workloads, they'll find plenty of a mention of \"zero copy\", that transferring a data buffer to/from CPU to GPU involves zero data copy, and only the pointer to the buffer is copied.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1tt6sl",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You should also be able to run Qwen 3 235B Q3_K_M or XL. Would be interesting to see the pp and tg for that on Strix Halo.&lt;/p&gt;\n\n&lt;p&gt;I find the unified memory argument in general very uninformed. People regurgitate it with zero understanding of what it means.&lt;/p&gt;\n\n&lt;p&gt;Intel has been doing &amp;quot;unified memory&amp;quot; for like 20 years, but never bothered marketing it. In general, unified memory doesn&amp;#39;t matter at all in integrated GPUs. Like zero difference. The reason is that all compute frameworks that work on iGPUs know they&amp;#39;re working with integrated hardware and the hardware itself doesn&amp;#39;t care where the compute buffer is allocated in physical RAM.&lt;/p&gt;\n\n&lt;p&gt;If anyone bothered to read the documentation for programming any iGPU for compute workloads, they&amp;#39;ll find plenty of a mention of &amp;quot;zero copy&amp;quot;, that transferring a data buffer to/from CPU to GPU involves zero data copy, and only the pointer to the buffer is copied.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tt6sl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751904132,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1tjqoi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "randomfoo2",
            "can_mod_post": false,
            "created_utc": 1751901403,
            "send_replies": true,
            "parent_id": "t3_1ltv847",
            "score": 6,
            "author_fullname": "t2_eztox",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There are a few things you've listed that I don't think are quite right:\n\n&gt;Max+ has to split memory between sys and vram, it's not unified like M4, and it needs to load models into sys ram first before offloading to vram, so realistically it's best to leave it at 64gb/64gb split.\n\nI have been able to load 100GB+ models without an issue, so you definitely don't need to load a model in system RAM before offloading to GPU memory. This is on Linux where I've been able to use up to 110GB of memory with ROCm (GTT-only) or 118GB with Vulkan (it can use GART+GTT) \n\n&gt;Neither has enough GPU power to run models larger models than would fit in 64gb at a fast enough speed so there's little to no benefit to more vram\n\nIn the age of MoE's I don't think this is correct.  Llama 4 Scout  is a 109B A17B and is 60GB at Q4\\_K\\_XL and inferences at 17 tok/s. As long as activations are low you can have even larger MoEs and run them at a reasonable speed. If you want to do longer context generation, the extra memory is also quite useful. There is also the consideration being able to use your computer for regular computing things while haven't a model loaded.\n\nStrix Halo has more raw TFLOPS, but it's hard to say how that translates to real-world image performance, I haven't tested that at all.\n\nI don't have an M4 Max, but for LLMs, the single point of comparison I have is that the M4 Max is reported on the official llama.cpp discussion thread: [https://github.com/ggml-org/llama.cpp/discussions/4167](https://github.com/ggml-org/llama.cpp/discussions/4167) to have a pp512/tg128 of 886 t/s and 83 t/s for llama 7b q4\\_0. On llama2 7b q4\\_0, Strix Halo on Vulkan gets pp512/tg128 of 884 t/s and 53 t/s.\n\nThe M4 Max has a theoretical max memory bandwidth of 546GB/s vs Strix Halo's theoretical max 256 GB/s (my real world measured is \\~212 GB/s) so the M4 Max tg128 number is actually lower than expected (but then again, Strix Halo's pp512 number is lower than expected compared to its raw TFLOPS).\n\nI feel like for LLM inference they are generally close enough that you can basically pick whichever one you think you'll enjoy better and you'll be equivalently satisfied or dissatisfied with either.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1tjqoi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There are a few things you&amp;#39;ve listed that I don&amp;#39;t think are quite right:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Max+ has to split memory between sys and vram, it&amp;#39;s not unified like M4, and it needs to load models into sys ram first before offloading to vram, so realistically it&amp;#39;s best to leave it at 64gb/64gb split.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I have been able to load 100GB+ models without an issue, so you definitely don&amp;#39;t need to load a model in system RAM before offloading to GPU memory. This is on Linux where I&amp;#39;ve been able to use up to 110GB of memory with ROCm (GTT-only) or 118GB with Vulkan (it can use GART+GTT) &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Neither has enough GPU power to run models larger models than would fit in 64gb at a fast enough speed so there&amp;#39;s little to no benefit to more vram&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;In the age of MoE&amp;#39;s I don&amp;#39;t think this is correct.  Llama 4 Scout  is a 109B A17B and is 60GB at Q4_K_XL and inferences at 17 tok/s. As long as activations are low you can have even larger MoEs and run them at a reasonable speed. If you want to do longer context generation, the extra memory is also quite useful. There is also the consideration being able to use your computer for regular computing things while haven&amp;#39;t a model loaded.&lt;/p&gt;\n\n&lt;p&gt;Strix Halo has more raw TFLOPS, but it&amp;#39;s hard to say how that translates to real-world image performance, I haven&amp;#39;t tested that at all.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have an M4 Max, but for LLMs, the single point of comparison I have is that the M4 Max is reported on the official llama.cpp discussion thread: &lt;a href=\"https://github.com/ggml-org/llama.cpp/discussions/4167\"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt; to have a pp512/tg128 of 886 t/s and 83 t/s for llama 7b q4_0. On llama2 7b q4_0, Strix Halo on Vulkan gets pp512/tg128 of 884 t/s and 53 t/s.&lt;/p&gt;\n\n&lt;p&gt;The M4 Max has a theoretical max memory bandwidth of 546GB/s vs Strix Halo&amp;#39;s theoretical max 256 GB/s (my real world measured is ~212 GB/s) so the M4 Max tg128 number is actually lower than expected (but then again, Strix Halo&amp;#39;s pp512 number is lower than expected compared to its raw TFLOPS).&lt;/p&gt;\n\n&lt;p&gt;I feel like for LLM inference they are generally close enough that you can basically pick whichever one you think you&amp;#39;ll enjoy better and you&amp;#39;ll be equivalently satisfied or dissatisfied with either.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tjqoi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751901403,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltv847",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n1tcwyv",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "cs668",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1taewq",
                                "score": 2,
                                "author_fullname": "t2_69gjd",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "This is what I find myself doing.  Using different size models for different use cases.  The response time that is acceptable for just text to text while interacting on some random subject is way to slow for a coding assistant.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1tcwyv",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is what I find myself doing.  Using different size models for different use cases.  The response time that is acceptable for just text to text while interacting on some random subject is way to slow for a coding assistant.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tcwyv/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751899390,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751899390,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1u2wd6",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "TaroOk7112",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1tto5j",
                                          "score": 1,
                                          "author_fullname": "t2_tsjh0dua",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I wish I could try dots.llm1 MOE in Strix Halo. This quants seem really interesting:\n\n[https://huggingface.co/unsloth/dots.llm1.inst-GGUF](https://huggingface.co/unsloth/dots.llm1.inst-GGUF)\n\nUD-Q4\\_K\\_XL: 86GB\n\nQ5\\_K\\_S: 101GB\n\nOnly 14B active parameters, performance comparable to Qwen3 235B without thinking.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1u2wd6",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wish I could try dots.llm1 MOE in Strix Halo. This quants seem really interesting:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/unsloth/dots.llm1.inst-GGUF\"&gt;https://huggingface.co/unsloth/dots.llm1.inst-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;UD-Q4_K_XL: 86GB&lt;/p&gt;\n\n&lt;p&gt;Q5_K_S: 101GB&lt;/p&gt;\n\n&lt;p&gt;Only 14B active parameters, performance comparable to Qwen3 235B without thinking.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1u2wd6/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751906930,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751906930,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1tto5j",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LevianMcBirdo",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1taewq",
                                "score": 1,
                                "author_fullname": "t2_cw9f6o0r",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You kinda forget about MoE options. You probably could get the big Qwen 3 at Q3 working and maybe there are more to follow. (You get up to 112gb into vram on Linux btw)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1tto5j",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You kinda forget about MoE options. You probably could get the big Qwen 3 at Q3 working and maybe there are more to follow. (You get up to 112gb into vram on Linux btw)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tto5j/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751904271,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751904271,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1taewq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "siegekeebsofficial",
                      "can_mod_post": false,
                      "created_utc": 1751898643,
                      "send_replies": true,
                      "parent_id": "t1_n1t9x2a",
                      "score": 1,
                      "author_fullname": "t2_8cqm44sa9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the feedback, I don't know that more than 64gb of vram really matters for the max+ anyway, as the GPU just isn't powerful enough to meaningfully do anything with a larger model. The only time I could see it being useful is if you wanted like... 96gb and have multiple models loaded at once or something - which would be fun.\n\nCan you tell me more about the terminal command for Mac? I haven't seen anything about that in googling...",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1taewq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the feedback, I don&amp;#39;t know that more than 64gb of vram really matters for the max+ anyway, as the GPU just isn&amp;#39;t powerful enough to meaningfully do anything with a larger model. The only time I could see it being useful is if you wanted like... 96gb and have multiple models loaded at once or something - which would be fun.&lt;/p&gt;\n\n&lt;p&gt;Can you tell me more about the terminal command for Mac? I haven&amp;#39;t seen anything about that in googling...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1taewq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751898643,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1t9x2a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "The_Hardcard",
            "can_mod_post": false,
            "created_utc": 1751898495,
            "send_replies": true,
            "parent_id": "t3_1ltv847",
            "score": 6,
            "author_fullname": "t2_aom89enmo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I’ve read that the need to have a 64GB sys RAM/64GB VRAM split on AMD is only for people who don’t know the proper commands. There are those who claim to use a lot more VRAM. I don’t know the details, but follow up on that if you are considering it.\n\nI do know that using nearly all the unified memory on Apple Silicon Macs ( minus 8GB or thereabouts for system stability is a simple terminal command and you can even set a Mac to boot with nearly all memory available to the GPU.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1t9x2a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’ve read that the need to have a 64GB sys RAM/64GB VRAM split on AMD is only for people who don’t know the proper commands. There are those who claim to use a lot more VRAM. I don’t know the details, but follow up on that if you are considering it.&lt;/p&gt;\n\n&lt;p&gt;I do know that using nearly all the unified memory on Apple Silicon Macs ( minus 8GB or thereabouts for system stability is a simple terminal command and you can even set a Mac to boot with nearly all memory available to the GPU.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1t9x2a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751898495,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltv847",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n1tupp5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "LevianMcBirdo",
                      "can_mod_post": false,
                      "created_utc": 1751904574,
                      "send_replies": true,
                      "parent_id": "t1_n1tlmcm",
                      "score": 2,
                      "author_fullname": "t2_cw9f6o0r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What do you care about ssd pricing. An enclosure is 20-80 bucks depending if you want 10 or 40 Gb/s (TB5 is still too expensive). If you buy a machine you don't take with you there is really no reason to give apple money for the SSD upgrade.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1tupp5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What do you care about ssd pricing. An enclosure is 20-80 bucks depending if you want 10 or 40 Gb/s (TB5 is still too expensive). If you buy a machine you don&amp;#39;t take with you there is really no reason to give apple money for the SSD upgrade.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tupp5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751904574,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1tx30o",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "FullstackSensei",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1tv8v0",
                                          "score": 2,
                                          "author_fullname": "t2_17n3nqtj56",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "If you intend to run it at full tilt 24/7 then yes it will be very expensive to run. More realistically, if you use it for 3-4 hours/day and shut it down when not in use, you're looking at 35-45/month at 0.30/kwh (I live in such a country). Mind you, this is assuming you're actually running them at full tilt for 3-4 hours. If you stop to actually read the output, you're looking at under 10€/month for 4hrs/day of use.\n\nOne of the niceties of using server platforms is having IPMI. I can start any or all my inference rigs using a one line command for each and they'll be up and running within 2 minutes. If you can't be bothered to wait said two minutes, you can program a pi or create a scheduled task (or cron) in your favorite OS to start the machine at a given time, and second cron job in that machine to shut down at a set time. \n\nI don't mind running the command by hand from shell and waiting the two minutes for it to boot. I like this arrangement so much that built two inference machines to use like this, and I'm upgrading one of them to 8 GPUs and building two more machines for large MoE models to also run like this.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1tx30o",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you intend to run it at full tilt 24/7 then yes it will be very expensive to run. More realistically, if you use it for 3-4 hours/day and shut it down when not in use, you&amp;#39;re looking at 35-45/month at 0.30/kwh (I live in such a country). Mind you, this is assuming you&amp;#39;re actually running them at full tilt for 3-4 hours. If you stop to actually read the output, you&amp;#39;re looking at under 10€/month for 4hrs/day of use.&lt;/p&gt;\n\n&lt;p&gt;One of the niceties of using server platforms is having IPMI. I can start any or all my inference rigs using a one line command for each and they&amp;#39;ll be up and running within 2 minutes. If you can&amp;#39;t be bothered to wait said two minutes, you can program a pi or create a scheduled task (or cron) in your favorite OS to start the machine at a given time, and second cron job in that machine to shut down at a set time. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t mind running the command by hand from shell and waiting the two minutes for it to boot. I like this arrangement so much that built two inference machines to use like this, and I&amp;#39;m upgrading one of them to 8 GPUs and building two more machines for large MoE models to also run like this.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tx30o/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751905261,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751905261,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1tv8v0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LevianMcBirdo",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1tqj6i",
                                "score": 1,
                                "author_fullname": "t2_cw9f6o0r",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It also will be quite expensive to run depending where you live. With 30 cents per kWh I'd probably go apple or 395 max for my stuff, just so I can experiment without paying an enormous electric bill. Then again I'd love to see the price per token, since you get a lot more out of the bigger machines. Wish we had some kind of board for this",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1tv8v0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It also will be quite expensive to run depending where you live. With 30 cents per kWh I&amp;#39;d probably go apple or 395 max for my stuff, just so I can experiment without paying an enormous electric bill. Then again I&amp;#39;d love to see the price per token, since you get a lot more out of the bigger machines. Wish we had some kind of board for this&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tv8v0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751904730,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751904730,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1tqj6i",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FullstackSensei",
                      "can_mod_post": false,
                      "created_utc": 1751903379,
                      "send_replies": true,
                      "parent_id": "t1_n1tlmcm",
                      "score": 1,
                      "author_fullname": "t2_17n3nqtj56",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "For 5k you can get a dual engineering sample (ES) Xeon with AMX support, ECC 512GB DDR5-4800, a motherboard, at least one 4090 (or two 3090s), and all ancillary components (PSU, coolers, fans, case, etc). You'll be able to run the full DeepSeek 671B at Q4 or higher with a lot of headroom for context. AMX will give a very good uplift for TG and the GPUs will take care of the PP. Sure, it's won't be as compact as the Mac studio nor as power efficient, but it will run practically any model out there.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1tqj6i",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For 5k you can get a dual engineering sample (ES) Xeon with AMX support, ECC 512GB DDR5-4800, a motherboard, at least one 4090 (or two 3090s), and all ancillary components (PSU, coolers, fans, case, etc). You&amp;#39;ll be able to run the full DeepSeek 671B at Q4 or higher with a lot of headroom for context. AMX will give a very good uplift for TG and the GPUs will take care of the PP. Sure, it&amp;#39;s won&amp;#39;t be as compact as the Mac studio nor as power efficient, but it will run practically any model out there.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tqj6i/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751903379,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1u46nu",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "DoldSchool",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1u0xpv",
                                          "score": 2,
                                          "author_fullname": "t2_1lgz6lbgr0",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "With 1/4 the vram",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1u46nu",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;With 1/4 the vram&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1u46nu/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751907292,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751907292,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1u0xpv",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pogo4Fufu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1tpjgi",
                                "score": 1,
                                "author_fullname": "t2_3q3msk1c",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The tokens per second on these small LLMs more or less correspond to the bandwidth and speed of the RAM. A NVidia RTX 5090 has a memory bandwidth of about 1.79 TB/s, the older RTX 4090 of 1.01 TB/s - twice as fast as a new Mac M4. In short, any old 4090 is at least twice as fast as a Mac M4.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1u0xpv",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The tokens per second on these small LLMs more or less correspond to the bandwidth and speed of the RAM. A NVidia RTX 5090 has a memory bandwidth of about 1.79 TB/s, the older RTX 4090 of 1.01 TB/s - twice as fast as a new Mac M4. In short, any old 4090 is at least twice as fast as a Mac M4.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1u0xpv/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751906372,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751906372,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1tpjgi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "siegekeebsofficial",
                      "can_mod_post": false,
                      "created_utc": 1751903095,
                      "send_replies": true,
                      "parent_id": "t1_n1tlmcm",
                      "score": 1,
                      "author_fullname": "t2_8cqm44sa9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I said M4 mac with 64gb, which is $2700ish - you're comparing bandwith, but from the test I've seen in real tests with side by side equipment the performance in t/s is within 20%. In posting this I was hoping to get people with real world experience with the hardware to provide insights, not just comparing specs online. Can you tell me how one acquires an H100 or MI300X for under $3k?",
                      "edited": 1751903673,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1tpjgi",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I said M4 mac with 64gb, which is $2700ish - you&amp;#39;re comparing bandwith, but from the test I&amp;#39;ve seen in real tests with side by side equipment the performance in t/s is within 20%. In posting this I was hoping to get people with real world experience with the hardware to provide insights, not just comparing specs online. Can you tell me how one acquires an H100 or MI300X for under $3k?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tpjgi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751903095,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1tlmcm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pogo4Fufu",
            "can_mod_post": false,
            "created_utc": 1751901954,
            "send_replies": true,
            "parent_id": "t3_1ltv847",
            "score": 2,
            "author_fullname": "t2_3q3msk1c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Comparable? An AMD AI 395 Mini PC with 128GB costs about $/€ 2000, a Mac M4 with 128GB is double the price - but with only 512GB SSD, a more reasonable 2TB -&gt; about $/€ 5000. Main difference is memory speed and thus overall LLM speed (Apple M4 Max \\~ 546 GB/s and AMD AI 395 \\~ 265 GB/s and up to 112GB 'VRAM'). But compared to 'real' solutions with NVidia hardware (H100 and 3.35 TB/s) or AMD (MI300X with 5.3 TB/s) the Mac isn't that great either. Or - in short: Apple is about double the price and about double the speed, but significantly slower then any 'real' AI hardware.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1tlmcm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Comparable? An AMD AI 395 Mini PC with 128GB costs about $/€ 2000, a Mac M4 with 128GB is double the price - but with only 512GB SSD, a more reasonable 2TB -&amp;gt; about $/€ 5000. Main difference is memory speed and thus overall LLM speed (Apple M4 Max ~ 546 GB/s and AMD AI 395 ~ 265 GB/s and up to 112GB &amp;#39;VRAM&amp;#39;). But compared to &amp;#39;real&amp;#39; solutions with NVidia hardware (H100 and 3.35 TB/s) or AMD (MI300X with 5.3 TB/s) the Mac isn&amp;#39;t that great either. Or - in short: Apple is about double the price and about double the speed, but significantly slower then any &amp;#39;real&amp;#39; AI hardware.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tlmcm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751901954,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltv847",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n1trz6s",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "siegekeebsofficial",
                      "can_mod_post": false,
                      "created_utc": 1751903785,
                      "send_replies": true,
                      "parent_id": "t1_n1tjdqf",
                      "score": 1,
                      "author_fullname": "t2_8cqm44sa9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for pointing that out, it also seems that the max+ system is not as limited in vram split as well based on randomfoo's experience",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1trz6s",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for pointing that out, it also seems that the max+ system is not as limited in vram split as well based on randomfoo&amp;#39;s experience&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1trz6s/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751903785,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1tjdqf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "synn89",
            "can_mod_post": false,
            "created_utc": 1751901297,
            "send_replies": true,
            "parent_id": "t3_1ltv847",
            "score": 1,
            "author_fullname": "t2_3jm4t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; Neither has enough GPU power to run models larger models than would fit in 64gb at a fast enough speed so there's little to no benefit to more vram\n\nI know Qwen3-235B runs pretty well on my M1 Ultra in 128GB of RAM. I'm not sure how well Max+ could handle MOE's that are 64-128GB VRAM.\n\n&gt; The M4 Max really isn't 64gb, since it can only use 75% of shared memory in vram, so it's really 48gb, which is a significant trade-off\n\nThis is something you can easily adjust via the command line. So on a 64GB Mac, you'll probably be able to use/access closer to 54-58GB of VRAM depending on other app usage.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1tjdqf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Neither has enough GPU power to run models larger models than would fit in 64gb at a fast enough speed so there&amp;#39;s little to no benefit to more vram&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I know Qwen3-235B runs pretty well on my M1 Ultra in 128GB of RAM. I&amp;#39;m not sure how well Max+ could handle MOE&amp;#39;s that are 64-128GB VRAM.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The M4 Max really isn&amp;#39;t 64gb, since it can only use 75% of shared memory in vram, so it&amp;#39;s really 48gb, which is a significant trade-off&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is something you can easily adjust via the command line. So on a 64GB Mac, you&amp;#39;ll probably be able to use/access closer to 54-58GB of VRAM depending on other app usage.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tjdqf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751901297,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltv847",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n1uegrc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "siegekeebsofficial",
                      "can_mod_post": false,
                      "created_utc": 1751910292,
                      "send_replies": true,
                      "parent_id": "t1_n1ucbki",
                      "score": 1,
                      "author_fullname": "t2_8cqm44sa9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the info, I wasn't questioning whether the bandwidth matters - but whether it had a significant impact in real world scenarios. It's hard to find side by side comparisons between the two, but in the one that I found they were roughly within margin of error. It's entirely possible I'm not looking in the right places though, which I was hoping for feedback here! 12 tps on Llama 3.3 70b q4 would be much nicer than 6 tps, I don't know if that's what I would actually see as a difference though when randomfoo2 gives his experience which is:\n\n\"I don't have an M4 Max, but for LLMs, the single point of comparison I have is that the M4 Max is reported on the official llama.cpp discussion thread: https://github.com/ggml-org/llama.cpp/discussions/4167 to have a pp512/tg128 of 886 t/s and 83 t/s for llama 7b q4_0. On llama2 7b q4_0, Strix Halo on Vulkan gets pp512/tg128 of 884 t/s and 53 t/s.\"",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1uegrc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the info, I wasn&amp;#39;t questioning whether the bandwidth matters - but whether it had a significant impact in real world scenarios. It&amp;#39;s hard to find side by side comparisons between the two, but in the one that I found they were roughly within margin of error. It&amp;#39;s entirely possible I&amp;#39;m not looking in the right places though, which I was hoping for feedback here! 12 tps on Llama 3.3 70b q4 would be much nicer than 6 tps, I don&amp;#39;t know if that&amp;#39;s what I would actually see as a difference though when randomfoo2 gives his experience which is:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I don&amp;#39;t have an M4 Max, but for LLMs, the single point of comparison I have is that the M4 Max is reported on the official llama.cpp discussion thread: &lt;a href=\"https://github.com/ggml-org/llama.cpp/discussions/4167\"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt; to have a pp512/tg128 of 886 t/s and 83 t/s for llama 7b q4_0. On llama2 7b q4_0, Strix Halo on Vulkan gets pp512/tg128 of 884 t/s and 53 t/s.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1uegrc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751910292,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1ucbki",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Baldur-Norddahl",
            "can_mod_post": false,
            "created_utc": 1751909651,
            "send_replies": true,
            "parent_id": "t3_1ltv847",
            "score": 1,
            "author_fullname": "t2_bvqb8ng0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Prompt processing is about the same. Token generation is double speed on the M4 Max. And then there is the price difference to decide if you want to invest in the higher tps.\n\nI understand that you are questioning that bandwidth really matters that much between those two platforms. But it absolutely does. Bandwidth sets a physical upper limit that is impossible to pass. For each token generated, the machine needs to read the whole of the model from ram. The theoretical max tps is bandwidth/model_size. For example 526GB/70b = 7.5 tps for a 70b model at 8 bit quantisation on the M4 Max. Double that for 4 bit (15 tps). I am actually getting 12 tps on Llama 3.3 70b q4. So fairly close to the theoretical max. It will be impossible to get anything near that on a machine with half the bandwidth.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1ucbki",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Prompt processing is about the same. Token generation is double speed on the M4 Max. And then there is the price difference to decide if you want to invest in the higher tps.&lt;/p&gt;\n\n&lt;p&gt;I understand that you are questioning that bandwidth really matters that much between those two platforms. But it absolutely does. Bandwidth sets a physical upper limit that is impossible to pass. For each token generated, the machine needs to read the whole of the model from ram. The theoretical max tps is bandwidth/model_size. For example 526GB/70b = 7.5 tps for a 70b model at 8 bit quantisation on the M4 Max. Double that for 4 bit (15 tps). I am actually getting 12 tps on Llama 3.3 70b q4. So fairly close to the theoretical max. It will be impossible to get anything near that on a machine with half the bandwidth.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1ucbki/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751909651,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltv847",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1tqunh",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "siegekeebsofficial",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1to2yr",
                                          "score": 1,
                                          "author_fullname": "t2_8cqm44sa9",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "the large vram is really the whole point in making this comparison. You can build a system with 2x 5060ti 16gb for under $1k?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1tqunh",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the large vram is really the whole point in making this comparison. You can build a system with 2x 5060ti 16gb for under $1k?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tqunh/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751903469,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751903469,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n1ts46c",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "UnknownGermanGuy",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n1to2yr",
                                          "score": 1,
                                          "author_fullname": "t2_u5mln",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "i ran the numbers recently on comparing the M3 with 512GB unified memory, and its my believe its the cheapest way to get the large MoE models V3/R1 running on a non-cpu/accelerated system. But those prompt-processing speeds are truly horrible, can be multiple minutes for like \\~5k ive heard",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n1ts46c",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i ran the numbers recently on comparing the M3 with 512GB unified memory, and its my believe its the cheapest way to get the large MoE models V3/R1 running on a non-cpu/accelerated system. But those prompt-processing speeds are truly horrible, can be multiple minutes for like ~5k ive heard&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ltv847",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1ts46c/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751903823,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751903823,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n1to2yr",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AdamDhahabi",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n1ti1z2",
                                "score": 1,
                                "author_fullname": "t2_x5lnbc2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I would say that filling that 64GB Unified Memory will result in slow speed and practically you'll probably stick to 24b/32b models. I found that M4 Max would give around 20 t/s for a 24b model and 15 t/s for a 32b model. That's roughly the same on the Intel system I proposed, but at half the price. M4 Max would be horrible at prompt processing compared to the NVIDIA-based system. Both systems give some room for larger MOE models and there I see advantage for the M4 Max 64GB.",
                                "edited": 1751902993,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n1to2yr",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would say that filling that 64GB Unified Memory will result in slow speed and practically you&amp;#39;ll probably stick to 24b/32b models. I found that M4 Max would give around 20 t/s for a 24b model and 15 t/s for a 32b model. That&amp;#39;s roughly the same on the Intel system I proposed, but at half the price. M4 Max would be horrible at prompt processing compared to the NVIDIA-based system. Both systems give some room for larger MOE models and there I see advantage for the M4 Max 64GB.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1ltv847",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1to2yr/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751902672,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751902672,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n1ti1z2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "siegekeebsofficial",
                      "can_mod_post": false,
                      "created_utc": 1751900911,
                      "send_replies": true,
                      "parent_id": "t1_n1tgvtw",
                      "score": 1,
                      "author_fullname": "t2_8cqm44sa9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Good to think of alternative systems for running things but 32gb of vram is not the same as 64gb vram - hopefully your comment helps someone though",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n1ti1z2",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Good to think of alternative systems for running things but 32gb of vram is not the same as 64gb vram - hopefully your comment helps someone though&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ltv847",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1ti1z2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751900911,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n1tgvtw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1751900566,
            "send_replies": true,
            "parent_id": "t3_1ltv847",
            "score": -2,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think you can get an Intel i5-based system with 2x 16GB 5060 Ti and 64 GB DDR5 for half the price of a M4 Max 64GB while both systems roughly perform the same.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1tgvtw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think you can get an Intel i5-based system with 2x 16GB 5060 Ti and 64 GB DDR5 for half the price of a M4 Max 64GB while both systems roughly perform the same.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/n1tgvtw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751900566,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltv847",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -2
          }
        }
      ],
      "before": null
    }
  }
]