[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have a macbook m4 pro with 16gb ram so I've made a list of the best models that should be able to run on it. I will be using llama.cpp without GUI for max efficiency but even still some of these quants might be too large to have enough space for reasoning tokens and some context, idk I'm a noob.\n\nHere are the best models and quants for under 16gb based on my research, but I'm a noob and I haven't tested these yet:\n\nBest Reasoning:\n\n1. Qwen3-32B   (IQ3\\_XXS   12.8 GB)\n2. Qwen3-30B-A3B-Thinking-2507   (IQ3\\_XS   12.7GB)\n3. Qwen 14B   (Q6\\_K\\_L 12.50GB)\n4. gpt-oss-20b   (12GB)\n5. Phi-4-reasoning-plus   (Q6\\_K\\_L   12.3 GB)\n\nBest non reasoning:\n\n1. gemma-3-27b   (IQ4\\_XS   14.77GB)\n2. Mistral-Small-3.2-24B-Instruct-2506   (Q4\\_K\\_L  14.83GB)\n3. gemma-3-12b    (Q8\\_0   12.5 GB)\n\nMy use cases:\n\n1. Accurately summarizing meeting transcripts.\n2. Creating an anonymized/censored version of a a document by removing confidential info while keeping everything else the same.\n3. Asking survival questions for scenarios without internet like camping. I think medgemma-27b-text would be cool for this scenario.\n\nI prefer maximum accuracy and intelligence over speed. How's my list and quants for my use cases? Am I missing any model or have something wrong? Any advice for getting the best performance with llama.cpp on a macbook m4pro 16gb?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best models under 16GB??",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjruwj",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_igdar",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754545257,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a macbook m4 pro with 16gb ram so I&amp;#39;ve made a list of the best models that should be able to run on it. I will be using llama.cpp without GUI for max efficiency but even still some of these quants might be too large to have enough space for reasoning tokens and some context, idk I&amp;#39;m a noob.&lt;/p&gt;\n\n&lt;p&gt;Here are the best models and quants for under 16gb based on my research, but I&amp;#39;m a noob and I haven&amp;#39;t tested these yet:&lt;/p&gt;\n\n&lt;p&gt;Best Reasoning:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Qwen3-32B   (IQ3_XXS   12.8 GB)&lt;/li&gt;\n&lt;li&gt;Qwen3-30B-A3B-Thinking-2507   (IQ3_XS   12.7GB)&lt;/li&gt;\n&lt;li&gt;Qwen 14B   (Q6_K_L 12.50GB)&lt;/li&gt;\n&lt;li&gt;gpt-oss-20b   (12GB)&lt;/li&gt;\n&lt;li&gt;Phi-4-reasoning-plus   (Q6_K_L   12.3 GB)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Best non reasoning:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;gemma-3-27b   (IQ4_XS   14.77GB)&lt;/li&gt;\n&lt;li&gt;Mistral-Small-3.2-24B-Instruct-2506   (Q4_K_L  14.83GB)&lt;/li&gt;\n&lt;li&gt;gemma-3-12b    (Q8_0   12.5 GB)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My use cases:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Accurately summarizing meeting transcripts.&lt;/li&gt;\n&lt;li&gt;Creating an anonymized/censored version of a a document by removing confidential info while keeping everything else the same.&lt;/li&gt;\n&lt;li&gt;Asking survival questions for scenarios without internet like camping. I think medgemma-27b-text would be cool for this scenario.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I prefer maximum accuracy and intelligence over speed. How&amp;#39;s my list and quants for my use cases? Am I missing any model or have something wrong? Any advice for getting the best performance with llama.cpp on a macbook m4pro 16gb?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjruwj",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Mr-Barack-Obama",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjruwj/best_models_under_16gb/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjruwj/best_models_under_16gb/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754545257,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7drpkz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DorphinPack",
            "can_mod_post": false,
            "created_utc": 1754555012,
            "send_replies": true,
            "parent_id": "t3_1mjruwj",
            "score": 6,
            "author_fullname": "t2_zebuyjw9s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Don’t sleep on 4bit MLX if you’re running an M chip. I would recommend something super easy like the newest Qwen3 4B at 4bit to let you get a feel for context sizes and how it interacts with your typical system RAM usage. Experiment with your first model for longer than you want to if you’re tempted to collect models right off the bat. I also recommend diligently watching token counts for a while to get a feel.\n\nGGUFs could be useful for higher precision on tiny models if you can’t find 6, 8 bit MLX quants of your model of choice. But if you want to go UNDER ~4bpw then GGUFs at the param size you can fit won’t be much use at all.\n\nBig/huge models can be surprisingly good for some things at Q2 (usually actually like 2.85bpw or around there) but smaller models suffer more.\n\n~4bpw is a common sweet spot for size/precision. IIRC only EXL2/EXL3 quants are the SOTA here with similar precision at ~3.5bpw. But those are strictly CUDA only.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7drpkz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Don’t sleep on 4bit MLX if you’re running an M chip. I would recommend something super easy like the newest Qwen3 4B at 4bit to let you get a feel for context sizes and how it interacts with your typical system RAM usage. Experiment with your first model for longer than you want to if you’re tempted to collect models right off the bat. I also recommend diligently watching token counts for a while to get a feel.&lt;/p&gt;\n\n&lt;p&gt;GGUFs could be useful for higher precision on tiny models if you can’t find 6, 8 bit MLX quants of your model of choice. But if you want to go UNDER ~4bpw then GGUFs at the param size you can fit won’t be much use at all.&lt;/p&gt;\n\n&lt;p&gt;Big/huge models can be surprisingly good for some things at Q2 (usually actually like 2.85bpw or around there) but smaller models suffer more.&lt;/p&gt;\n\n&lt;p&gt;~4bpw is a common sweet spot for size/precision. IIRC only EXL2/EXL3 quants are the SOTA here with similar precision at ~3.5bpw. But those are strictly CUDA only.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjruwj/best_models_under_16gb/n7drpkz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754555012,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjruwj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dqtux",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "GreenTreeAndBlueSky",
                      "can_mod_post": false,
                      "created_utc": 1754554500,
                      "send_replies": true,
                      "parent_id": "t1_n7ddsbo",
                      "score": 1,
                      "author_fullname": "t2_1p50pl73j2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah op is better off with a lower quant of qwen3 14b. I don't know how much ram macos uses while idle tho",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dqtux",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah op is better off with a lower quant of qwen3 14b. I don&amp;#39;t know how much ram macos uses while idle tho&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjruwj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjruwj/best_models_under_16gb/n7dqtux/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754554500,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7ddsbo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TheToi",
            "can_mod_post": false,
            "created_utc": 1754547194,
            "send_replies": true,
            "parent_id": "t3_1mjruwj",
            "score": 2,
            "author_fullname": "t2_y748f2r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You need RAM for store the context and for the rest of your system.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ddsbo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You need RAM for store the context and for the rest of your system.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjruwj/best_models_under_16gb/n7ddsbo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754547194,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjruwj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7djb77",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1754550196,
            "send_replies": true,
            "parent_id": "t3_1mjruwj",
            "score": 1,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Likely some deepseek distils should go there",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7djb77",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Likely some deepseek distils should go there&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjruwj/best_models_under_16gb/n7djb77/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754550196,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjruwj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ecz5j",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AliNT77",
            "can_mod_post": false,
            "created_utc": 1754566104,
            "send_replies": true,
            "parent_id": "t3_1mjruwj",
            "score": 1,
            "author_fullname": "t2_66tlmx2l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "look for 4bit MLX DWQ quants, they've much better accuracy than normal 4bit quants. \n\n  \nas for the models, I think Qwen3-30B-A3B-Instruct is both fast and high-quality.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ecz5j",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;look for 4bit MLX DWQ quants, they&amp;#39;ve much better accuracy than normal 4bit quants. &lt;/p&gt;\n\n&lt;p&gt;as for the models, I think Qwen3-30B-A3B-Instruct is both fast and high-quality.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjruwj/best_models_under_16gb/n7ecz5j/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754566104,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjruwj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]