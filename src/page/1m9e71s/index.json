[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.\n\nThe one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.\n\n\nIs there a easy to use mobile/desktop app and model combo that a semi-layman can setup?\n\nCurrently I use https://chatboxai.app/en + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).\n\nThanks in advance! ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "App for voice interaction with LocalLLaMA. Looking for help/app/model etc.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9e71s",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_4t06y10d",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753485878,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI have been self hosting Ollama and mostly just use it to throw random questions or helping me dumb down a complex topic to answer a question my daughter asks.&lt;/p&gt;\n\n&lt;p&gt;The one thing I love about ChatGPT/Gemini is the ability to voice chat back and forth.&lt;/p&gt;\n\n&lt;p&gt;Is there a easy to use mobile/desktop app and model combo that a semi-layman can setup?&lt;/p&gt;\n\n&lt;p&gt;Currently I use &lt;a href=\"https://chatboxai.app/en\"&gt;https://chatboxai.app/en&lt;/a&gt; + tailscale to access my Ollama/LLM remotely that runs on my RTX 3060 (12GB VRAM).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?auto=webp&amp;s=292ee07d261c9960edd9e6bc5216b120e3ca8c70",
                    "width": 1200,
                    "height": 630
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=279a09b67459be926a08944e6c9ea50312a63a5f",
                      "width": 108,
                      "height": 56
                    },
                    {
                      "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=71854addc096fa99604724a66b8d210353b93453",
                      "width": 216,
                      "height": 113
                    },
                    {
                      "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb617badf513ed94cfbeb3e2cbe71000e2592028",
                      "width": 320,
                      "height": 168
                    },
                    {
                      "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b440db62ce02b358f27186c315e179af0a46a940",
                      "width": 640,
                      "height": 336
                    },
                    {
                      "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3fbbeab38b3b11b83f247b980db93408afdf989",
                      "width": 960,
                      "height": 504
                    },
                    {
                      "url": "https://external-preview.redd.it/JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a139fe4e602dba248adc60f4bda3146ef969fd06",
                      "width": 1080,
                      "height": 567
                    }
                  ],
                  "variants": {},
                  "id": "JQJYWP9EtIyW64HOx_ngOVbE5TF6SXekcj5FkVZaVII"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m9e71s",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Dark_Mesh",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753485878,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n580e6s",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "dedreo58",
            "can_mod_post": false,
            "created_utc": 1753510081,
            "send_replies": true,
            "parent_id": "t3_1m9e71s",
            "score": 2,
            "author_fullname": "t2_nmspgzu7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Funny timing—I just posted about wanting a resource like CivitAI, but focused on local LLM usage. Not just models, but something that covers tools, frontends, configs, UI compatibility, etc.\n\nThat thread you linked is exactly the kind of use case I had in mind: someone with a solid setup who just wants voice interaction with their local model, without having to dig through 20 disconnected sources.\n\nWhat that user has:\n\n* Ollama running on an RTX 3060 (12GB)\n* Using ChatboxAI + Tailscale to connect remotely\n* Wants to use voice chat like ChatGPT/Gemini\n* Isn't trying to be a power user—just wants something simple that works\n\nWhat they’d actually need:\n\n* A working combo of Whisper + Piper or Bark, hooked into SillyTavern or a similar UI\n* A guide that says “Here’s what works well on a 3060”\n* Maybe a plug-and-play setup script: something like “VoiceChatKit for Ollama”—drop in your model, click run\n\nThis is where the hub idea kicks in. If there were a single place that listed:\n\n* What models work well with what frontends\n* What hardware loads what quant\n* STT/TTS combos that actually play nice together\n* Sample setups for specific GPUs\n* And guides that are actually readable\n\n...people like this wouldn’t have to keep asking the same integration questions over and over. There’s no reason it should still feel like Skyrim modding in 2008.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n580e6s",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Funny timing—I just posted about wanting a resource like CivitAI, but focused on local LLM usage. Not just models, but something that covers tools, frontends, configs, UI compatibility, etc.&lt;/p&gt;\n\n&lt;p&gt;That thread you linked is exactly the kind of use case I had in mind: someone with a solid setup who just wants voice interaction with their local model, without having to dig through 20 disconnected sources.&lt;/p&gt;\n\n&lt;p&gt;What that user has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ollama running on an RTX 3060 (12GB)&lt;/li&gt;\n&lt;li&gt;Using ChatboxAI + Tailscale to connect remotely&lt;/li&gt;\n&lt;li&gt;Wants to use voice chat like ChatGPT/Gemini&lt;/li&gt;\n&lt;li&gt;Isn&amp;#39;t trying to be a power user—just wants something simple that works&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What they’d actually need:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A working combo of Whisper + Piper or Bark, hooked into SillyTavern or a similar UI&lt;/li&gt;\n&lt;li&gt;A guide that says “Here’s what works well on a 3060”&lt;/li&gt;\n&lt;li&gt;Maybe a plug-and-play setup script: something like “VoiceChatKit for Ollama”—drop in your model, click run&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is where the hub idea kicks in. If there were a single place that listed:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What models work well with what frontends&lt;/li&gt;\n&lt;li&gt;What hardware loads what quant&lt;/li&gt;\n&lt;li&gt;STT/TTS combos that actually play nice together&lt;/li&gt;\n&lt;li&gt;Sample setups for specific GPUs&lt;/li&gt;\n&lt;li&gt;And guides that are actually readable&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...people like this wouldn’t have to keep asking the same integration questions over and over. There’s no reason it should still feel like Skyrim modding in 2008.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9e71s/app_for_voice_interaction_with_localllama_looking/n580e6s/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753510081,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9e71s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]