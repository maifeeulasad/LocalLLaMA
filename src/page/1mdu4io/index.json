[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Like the title says, I ran **GLM 4.5 Air Q4** on my local machine using **RooCode** inside **VS Code**, and I was able to build a functional CRUD-style web application.\n\nUsers can register with a password, log in, and log out from the client side. All authentication is handled using JWTs.\n\nThe experience honestly exceeded my expectations. Compared to my past attempts with local LLMs and RooCode (which sometimes struggled to generate even a basic webpage), this felt like a major step forward. The results were genuinely satisfying.\n\nThe entire app took about an hour to generate, with a bit of debugging and prompt tweaking along the way. With more deliberate prompting and a little more patience, I think I could have pushed it further. But for now, it’s a solid starting point.\n\nIf anyone else is experimenting with local models for full stack projects, I’d love to hear how it’s going. Happy to answer questions or share what I’ve learned.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "I Built a Full Stack App Using a Local LLM (GLM 4.5 Air) and RooCode. Here's How It Went",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdu4io",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_fz3utn30",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753943029,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says, I ran &lt;strong&gt;GLM 4.5 Air Q4&lt;/strong&gt; on my local machine using &lt;strong&gt;RooCode&lt;/strong&gt; inside &lt;strong&gt;VS Code&lt;/strong&gt;, and I was able to build a functional CRUD-style web application.&lt;/p&gt;\n\n&lt;p&gt;Users can register with a password, log in, and log out from the client side. All authentication is handled using JWTs.&lt;/p&gt;\n\n&lt;p&gt;The experience honestly exceeded my expectations. Compared to my past attempts with local LLMs and RooCode (which sometimes struggled to generate even a basic webpage), this felt like a major step forward. The results were genuinely satisfying.&lt;/p&gt;\n\n&lt;p&gt;The entire app took about an hour to generate, with a bit of debugging and prompt tweaking along the way. With more deliberate prompting and a little more patience, I think I could have pushed it further. But for now, it’s a solid starting point.&lt;/p&gt;\n\n&lt;p&gt;If anyone else is experimenting with local models for full stack projects, I’d love to hear how it’s going. Happy to answer questions or share what I’ve learned.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mdu4io",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "gamblingapocalypse",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/",
            "subreddit_subscribers": 507575,
            "created_utc": 1753943029,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64dxvk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gamblingapocalypse",
                      "can_mod_post": false,
                      "created_utc": 1753944193,
                      "send_replies": true,
                      "parent_id": "t1_n64ckzy",
                      "score": 1,
                      "author_fullname": "t2_fz3utn30",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That’s really interesting. I’ve been focused mostly on local models.  I haven’t tried RooCode with cloud hosted models like Opus yet.  I was surprised how well GLM 4.5 performed in a local setup, though. It’s encouraging to hear RooCode scales well with different backends.  Fun times! :)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64dxvk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s really interesting. I’ve been focused mostly on local models.  I haven’t tried RooCode with cloud hosted models like Opus yet.  I was surprised how well GLM 4.5 performed in a local setup, though. It’s encouraging to hear RooCode scales well with different backends.  Fun times! :)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64dxvk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753944193,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64ckzy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "dfgvbsrdfgaregzf",
            "can_mod_post": false,
            "created_utc": 1753943448,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 6,
            "author_fullname": "t2_pup33y79c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Roo Code is extremely powerful with the right model. I benchmarked it using full context length models from OpenRouter and it equaled Cursor with Opus 4 Max in a stress test.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64ckzy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Roo Code is extremely powerful with the right model. I benchmarked it using full context length models from OpenRouter and it equaled Cursor with Opus 4 Max in a stress test.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64ckzy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753943448,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n64frnp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fueled_by_caffeine",
            "can_mod_post": false,
            "created_utc": 1753945184,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 0,
            "author_fullname": "t2_7fkqhhi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I spent a couple of hours playing yesterday, it worked a lot better than any other model I’d tried previously for agentic stuff, but it’s still quite a lot worse than Claude 4 sonnet in terms of coding strategy and architecture (far more interventions required with incorrect approaches needing to be intercepted vs Claude) and on my M4 Max MBP with 128GB RAM q4 with 128k context felt too slow to be useful professionally though honestly I feel like that could be more on LM Studio than anything else as in direct chat rather than API I was seeing a little over 40 tps.  Overall the model felt very very thinky with often entire responses being synthesized as thoughts before being regenerated again as an actual response.\n\nComing on leaps and bounds but I couldn’t use it over a bigger hosted model yet.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64frnp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I spent a couple of hours playing yesterday, it worked a lot better than any other model I’d tried previously for agentic stuff, but it’s still quite a lot worse than Claude 4 sonnet in terms of coding strategy and architecture (far more interventions required with incorrect approaches needing to be intercepted vs Claude) and on my M4 Max MBP with 128GB RAM q4 with 128k context felt too slow to be useful professionally though honestly I feel like that could be more on LM Studio than anything else as in direct chat rather than API I was seeing a little over 40 tps.  Overall the model felt very very thinky with often entire responses being synthesized as thoughts before being regenerated again as an actual response.&lt;/p&gt;\n\n&lt;p&gt;Coming on leaps and bounds but I couldn’t use it over a bigger hosted model yet.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64frnp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753945184,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64jyv7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "robberviet",
                      "can_mod_post": false,
                      "created_utc": 1753947543,
                      "send_replies": true,
                      "parent_id": "t1_n64hxk2",
                      "score": 3,
                      "author_fullname": "t2_jxc5a",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "You can use it. Use and tell us what is the difference. \n\nEach person feel different. I feel Cline is better than VSCode. RooCode and Cline are the same.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64jyv7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can use it. Use and tell us what is the difference. &lt;/p&gt;\n\n&lt;p&gt;Each person feel different. I feel Cline is better than VSCode. RooCode and Cline are the same.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64jyv7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753947543,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64hxk2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Glittering-Call8746",
            "can_mod_post": false,
            "created_utc": 1753946386,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 1,
            "author_fullname": "t2_tqwl6sawb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why can't I just use vs code insiders ? What's the difference between the AI extensions ? Ie Cline and Roo",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64hxk2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why can&amp;#39;t I just use vs code insiders ? What&amp;#39;s the difference between the AI extensions ? Ie Cline and Roo&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64hxk2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753946386,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n64uk6q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Glittering-Call8746",
            "can_mod_post": false,
            "created_utc": 1753953591,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 1,
            "author_fullname": "t2_tqwl6sawb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I can't get vs code insiders to work on remote ollama endpoint.. so just cline atm.. I only have begin my setup. It took me weeks as I'm busy working. Now I gotta figure out why vs code insiders not working with ollama.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64uk6q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t get vs code insiders to work on remote ollama endpoint.. so just cline atm.. I only have begin my setup. It took me weeks as I&amp;#39;m busy working. Now I gotta figure out why vs code insiders not working with ollama.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64uk6q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753953591,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]