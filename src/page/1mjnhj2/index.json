[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm using a RX 6800 16GB on Linux.\n\nWhen did the Vulkan backend get so much better? Last time I tried it (probably a year ago) it was way behind ROCm, now it's up to **50% faster** at token generation depending on the model.\n\nWith Qwen3-Coder-30B-A3B-Instruct-UD-Q3_K_XL.gguf\n\n    ROCm   = 67 tokens/sec\n    Vulkan = 105 tokens/sec\n\nWTF?!?\n\nSome other models I've tested don't see nearly that much difference but the token generation speed is always better with Vulkan and sometimes considerably so. Perhaps it depends on the quantization type?\n\nThe only problem is that the prompt processing speed is tanked. On most of my tests it's about 1.5-2x slower but on this particular model it's **9x slower**. Anyone else encountered that? I'm wondering if it's to do with this GTT spilling issue in RADV;\n\nhttps://github.com/ggml-org/llama.cpp/issues/13765#issuecomment-2951505215\n\nThe PR mentioned there was released today in Mesa 25.2.0 (`RADV_PERFTEST=nogttspill`) so I guess I need to build and install that when I have time... or build a patched version of my current Mesa 25.1.\n\nWould be very nice if I could just use the pre-built Linux Vulkan binaries AND get better performance. \n\n\n    $ llama-bench -m models/local/Qwen3-Coder-30B-A3B-Instruct-UD-Q3_K_XL.gguf\n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    ggml_cuda_init: found 1 ROCm devices:\n      Device 0: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | ROCm       |  99 |           pp512 |       1004.02 ± 1.57 |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | ROCm       |  99 |           tg128 |         67.02 ± 0.06 |\n    build: 3db4da56 (6103)\n\n\n    $ llama-bench -m /hdd/llm-models/Qwen3-Coder-30B-A3B-Instruct-UD-Q3_K_XL.gguf\n    load_backend: loaded RPC backend from /home/xxx/llama-6103-vulkan/bin/libggml-rpc.so\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon RX 6800 (RADV NAVI21) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none\n    load_backend: loaded Vulkan backend from /home/xxx/llama-6103-vulkan/bin/libggml-vulkan.so\n    load_backend: loaded CPU backend from /home/xxx/llama-6103-vulkan/bin/libggml-cpu-haswell.so\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | RPC,Vulkan |  99 |           pp512 |        110.61 ± 0.03 |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | RPC,Vulkan |  99 |           tg128 |        105.28 ± 0.03 |\n    build: 3db4da56 (6103)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Llama.cpp Vulkan backend is up to 50% faster than ROCm?!?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjnhj2",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": "#bbbdbf",
            "subreddit_type": "public",
            "ups": 24,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "is_original_content": false,
            "author_fullname": "t2_lspqn",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 24,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754531692,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using a RX 6800 16GB on Linux.&lt;/p&gt;\n\n&lt;p&gt;When did the Vulkan backend get so much better? Last time I tried it (probably a year ago) it was way behind ROCm, now it&amp;#39;s up to &lt;strong&gt;50% faster&lt;/strong&gt; at token generation depending on the model.&lt;/p&gt;\n\n&lt;p&gt;With Qwen3-Coder-30B-A3B-Instruct-UD-Q3_K_XL.gguf&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ROCm   = 67 tokens/sec\nVulkan = 105 tokens/sec\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;WTF?!?&lt;/p&gt;\n\n&lt;p&gt;Some other models I&amp;#39;ve tested don&amp;#39;t see nearly that much difference but the token generation speed is always better with Vulkan and sometimes considerably so. Perhaps it depends on the quantization type?&lt;/p&gt;\n\n&lt;p&gt;The only problem is that the prompt processing speed is tanked. On most of my tests it&amp;#39;s about 1.5-2x slower but on this particular model it&amp;#39;s &lt;strong&gt;9x slower&lt;/strong&gt;. Anyone else encountered that? I&amp;#39;m wondering if it&amp;#39;s to do with this GTT spilling issue in RADV;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/issues/13765#issuecomment-2951505215\"&gt;https://github.com/ggml-org/llama.cpp/issues/13765#issuecomment-2951505215&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The PR mentioned there was released today in Mesa 25.2.0 (&lt;code&gt;RADV_PERFTEST=nogttspill&lt;/code&gt;) so I guess I need to build and install that when I have time... or build a patched version of my current Mesa 25.1.&lt;/p&gt;\n\n&lt;p&gt;Would be very nice if I could just use the pre-built Linux Vulkan binaries AND get better performance. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ llama-bench -m models/local/Qwen3-Coder-30B-A3B-Instruct-UD-Q3_K_XL.gguf\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 6800, gfx1030 (0x1030), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | ROCm       |  99 |           pp512 |       1004.02 ± 1.57 |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | ROCm       |  99 |           tg128 |         67.02 ± 0.06 |\nbuild: 3db4da56 (6103)\n\n\n$ llama-bench -m /hdd/llm-models/Qwen3-Coder-30B-A3B-Instruct-UD-Q3_K_XL.gguf\nload_backend: loaded RPC backend from /home/xxx/llama-6103-vulkan/bin/libggml-rpc.so\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon RX 6800 (RADV NAVI21) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none\nload_backend: loaded Vulkan backend from /home/xxx/llama-6103-vulkan/bin/libggml-vulkan.so\nload_backend: loaded CPU backend from /home/xxx/llama-6103-vulkan/bin/libggml-cpu-haswell.so\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | RPC,Vulkan |  99 |           pp512 |        110.61 ± 0.03 |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.85 GiB |    30.53 B | RPC,Vulkan |  99 |           tg128 |        105.28 ± 0.03 |\nbuild: 3db4da56 (6103)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo.png?auto=webp&amp;s=daecc21d4e3742536275d86f67b41f3383e73073",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=044e21b5b0784d62c086f300db49fc70cafffacc",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe4f078b5ff91fb69c3f75f515e39edf26c5db98",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e198418399d5e6f827602e7625cea4dbde96ab11",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3fb67766fffbac352c38b3db665af8862e7a49ef",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ecd23fdaa297e2a29062aa59d22b58995ace620",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b2d0bbfef0b96cbe48245e4c5a18953968b594a5",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "X92iU733D06TrlCen_xHl-SjFVuhypoj1xjA-iJyhoo"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjnhj2",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "mine49er",
            "discussion_type": null,
            "num_comments": 17,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "light",
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754531692,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "richtext",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "richtext",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n7cnxg5",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "mine49er",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n7clhlk",
                                                              "score": 2,
                                                              "author_fullname": "t2_lspqn",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Well I'll definitely do more testing. Haven't compared with offloading and given the speed of the Qwen 3 30B A3B models a 4-bit quantization with offloading is probably a better idea tbh.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n7cnxg5",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [
                                                                {
                                                                  "e": "text",
                                                                  "t": "llama.cpp"
                                                                }
                                                              ],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well I&amp;#39;ll definitely do more testing. Haven&amp;#39;t compared with offloading and given the speed of the Qwen 3 30B A3B models a 4-bit quantization with offloading is probably a better idea tbh.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mjnhj2",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": "light",
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7cnxg5/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754535445,
                                                              "author_flair_text": "llama.cpp",
                                                              "treatment_tags": [],
                                                              "created_utc": 1754535445,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": "#bbbdbf",
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 2
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7clhlk",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "BlueSwordM",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7ckc9z",
                                                    "score": 2,
                                                    "author_fullname": "t2_qhqon",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "You'd be surprised since ROCm 6.3.0 brought massive speed increases across the board.\n\nAnyway, I tested both on my 6700XT and Radeon VII (GCN5.1).\n\n6700XT: ROCm is only faster when offloading models for some reason.\n\nRadeon VII: ROCm is faster for &gt;=4-bit quants, slower for &lt;= 4-bit quants; it might be better for modern CDNA2/3 cards.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7clhlk",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [
                                                      {
                                                        "e": "text",
                                                        "t": "llama.cpp"
                                                      }
                                                    ],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;d be surprised since ROCm 6.3.0 brought massive speed increases across the board.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I tested both on my 6700XT and Radeon VII (GCN5.1).&lt;/p&gt;\n\n&lt;p&gt;6700XT: ROCm is only faster when offloading models for some reason.&lt;/p&gt;\n\n&lt;p&gt;Radeon VII: ROCm is faster for &amp;gt;=4-bit quants, slower for &amp;lt;= 4-bit quants; it might be better for modern CDNA2/3 cards.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mjnhj2",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": "light",
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7clhlk/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754534525,
                                                    "author_flair_text": "llama.cpp",
                                                    "collapsed": false,
                                                    "created_utc": 1754534525,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": "#bbbdbf",
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7ckc9z",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "mine49er",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7cjv1w",
                                          "score": 1,
                                          "author_fullname": "t2_lspqn",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "6.2.4. I doubt that anything later offers much for RDNA2.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7ckc9z",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;6.2.4. I doubt that anything later offers much for RDNA2.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mjnhj2",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7ckc9z/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754534097,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1754534097,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7cjv1w",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "BlueSwordM",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7cih3e",
                                "score": 1,
                                "author_fullname": "t2_qhqon",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What ROCm version do you have?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7cjv1w",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What ROCm version do you have?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjnhj2",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7cjv1w/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754533921,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754533921,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7cih3e",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "mine49er",
                      "can_mod_post": false,
                      "created_utc": 1754533416,
                      "send_replies": true,
                      "parent_id": "t1_n7cgs0z",
                      "score": 3,
                      "author_fullname": "t2_lspqn",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf was one that I tested.\n\nROCm = 63 t/s, Vulkan = 83 t/s\n\nThe ones I've seen the smallest difference so far are K6_L models which I have a few of, but even there Vulkan is slightly ahead.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7cih3e",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf was one that I tested.&lt;/p&gt;\n\n&lt;p&gt;ROCm = 63 t/s, Vulkan = 83 t/s&lt;/p&gt;\n\n&lt;p&gt;The ones I&amp;#39;ve seen the smallest difference so far are K6_L models which I have a few of, but even there Vulkan is slightly ahead.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjnhj2",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7cih3e/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754533416,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7cgs0z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "BlueSwordM",
            "can_mod_post": false,
            "created_utc": 1754532798,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 12,
            "author_fullname": "t2_qhqon",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For token generation speeds, yes, this is normal.\n\nExotic 1-3bit quants are faster with Vulkan, while \"normal\" quants like Q4_K_M and up are as fast or faster on ROCm.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7cgs0z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For token generation speeds, yes, this is normal.&lt;/p&gt;\n\n&lt;p&gt;Exotic 1-3bit quants are faster with Vulkan, while &amp;quot;normal&amp;quot; quants like Q4_K_M and up are as fast or faster on ROCm.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7cgs0z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754532798,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mjnhj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dhulf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "zdy1995",
                      "can_mod_post": false,
                      "created_utc": 1754549400,
                      "send_replies": true,
                      "parent_id": "t1_n7ch1qm",
                      "score": 2,
                      "author_fullname": "t2_ok06y1b2i",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "wait what? vulcan faster than rocm on MI60?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dhulf",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;wait what? vulcan faster than rocm on MI60?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjnhj2",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7dhulf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754549400,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dkuis",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "dsanft",
                      "can_mod_post": false,
                      "created_utc": 1754551060,
                      "send_replies": true,
                      "parent_id": "t1_n7ch1qm",
                      "score": 2,
                      "author_fullname": "t2_1uxcrk2ui9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have Mi50s and I find ROCm to be ~25% faster than Vulkan for Qwen3 32B Q6_K.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dkuis",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have Mi50s and I find ROCm to be ~25% faster than Vulkan for Qwen3 32B Q6_K.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjnhj2",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7dkuis/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754551060,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7ch1qm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754532896,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 12,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yeah, the llama.cpp devs have been putting a lot of effort into making the Vulkan back-end more performant.  It reached parity (more or less) with ROCm several months ago.\n\nAs an MI60 user, this pleases me greatly.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ch1qm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, the llama.cpp devs have been putting a lot of effort into making the Vulkan back-end more performant.  It reached parity (more or less) with ROCm several months ago.&lt;/p&gt;\n\n&lt;p&gt;As an MI60 user, this pleases me greatly.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7ch1qm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754532896,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mjnhj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7eaz4h",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Mushoz",
                      "can_mod_post": false,
                      "created_utc": 1754565229,
                      "send_replies": true,
                      "parent_id": "t1_n7d7hnu",
                      "score": 2,
                      "author_fullname": "t2_gwpq7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Mesa RADV vs AMDVLK - the latter is usually a fair bit faster than the former for pp\n\nI initially had the same conclusion as you did, but I then found that radv is actually faster for the models I tested as soon as the context depth increased. For example, with `-d 10000` and `-p 2048` I found that radv was faster both in pp and tg, at least on Strix Halo. Seeing as context starts at 10k and only grows from there in applications such as Roocode, I prefer radv",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7eaz4h",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Mesa RADV vs AMDVLK - the latter is usually a fair bit faster than the former for pp&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I initially had the same conclusion as you did, but I then found that radv is actually faster for the models I tested as soon as the context depth increased. For example, with &lt;code&gt;-d 10000&lt;/code&gt; and &lt;code&gt;-p 2048&lt;/code&gt; I found that radv was faster both in pp and tg, at least on Strix Halo. Seeing as context starts at 10k and only grows from there in applications such as Roocode, I prefer radv&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjnhj2",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7eaz4h/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754565229,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7d7hnu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "randomfoo2",
            "can_mod_post": false,
            "created_utc": 1754543940,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 8,
            "author_fullname": "t2_eztox",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A couple notes:\n\n* Performance between backends can vary a lot for different GPUs. I've found ROCm with 7900 cards (gfx1100) to be a lot faster/competitive than say with Strix Halo (gfx1151)\n* As you saw, different models and quants can have very different performance as well. I've run sweeps of many different model types and quants (just specific quants, takes forever to run these). If you click into each model folder you will see that there's a lot more variability between which backends/flags perform better for each specific model: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench)\n* For ROCm, you may find using hipblaslt `ROCBLAS_USE_HIPBLASLT=1` can sometimes be much faster than rocblas\n* For Vulkan there are two important things you want to test - Mesa RADV vs AMDVLK - the latter is usually a fair bit faster than the former for pp. You can have both installed but amdvlk will take precedence and you will need`AMD_VULKAN_ICD=RADV`to enable Mesa RADV over amdvlk after installation. I'd recommend installing amdvlk and seeing if the pp512 improves.\n* Your Vulkan numbers seem a bit slower than they should be...\n\nOn my W7900 (gfx1100), Mesa RADV:\n\n    ❯ AMD_VULKAN_ICD=RADV GGML_VK_VISIBLE_DEVICES=0 llama.cpp-vulkan/build/bin/llama-bench -m /models/gguf/Qwen3-30B-A3B-128K-UD-Q3_K_XL.gguf\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon Pro W7900 (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           pp512 |        909.89 ± 8.66 |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           tg128 |        122.90 ± 1.46 |\n\nvs AMDVLK:\n\n    ❯ GGML_VK_VISIBLE_DEVICES=0 llama.cpp-vulkan/build/bin/llama-bench -m /models/gguf/Qwen3-30B-A3B-128K-UD-Q3_K_XL.gguf\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon Pro W7900 (AMD open-source driver) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           pp512 |      1540.10 ± 11.76 |\n    | qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           tg128 |        136.47 ± 0.19 |\n    \n    build: 36d3f00e (6107)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7d7hnu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A couple notes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Performance between backends can vary a lot for different GPUs. I&amp;#39;ve found ROCm with 7900 cards (gfx1100) to be a lot faster/competitive than say with Strix Halo (gfx1151)&lt;/li&gt;\n&lt;li&gt;As you saw, different models and quants can have very different performance as well. I&amp;#39;ve run sweeps of many different model types and quants (just specific quants, takes forever to run these). If you click into each model folder you will see that there&amp;#39;s a lot more variability between which backends/flags perform better for each specific model: &lt;a href=\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;For ROCm, you may find using hipblaslt &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; can sometimes be much faster than rocblas&lt;/li&gt;\n&lt;li&gt;For Vulkan there are two important things you want to test - Mesa RADV vs AMDVLK - the latter is usually a fair bit faster than the former for pp. You can have both installed but amdvlk will take precedence and you will need&lt;code&gt;AMD_VULKAN_ICD=RADV&lt;/code&gt;to enable Mesa RADV over amdvlk after installation. I&amp;#39;d recommend installing amdvlk and seeing if the pp512 improves.&lt;/li&gt;\n&lt;li&gt;Your Vulkan numbers seem a bit slower than they should be...&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;On my W7900 (gfx1100), Mesa RADV:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;❯ AMD_VULKAN_ICD=RADV GGML_VK_VISIBLE_DEVICES=0 llama.cpp-vulkan/build/bin/llama-bench -m /models/gguf/Qwen3-30B-A3B-128K-UD-Q3_K_XL.gguf\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon Pro W7900 (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           pp512 |        909.89 ± 8.66 |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           tg128 |        122.90 ± 1.46 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;vs AMDVLK:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;❯ GGML_VK_VISIBLE_DEVICES=0 llama.cpp-vulkan/build/bin/llama-bench -m /models/gguf/Qwen3-30B-A3B-128K-UD-Q3_K_XL.gguf\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon Pro W7900 (AMD open-source driver) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           pp512 |      1540.10 ± 11.76 |\n| qwen3moe 30B.A3B Q3_K - Medium |  12.88 GiB |    30.53 B | Vulkan     |  99 |           tg128 |        136.47 ± 0.19 |\n\nbuild: 36d3f00e (6107)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7d7hnu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754543940,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjnhj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dlzya",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "b3081a",
            "can_mod_post": false,
            "created_utc": 1754551714,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 2,
            "author_fullname": "t2_17n5yh7l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "llama.cpp CUDA/ROCm backend isn't good at MoEs with small active parameter size due to their extremely high overhead on expert selection (basically done on CPU). For Qwen 3 this applies to NVIDIA GPUs as well.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dlzya",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llama.cpp CUDA/ROCm backend isn&amp;#39;t good at MoEs with small active parameter size due to their extremely high overhead on expert selection (basically done on CPU). For Qwen 3 this applies to NVIDIA GPUs as well.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7dlzya/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754551714,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mjnhj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dikt9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Final-Rush759",
            "can_mod_post": false,
            "created_utc": 1754549795,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 1,
            "author_fullname": "t2_817ilmxg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GPUs are far below saturation in these inference situation. These are not testing Vulcan vs Rocm. Probably, the code linked to Vulcan and ROCM performs very differently in speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dikt9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GPUs are far below saturation in these inference situation. These are not testing Vulcan vs Rocm. Probably, the code linked to Vulcan and ROCM performs very differently in speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7dikt9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754549795,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjnhj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "total_awards_received": 0,
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "ups": 1,
            "removal_reason": null,
            "link_id": "t3_1mjnhj2",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7e0zoo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "i-eat-kittens",
                      "can_mod_post": false,
                      "created_utc": 1754560342,
                      "send_replies": true,
                      "parent_id": "t1_n7dvx9g",
                      "score": 1,
                      "author_fullname": "t2_z290n",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Enabling FA seems to drop pp performance for ROCm, while not affecting Vulkan too much:\n\n    ~/src/llama.cpp/build/bin/llama-bench -ngl 99 -fa 1 -m ~/.cache/huggingface/hub/models--unsloth--Qwen3-4B-Instruct-2507-GGUF/snapshots/b48eaa0431fbfc07e852bc574f440def545d5ccb/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = AMD Radeon RX 7600 (RADV NAVI33) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | Vulkan     |  99 |  1 |           pp512 |       995.55 ± 20.70 |\n    | qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | Vulkan     |  99 |  1 |           tg128 |         43.77 ± 0.05 |\n    \n    build: 20638e4f (6108)\n    \n    ~/src/llama.cpp/build-hip/bin/llama-bench -ngl 99 -fa 1 -m ~/.cache/huggingface/hub/models--unsloth--Qwen3-4B-Instruct-2507-GGUF/snapshots/b48eaa0431fbfc07e852bc574f440def545d5ccb/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf\n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    ggml_cuda_init: found 1 ROCm devices:\n      Device 0: AMD Radeon RX 7600, gfx1102 (0x1102), VMM: no, Wave Size: 32\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | ROCm       |  99 |  1 |           pp512 |        771.45 ± 9.40 |\n    | qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | ROCm       |  99 |  1 |           tg128 |         50.86 ± 0.08 |\n    \n    build: 20638e4f (6108)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7e0zoo",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Enabling FA seems to drop pp performance for ROCm, while not affecting Vulkan too much:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;~/src/llama.cpp/build/bin/llama-bench -ngl 99 -fa 1 -m ~/.cache/huggingface/hub/models--unsloth--Qwen3-4B-Instruct-2507-GGUF/snapshots/b48eaa0431fbfc07e852bc574f440def545d5ccb/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf\nggml_vulkan: Found 1 Vulkan devices:\nggml_vulkan: 0 = AMD Radeon RX 7600 (RADV NAVI33) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | Vulkan     |  99 |  1 |           pp512 |       995.55 ± 20.70 |\n| qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | Vulkan     |  99 |  1 |           tg128 |         43.77 ± 0.05 |\n\nbuild: 20638e4f (6108)\n\n~/src/llama.cpp/build-hip/bin/llama-bench -ngl 99 -fa 1 -m ~/.cache/huggingface/hub/models--unsloth--Qwen3-4B-Instruct-2507-GGUF/snapshots/b48eaa0431fbfc07e852bc574f440def545d5ccb/Qwen3-4B-Instruct-2507-UD-Q6_K_XL.gguf\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7600, gfx1102 (0x1102), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | ROCm       |  99 |  1 |           pp512 |        771.45 ± 9.40 |\n| qwen3 4B Q6_K                  |   3.40 GiB |     4.02 B | ROCm       |  99 |  1 |           tg128 |         50.86 ± 0.08 |\n\nbuild: 20638e4f (6108)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjnhj2",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7e0zoo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754560342,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7dvx9g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "DELETED",
            "no_follow": true,
            "author": "[deleted]",
            "can_mod_post": false,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 1,
            "approved_by": null,
            "report_reasons": null,
            "all_awardings": [],
            "subreddit_id": "t5_81eyvm",
            "body": "[removed]",
            "edited": 1754565755,
            "downs": 0,
            "author_flair_css_class": null,
            "collapsed": true,
            "is_submitter": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;",
            "gildings": {},
            "collapsed_reason": null,
            "associated_award": null,
            "stickied": false,
            "subreddit_type": "public",
            "can_gild": false,
            "top_awarded_type": null,
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7dvx9g/",
            "num_reports": null,
            "locked": false,
            "name": "t1_n7dvx9g",
            "created": 1754557459,
            "subreddit": "LocalLLaMA",
            "author_flair_text": null,
            "treatment_tags": [],
            "created_utc": 1754557459,
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "mod_note": null,
            "distinguished": null
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7cq74z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MeteoriteImpact",
            "can_mod_post": false,
            "created_utc": 1754536332,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 1,
            "author_fullname": "t2_41dxjep4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes same here for a while now on Linux and recently on windows but a new ROCm 7 is coming.\n\nhttps://www.amd.com/en/products/software/rocm/whats-new.html",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7cq74z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes same here for a while now on Linux and recently on windows but a new ROCm 7 is coming.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amd.com/en/products/software/rocm/whats-new.html\"&gt;https://www.amd.com/en/products/software/rocm/whats-new.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7cq74z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754536332,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjnhj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7d4rqw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Tyme4Trouble",
            "can_mod_post": false,
            "created_utc": 1754542610,
            "send_replies": true,
            "parent_id": "t3_1mjnhj2",
            "score": 0,
            "author_fullname": "t2_973amyap",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Also while Vulcan can be faster for decode. It’s usually much slower for prefill.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7d4rqw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also while Vulcan can be faster for decode. It’s usually much slower for prefill.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjnhj2/llamacpp_vulkan_backend_is_up_to_50_faster_than/n7d4rqw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754542610,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjnhj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]