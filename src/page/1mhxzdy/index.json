[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "[DoCoreAI is Back as Saas](https://preview.redd.it/df8uzh8da4hf1.png?width=1536&amp;format=png&amp;auto=webp&amp;s=1a0615717de9975bb1ca0c2d2f48022272ca6c7c)\n\nA few months ago, I shared a GitHub CLI tool here for optimizing local LLM prompts. It quietly grew to 16K+ downloads â€” but most users skip the dashboard where all the real insights are.\n\nNow, Iâ€™ve brought it back as a SaaS-powered prompt analytics layer â€” still CLI-first, still dev-friendly.\n\nI recently built a tool called DoCoreAI â€” originally meant to help devs and teams optimize LLM prompts and see behind-the-scenes telemetry (usage, cost, tokens, efficiency, etc.). It went live on PyPI and surprisingly crossed **16,000+ downloads**.\n\nBut here's the strange part:\n\nAlmost no one is actually using the charts we built into the dashboard â€” which is where all the insights really live.\n\nWe realized most devs install it like any normal CLI tool (pip install docoreai), run a few prompt tests, and never connect it to the dashboard. So we decided to fix the docs and write a proper **getting started blog**.\n\nHereâ€™s what the dashboard shows now after running a few prompt sessions:\n\nðŸ“Š **Developer Time Saved**\n\nðŸ’° **Token Cost Savings**\n\nðŸ“ˆ **Prompt Health Score**\n\nðŸ§  **Model Temperature Trends**\n\nIt works with both OpenAI and Groq. No original prompt data leaves your machine â€” it just sends optimization metrics.\n\nHereâ€™s a sample CLI session:\n\n    $ docoreai start\n    [âœ“] Running: Prompt telemetry enabled\n    [âœ“] Optimization: Bloat reduced by 41%\n    [âœ“] See dashboard at: https://docoreai.com/dashboard\n\nAnd here's one of my favorite charts:\n\nhttps://preview.redd.it/m0we2tdfc4hf1.png?width=600&amp;format=png&amp;auto=webp&amp;s=852199c02b5b69ee9e53ca4544a1f17dc59f6f39\n\nðŸ‘‰ **Full post with setup guide &amp; dashboard screenshots:**\n\n[https://docoreai.com/pypi-downloads-docoreai-dashboard-insights/](https://docoreai.com/pypi-downloads-docoreai-dashboard-insights/)\n\nWould love feedback â€” especially from devs who care about making their LLM usage less of a black box.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "I built a tool that got 16K downloads, but no one uses the charts. Here's what they're missing.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 70,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "df8uzh8da4hf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 72,
                    "x": 108,
                    "u": "https://preview.redd.it/df8uzh8da4hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=674245e78604e3169bd86adf1ff5ad7b70e2c2df"
                  },
                  {
                    "y": 144,
                    "x": 216,
                    "u": "https://preview.redd.it/df8uzh8da4hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cb1eafb289cae687f89414b3f3f48e3b2fc9126"
                  },
                  {
                    "y": 213,
                    "x": 320,
                    "u": "https://preview.redd.it/df8uzh8da4hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c902ee5b09da60f7973d7c02bc7e8f4648a1832"
                  },
                  {
                    "y": 426,
                    "x": 640,
                    "u": "https://preview.redd.it/df8uzh8da4hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56b5d952ab98ca0399c8532863b3716912111705"
                  },
                  {
                    "y": 640,
                    "x": 960,
                    "u": "https://preview.redd.it/df8uzh8da4hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7d31b9b66b13a64ff9f28c1d013df0f4323e09e"
                  },
                  {
                    "y": 720,
                    "x": 1080,
                    "u": "https://preview.redd.it/df8uzh8da4hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34f26809546a2ebada7c6877eef62974eb5845e7"
                  }
                ],
                "s": {
                  "y": 1024,
                  "x": 1536,
                  "u": "https://preview.redd.it/df8uzh8da4hf1.png?width=1536&amp;format=png&amp;auto=webp&amp;s=1a0615717de9975bb1ca0c2d2f48022272ca6c7c"
                },
                "id": "df8uzh8da4hf1"
              },
              "m0we2tdfc4hf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 63,
                    "x": 108,
                    "u": "https://preview.redd.it/m0we2tdfc4hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80eaabff3dcaf397e2ccdd943a79797ecc2feb39"
                  },
                  {
                    "y": 126,
                    "x": 216,
                    "u": "https://preview.redd.it/m0we2tdfc4hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c1ffc052b0d4e9daffb819e833bfd368143f640"
                  },
                  {
                    "y": 186,
                    "x": 320,
                    "u": "https://preview.redd.it/m0we2tdfc4hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d03386451dc5a4695758d4a91b3ad6cb9a75eb8"
                  }
                ],
                "s": {
                  "y": 350,
                  "x": 600,
                  "u": "https://preview.redd.it/m0we2tdfc4hf1.png?width=600&amp;format=png&amp;auto=webp&amp;s=852199c02b5b69ee9e53ca4544a1f17dc59f6f39"
                },
                "id": "m0we2tdfc4hf1"
              }
            },
            "name": "t3_1mhxzdy",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_akz5120l",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=c9f87e578fd282dbc3becb7e5d7ca72f3c915d86",
            "edited": 1754363857,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "subreddit_type": "public",
            "created": 1754362944,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/df8uzh8da4hf1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a0615717de9975bb1ca0c2d2f48022272ca6c7c\"&gt;DoCoreAI is Back as Saas&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A few months ago, I shared a GitHub CLI tool here for optimizing local LLM prompts. It quietly grew to 16K+ downloads â€” but most users skip the dashboard where all the real insights are.&lt;/p&gt;\n\n&lt;p&gt;Now, Iâ€™ve brought it back as a SaaS-powered prompt analytics layer â€” still CLI-first, still dev-friendly.&lt;/p&gt;\n\n&lt;p&gt;I recently built a tool called DoCoreAI â€” originally meant to help devs and teams optimize LLM prompts and see behind-the-scenes telemetry (usage, cost, tokens, efficiency, etc.). It went live on PyPI and surprisingly crossed &lt;strong&gt;16,000+ downloads&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;But here&amp;#39;s the strange part:&lt;/p&gt;\n\n&lt;p&gt;Almost no one is actually using the charts we built into the dashboard â€” which is where all the insights really live.&lt;/p&gt;\n\n&lt;p&gt;We realized most devs install it like any normal CLI tool (pip install docoreai), run a few prompt tests, and never connect it to the dashboard. So we decided to fix the docs and write a proper &lt;strong&gt;getting started blog&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Hereâ€™s what the dashboard shows now after running a few prompt sessions:&lt;/p&gt;\n\n&lt;p&gt;ðŸ“Š &lt;strong&gt;Developer Time Saved&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;ðŸ’° &lt;strong&gt;Token Cost Savings&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;ðŸ“ˆ &lt;strong&gt;Prompt Health Score&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;ðŸ§  &lt;strong&gt;Model Temperature Trends&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It works with both OpenAI and Groq. No original prompt data leaves your machine â€” it just sends optimization metrics.&lt;/p&gt;\n\n&lt;p&gt;Hereâ€™s a sample CLI session:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ docoreai start\n[âœ“] Running: Prompt telemetry enabled\n[âœ“] Optimization: Bloat reduced by 41%\n[âœ“] See dashboard at: https://docoreai.com/dashboard\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And here&amp;#39;s one of my favorite charts:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/m0we2tdfc4hf1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=852199c02b5b69ee9e53ca4544a1f17dc59f6f39\"&gt;https://preview.redd.it/m0we2tdfc4hf1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=852199c02b5b69ee9e53ca4544a1f17dc59f6f39&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;ðŸ‘‰ &lt;strong&gt;Full post with setup guide &amp;amp; dashboard screenshots:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docoreai.com/pypi-downloads-docoreai-dashboard-insights/\"&gt;https://docoreai.com/pypi-downloads-docoreai-dashboard-insights/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback â€” especially from devs who care about making their LLM usage less of a black box.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?auto=webp&amp;s=2f4ed751d9647a565eebea446804e3d504e57d88",
                    "width": 1280,
                    "height": 640
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eee87d373fea3c9717fca18d6d6a4ca45e50a554",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=03bee598a7514df4650898e2368723c31b3fb6be",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c90bcc8be0c6d516d153f910f346a5476bfe0f86",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=427619fc262d84a1091337db14b6266a39db5336",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ed710acf350c318733182dbe2c3e9ddbdcbf17cb",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9e073d7e72df1c8d5d7067eea0313e5d5a2e90c2",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "4LsBzhGZ9Abov7rw0R6iAiqHWhYFbNfYU9nhF4zmqkU"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1mhxzdy",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "MobiLights",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754362944,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n70s1p1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HiddenoO",
                      "can_mod_post": false,
                      "created_utc": 1754384138,
                      "send_replies": true,
                      "parent_id": "t1_n7042ja",
                      "score": 2,
                      "author_fullname": "t2_8127x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is basically the whole code:\n\n[https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore\\_ai/model.py](https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore_ai/model.py)\n\nNot only does it hardcode OpenAI/Groq, the functionality seems dubious at best (see my response below) so I wouldn't waste my time (and especially not my money) with this.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n70s1p1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is basically the whole code:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore_ai/model.py\"&gt;https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore_ai/model.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Not only does it hardcode OpenAI/Groq, the functionality seems dubious at best (see my response below) so I wouldn&amp;#39;t waste my time (and especially not my money) with this.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhxzdy",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/n70s1p1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754384138,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7042ja",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lissanro",
            "can_mod_post": false,
            "created_utc": 1754370779,
            "send_replies": true,
            "parent_id": "t3_1mhxzdy",
            "score": 3,
            "author_fullname": "t2_fpfao9g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;It works with both OpenAI and Groq\n\nDoes it work with OpenAI-compatible local running LLMs? I checked the site, but it kind of unclear:\n\n&gt;Can I use DoCoreAI with any LLM like Claude or Gemini?\n\n&gt;Yes, DoCoreAI is compatible with various LLMs, including GPT, LLM access via Groq , other LLMs like Gemini, Claude integrations will be released soon. You can analyze prompts from any of these models using our platform.\n\nNothing about llama.cpp, TabbyAPI or other OpenAI-compatible backends. Given support for local LLMs is the most important part, it would be appreciated if you cover it explicitly. Otherwise not really relevant for r/locallama.",
            "edited": 1754374941,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7042ja",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It works with both OpenAI and Groq&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Does it work with OpenAI-compatible local running LLMs? I checked the site, but it kind of unclear:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Can I use DoCoreAI with any LLM like Claude or Gemini?&lt;/p&gt;\n\n&lt;p&gt;Yes, DoCoreAI is compatible with various LLMs, including GPT, LLM access via Groq , other LLMs like Gemini, Claude integrations will be released soon. You can analyze prompts from any of these models using our platform.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Nothing about llama.cpp, TabbyAPI or other OpenAI-compatible backends. Given support for local LLMs is the most important part, it would be appreciated if you cover it explicitly. Otherwise not really relevant for &lt;a href=\"/r/locallama\"&gt;r/locallama&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/n7042ja/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754370779,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhxzdy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70qslt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HiddenoO",
            "can_mod_post": false,
            "created_utc": 1754383415,
            "send_replies": true,
            "parent_id": "t3_1mhxzdy",
            "score": 2,
            "author_fullname": "t2_8127x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I checked the repo and frankly don't see how you can get any statistical insights that are even remotely accurate based on the code. This is basically the whole functionality: [https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore\\_ai/model.py](https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore_ai/model.py)\n\nThe 'intelligence\\_profiler' practically just appends a system prompt telling the LLM to choose an appropriate temperature and generate the response using that temperature:\n\n        system_message = f\"\"\"\n            You are a system prompt profiler. Analyze the user input and estimate what temperature setting would best match the tone, ambiguity, and specificity of the request.\n            Return the estimated temperature value only, between 0.1 and 1.0, based on the following:\n            - Low temperature (~0.1â€“0.3): Precise, factual, deterministic answers.\n            - Medium temperature (~0.4â€“0.6): Balanced creativity and reasoning.\n            - High temperature (~0.7â€“1.0): Creative, open-ended, speculative, or abstract.\n    \n            You MUST generate responses using the estimated temperature.\n            The response must be coherent and informative\n    \n            Return **ONLY** the following JSON format:  \n            {{\n                \"optimized_response\": \"&lt;AI-generated response&gt;\",\n                {{ \"temperature\": &lt;value&gt;}}\n            }}\n        \"\"\"\n\nNot only does this not work the way the system prompt suggests you think it does (telling an LLM to use a temperature does not actually make it do so, it makes it try to simulate doing so), it bloats the context window (and thus cost), and I don't see how you could translate its results into objective metrics such as the ones you're presenting in your charts.\n\nThe other relevant part is the \"token\\_profiler\" which may even be worse. What it essentially does is check the average number of tokens per word and set a flag \"bloated\" if it's roughly &gt;1.3 and then suggest you can save 30% if that flag is true.\n\nNot only does that heuristic not work for different practical scenarios (different tokenizers use different numbers of tokens per word, and so do different languages, application areas, etc.), the 30% savings are also just completely made up and there is no feedback on how to even get any savings.\n\nUnless your SaaS is something entirely different from what's on your Github for the project (in which case I'd be worried about entering my API keys), I wouldn't touch this with a 10-foot-pole as either a developer or an investor.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70qslt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I checked the repo and frankly don&amp;#39;t see how you can get any statistical insights that are even remotely accurate based on the code. This is basically the whole functionality: &lt;a href=\"https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore_ai/model.py\"&gt;https://github.com/SajiJohnMiranda/DoCoreAI/blob/main/docore_ai/model.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The &amp;#39;intelligence_profiler&amp;#39; practically just appends a system prompt telling the LLM to choose an appropriate temperature and generate the response using that temperature:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    system_message = f&amp;quot;&amp;quot;&amp;quot;\n        You are a system prompt profiler. Analyze the user input and estimate what temperature setting would best match the tone, ambiguity, and specificity of the request.\n        Return the estimated temperature value only, between 0.1 and 1.0, based on the following:\n        - Low temperature (~0.1â€“0.3): Precise, factual, deterministic answers.\n        - Medium temperature (~0.4â€“0.6): Balanced creativity and reasoning.\n        - High temperature (~0.7â€“1.0): Creative, open-ended, speculative, or abstract.\n\n        You MUST generate responses using the estimated temperature.\n        The response must be coherent and informative\n\n        Return **ONLY** the following JSON format:  \n        {{\n            &amp;quot;optimized_response&amp;quot;: &amp;quot;&amp;lt;AI-generated response&amp;gt;&amp;quot;,\n            {{ &amp;quot;temperature&amp;quot;: &amp;lt;value&amp;gt;}}\n        }}\n    &amp;quot;&amp;quot;&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Not only does this not work the way the system prompt suggests you think it does (telling an LLM to use a temperature does not actually make it do so, it makes it try to simulate doing so), it bloats the context window (and thus cost), and I don&amp;#39;t see how you could translate its results into objective metrics such as the ones you&amp;#39;re presenting in your charts.&lt;/p&gt;\n\n&lt;p&gt;The other relevant part is the &amp;quot;token_profiler&amp;quot; which may even be worse. What it essentially does is check the average number of tokens per word and set a flag &amp;quot;bloated&amp;quot; if it&amp;#39;s roughly &amp;gt;1.3 and then suggest you can save 30% if that flag is true.&lt;/p&gt;\n\n&lt;p&gt;Not only does that heuristic not work for different practical scenarios (different tokenizers use different numbers of tokens per word, and so do different languages, application areas, etc.), the 30% savings are also just completely made up and there is no feedback on how to even get any savings.&lt;/p&gt;\n\n&lt;p&gt;Unless your SaaS is something entirely different from what&amp;#39;s on your Github for the project (in which case I&amp;#39;d be worried about entering my API keys), I wouldn&amp;#39;t touch this with a 10-foot-pole as either a developer or an investor.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/n70qslt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754383415,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhxzdy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n707tnx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754372727,
            "send_replies": true,
            "parent_id": "t3_1mhxzdy",
            "score": 1,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Does it support local LLMs?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n707tnx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does it support local LLMs?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/n707tnx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754372727,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhxzdy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6zt4t0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "No_Efficiency_1144",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6zpm19",
                                "score": 1,
                                "author_fullname": "t2_1nkj9l14b0",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Gemini for sure seems to do well with the professor role compared to its default. I found that sometimes just giving it the role wasnâ€™t enough, but a further boost could come by giving it some example responses that were in the role, essentially few-shot but for the role. Gemini in particular seems to have an ability to hold multiple personas at once even, it does a lot better at this than older models. I tried getting them to debate within one context window and it sort of works. At that point the LLM is more like play-write than actor. Iâ€™m not saying that is as good as multi-agent, because itâ€™s not, but itâ€™s a far cheaper approximation which is sometimes enough. I love multi-agent stuff but often they are like $1,000 for a single run of the multi-agent system on one task. Ironically LLM costs were kinda going down but then with reasoning and then multi-agent they are right back up again. If we can sometimes avoid that cost then it would be nice.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6zt4t0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gemini for sure seems to do well with the professor role compared to its default. I found that sometimes just giving it the role wasnâ€™t enough, but a further boost could come by giving it some example responses that were in the role, essentially few-shot but for the role. Gemini in particular seems to have an ability to hold multiple personas at once even, it does a lot better at this than older models. I tried getting them to debate within one context window and it sort of works. At that point the LLM is more like play-write than actor. Iâ€™m not saying that is as good as multi-agent, because itâ€™s not, but itâ€™s a far cheaper approximation which is sometimes enough. I love multi-agent stuff but often they are like $1,000 for a single run of the multi-agent system on one task. Ironically LLM costs were kinda going down but then with reasoning and then multi-agent they are right back up again. If we can sometimes avoid that cost then it would be nice.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhxzdy",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/n6zt4t0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754365730,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754365730,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6zpm19",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MobiLights",
                      "can_mod_post": false,
                      "created_utc": 1754364259,
                      "send_replies": true,
                      "parent_id": "t1_n6zo51e",
                      "score": 1,
                      "author_fullname": "t2_akz5120l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks! Yeah, that surprised me too. Turns out roles like Language Teachers and Engineers save a ton of time with structured prompt workflows, while others like Nutritionists or Product Managers tend to have shorter prompt chains or use cases.\n\nThe tool doesnâ€™t assume use case â€” it just measures actual savings based on token/time telemetry. Would love to hear if others are seeing similar trends in their prompts.\n\nThese roles you see are actually **semantic roles** that were explicitly sent along with the prompts â€” kind of like a simplified version of **agentic AI**. It helps understand how different \"intent profiles\" affect optimization and efficiency.\n\nWould love to hear if others are seeing similar trends in their prompt usage.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6zpm19",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks! Yeah, that surprised me too. Turns out roles like Language Teachers and Engineers save a ton of time with structured prompt workflows, while others like Nutritionists or Product Managers tend to have shorter prompt chains or use cases.&lt;/p&gt;\n\n&lt;p&gt;The tool doesnâ€™t assume use case â€” it just measures actual savings based on token/time telemetry. Would love to hear if others are seeing similar trends in their prompts.&lt;/p&gt;\n\n&lt;p&gt;These roles you see are actually &lt;strong&gt;semantic roles&lt;/strong&gt; that were explicitly sent along with the prompts â€” kind of like a simplified version of &lt;strong&gt;agentic AI&lt;/strong&gt;. It helps understand how different &amp;quot;intent profiles&amp;quot; affect optimization and efficiency.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear if others are seeing similar trends in their prompt usage.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhxzdy",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/n6zpm19/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754364259,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6zo51e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1754363670,
            "send_replies": true,
            "parent_id": "t3_1mhxzdy",
            "score": -1,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Itâ€™s a nice tool. The difference between roles is huge it seems",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6zo51e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Itâ€™s a nice tool. The difference between roles is huge it seems&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhxzdy/i_built_a_tool_that_got_16k_downloads_but_no_one/n6zo51e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754363670,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhxzdy",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        }
      ],
      "before": null
    }
  }
]