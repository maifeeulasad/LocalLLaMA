[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of \"ultra high bandwidth memory\", but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it's going to let an on device assistant use a lot more context.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "News"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 102,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1m5fmlp",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "ups": 35,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_lkljr",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 35,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=140&amp;height=102&amp;crop=140:102,smart&amp;auto=webp&amp;s=f46b99f9160b02acaab35b0793c2419a717f902a",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1753094763,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "cnx-software.com",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of &amp;quot;ultra high bandwidth memory&amp;quot;, but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it&amp;#39;s going to let an on device assistant use a lot more context.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?auto=webp&amp;s=a3357893385aa57e61c85776502b465fe73661a4",
                    "width": 1184,
                    "height": 868
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19d2a2efbd9333bbc8e7495d96c37dc9a67f94f7",
                      "width": 108,
                      "height": 79
                    },
                    {
                      "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4320f14e39faee0bd0ec022e73a0260326f90d4",
                      "width": 216,
                      "height": 158
                    },
                    {
                      "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=36df6ea44afc6e837f534a02a66625d799b64a88",
                      "width": 320,
                      "height": 234
                    },
                    {
                      "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ec21866ca44554bfe2c9ada5521c937f241afc3",
                      "width": 640,
                      "height": 469
                    },
                    {
                      "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f1fc5136f191685dd65bd8cc69f5a732d025a2",
                      "width": 960,
                      "height": 703
                    },
                    {
                      "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24e96efb97efa4c62ad46fa709863ab37c9c8266",
                      "width": 1080,
                      "height": 791
                    }
                  ],
                  "variants": {},
                  "id": "p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#cc3600",
            "id": "1m5fmlp",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "PmMeForPCBuilds",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/",
            "stickied": false,
            "url": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
            "subreddit_subscribers": 502273,
            "created_utc": 1753094763,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4bnqog",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AnomalyNexus",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4bn4xc",
                                "score": 1,
                                "author_fullname": "t2_3q8dd",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah they must have done something special there. The discrepancy seems way higher than on other hardware &amp; I thought both are roughly under the same hardware constraints - GPU compute and memory.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4bnqog",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah they must have done something special there. The discrepancy seems way higher than on other hardware &amp;amp; I thought both are roughly under the same hardware constraints - GPU compute and memory.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5fmlp",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bnqog/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753097638,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753097638,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bn4xc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "National_Meeting_749",
                      "can_mod_post": false,
                      "created_utc": 1753097369,
                      "send_replies": true,
                      "parent_id": "t1_n4bjcnb",
                      "score": 4,
                      "author_fullname": "t2_drm5tg5d",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is pure guessing from my part,\nBut there is probably some bit of the math for prompt processing that they were able to 'hardwire' and make an ASIC component for the chip that is much faster than multi-purpose cores would be able to process them.\n\nThat generally what happens when some process gets accelerated quite a bit by a piece of hardware. \n\nMining Bitcoin on GPU's became obsolete when the ASIC miners came out, which is what I'm hoping happens with LLM's. These AI accelerator cards become the best thing to run LLMs on, and the GPU market will have pressure taken off of it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bn4xc",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is pure guessing from my part,\nBut there is probably some bit of the math for prompt processing that they were able to &amp;#39;hardwire&amp;#39; and make an ASIC component for the chip that is much faster than multi-purpose cores would be able to process them.&lt;/p&gt;\n\n&lt;p&gt;That generally what happens when some process gets accelerated quite a bit by a piece of hardware. &lt;/p&gt;\n\n&lt;p&gt;Mining Bitcoin on GPU&amp;#39;s became obsolete when the ASIC miners came out, which is what I&amp;#39;m hoping happens with LLM&amp;#39;s. These AI accelerator cards become the best thing to run LLMs on, and the GPU market will have pressure taken off of it.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bn4xc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753097369,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bnsqu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Amazing_Athlete_2265",
                      "can_mod_post": false,
                      "created_utc": 1753097664,
                      "send_replies": true,
                      "parent_id": "t1_n4bjcnb",
                      "score": 3,
                      "author_fullname": "t2_1nw9fzb7dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Almost all of my benchmarks show this is the case for most local models. For example, for falcon-h1-7b-instruct I am showing prompt processing rate of 104 t/s and inference rate of 7 t/s.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bnsqu",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Almost all of my benchmarks show this is the case for most local models. For example, for falcon-h1-7b-instruct I am showing prompt processing rate of 104 t/s and inference rate of 7 t/s.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bnsqu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753097664,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bpc5h",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Afternoon_4260",
                      "can_mod_post": false,
                      "created_utc": 1753098330,
                      "send_replies": true,
                      "parent_id": "t1_n4bjcnb",
                      "score": 1,
                      "author_fullname": "t2_cj9kap4bx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "[flash attention ](https://arxiv.org/abs/2205.14135)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bpc5h",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2205.14135\"&gt;flash attention &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bpc5h/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753098330,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bqnfq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AppearanceHeavy6724",
                      "can_mod_post": false,
                      "created_utc": 1753098867,
                      "send_replies": true,
                      "parent_id": "t1_n4bjcnb",
                      "score": 1,
                      "author_fullname": "t2_uz37qfx5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is an odd statement for someone who run models locally, as it is well known fact that PP is faster than TG on any accelerated platform, but not on cpus. Token generation is bottlenecked by memory bandwidth, which difficult to scale. PP is limited by compute, which is easier to scale by dropping more computation units on the chip, without need to reengineer bus interface.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bqnfq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is an odd statement for someone who run models locally, as it is well known fact that PP is faster than TG on any accelerated platform, but not on cpus. Token generation is bottlenecked by memory bandwidth, which difficult to scale. PP is limited by compute, which is easier to scale by dropping more computation units on the chip, without need to reengineer bus interface.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bqnfq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753098867,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bjp2x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Vas1le",
                      "can_mod_post": false,
                      "created_utc": 1753095792,
                      "send_replies": true,
                      "parent_id": "t1_n4bjcnb",
                      "score": -1,
                      "author_fullname": "t2_9mhfdr5f",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It connects to China servers for processing \n\n\n^/s",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bjp2x",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It connects to China servers for processing &lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;/s&lt;/sup&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bjp2x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753095792,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4blrdt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Jack-of-the-Shadows",
                      "can_mod_post": false,
                      "created_utc": 1753096751,
                      "send_replies": true,
                      "parent_id": "t1_n4bjcnb",
                      "score": 0,
                      "author_fullname": "t2_2pykl6xb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Memory bandwith?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4blrdt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Memory bandwith?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4blrdt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753096751,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4bjcnb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AnomalyNexus",
            "can_mod_post": false,
            "created_utc": 1753095629,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 7,
            "author_fullname": "t2_3q8dd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Wonder why it’s so much faster on prompt processing",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bjcnb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wonder why it’s so much faster on prompt processing&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bjcnb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753095629,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4blqgs",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "HiddenoO",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4bi4po",
                                "score": 5,
                                "author_fullname": "t2_8127x",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Sequence length is the actual length of the input (context), not the maximum length. Obviously, this also means that the numbers presented will get worse if your input is longer than 1024, assuming longer input fits in the memory.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4blqgs",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sequence length is the actual length of the input (context), not the maximum length. Obviously, this also means that the numbers presented will get worse if your input is longer than 1024, assuming longer input fits in the memory.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5fmlp",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4blqgs/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753096740,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753096740,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4bvby6",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "PmMeForPCBuilds",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4bi4po",
                                "score": 1,
                                "author_fullname": "t2_lkljr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It has 5GB of memory and 3.5GB are taken by the model (for Qwen 7B), so you'd have 1.5GB left over for context. That should be able to fit more than 2048 tokens, but I'm not sure what the limit is.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4bvby6",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It has 5GB of memory and 3.5GB are taken by the model (for Qwen 7B), so you&amp;#39;d have 1.5GB left over for context. That should be able to fit more than 2048 tokens, but I&amp;#39;m not sure what the limit is.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m5fmlp",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bvby6/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753100707,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753100707,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bi4po",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Fast-Satisfaction482",
                      "can_mod_post": false,
                      "created_utc": 1753095052,
                      "send_replies": true,
                      "parent_id": "t1_n4bhluo",
                      "score": 1,
                      "author_fullname": "t2_9ceux4xp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I hope the given Seq len number does not mean how big the context can be, because 1024 is a bit low.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bi4po",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I hope the given Seq len number does not mean how big the context can be, because 1024 is a bit low.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bi4po/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753095052,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4bhluo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "PmMeForPCBuilds",
            "can_mod_post": false,
            "created_utc": 1753094800,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 4,
            "author_fullname": "t2_lkljr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://preview.redd.it/zygp6nfvi7ef1.png?width=1536&amp;format=png&amp;auto=webp&amp;s=541621dcd1c91b62ac181228183adeb15a035351",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bhluo",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/zygp6nfvi7ef1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=541621dcd1c91b62ac181228183adeb15a035351\"&gt;https://preview.redd.it/zygp6nfvi7ef1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=541621dcd1c91b62ac181228183adeb15a035351&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bhluo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753094800,
            "media_metadata": {
              "zygp6nfvi7ef1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 60,
                    "x": 108,
                    "u": "https://preview.redd.it/zygp6nfvi7ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf3df163f99ba738c77913a2ea98eb71546083f5"
                  },
                  {
                    "y": 121,
                    "x": 216,
                    "u": "https://preview.redd.it/zygp6nfvi7ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=604cc4d81fd79d2a368de88151ca63b8d8a4cfa8"
                  },
                  {
                    "y": 180,
                    "x": 320,
                    "u": "https://preview.redd.it/zygp6nfvi7ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96e932260543797a7e9287e33e60606ca1ce20a1"
                  },
                  {
                    "y": 360,
                    "x": 640,
                    "u": "https://preview.redd.it/zygp6nfvi7ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1781a83910ec7cad78f7780afe57677a8e32ed50"
                  },
                  {
                    "y": 540,
                    "x": 960,
                    "u": "https://preview.redd.it/zygp6nfvi7ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bbfcc0b3d7549e83daec1d5fcc1c569829eaeb1"
                  },
                  {
                    "y": 607,
                    "x": 1080,
                    "u": "https://preview.redd.it/zygp6nfvi7ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a00791863ecd708df5c2d8f0aa3eddb26025135a"
                  }
                ],
                "s": {
                  "y": 864,
                  "x": 1536,
                  "u": "https://preview.redd.it/zygp6nfvi7ef1.png?width=1536&amp;format=png&amp;auto=webp&amp;s=541621dcd1c91b62ac181228183adeb15a035351"
                },
                "id": "zygp6nfvi7ef1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4bldtl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Thellton",
            "can_mod_post": false,
            "created_utc": 1753096580,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 3,
            "author_fullname": "t2_gzzli",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "that link also makes mention of an announcement for an RK3668 SoC. \n\n&gt;CPU – 4x Cortex-A730 + 6x Cortex-A530 Armv9.3 cores delivering around 200K DMIPS; note: neither core has been announced by Arm yet\n\n&gt;GPU – Arm Magni GPU delivering up to 1-1.5 TFLOPS of performance\n\n&gt;***AI accelerator – 16 TOPS RKNN-P3 NPU***\n\n&gt;VPU – 8K 60 FPS video decoder\n\n&gt;ISP – AI-enhanced ISP supporting up to 8K @ 30 FPS\n\n&gt;Memory – LPDDR5/5x/6 up to 100 GB/s\n\n&gt;Storage – UFS 4.0\n\n&gt;Video Output – HDMI 2.1 up to 8K 60 FPS, MIPI DSI\n\n&gt;Peripherals interfaces – PCIe, UCIe\n\n&gt;Manufacturing Process- 5~6nm         \n\nwhich is much more interesting as that'll likely support up to 48GB of RAM going by its predecessor (the RK3588), which supports 32GB of RAM. would definitely make for a way better base for a mobile inferencing device.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bldtl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;that link also makes mention of an announcement for an RK3668 SoC. &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;CPU – 4x Cortex-A730 + 6x Cortex-A530 Armv9.3 cores delivering around 200K DMIPS; note: neither core has been announced by Arm yet&lt;/p&gt;\n\n&lt;p&gt;GPU – Arm Magni GPU delivering up to 1-1.5 TFLOPS of performance&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;AI accelerator – 16 TOPS RKNN-P3 NPU&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;VPU – 8K 60 FPS video decoder&lt;/p&gt;\n\n&lt;p&gt;ISP – AI-enhanced ISP supporting up to 8K @ 30 FPS&lt;/p&gt;\n\n&lt;p&gt;Memory – LPDDR5/5x/6 up to 100 GB/s&lt;/p&gt;\n\n&lt;p&gt;Storage – UFS 4.0&lt;/p&gt;\n\n&lt;p&gt;Video Output – HDMI 2.1 up to 8K 60 FPS, MIPI DSI&lt;/p&gt;\n\n&lt;p&gt;Peripherals interfaces – PCIe, UCIe&lt;/p&gt;\n\n&lt;p&gt;Manufacturing Process- 5~6nm         &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;which is much more interesting as that&amp;#39;ll likely support up to 48GB of RAM going by its predecessor (the RK3588), which supports 32GB of RAM. would definitely make for a way better base for a mobile inferencing device.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bldtl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753096580,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4bnq5a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Roubbes",
            "can_mod_post": false,
            "created_utc": 1753097631,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 2,
            "author_fullname": "t2_aoir7erh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Power consumption?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bnq5a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Power consumption?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bnq5a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753097631,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4bq6wz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SkyFeistyLlama8",
            "can_mod_post": false,
            "created_utc": 1753098684,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 1,
            "author_fullname": "t2_1hgbaqgbnq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I hope this is a wakeup call for Qualcomm. The problem is that Qualcomm's developer tooling is a pain to deal with and the Hexagon Tensor Processor (the internal name for the NPU) can't be used with GGUF models, not without Qualcomm developers coming in. They actually did that with the Adreno GPU OpenCL backend and it's a nice low-power option for users running Snapdragon X laptops.\n\nAI at the edge doesn't need kilowatt GPUs, it needs NPUs running at 5W or 10W on smaller models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bq6wz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I hope this is a wakeup call for Qualcomm. The problem is that Qualcomm&amp;#39;s developer tooling is a pain to deal with and the Hexagon Tensor Processor (the internal name for the NPU) can&amp;#39;t be used with GGUF models, not without Qualcomm developers coming in. They actually did that with the Adreno GPU OpenCL backend and it&amp;#39;s a nice low-power option for users running Snapdragon X laptops.&lt;/p&gt;\n\n&lt;p&gt;AI at the edge doesn&amp;#39;t need kilowatt GPUs, it needs NPUs running at 5W or 10W on smaller models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bq6wz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753098684,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4bvy1m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bene_42069",
            "can_mod_post": false,
            "created_utc": 1753100940,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 1,
            "author_fullname": "t2_9yo3ah1u",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The same Rockchip that powers my dollar store android tv box?\n\nhttps://preview.redd.it/yjcprts418ef1.png?width=764&amp;format=png&amp;auto=webp&amp;s=410601eb65032ef34c222116433ace0e4cfc985c",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bvy1m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The same Rockchip that powers my dollar store android tv box?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yjcprts418ef1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=410601eb65032ef34c222116433ace0e4cfc985c\"&gt;https://preview.redd.it/yjcprts418ef1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=410601eb65032ef34c222116433ace0e4cfc985c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bvy1m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753100940,
            "media_metadata": {
              "yjcprts418ef1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 33,
                    "x": 108,
                    "u": "https://preview.redd.it/yjcprts418ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=274f7eadd66862adb5301138aa45e74d2b9cbd7a"
                  },
                  {
                    "y": 67,
                    "x": 216,
                    "u": "https://preview.redd.it/yjcprts418ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d5eba47a92aeea128070c3b38e9ff52b05a77165"
                  },
                  {
                    "y": 99,
                    "x": 320,
                    "u": "https://preview.redd.it/yjcprts418ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd2c28fdcfdd4787f8338fb67dbdf7dc591c5e1e"
                  },
                  {
                    "y": 198,
                    "x": 640,
                    "u": "https://preview.redd.it/yjcprts418ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8edd42ef0ebbfcc5d1794b6929978c220fb099e8"
                  }
                ],
                "s": {
                  "y": 237,
                  "x": 764,
                  "u": "https://preview.redd.it/yjcprts418ef1.png?width=764&amp;format=png&amp;auto=webp&amp;s=410601eb65032ef34c222116433ace0e4cfc985c"
                },
                "id": "yjcprts418ef1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4bqgeg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "oxygen_addiction",
                      "can_mod_post": false,
                      "created_utc": 1753098789,
                      "send_replies": true,
                      "parent_id": "t1_n4bjscn",
                      "score": 1,
                      "author_fullname": "t2_66k6x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Probably doesn't have enough memory for it?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4bqgeg",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Probably doesn&amp;#39;t have enough memory for it?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m5fmlp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bqgeg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753098789,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4bjscn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Vas1le",
            "can_mod_post": false,
            "created_utc": 1753095836,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 1,
            "author_fullname": "t2_9mhfdr5f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Wonder why qwen 3 wasn't in the benchmark. \n\nDoesn't rock already have a NPU for LLM?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bjscn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wonder why qwen 3 wasn&amp;#39;t in the benchmark. &lt;/p&gt;\n\n&lt;p&gt;Doesn&amp;#39;t rock already have a NPU for LLM?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bjscn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753095836,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4bqt4w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1753098934,
            "send_replies": true,
            "parent_id": "t3_1m5fmlp",
            "score": 0,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "4060 but more energy efficient. Great.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4bqt4w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;4060 but more energy efficient. Great.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/n4bqt4w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753098934,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m5fmlp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]