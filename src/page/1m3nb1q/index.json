[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Long story short I've got a system with 16GB RAM and a 6750XT GPU with 12GB VRAM, I'm happy with it for my daily usage but for AI stuff (coding/roleplay using koboldcpp) it's quite limiting. \n\nFor a cheapskate upgrade, do you think it'd be worth it to buy 2 RAM sticks of 16GB for ~40$ each (bringing me to 48GB total) in order to run MOE models like Qwen 30B.A3B / bigger ? Or should I stick with my current setup instead and keep running quantized models like mistrall 24B ?\n\nIdeally I just want to avoid buying a new GPU while also being able to use better models and have bigger context. I'm quite a noob and I don't know what I should really do, so any help/suggestion is more than welcomed.\n\nThanks in advance :)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Is it worth getting 48GB of RAM alongside my 12GB VRAM GPU ? (cheapskate upgrade)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m3nb1q",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ux1pavfwr",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1752900827,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752899555,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I&amp;#39;ve got a system with 16GB RAM and a 6750XT GPU with 12GB VRAM, I&amp;#39;m happy with it for my daily usage but for AI stuff (coding/roleplay using koboldcpp) it&amp;#39;s quite limiting. &lt;/p&gt;\n\n&lt;p&gt;For a cheapskate upgrade, do you think it&amp;#39;d be worth it to buy 2 RAM sticks of 16GB for ~40$ each (bringing me to 48GB total) in order to run MOE models like Qwen 30B.A3B / bigger ? Or should I stick with my current setup instead and keep running quantized models like mistrall 24B ?&lt;/p&gt;\n\n&lt;p&gt;Ideally I just want to avoid buying a new GPU while also being able to use better models and have bigger context. I&amp;#39;m quite a noob and I don&amp;#39;t know what I should really do, so any help/suggestion is more than welcomed.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m3nb1q",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "QuackMania",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/",
            "subreddit_subscribers": 501103,
            "created_utc": 1752899555,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n3y032x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kironlau",
            "can_mod_post": false,
            "created_utc": 1752900498,
            "send_replies": true,
            "parent_id": "t3_1m3nb1q",
            "score": 1,
            "author_fullname": "t2_tb0dz2ds",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes, for MOE Qwen-30B-A3B. (buy a pair of lower latency ram, oc to 3600+ mhz for ddr4, if you using a CPU/motherboard doesn't support XMP/ram OC, forget about MOE. Qwen 14B is much faster.)\n\nNot much gain in mistral 24B (any layers offloading to the ram...will get you a huge drop of speed)....  \nI am using 4070 12GB VRAM, I suggest you could use a smaller size of quant of mistral 24b. (offloading the vision part and context to cpu, you could host a Q3 model in VRAM.)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3y032x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, for MOE Qwen-30B-A3B. (buy a pair of lower latency ram, oc to 3600+ mhz for ddr4, if you using a CPU/motherboard doesn&amp;#39;t support XMP/ram OC, forget about MOE. Qwen 14B is much faster.)&lt;/p&gt;\n\n&lt;p&gt;Not much gain in mistral 24B (any layers offloading to the ram...will get you a huge drop of speed)....&lt;br/&gt;\nI am using 4070 12GB VRAM, I suggest you could use a smaller size of quant of mistral 24b. (offloading the vision part and context to cpu, you could host a Q3 model in VRAM.)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3y032x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752900498,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3nb1q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n3y5jxm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ironcodegaming",
            "can_mod_post": false,
            "created_utc": 1752903181,
            "send_replies": true,
            "parent_id": "t3_1m3nb1q",
            "score": 1,
            "author_fullname": "t2_eeaio",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Adding RAM is generally useful. But unless you have a reasonably fast system, offloading to CPU will be a big hit to speed.\n\nIf possible, and if it can be installed in your PC, buy a cheap 8GB Card!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3y5jxm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Adding RAM is generally useful. But unless you have a reasonably fast system, offloading to CPU will be a big hit to speed.&lt;/p&gt;\n\n&lt;p&gt;If possible, and if it can be installed in your PC, buy a cheap 8GB Card!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3y5jxm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752903181,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3nb1q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n3ybhmv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArchdukeofHyperbole",
            "can_mod_post": false,
            "created_utc": 1752906294,
            "send_replies": true,
            "parent_id": "t3_1m3nb1q",
            "score": 1,
            "author_fullname": "t2_1p41v97q5d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can already run qwen 3AB. Q4 is 16GB or so file size. offload like 11.5 to vram and do the rest on regular. But I'd say upgrade it anyway if you can, and go for 64GB ram 😏",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3ybhmv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can already run qwen 3AB. Q4 is 16GB or so file size. offload like 11.5 to vram and do the rest on regular. But I&amp;#39;d say upgrade it anyway if you can, and go for 64GB ram 😏&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3ybhmv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752906294,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3nb1q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]