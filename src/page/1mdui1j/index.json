[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hello fellow Redditors, i need to ask for your wisdom.\n\nI got the request from my Boss to implement a localy running AI System to analyze our company data and let our employes check for information, create knowleagebases and create nice sounding emails.\n\nSo now to my \"Problem\", i got a budget of about 5000€ to build a machine capable of running quite fast in respoding to the User and also at least 2-3 requests of users simultaniously. (If possible). I got a Testsystem with 2x 12GB RTX A2000 cards (64GB Ram and I7 14700k that can run single responses with 30b LLMs with about 8-9Tokens 24b with about 13 and 7B Models with 18 Token.\n\nEdit: It would be fine to run just 30-70b models if possible.\n\nI run Ollama with Open Webui.\n\nAs 5000€ is not a very high budget and everything needs to be bought New i cant just buy used 3090s.\n\nI though about getting something like a midrange for CPU with 128gb Ram, DDR5 Mainboard with Dual GPU slot but i dont know about the GPU's. \n\nWhat would be the best combo for that money?\n\nCloud is not an option as the data is very sensitive.\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Help for new LLM Rig",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdui1j",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_yxgzw",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753944900,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753944455,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow Redditors, i need to ask for your wisdom.&lt;/p&gt;\n\n&lt;p&gt;I got the request from my Boss to implement a localy running AI System to analyze our company data and let our employes check for information, create knowleagebases and create nice sounding emails.&lt;/p&gt;\n\n&lt;p&gt;So now to my &amp;quot;Problem&amp;quot;, i got a budget of about 5000€ to build a machine capable of running quite fast in respoding to the User and also at least 2-3 requests of users simultaniously. (If possible). I got a Testsystem with 2x 12GB RTX A2000 cards (64GB Ram and I7 14700k that can run single responses with 30b LLMs with about 8-9Tokens 24b with about 13 and 7B Models with 18 Token.&lt;/p&gt;\n\n&lt;p&gt;Edit: It would be fine to run just 30-70b models if possible.&lt;/p&gt;\n\n&lt;p&gt;I run Ollama with Open Webui.&lt;/p&gt;\n\n&lt;p&gt;As 5000€ is not a very high budget and everything needs to be bought New i cant just buy used 3090s.&lt;/p&gt;\n\n&lt;p&gt;I though about getting something like a midrange for CPU with 128gb Ram, DDR5 Mainboard with Dual GPU slot but i dont know about the GPU&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;What would be the best combo for that money?&lt;/p&gt;\n\n&lt;p&gt;Cloud is not an option as the data is very sensitive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdui1j",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "juli199696",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/",
            "subreddit_subscribers": 507575,
            "created_utc": 1753944455,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n64ykpp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "zipperlein",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n64nb4j",
                                "score": 1,
                                "author_fullname": "t2_x3duw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I think so. Ollama has support for AMD for over a year now. U can for example check their open issues, if there are any problems currently.  \n[https://github.com/ollama/ollama/issues?q=is%3Aissue%20state%3Aopen%20rtx%207900](https://github.com/ollama/ollama/issues?q=is%3Aissue%20state%3Aopen%20rtx%207900)\n\nBut if u want maximum throuput I'd use vllm. Ollama is good for trying things out or if you are using multiple models dynamically.  \nOpen-webui can work with any openai-compatible inference server.\n\nNvidia is the way to go because of CUDA. Some projects do not support AMD but for inference it is an option to consider.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n64ykpp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think so. Ollama has support for AMD for over a year now. U can for example check their open issues, if there are any problems currently.&lt;br/&gt;\n&lt;a href=\"https://github.com/ollama/ollama/issues?q=is%3Aissue%20state%3Aopen%20rtx%207900\"&gt;https://github.com/ollama/ollama/issues?q=is%3Aissue%20state%3Aopen%20rtx%207900&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But if u want maximum throuput I&amp;#39;d use vllm. Ollama is good for trying things out or if you are using multiple models dynamically.&lt;br/&gt;\nOpen-webui can work with any openai-compatible inference server.&lt;/p&gt;\n\n&lt;p&gt;Nvidia is the way to go because of CUDA. Some projects do not support AMD but for inference it is an option to consider.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdui1j",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/n64ykpp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753955815,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753955815,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n64nb4j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "juli199696",
                      "can_mod_post": false,
                      "created_utc": 1753949429,
                      "send_replies": true,
                      "parent_id": "t1_n64isxl",
                      "score": 1,
                      "author_fullname": "t2_yxgzw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is AMD Support for Ollama working good now? or more like in general? The Testsystem cant be used forever as its just borrowed from our IT Supply we have here. In the past Nvidia was the thing to go for AI usage in general. 96GB VRAM would be awesome. But i need to make sure that it works with ollama.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64nb4j",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is AMD Support for Ollama working good now? or more like in general? The Testsystem cant be used forever as its just borrowed from our IT Supply we have here. In the past Nvidia was the thing to go for AI usage in general. 96GB VRAM would be awesome. But i need to make sure that it works with ollama.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdui1j",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/n64nb4j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753949429,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64isxl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zipperlein",
            "can_mod_post": false,
            "created_utc": 1753946877,
            "send_replies": true,
            "parent_id": "t3_1mdui1j",
            "score": 1,
            "author_fullname": "t2_x3duw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What about a setup with 2/4 7900xtx, u can get them \"new\" for relative good price compared to nvidia cards. It would give u 48/96GB of VRAM, thtats plenty for 30b models. No need for mixed compute. Maybe u can even adapt your test system, saves money for GPUs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64isxl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What about a setup with 2/4 7900xtx, u can get them &amp;quot;new&amp;quot; for relative good price compared to nvidia cards. It would give u 48/96GB of VRAM, thtats plenty for 30b models. No need for mixed compute. Maybe u can even adapt your test system, saves money for GPUs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/n64isxl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753946877,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdui1j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n64or6j",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Toooooool",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n64ngsm",
                                "score": 1,
                                "author_fullname": "t2_8llornh4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I've been told they can run thanks to Vulkan support but I've also been told you need to use a different fork of llama to run it efficiently.\n\nThe B60 is also 2x24GB rather than a true 48GB, so, eh.. ¯\\\\\\_(ツ)\\_/¯",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n64or6j",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been told they can run thanks to Vulkan support but I&amp;#39;ve also been told you need to use a different fork of llama to run it efficiently.&lt;/p&gt;\n\n&lt;p&gt;The B60 is also 2x24GB rather than a true 48GB, so, eh.. ¯\\_(ツ)_/¯&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdui1j",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/n64or6j/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753950259,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753950259,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n64ngsm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "juli199696",
                      "can_mod_post": false,
                      "created_utc": 1753949521,
                      "send_replies": true,
                      "parent_id": "t1_n64n0ng",
                      "score": 1,
                      "author_fullname": "t2_yxgzw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I dont know about Intel actually. Is there even support for Intel Cards for llama? The amd card could be an option maybe. I will take a look into that.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64ngsm",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I dont know about Intel actually. Is there even support for Intel Cards for llama? The amd card could be an option maybe. I will take a look into that.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdui1j",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/n64ngsm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753949521,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64n0ng",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Toooooool",
            "can_mod_post": false,
            "created_utc": 1753949264,
            "send_replies": true,
            "parent_id": "t3_1mdui1j",
            "score": 2,
            "author_fullname": "t2_8llornh4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "As much as it's stretching the line of \"bought new\" I'd suggest considering a modded 48GB 4090, they go for around $3k and is likely your best option to run 70b models with a \"bought new\" card.\n\nAlternatively get a 32GB 5090 for $3k and run 30b models.\n\nSecond alternative is to wait 1-2 more months for the Intel B60 dual-GPU 48GB or the AMD Radeon PRO R9700 32GB to be publicly released, they're both rumored to cost around $1k.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64n0ng",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As much as it&amp;#39;s stretching the line of &amp;quot;bought new&amp;quot; I&amp;#39;d suggest considering a modded 48GB 4090, they go for around $3k and is likely your best option to run 70b models with a &amp;quot;bought new&amp;quot; card.&lt;/p&gt;\n\n&lt;p&gt;Alternatively get a 32GB 5090 for $3k and run 30b models.&lt;/p&gt;\n\n&lt;p&gt;Second alternative is to wait 1-2 more months for the Intel B60 dual-GPU 48GB or the AMD Radeon PRO R9700 32GB to be publicly released, they&amp;#39;re both rumored to cost around $1k.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/n64n0ng/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753949264,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdui1j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n658ihg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1753960642,
            "send_replies": true,
            "parent_id": "t3_1mdui1j",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "you need as much VRAM as possible, RAM is not that important. Check RTX Pro 5000 48GB, it might be within your budget. The absolute minimum you'd want is 5090 32GB",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n658ihg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you need as much VRAM as possible, RAM is not that important. Check RTX Pro 5000 48GB, it might be within your budget. The absolute minimum you&amp;#39;d want is 5090 32GB&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdui1j/help_for_new_llm_rig/n658ihg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753960642,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdui1j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]