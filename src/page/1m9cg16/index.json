[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Was thinking how to scale a GPU cluster. Not talking about CPUs here.  \nUsually have heard that \"buy Epyc\" and add 6-8 GPUs in it. but thats it then, it wont scale more.  \nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   \n  \nHas anyone done it? \n\nI happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.   \nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.   \nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?  \nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. \n\nLater when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. \n\nSo with this kind of setup, even 100 gpus could server the same model? \n\n\"**RDMA over Converged Ethernet (RoCE):** The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.\"",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Multi GPU multi server inference",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9cg16",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1jk2ep8a52",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753481338,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was thinking how to scale a GPU cluster. Not talking about CPUs here.&lt;br/&gt;\nUsually have heard that &amp;quot;buy Epyc&amp;quot; and add 6-8 GPUs in it. but thats it then, it wont scale more.&lt;br/&gt;\nBut now that I have learned how to use vLLM, and it can utilize multi GPU and also multi server GPUs, was thinking what if creating a cluster with fast networking and vLLM RAY?   &lt;/p&gt;\n\n&lt;p&gt;Has anyone done it? &lt;/p&gt;\n\n&lt;p&gt;I happen to have spare Mellanox Connect-x6 cards, 2x25GB with ROCE, some 25gb and 100gb switches.&lt;br/&gt;\nI do not have any Epycs, but loads of AM5 boards and 7000 cpus and memory.&lt;br/&gt;\nSo my understanding is, if creating multiple servers, with 1-2 GPUs in each 8x or 16x pcie 4.0 connected, and then creating a NFS file server for model sharing and connecting all them with 2x25GB DAC, I guess it would work?&lt;br/&gt;\nThat 5GB/s connection will be in tensor parallel a bottleneck but how much? Some say even 4x pcie 4.0 is not a bottleneck in vLLM tensor parallel and its about 8GB/s. &lt;/p&gt;\n\n&lt;p&gt;Later when pcie 5.0 4x network cards are available it could be upgraded to 100GB networking. &lt;/p&gt;\n\n&lt;p&gt;So with this kind of setup, even 100 gpus could server the same model? &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&lt;strong&gt;RDMA over Converged Ethernet (RoCE):&lt;/strong&gt; The ConnectX-6 cards are designed for RoCE. This is a critical advantage. RoCE allows Remote Direct Memory Access, meaning data can be transferred directly between the GPU memories on different servers, bypassing the CPU.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m9cg16",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Rich_Artist_8327",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/",
            "subreddit_subscribers": 504486,
            "created_utc": 1753481338,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n56et3a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1753486357,
            "send_replies": true,
            "parent_id": "t3_1m9cg16",
            "score": 1,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I only saw people doing it with RPC on llama.cpp. Then again, that's not tensor parallel. \n\nIt sounds like it would work but consume a ton of electricity. Do you have the power supplies,etc? And of course having to get the GPUs as well.\n\nedit: Does VLLM tp work over nodes? I thought it did PP for that.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n56et3a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I only saw people doing it with RPC on llama.cpp. Then again, that&amp;#39;s not tensor parallel. &lt;/p&gt;\n\n&lt;p&gt;It sounds like it would work but consume a ton of electricity. Do you have the power supplies,etc? And of course having to get the GPUs as well.&lt;/p&gt;\n\n&lt;p&gt;edit: Does VLLM tp work over nodes? I thought it did PP for that.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/n56et3a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753486357,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9cg16",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n56od5p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Normal-Ad-7114",
            "can_mod_post": false,
            "created_utc": 1753489705,
            "send_replies": true,
            "parent_id": "t3_1m9cg16",
            "score": 1,
            "author_fullname": "t2_8fu8sqhz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If it doesn't cost you any money, try to connect several (2-5) servers and set everything up, and see for yourself how's the performance, and then tell us :)\n\n\nI recall people here connected several raspberry pis and ran inference, IIRC it was working fine (I mean the scaling; obviously they were still too slow for any practical usage), so your experience might be very interesting for everyone\n\n\nI also wondered what could be done with several regular desktops/laptops (i.e. 2.5gb or even 1gb ethernet) - could it be of any use in \"multiplying\" RAM/VRAM, but never got around to actually testing it",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n56od5p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If it doesn&amp;#39;t cost you any money, try to connect several (2-5) servers and set everything up, and see for yourself how&amp;#39;s the performance, and then tell us :)&lt;/p&gt;\n\n&lt;p&gt;I recall people here connected several raspberry pis and ran inference, IIRC it was working fine (I mean the scaling; obviously they were still too slow for any practical usage), so your experience might be very interesting for everyone&lt;/p&gt;\n\n&lt;p&gt;I also wondered what could be done with several regular desktops/laptops (i.e. 2.5gb or even 1gb ethernet) - could it be of any use in &amp;quot;multiplying&amp;quot; RAM/VRAM, but never got around to actually testing it&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9cg16/multi_gpu_multi_server_inference/n56od5p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753489705,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9cg16",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]