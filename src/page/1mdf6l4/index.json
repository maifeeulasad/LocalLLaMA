[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "4 5090 or rtx pro 6000, what's your take?\n5090 have a tad bit lower $/gig, you get 128gb instead of 96 and should have some good speeds with \"tp\".\nIf density isn't an issue, what's your take?\nFor inference and for training\n\n[View Poll](https://www.reddit.com/poll/1mdf6l4)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "4 5090 or rtx pro 6000?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdf6l4",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": "#bbbdbf",
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "is_original_content": false,
            "author_fullname": "t2_cj9kap4bx",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753901855,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;4 5090 or rtx pro 6000, what&amp;#39;s your take?\n5090 have a tad bit lower $/gig, you get 128gb instead of 96 and should have some good speeds with &amp;quot;tp&amp;quot;.\nIf density isn&amp;#39;t an issue, what&amp;#39;s your take?\nFor inference and for training&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1mdf6l4\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdf6l4",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "No_Afternoon_4260",
            "discussion_type": null,
            "num_comments": 22,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "poll_data": {
              "prediction_status": null,
              "tournament_id": null,
              "voting_end_timestamp": 1754247455536,
              "total_vote_count": 114,
              "vote_updates_remained": null,
              "is_prediction": false,
              "resolved_option_id": null,
              "user_won_amount": null,
              "user_selection": null,
              "options": [
                {
                  "text": "4 * 5090",
                  "id": "31252888"
                },
                {
                  "text": "rtx pro 6000",
                  "id": "31252889"
                },
                {
                  "text": "just give me 8 3090 at this price",
                  "id": "31252890"
                },
                {
                  "text": "just give me some cloud credit",
                  "id": "31252891"
                }
              ],
              "total_stake_amount": null
            },
            "author_patreon_flair": false,
            "author_flair_text_color": "light",
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/",
            "subreddit_subscribers": 507275,
            "created_utc": 1753901855,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n61kynw",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "No_Afternoon_4260",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n61kcdq",
                                          "score": 1,
                                          "author_fullname": "t2_cj9kap4bx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Ho I see you are completely right! Thx I'll see if I can change that..\n\n\nI didn't know salad, thx\n\n&gt;you mention \"i hate gpu instances idling in the cloud it stresses me,\"\n\nSometimes renting big gpu instances to test agent in real conditions",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n61kynw",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ho I see you are completely right! Thx I&amp;#39;ll see if I can change that..&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t know salad, thx&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;you mention &amp;quot;i hate gpu instances idling in the cloud it stresses me,&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Sometimes renting big gpu instances to test agent in real conditions&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdf6l4",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61kynw/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753907196,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753907196,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n61l6dt",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "No_Afternoon_4260",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n61kcdq",
                                          "score": 1,
                                          "author_fullname": "t2_cj9kap4bx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Too bad I cannot modify the poll..",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n61l6dt",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Too bad I cannot modify the poll..&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdf6l4",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61l6dt/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753907254,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753907254,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n61kcdq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "offlinesir",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n61h6o9",
                                "score": 1,
                                "author_fullname": "t2_jn5ft2le",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "While you can see the results of the survey, people who are voting cannot see the results until they vote. So when somebody wants to see the results of the survey, but they don't want to actually vote, they will often just click a random choice, which, of course, messes up the data. You may be thinking this isn't actually too important, but I've seen polls on reddit in which the \"results\" option is actually the most selected as most people don't know the answer.\n\nAs for your other point, you mention \"i hate gpu instances idling in the cloud it stresses me,\" but for services like openrouter, you are only charged per request. But, if you are going to use this for 3 people at once, I do understand how a physical solution might be best. If you are ever not using the GPU's, you could rent them out to sites like [salad.io](http://salad.io) if needed for some extra cashback (I'm not trying to endorse salad here).",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n61kcdq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While you can see the results of the survey, people who are voting cannot see the results until they vote. So when somebody wants to see the results of the survey, but they don&amp;#39;t want to actually vote, they will often just click a random choice, which, of course, messes up the data. You may be thinking this isn&amp;#39;t actually too important, but I&amp;#39;ve seen polls on reddit in which the &amp;quot;results&amp;quot; option is actually the most selected as most people don&amp;#39;t know the answer.&lt;/p&gt;\n\n&lt;p&gt;As for your other point, you mention &amp;quot;i hate gpu instances idling in the cloud it stresses me,&amp;quot; but for services like openrouter, you are only charged per request. But, if you are going to use this for 3 people at once, I do understand how a physical solution might be best. If you are ever not using the GPU&amp;#39;s, you could rent them out to sites like &lt;a href=\"http://salad.io\"&gt;salad.io&lt;/a&gt; if needed for some extra cashback (I&amp;#39;m not trying to endorse salad here).&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdf6l4",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61kcdq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753907028,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753907028,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n61h6o9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Afternoon_4260",
                      "can_mod_post": false,
                      "created_utc": 1753906150,
                      "send_replies": true,
                      "parent_id": "t1_n6152v7",
                      "score": 1,
                      "author_fullname": "t2_cj9kap4bx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;every reddit survey is basically invalid if you don't include a \"results\" option!\n\nSorry care to elaborate?\n\nI'm using openrouter a lot, watching my 3090s idling and throwing 20 bucks every couple of days, at the end of the year, not sure some rtx pro will depreciate that much.. + i hate gpu instances idling in the cloud it stresses me.\nWe are 3 in the office. At some point we need custom models and other things that apis can't give us, or it's just not the route we are taking.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n61h6o9",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;every reddit survey is basically invalid if you don&amp;#39;t include a &amp;quot;results&amp;quot; option!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Sorry care to elaborate?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using openrouter a lot, watching my 3090s idling and throwing 20 bucks every couple of days, at the end of the year, not sure some rtx pro will depreciate that much.. + i hate gpu instances idling in the cloud it stresses me.\nWe are 3 in the office. At some point we need custom models and other things that apis can&amp;#39;t give us, or it&amp;#39;s just not the route we are taking.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdf6l4",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61h6o9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753906150,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6152v7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "offlinesir",
            "can_mod_post": false,
            "created_utc": 1753902674,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 7,
            "author_fullname": "t2_jn5ft2le",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "every reddit survey is basically invalid if you don't include a \"results\" option!\n\nAnyways, I would recomend you go on openrouter, either the free or paid models, and test out the models that you would actually be running locally. make sure they are able to complete what tasks you wish to complete. Also, I'm completely aware this is r/LocalLLaMA, but be considerate on how much money that is in cloud credit on a site like openrouter, on hardware that will depreciate.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6152v7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;every reddit survey is basically invalid if you don&amp;#39;t include a &amp;quot;results&amp;quot; option!&lt;/p&gt;\n\n&lt;p&gt;Anyways, I would recomend you go on openrouter, either the free or paid models, and test out the models that you would actually be running locally. make sure they are able to complete what tasks you wish to complete. Also, I&amp;#39;m completely aware this is &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, but be considerate on how much money that is in cloud credit on a site like openrouter, on hardware that will depreciate.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n6152v7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753902674,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n613cgd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "mayo551",
            "can_mod_post": false,
            "created_utc": 1753902170,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 4,
            "author_fullname": "t2_vsz5kd9o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm sorry but this question depends entirely on your setup.\n\nIf you have:\n\nA) The cooling (for the room)\nB) The electrcity/circuits\nC) The PCI lanes\nD) The hardware setup needed to run 8 3090\n\nThen go with the 8 3090.\n\nSame question for the 4x5090....\n\nThe pro 6000 only makes sense if you need the density, the power savings and are using hardware that can't support other setups.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n613cgd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sorry but this question depends entirely on your setup.&lt;/p&gt;\n\n&lt;p&gt;If you have:&lt;/p&gt;\n\n&lt;p&gt;A) The cooling (for the room)\nB) The electrcity/circuits\nC) The PCI lanes\nD) The hardware setup needed to run 8 3090&lt;/p&gt;\n\n&lt;p&gt;Then go with the 8 3090.&lt;/p&gt;\n\n&lt;p&gt;Same question for the 4x5090....&lt;/p&gt;\n\n&lt;p&gt;The pro 6000 only makes sense if you need the density, the power savings and are using hardware that can&amp;#39;t support other setups.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n613cgd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753902170,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62lomx",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "No_Afternoon_4260",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62l4np",
                                          "score": 1,
                                          "author_fullname": "t2_cj9kap4bx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Interesting.\nHo this TP I see haha that will be really cool for video \nthx for taking the time to answer",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62lomx",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting.\nHo this TP I see haha that will be really cool for video \nthx for taking the time to answer&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdf6l4",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62lomx/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753918325,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753918325,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n62l4np",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kmouratidis",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n61ht0o",
                                "score": 1,
                                "author_fullname": "t2_k6u7rfxb",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "&gt; With similar backends?\n\nThe post doesn't say which backend was used, but the testing script was from vLLM and the post mentioned GPTQ for the INT4 model, so it likely means vLLM is the backend. I'm using sglang, so it should be in the same ballpark.\n\n&gt; What do you call TP in the image/video realm? I guess it also applies to future world models\n\nIt's still called the same, I just don't think it's commonly implemented. For example, a few months ago I remember trying to get Hunyuan Video to work ([instructions in the readme say they support TP](https://github.com/Tencent-Hunyuan/HunyuanVideo?tab=readme-ov-file#-parallel-inference-on-multiple-gpus-by-xdit)) but I couldn't and eventually gave up. It's not a simple `-tp` flag like in most LLM frameworks.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62l4np",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;With similar backends?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The post doesn&amp;#39;t say which backend was used, but the testing script was from vLLM and the post mentioned GPTQ for the INT4 model, so it likely means vLLM is the backend. I&amp;#39;m using sglang, so it should be in the same ballpark.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;What do you call TP in the image/video realm? I guess it also applies to future world models&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s still called the same, I just don&amp;#39;t think it&amp;#39;s commonly implemented. For example, a few months ago I remember trying to get Hunyuan Video to work (&lt;a href=\"https://github.com/Tencent-Hunyuan/HunyuanVideo?tab=readme-ov-file#-parallel-inference-on-multiple-gpus-by-xdit\"&gt;instructions in the readme say they support TP&lt;/a&gt;) but I couldn&amp;#39;t and eventually gave up. It&amp;#39;s not a simple &lt;code&gt;-tp&lt;/code&gt; flag like in most LLM frameworks.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdf6l4",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62l4np/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753918146,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753918146,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n61ht0o",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Afternoon_4260",
                      "can_mod_post": false,
                      "created_utc": 1753906330,
                      "send_replies": true,
                      "parent_id": "t1_n615toi",
                      "score": 1,
                      "author_fullname": "t2_cj9kap4bx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; [This fella with dual Pro 6000](https://www.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/) gets similar output throughput on Qwen3-30B-A3B as my 4x3090s.\n\nWith similar backends?\n\n&gt;Image/video stuff: 6000. TP with image/video isn't as advanced as with LLMs.\n\nWhat do you call TP in the image/video realm? I guess it also applies to future world models",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n61ht0o",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/\"&gt;This fella with dual Pro 6000&lt;/a&gt; gets similar output throughput on Qwen3-30B-A3B as my 4x3090s.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;With similar backends?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Image/video stuff: 6000. TP with image/video isn&amp;#39;t as advanced as with LLMs.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;What do you call TP in the image/video realm? I guess it also applies to future world models&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdf6l4",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61ht0o/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753906330,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n615toi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kmouratidis",
            "can_mod_post": false,
            "created_utc": 1753902888,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 2,
            "author_fullname": "t2_k6u7rfxb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\"for training\" depends on what you want to train. 1B models? The 3090s are fine. 20B models? Maybe they're still fine for QLoRA and low context. FP8 training of 30B model? likely the 6000. I cannot imagine a training scenario where 4x5090 would be preferable to both 8x3090 *and* the pro 6000, it's by literally the middle ground between the two extremes.\n\nFor inference: more VRAM -&gt; more options. Newer GPU -&gt; better support going forward (still slightly spotty support now). Depending on the model, you might even get better speeds with the 3090s, e.g. if your model entirely doesn't fit the 5090s and you end up running llama.cpp, vs fitting well in the 3090s and you get to run on vllm/sglang. [This fella with dual Pro 6000](https://www.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/) gets similar output throughput on Qwen3-30B-A3B as my 4x3090s.\n\nImage/video stuff: 6000. TP with image/video isn't as advanced as with LLMs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n615toi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;for training&amp;quot; depends on what you want to train. 1B models? The 3090s are fine. 20B models? Maybe they&amp;#39;re still fine for QLoRA and low context. FP8 training of 30B model? likely the 6000. I cannot imagine a training scenario where 4x5090 would be preferable to both 8x3090 &lt;em&gt;and&lt;/em&gt; the pro 6000, it&amp;#39;s by literally the middle ground between the two extremes.&lt;/p&gt;\n\n&lt;p&gt;For inference: more VRAM -&amp;gt; more options. Newer GPU -&amp;gt; better support going forward (still slightly spotty support now). Depending on the model, you might even get better speeds with the 3090s, e.g. if your model entirely doesn&amp;#39;t fit the 5090s and you end up running llama.cpp, vs fitting well in the 3090s and you get to run on vllm/sglang. &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/\"&gt;This fella with dual Pro 6000&lt;/a&gt; gets similar output throughput on Qwen3-30B-A3B as my 4x3090s.&lt;/p&gt;\n\n&lt;p&gt;Image/video stuff: 6000. TP with image/video isn&amp;#39;t as advanced as with LLMs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n615toi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753902888,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61pg1a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Popular_Brief335",
            "can_mod_post": false,
            "created_utc": 1753908433,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 2,
            "author_fullname": "t2_1j9oxxzd6c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The 6000 will save it's cost in power usage alone",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61pg1a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The 6000 will save it&amp;#39;s cost in power usage alone&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61pg1a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753908433,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62ditr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "PermanentLiminality",
            "can_mod_post": false,
            "created_utc": 1753915697,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 2,
            "author_fullname": "t2_19zqycaf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The 6000 is the best choice.  You don't need a giant server system to run it and you can even add a second card in a regular non server system.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62ditr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The 6000 is the best choice.  You don&amp;#39;t need a giant server system to run it and you can even add a second card in a regular non server system.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62ditr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753915697,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n617bm8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1753903322,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 1,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It depends, do you live in the arctic where you need a 3000w space heater next to you? if not, then the 6000. I believe even 8x3090 take less power than 4x5090",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n617bm8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends, do you live in the arctic where you need a 3000w space heater next to you? if not, then the 6000. I believe even 8x3090 take less power than 4x5090&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n617bm8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753903322,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62ucep",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753921240,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Each 5090 is 600 wats so there is no way 4x is on the table lol",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62ucep",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Each 5090 is 600 wats so there is no way 4x is on the table lol&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62ucep/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753921240,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62gyxp",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "No_Afternoon_4260",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n61se5q",
                                          "score": 1,
                                          "author_fullname": "t2_cj9kap4bx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Wow that's a deep localllama rig ",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62gyxp",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow that&amp;#39;s a deep localllama rig &lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdf6l4",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62gyxp/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753916804,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753916804,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n61se5q",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "panchovix",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n61qkgc",
                                "score": 1,
                                "author_fullname": "t2_j1kqr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "[https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance\\_benchmarks\\_on\\_deepseek/](https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n61se5q",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 405B"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdf6l4",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61se5q/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753909246,
                                "author_flair_text": "Llama 405B",
                                "treatment_tags": [],
                                "created_utc": 1753909246,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n61qkgc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Afternoon_4260",
                      "can_mod_post": false,
                      "created_utc": 1753908740,
                      "send_replies": true,
                      "parent_id": "t1_n61pwpd",
                      "score": 1,
                      "author_fullname": "t2_cj9kap4bx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hey I see you have a llama 405B tag. What's your rig, happy with it?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n61qkgc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey I see you have a llama 405B tag. What&amp;#39;s your rig, happy with it?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdf6l4",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61qkgc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753908740,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61pwpd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "panchovix",
            "can_mod_post": false,
            "created_utc": 1753908560,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 1,
            "author_fullname": "t2_j1kqr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Both 8x3090 or 4x5090 would be faster than a single 6000 PRO assuming you have a good use and implementation of multiGPU.\n\nSingle GPU is way more easier to manage tho and also better for other diffusion tasks (like video, wan 2.2 at full fp16 uses like 60-70GB VRAM) and uses LESS way power.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61pwpd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 405B"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Both 8x3090 or 4x5090 would be faster than a single 6000 PRO assuming you have a good use and implementation of multiGPU.&lt;/p&gt;\n\n&lt;p&gt;Single GPU is way more easier to manage tho and also better for other diffusion tasks (like video, wan 2.2 at full fp16 uses like 60-70GB VRAM) and uses LESS way power.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n61pwpd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753908560,
            "author_flair_text": "Llama 405B",
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62i7d3",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "No_Afternoon_4260",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62hr49",
                                          "score": 1,
                                          "author_fullname": "t2_cj9kap4bx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yeah another drawback is quickly becoming bottlenecked by pcie 4.0 x8 as modern stuff runs pcie 5.0 x16.  \nGpu2gpu and model loading (with the correct storage) impacted. Too bad for an agent system that needs to swap big weights etc..",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62i7d3",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah another drawback is quickly becoming bottlenecked by pcie 4.0 x8 as modern stuff runs pcie 5.0 x16.&lt;br/&gt;\nGpu2gpu and model loading (with the correct storage) impacted. Too bad for an agent system that needs to swap big weights etc..&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdf6l4",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62i7d3/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753917200,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753917200,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n62hr49",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DepthHour1669",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62hcxy",
                                "score": 1,
                                "author_fullname": "t2_t6glzswk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "8x 3090 is actually the best option if you can find a motherboard and power supply which works for that. Try to connect them together with nvlink, it will be much faster for finetuning (same speed for inference though).",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62hr49",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;8x 3090 is actually the best option if you can find a motherboard and power supply which works for that. Try to connect them together with nvlink, it will be much faster for finetuning (same speed for inference though).&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdf6l4",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62hr49/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753917055,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753917055,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62hcxy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Afternoon_4260",
                      "can_mod_post": false,
                      "created_utc": 1753916929,
                      "send_replies": true,
                      "parent_id": "t1_n62f12i",
                      "score": 0,
                      "author_fullname": "t2_cj9kap4bx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "My thoughts exactly, but are we right?  \nI've added the 8x 3090 as a joke last minute, but I might not be that stupid may be.. Just risking faster depreciation",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62hcxy",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My thoughts exactly, but are we right?&lt;br/&gt;\nI&amp;#39;ve added the 8x 3090 as a joke last minute, but I might not be that stupid may be.. Just risking faster depreciation&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdf6l4",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62hcxy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753916929,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62f12i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DepthHour1669",
            "can_mod_post": false,
            "created_utc": 1753916190,
            "send_replies": true,
            "parent_id": "t3_1mdf6l4",
            "score": 0,
            "author_fullname": "t2_t6glzswk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "4x 5090 for sure. \n\nYou can run vLLM as long as you have 1/2/4/8 of the same GPU. You will want to run vllm with that much power, not llama.cpp. \n\nThen you'll get more vram and more computational power with 4x 5090.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62f12i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;4x 5090 for sure. &lt;/p&gt;\n\n&lt;p&gt;You can run vLLM as long as you have 1/2/4/8 of the same GPU. You will want to run vllm with that much power, not llama.cpp. &lt;/p&gt;\n\n&lt;p&gt;Then you&amp;#39;ll get more vram and more computational power with 4x 5090.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdf6l4/4_5090_or_rtx_pro_6000/n62f12i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753916190,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdf6l4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]