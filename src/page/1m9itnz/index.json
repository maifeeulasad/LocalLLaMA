[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I was told trying to run non-tiny LLM's on a CPU was unusable.  But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090's.  Note, I'm getting barely 48% processing usage on the 5090's and wondering what I can do to improve that.\n\nLlama.cpp thread affinity seems to not do anything on Ubuntu. For my CPU's runs I had to do my own fix for this.  I mainly did this to see how well layer overflowing will work for even larger models.  \nThe problem is the nearly continuous stream of new models to try.  \nWas going with qwen2.5-coder-32b-instruct.  \nThen today I see Qwen3-235B-A22B-Thinking-2507-FP8 and just now [Llama-3\\_3-Nemotron-Super-49B-v1\\_5](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5)   \nToo many choices.\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "My 7985WX, dual 5090's, and 256GB's of DDR5-6000 has landed.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9itnz",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 11,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rxk6hx4t",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 11,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753499460,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was told trying to run non-tiny LLM&amp;#39;s on a CPU was unusable.  But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090&amp;#39;s.  Note, I&amp;#39;m getting barely 48% processing usage on the 5090&amp;#39;s and wondering what I can do to improve that.&lt;/p&gt;\n\n&lt;p&gt;Llama.cpp thread affinity seems to not do anything on Ubuntu. For my CPU&amp;#39;s runs I had to do my own fix for this.  I mainly did this to see how well layer overflowing will work for even larger models.&lt;br/&gt;\nThe problem is the nearly continuous stream of new models to try.&lt;br/&gt;\nWas going with qwen2.5-coder-32b-instruct.&lt;br/&gt;\nThen today I see Qwen3-235B-A22B-Thinking-2507-FP8 and just now &lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;br/&gt;\nToo many choices.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?auto=webp&amp;s=af31b2002f0236c31cf3c91755fd855ed95ae985",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45e7c8c14055c57d8c62dad0b150faa3212ce087",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82619b61b919798034ba0f0b798bd1e75640c0b9",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eaf3accf5409bb25bb8728256d4e2f61e2bbbeec",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8ce793850ca6936254a722184eb2367e6423fa1",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2e1d1bb2008c0cfb9abdbf16638bc668942167e7",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=313d9b1115102aa6244b349a8e99c1ee840c4702",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "Dk44WxUWzwyFVy1Yr9Co1zKqM1MqmFo8Qo97aSXpZNs"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m9itnz",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Guilty-History-9249",
            "discussion_type": null,
            "num_comments": 15,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/",
            "subreddit_subscribers": 505255,
            "created_utc": 1753499460,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5c1ioy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MachinaVerum",
                      "can_mod_post": false,
                      "created_utc": 1753566696,
                      "send_replies": true,
                      "parent_id": "t1_n57yg2g",
                      "score": 1,
                      "author_fullname": "t2_6ybfgv5i",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "that 7985wx is definitely a good call. i'm still kicking myself for building with 7975wx (its half the bandwidth with only 4 ccds)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5c1ioy",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;that 7985wx is definitely a good call. i&amp;#39;m still kicking myself for building with 7975wx (its half the bandwidth with only 4 ccds)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9itnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n5c1ioy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753566696,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n57yg2g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1753509026,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 23,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;I was told trying to run non-tiny LLM's on a CPU was unusable.\n\nDid you tell this person you were going to spend $5,000+ on just CPU and RAM to attain memory bandwidth greater than mid-class GPUs before they told you this?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n57yg2g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I was told trying to run non-tiny LLM&amp;#39;s on a CPU was unusable.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Did you tell this person you were going to spend $5,000+ on just CPU and RAM to attain memory bandwidth greater than mid-class GPUs before they told you this?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n57yg2g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753509026,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 23
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n57fwan",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "koushd",
            "can_mod_post": false,
            "created_utc": 1753500292,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 16,
            "author_fullname": "t2_4yut6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "use vllm which supports tensor parallel for 60tps.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n57fwan",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;use vllm which supports tensor parallel for 60tps.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n57fwan/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753500292,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 16
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n58glrs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "MichaelXie4645",
            "can_mod_post": false,
            "created_utc": 1753519360,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 8,
            "author_fullname": "t2_a06q0mmx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Run VLLM with fp8 built in quant since 5090 supports native fp8 inferencing then you will see light speed tps at 32B qwen coder",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n58glrs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 405B"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Run VLLM with fp8 built in quant since 5090 supports native fp8 inferencing then you will see light speed tps at 32B qwen coder&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n58glrs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753519360,
            "author_flair_text": "Llama 405B",
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n59fbfn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "henfiber",
            "can_mod_post": false,
            "created_utc": 1753536243,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 5,
            "author_fullname": "t2_lw9me25",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;I was told trying to run non-tiny LLM's on a CPU was unusable. But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090's\n\nIt is the Input/Prompt Processing speed (PP t/s) that makes CPUs unusable.\n\nYou 64-core CPU will do better than most, but will still be \\~60x times slower than your 5090s (6-7 AVX512 FP16 Tflops Vs 420 FP16 Tflops).  \ni.e. if it takes 3 seconds for your 5090 to process a large prompt, it will take 3 minutes with your CPU.\n\nThe difference is even greater in FP8/FP4.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n59fbfn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I was told trying to run non-tiny LLM&amp;#39;s on a CPU was unusable. But I got 8.3 token/sec for qwen2.5-coder-32b-instruct Q8 without using the GPU.  38.6 tokens/sec using both 5090&amp;#39;s&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It is the Input/Prompt Processing speed (PP t/s) that makes CPUs unusable.&lt;/p&gt;\n\n&lt;p&gt;You 64-core CPU will do better than most, but will still be ~60x times slower than your 5090s (6-7 AVX512 FP16 Tflops Vs 420 FP16 Tflops).&lt;br/&gt;\ni.e. if it takes 3 seconds for your 5090 to process a large prompt, it will take 3 minutes with your CPU.&lt;/p&gt;\n\n&lt;p&gt;The difference is even greater in FP8/FP4.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n59fbfn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753536243,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n580fz3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "niellsro",
            "can_mod_post": false,
            "created_utc": 1753510108,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 2,
            "author_fullname": "t2_17f4e4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would suggest you to setup docker containers - vllm for fp8/gptq/awq models, llama.cpp server container for gguf models\n\nHave the models stored in a folder that is bind mounted on both vllm and llama.cpp containers. This way you can easily switch depending on what model you want to use and also keep your OS clean and avoid any possible dependency conflicts.\n\nPS: very nice hardware setup",
            "edited": 1753510296,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n580fz3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would suggest you to setup docker containers - vllm for fp8/gptq/awq models, llama.cpp server container for gguf models&lt;/p&gt;\n\n&lt;p&gt;Have the models stored in a folder that is bind mounted on both vllm and llama.cpp containers. This way you can easily switch depending on what model you want to use and also keep your OS clean and avoid any possible dependency conflicts.&lt;/p&gt;\n\n&lt;p&gt;PS: very nice hardware setup&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n580fz3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753510108,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5815x4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zipperlein",
            "can_mod_post": false,
            "created_utc": 1753510504,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 2,
            "author_fullname": "t2_x3duw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If u want more throughput u need to use vllm with tensor-parallel.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5815x4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If u want more throughput u need to use vllm with tensor-parallel.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n5815x4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753510504,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5c30gp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Guilty-History-9249",
                      "can_mod_post": false,
                      "created_utc": 1753567202,
                      "send_replies": true,
                      "parent_id": "t1_n586q32",
                      "score": 2,
                      "author_fullname": "t2_rxk6hx4t",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm using all 8 memory channels.  Yes, I could have got the 7995WX with 12 mem channels but decided against the $10,000  CPU.  I kind of wish I just went for it.\n\nI'll have to look up what ik\\_llama.cpp is.  Just did and it looks worth trying.  Thanks.  \nI only got 8.3 tokens per second when I manually bound llama.cpp's worker threads evenly spaced across the threadripper's CCD's.  llama.cpp's affinity code is broke.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5c30gp",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using all 8 memory channels.  Yes, I could have got the 7995WX with 12 mem channels but decided against the $10,000  CPU.  I kind of wish I just went for it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll have to look up what ik_llama.cpp is.  Just did and it looks worth trying.  Thanks.&lt;br/&gt;\nI only got 8.3 tokens per second when I manually bound llama.cpp&amp;#39;s worker threads evenly spaced across the threadripper&amp;#39;s CCD&amp;#39;s.  llama.cpp&amp;#39;s affinity code is broke.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9itnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n5c30gp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753567202,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n586q32",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "un_passant",
            "can_mod_post": false,
            "created_utc": 1753513575,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 2,
            "author_fullname": "t2_7rqtc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How many memory channels are you using ? How much more expensive would have it been to have 12 memory channels with an Epyc Gen 4 ?\n\nWhy don't you use ik\\_llama.cpp ?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n586q32",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How many memory channels are you using ? How much more expensive would have it been to have 12 memory channels with an Epyc Gen 4 ?&lt;/p&gt;\n\n&lt;p&gt;Why don&amp;#39;t you use ik_llama.cpp ?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n586q32/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753513575,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5adkdz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTshop_ai",
            "can_mod_post": false,
            "created_utc": 1753547420,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 2,
            "author_fullname": "t2_rkmud0isr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Blackwell was made for FP4. use only the best model Qwen3-235B-A22B-Thinking-2507. vllm, sglang, dynamo.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5adkdz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Blackwell was made for FP4. use only the best model Qwen3-235B-A22B-Thinking-2507. vllm, sglang, dynamo.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n5adkdz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753547420,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5c3d1s",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Guilty-History-9249",
                      "can_mod_post": false,
                      "created_utc": 1753567321,
                      "send_replies": true,
                      "parent_id": "t1_n58sp0z",
                      "score": 1,
                      "author_fullname": "t2_rxk6hx4t",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That'll probably be the next model I try.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5c3d1s",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;ll probably be the next model I try.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9itnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n5c3d1s/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753567321,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n58sp0z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Zyguard7777777",
            "can_mod_post": false,
            "created_utc": 1753526422,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 1,
            "author_fullname": "t2_zo1h5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What speed do you get with cpu only for qwen3 235b q4? ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n58sp0z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What speed do you get with cpu only for qwen3 235b q4? &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n58sp0z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753526422,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n57j42p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MikeRoz",
            "can_mod_post": false,
            "created_utc": 1753501674,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 1,
            "author_fullname": "t2_ht2fg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Row split on llama.cpp, or tensor parallelism on exllamav2. For Exllamav2, note that not all supported architectures also support tensor parallelism - Qwen3 MoE being the most recent example.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n57j42p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Row split on llama.cpp, or tensor parallelism on exllamav2. For Exllamav2, note that not all supported architectures also support tensor parallelism - Qwen3 MoE being the most recent example.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n57j42p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753501674,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5c13pa",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Guilty-History-9249",
                      "can_mod_post": false,
                      "created_utc": 1753566554,
                      "send_replies": true,
                      "parent_id": "t1_n58wnjx",
                      "score": 1,
                      "author_fullname": "t2_rxk6hx4t",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'll buy the venmo company for you and then send a penny.  :-)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5c13pa",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll buy the venmo company for you and then send a penny.  :-)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9itnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n5c13pa/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753566554,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n58wnjx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "getmevodka",
            "can_mod_post": false,
            "created_utc": 1753528447,
            "send_replies": true,
            "parent_id": "t3_1m9itnz",
            "score": 1,
            "author_fullname": "t2_7uoa6r1b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "damn, can you venmo some money ? (i have no venmo) 🤣👊",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n58wnjx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;damn, can you venmo some money ? (i have no venmo) 🤣👊&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9itnz/my_7985wx_dual_5090s_and_256gbs_of_ddr56000_has/n58wnjx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753528447,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9itnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]