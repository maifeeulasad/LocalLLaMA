[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "First, thank you so much to everyone who has helped me work through and suggested how to build out my rig.\n\nFor those of you who haven’t seen those, I have posted twice with slightly different ideas and let me tell you this community has shown up! \n\n\nI have to taken this approach as the technical side of hybrid inferences finally sunk in. While typically self hosted inference on dense models would ideally be run on just a GPU. \nThe paradigm of hybrid inference kind of flips it on a head. The GPU just becomes a utility for the overall CPU based inference to use and not vice versa.\n\nSo here is the new context and question.\n\nContext: I have one existing 5090 FE (i have a second but would like to use it to upgrade one of my gaming pcs, which current have a 4090 and a 5080 in them)\n\nQuestion:\nWith a remaining budget of $10,000, how would you build out an inference rig that is especially optimized for CPU inference, and would pair well with the 5090(I assume for kv cache and FFN)\n\n\nLong live local llama!\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "The Final build: help me finish a CPU FIRST hybrid MOE rig",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m3xbj7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_eqtnew30",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": true,
            "thumbnail": "self",
            "edited": 1752935148,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752934621,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First, thank you so much to everyone who has helped me work through and suggested how to build out my rig.&lt;/p&gt;\n\n&lt;p&gt;For those of you who haven’t seen those, I have posted twice with slightly different ideas and let me tell you this community has shown up! &lt;/p&gt;\n\n&lt;p&gt;I have to taken this approach as the technical side of hybrid inferences finally sunk in. While typically self hosted inference on dense models would ideally be run on just a GPU. \nThe paradigm of hybrid inference kind of flips it on a head. The GPU just becomes a utility for the overall CPU based inference to use and not vice versa.&lt;/p&gt;\n\n&lt;p&gt;So here is the new context and question.&lt;/p&gt;\n\n&lt;p&gt;Context: I have one existing 5090 FE (i have a second but would like to use it to upgrade one of my gaming pcs, which current have a 4090 and a 5080 in them)&lt;/p&gt;\n\n&lt;p&gt;Question:\nWith a remaining budget of $10,000, how would you build out an inference rig that is especially optimized for CPU inference, and would pair well with the 5090(I assume for kv cache and FFN)&lt;/p&gt;\n\n&lt;p&gt;Long live local llama!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m3xbj7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "novel_market_21",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/",
            "subreddit_subscribers": 501526,
            "created_utc": 1752934621,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4024qp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LagOps91",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n400za8",
                                "score": 2,
                                "author_fullname": "t2_3wi6j7vwh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "you need about 30-31 gb for Q8 KV cache for 32k context on R1 (according to HF VRAM calculator).  \n  \nSo with one 5090, that would be the maximum for you. I'm not sure how much context you want to run and you likely also want to load all kinds of commonly/always used tensors to GPU. having another GPU for this purpose should result in a noticable speedup or more context. I don't think it needs to be a 5090 tho - your 4090 should already help a lot in this regard for instance.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4024qp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you need about 30-31 gb for Q8 KV cache for 32k context on R1 (according to HF VRAM calculator).  &lt;/p&gt;\n\n&lt;p&gt;So with one 5090, that would be the maximum for you. I&amp;#39;m not sure how much context you want to run and you likely also want to load all kinds of commonly/always used tensors to GPU. having another GPU for this purpose should result in a noticable speedup or more context. I don&amp;#39;t think it needs to be a 5090 tho - your 4090 should already help a lot in this regard for instance.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m3xbj7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/n4024qp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752935476,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752935476,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n400za8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "novel_market_21",
                      "can_mod_post": false,
                      "created_utc": 1752935109,
                      "send_replies": true,
                      "parent_id": "t1_n400pwz",
                      "score": 1,
                      "author_fullname": "t2_eqtnew30",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "So i have a 4090 a 2nd 5090, and a 5080 in other machine i can do some musical chairs with. how much more vram do you think i would need? ideally id like to use the 2nd 5090 for a gaming pc upgrade.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n400za8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So i have a 4090 a 2nd 5090, and a 5080 in other machine i can do some musical chairs with. how much more vram do you think i would need? ideally id like to use the 2nd 5090 for a gaming pc upgrade.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": true,
                      "can_gild": false,
                      "link_id": "t3_1m3xbj7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/n400za8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752935109,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n400pwz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1752935027,
            "send_replies": true,
            "parent_id": "t3_1m3xbj7",
            "score": 4,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "kv cache can be quite heavy, especially for large models. R1 has very heavy kv cache for instance and one 5090 won't be enough to hold the context (correct me if that is wrong). aside from (at least) one more 5090, you likely want as much fast ram on as many channels as possible. 512 GB DDR5 ram should be a good target. In terms of server boards with a ton of ram channels (12-channel?), i'm not so familliar myself.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n400pwz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;kv cache can be quite heavy, especially for large models. R1 has very heavy kv cache for instance and one 5090 won&amp;#39;t be enough to hold the context (correct me if that is wrong). aside from (at least) one more 5090, you likely want as much fast ram on as many channels as possible. 512 GB DDR5 ram should be a good target. In terms of server boards with a ton of ram channels (12-channel?), i&amp;#39;m not so familliar myself.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/n400pwz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752935027,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3xbj7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n402lv8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kryptkpr",
            "can_mod_post": false,
            "created_utc": 1752935627,
            "send_replies": true,
            "parent_id": "t3_1m3xbj7",
            "score": 3,
            "author_fullname": "t2_30i1a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "EPYC 9175F\n\n12x 64GB 6400mhz DIMMs\n\nA compatible SP5 motherboard such as Gigabyte MZ33\n\nRTX4090+ for prompt progressing and non-MoE layers\n\nIf this doesn't fit budget in your country fall back to a zen4. I built a zen2 rig for under $2K and am super happy with it, getting 140 GB/sec memory bw.",
            "edited": 1752935816,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n402lv8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;EPYC 9175F&lt;/p&gt;\n\n&lt;p&gt;12x 64GB 6400mhz DIMMs&lt;/p&gt;\n\n&lt;p&gt;A compatible SP5 motherboard such as Gigabyte MZ33&lt;/p&gt;\n\n&lt;p&gt;RTX4090+ for prompt progressing and non-MoE layers&lt;/p&gt;\n\n&lt;p&gt;If this doesn&amp;#39;t fit budget in your country fall back to a zen4. I built a zen2 rig for under $2K and am super happy with it, getting 140 GB/sec memory bw.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/n402lv8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752935627,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1m3xbj7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n402nb5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MDT-49",
            "can_mod_post": false,
            "created_utc": 1752935639,
            "send_replies": true,
            "parent_id": "t3_1m3xbj7",
            "score": 1,
            "author_fullname": "t2_h8yrica5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Probably pair it with an AMD EPYC Zen5 CPU and make sure to utilize all twelve memory channels with DDR5-6000\n\nI think in theory, a dual-CPU/socket approach with the right NUMA-aware workload could double the memory bandwidth, but from what I've read this is difficult to get right in practice right now. \n\nThis is just my take, though, so take it a with a grain of salt.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n402nb5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Probably pair it with an AMD EPYC Zen5 CPU and make sure to utilize all twelve memory channels with DDR5-6000&lt;/p&gt;\n\n&lt;p&gt;I think in theory, a dual-CPU/socket approach with the right NUMA-aware workload could double the memory bandwidth, but from what I&amp;#39;ve read this is difficult to get right in practice right now. &lt;/p&gt;\n\n&lt;p&gt;This is just my take, though, so take it a with a grain of salt.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/n402nb5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752935639,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3xbj7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n41s2qu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bick_nyers",
            "can_mod_post": false,
            "created_utc": 1752954918,
            "send_replies": true,
            "parent_id": "t3_1m3xbj7",
            "score": 1,
            "author_fullname": "t2_6nwld4d3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "768GB gives you access to 4bit quant Kimi K2 btw.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n41s2qu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;768GB gives you access to 4bit quant Kimi K2 btw.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3xbj7/the_final_build_help_me_finish_a_cpu_first_hybrid/n41s2qu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752954918,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3xbj7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]