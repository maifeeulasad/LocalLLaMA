[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I am building in the voice AI space and playing with open source TTS models. For single requests, they are great! But when it comes to supporting concurrent requests for streaming pretty much all Youtube videos, docs, tutorials, blog posts cease to exist... \n\nFor example, I have a TTS model which takes about 1.8GB on disk space but when loaded onto VRAM it takes about 2.5GB in GPU. When I run concurrent requests it just doesn't work. There are bunch of shared states overwriting each other, tensors getting corrupt, tensor size mismatches caused by the concurrent requests. \n\nI then tried setting up a pool with 2 model instances. It worked a bit better in that I could now run 2 parallel requests achieving about 600ms-1s for first audio chunk in both requests. But anything beyond 2 concurrent requests end up waiting for GPU to become available. \n\nSome benchmark analysis examples:\n\n✅ 1 concurrent users:  \n   • Throughput: 0.31 req/s  \n   • First byte: 623ms  \n   • First audio: 623ms  \n   • Total latency: 3197ms (P95: 3197ms)  \n   • Real-time factor: 0.70x (lower is better)  \n   • GPU utilization: 34.8%  \n   • Memory usage: 38.4%\n\n✅ 2 concurrent users:  \n   • Throughput: 0.38 req/s  \n   • First byte: 1041ms  \n   • First audio: 1041ms  \n   • Total latency: 5196ms (P95: 5284ms)  \n   • Real-time factor: 1.16x (lower is better)  \n   • GPU utilization: 41.8%  \n   • Memory usage: 39.2%\n\n✅ 4 concurrent users:  \n   • Throughput: 0.38 req/s  \n   • First byte: 5719ms  \n   • First audio: 5719ms  \n   • Total latency: 10173ms (P95: 12510ms)  \n   • Real-time factor: 2.28x (lower is better)  \n   • GPU utilization: 41.6%  \n   • Memory usage: 39.4%\n\n✅ 8 concurrent users:  \n   • Throughput: 0.38 req/s  \n   • First byte: 10436ms  \n   • First audio: 10436ms  \n   • Total latency: 14962ms (P95: 31506ms)  \n   • Real-time factor: 3.36x (lower is better)  \n   • GPU utilization: 41.2%  \n   • Memory usage: 39.4%\n\nThis was done on an T4 GPU with 2vCPU and 7.5GB VRAM.   \nThen I tested T4 with 4vCPU, 15GB RAM and achieved bout 0.50 req/s instead of 0.38. So, some improvement and slight gain on FFTB, first audio. But not much.   \nThen I tested on L4 with 4vCPU, 16GB RAM and I could achieve 0.74\\~ req/s which was much better. But still concurrent users beyond 2 struggle. \n\nI tried bumping up the pool size beyond 2 and there wasn't anything noticeably better. At 4, some latency was in fact slower. At 6 it was about same as 2 or worse. \n\nSo at L4, which comes with 24GB VRAM and cost of like $700/month on cloud, I could only load 2 instances of the TTS model to achieve the \"best\" parallelization. It takes about 5GB VRAM so I am essentially wasting 19GB VRAM of space and money. If I go down to something like T4 it is about 2x cheaper but the performance is about 2x slower as well. \n\nIt is almost like I need 5GB VRAM of L4 GPU kind of a situation but I don't know if it is possile. Because if I rent L4, it just comes with 24GB VRAM and that't it. \n\nSo, now back to the question of enabling parallelization on GPU. I have done a bunch of research online. It looks like \"batch processing\" is the only way to get around this because unlike CPU/Memory there is no concept of \"multi-processing\" on GPU. There are things like CUDA Streams but it didn't make any difference. Inference frameworks like Triton does support streaming batch processing but the underlying model has to support it. In my case, the underlying model has several layers like GPT, Vocoder...etc. And I can't figure out how get the model to support batch processing without taking apart the model or trying to rebuild it from scratch. \n\nI have seen frameworks like Auralis, RealtimeTTS -- but none of them provides true streaming for concurrent users. I have also looked into AllTalk V2 but not seeing true streaming for concurrent users either. I have looked at Baseten's blogs and examples. But renting an H100 to get concurrent user support for up to 24 just doesn't work economically. But they use OrpheusTTS which is an LLM based model that generates discrete audio tokens directly and it works with batch processing. \n\nReaching out to the community to see if anyone has seen any example of how to exactly achieve true streaming support for concurrent users for any open-source TTS models? ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Scaling GPU - How to add concurrency support?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mh3wzs",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_vgd5f4x3",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754281424,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am building in the voice AI space and playing with open source TTS models. For single requests, they are great! But when it comes to supporting concurrent requests for streaming pretty much all Youtube videos, docs, tutorials, blog posts cease to exist... &lt;/p&gt;\n\n&lt;p&gt;For example, I have a TTS model which takes about 1.8GB on disk space but when loaded onto VRAM it takes about 2.5GB in GPU. When I run concurrent requests it just doesn&amp;#39;t work. There are bunch of shared states overwriting each other, tensors getting corrupt, tensor size mismatches caused by the concurrent requests. &lt;/p&gt;\n\n&lt;p&gt;I then tried setting up a pool with 2 model instances. It worked a bit better in that I could now run 2 parallel requests achieving about 600ms-1s for first audio chunk in both requests. But anything beyond 2 concurrent requests end up waiting for GPU to become available. &lt;/p&gt;\n\n&lt;p&gt;Some benchmark analysis examples:&lt;/p&gt;\n\n&lt;p&gt;✅ 1 concurrent users:&lt;br/&gt;\n   • Throughput: 0.31 req/s&lt;br/&gt;\n   • First byte: 623ms&lt;br/&gt;\n   • First audio: 623ms&lt;br/&gt;\n   • Total latency: 3197ms (P95: 3197ms)&lt;br/&gt;\n   • Real-time factor: 0.70x (lower is better)&lt;br/&gt;\n   • GPU utilization: 34.8%&lt;br/&gt;\n   • Memory usage: 38.4%&lt;/p&gt;\n\n&lt;p&gt;✅ 2 concurrent users:&lt;br/&gt;\n   • Throughput: 0.38 req/s&lt;br/&gt;\n   • First byte: 1041ms&lt;br/&gt;\n   • First audio: 1041ms&lt;br/&gt;\n   • Total latency: 5196ms (P95: 5284ms)&lt;br/&gt;\n   • Real-time factor: 1.16x (lower is better)&lt;br/&gt;\n   • GPU utilization: 41.8%&lt;br/&gt;\n   • Memory usage: 39.2%&lt;/p&gt;\n\n&lt;p&gt;✅ 4 concurrent users:&lt;br/&gt;\n   • Throughput: 0.38 req/s&lt;br/&gt;\n   • First byte: 5719ms&lt;br/&gt;\n   • First audio: 5719ms&lt;br/&gt;\n   • Total latency: 10173ms (P95: 12510ms)&lt;br/&gt;\n   • Real-time factor: 2.28x (lower is better)&lt;br/&gt;\n   • GPU utilization: 41.6%&lt;br/&gt;\n   • Memory usage: 39.4%&lt;/p&gt;\n\n&lt;p&gt;✅ 8 concurrent users:&lt;br/&gt;\n   • Throughput: 0.38 req/s&lt;br/&gt;\n   • First byte: 10436ms&lt;br/&gt;\n   • First audio: 10436ms&lt;br/&gt;\n   • Total latency: 14962ms (P95: 31506ms)&lt;br/&gt;\n   • Real-time factor: 3.36x (lower is better)&lt;br/&gt;\n   • GPU utilization: 41.2%&lt;br/&gt;\n   • Memory usage: 39.4%&lt;/p&gt;\n\n&lt;p&gt;This was done on an T4 GPU with 2vCPU and 7.5GB VRAM.&lt;br/&gt;\nThen I tested T4 with 4vCPU, 15GB RAM and achieved bout 0.50 req/s instead of 0.38. So, some improvement and slight gain on FFTB, first audio. But not much.&lt;br/&gt;\nThen I tested on L4 with 4vCPU, 16GB RAM and I could achieve 0.74~ req/s which was much better. But still concurrent users beyond 2 struggle. &lt;/p&gt;\n\n&lt;p&gt;I tried bumping up the pool size beyond 2 and there wasn&amp;#39;t anything noticeably better. At 4, some latency was in fact slower. At 6 it was about same as 2 or worse. &lt;/p&gt;\n\n&lt;p&gt;So at L4, which comes with 24GB VRAM and cost of like $700/month on cloud, I could only load 2 instances of the TTS model to achieve the &amp;quot;best&amp;quot; parallelization. It takes about 5GB VRAM so I am essentially wasting 19GB VRAM of space and money. If I go down to something like T4 it is about 2x cheaper but the performance is about 2x slower as well. &lt;/p&gt;\n\n&lt;p&gt;It is almost like I need 5GB VRAM of L4 GPU kind of a situation but I don&amp;#39;t know if it is possile. Because if I rent L4, it just comes with 24GB VRAM and that&amp;#39;t it. &lt;/p&gt;\n\n&lt;p&gt;So, now back to the question of enabling parallelization on GPU. I have done a bunch of research online. It looks like &amp;quot;batch processing&amp;quot; is the only way to get around this because unlike CPU/Memory there is no concept of &amp;quot;multi-processing&amp;quot; on GPU. There are things like CUDA Streams but it didn&amp;#39;t make any difference. Inference frameworks like Triton does support streaming batch processing but the underlying model has to support it. In my case, the underlying model has several layers like GPT, Vocoder...etc. And I can&amp;#39;t figure out how get the model to support batch processing without taking apart the model or trying to rebuild it from scratch. &lt;/p&gt;\n\n&lt;p&gt;I have seen frameworks like Auralis, RealtimeTTS -- but none of them provides true streaming for concurrent users. I have also looked into AllTalk V2 but not seeing true streaming for concurrent users either. I have looked at Baseten&amp;#39;s blogs and examples. But renting an H100 to get concurrent user support for up to 24 just doesn&amp;#39;t work economically. But they use OrpheusTTS which is an LLM based model that generates discrete audio tokens directly and it works with batch processing. &lt;/p&gt;\n\n&lt;p&gt;Reaching out to the community to see if anyone has seen any example of how to exactly achieve true streaming support for concurrent users for any open-source TTS models? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mh3wzs",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Unfair-Enthusiasm-30",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mh3wzs/scaling_gpu_how_to_add_concurrency_support/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mh3wzs/scaling_gpu_how_to_add_concurrency_support/",
            "subreddit_subscribers": 509911,
            "created_utc": 1754281424,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6uah2x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Icy_Bid6597",
            "can_mod_post": false,
            "created_utc": 1754299180,
            "send_replies": true,
            "parent_id": "t3_1mh3wzs",
            "score": 1,
            "author_fullname": "t2_trc4foci4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "While implementing concurrency in any ML model you always want to utilize SIMD. So you want to increase the batch size and perform the same operations on different data, instead of calling a model from multiple threads (none of mainstream ml framework is thread save by design BTW), or having multiple instances of the model (wasting resources, and while doing two predictions on same GPU you force both of instances to fight for resources)\n\n  \nDepending on the model architecture it can be easier or harder. A lot of TTS models are autoregressive (same as LLMs) so this issue on conceptual level is already solved in other tools (like vLLM for LLMs). \n\nSo as long as your model is autoregressive you want to implement \"dynamic batching\". Keeping incoming request queue in memory, and dynamically processing as many as your VRAM allows you to. \n\nThere are different possible implementations. Simples one is to queue all the incoming requests, wait any arbitrary amount of time to collect a batch, and just pass them into the model at once. No hardcore engineering, but it have some downsides:\n\n\\- if a batch is already being processed all other incoming requests must wait\n\n\\- you are increasing first response time by selected arbitrary time\n\n  \nOther then that you can try to implement dynamic batching (exactly like vllm/sglang and others are doing for LLMs). \n\n\\- first of all you calculate how much VRAM you need to process one request (this can be tricky, as it can vary between them. But most of the models have some arbitrary max output length that you can adjust to your needs)\n\n\\- then you implement in memory queue for requests\n\n\\- next step is understanding the information flow inside the model. Most of TTS generate intermediate output step by step and then they decode it into audio (with separate decoder/vocoder or whatever). You need to identify both of these parts\n\n\\- first part is highly concurrent and independent. You generate one step, then if new request came into the queue you can batch them together and pass through a model at the same time (for first request you are generating output #2 for first #1).\n\n\\- then at any moment you can gather generated outputs and process them with the decoder into actual audio (that allows you to stream). \n\n  \nNone of steps above are easy, but it is definitely doable. Personally I am not aware of any mature open source implementation of concurrent TTS server with streaming.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6uah2x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While implementing concurrency in any ML model you always want to utilize SIMD. So you want to increase the batch size and perform the same operations on different data, instead of calling a model from multiple threads (none of mainstream ml framework is thread save by design BTW), or having multiple instances of the model (wasting resources, and while doing two predictions on same GPU you force both of instances to fight for resources)&lt;/p&gt;\n\n&lt;p&gt;Depending on the model architecture it can be easier or harder. A lot of TTS models are autoregressive (same as LLMs) so this issue on conceptual level is already solved in other tools (like vLLM for LLMs). &lt;/p&gt;\n\n&lt;p&gt;So as long as your model is autoregressive you want to implement &amp;quot;dynamic batching&amp;quot;. Keeping incoming request queue in memory, and dynamically processing as many as your VRAM allows you to. &lt;/p&gt;\n\n&lt;p&gt;There are different possible implementations. Simples one is to queue all the incoming requests, wait any arbitrary amount of time to collect a batch, and just pass them into the model at once. No hardcore engineering, but it have some downsides:&lt;/p&gt;\n\n&lt;p&gt;- if a batch is already being processed all other incoming requests must wait&lt;/p&gt;\n\n&lt;p&gt;- you are increasing first response time by selected arbitrary time&lt;/p&gt;\n\n&lt;p&gt;Other then that you can try to implement dynamic batching (exactly like vllm/sglang and others are doing for LLMs). &lt;/p&gt;\n\n&lt;p&gt;- first of all you calculate how much VRAM you need to process one request (this can be tricky, as it can vary between them. But most of the models have some arbitrary max output length that you can adjust to your needs)&lt;/p&gt;\n\n&lt;p&gt;- then you implement in memory queue for requests&lt;/p&gt;\n\n&lt;p&gt;- next step is understanding the information flow inside the model. Most of TTS generate intermediate output step by step and then they decode it into audio (with separate decoder/vocoder or whatever). You need to identify both of these parts&lt;/p&gt;\n\n&lt;p&gt;- first part is highly concurrent and independent. You generate one step, then if new request came into the queue you can batch them together and pass through a model at the same time (for first request you are generating output #2 for first #1).&lt;/p&gt;\n\n&lt;p&gt;- then at any moment you can gather generated outputs and process them with the decoder into actual audio (that allows you to stream). &lt;/p&gt;\n\n&lt;p&gt;None of steps above are easy, but it is definitely doable. Personally I am not aware of any mature open source implementation of concurrent TTS server with streaming.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mh3wzs/scaling_gpu_how_to_add_concurrency_support/n6uah2x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754299180,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mh3wzs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]