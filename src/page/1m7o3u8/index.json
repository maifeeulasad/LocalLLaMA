[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm seeing a trend in recent advancements in open source models, they're getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Is there a future for local models?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m7o3u8",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.82,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 18,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_e11po",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 18,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753311706,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m seeing a trend in recent advancements in open source models, they&amp;#39;re getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m7o3u8",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ASTRdeca",
            "discussion_type": null,
            "num_comments": 36,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/",
            "subreddit_subscribers": 503516,
            "created_utc": 1753311706,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t05i3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Macestudios32",
            "can_mod_post": false,
            "created_utc": 1753312905,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 20,
            "author_fullname": "t2_1mbsf8cel3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My view is just the opposite. Positive.\n\n\n The fact that free models are advancing even with giant monsters is good news. With time, you can distill them to small, quantify, improve HW, make more money... \n\n\nMy pain point will be when there are offline conversational agents or models and the HW necessary for their normal functioning is unaffordable for me. \nNo one wants to hear one token per second!\n\n\n This is a long-term race that includes privacy and freedom. \n\n\nThis year maybe an 8 gigabyte graph, and in 2 years 16, or NPUs will come out. etc.\n\n\n Slow responses depend on the patience of each one, but when they do actions..... Hal, perform the optimization of the system and security checks (and you go to sleep) if the system is reliable, works well and can be loaded, any action that the system does is not done by me, no matter how slow it is, it is a gain.\n\n\n In consumer HW I include everything from 512 gigabyte macs to homelab servers...\n\n\n May the progress not stop! Radios, televisions, telephones, everything for the rich at the beginning and now... Look at us.\n\n\n Best regards",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t05i3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My view is just the opposite. Positive.&lt;/p&gt;\n\n&lt;p&gt; The fact that free models are advancing even with giant monsters is good news. With time, you can distill them to small, quantify, improve HW, make more money... &lt;/p&gt;\n\n&lt;p&gt;My pain point will be when there are offline conversational agents or models and the HW necessary for their normal functioning is unaffordable for me. \nNo one wants to hear one token per second!&lt;/p&gt;\n\n&lt;p&gt; This is a long-term race that includes privacy and freedom. &lt;/p&gt;\n\n&lt;p&gt;This year maybe an 8 gigabyte graph, and in 2 years 16, or NPUs will come out. etc.&lt;/p&gt;\n\n&lt;p&gt; Slow responses depend on the patience of each one, but when they do actions..... Hal, perform the optimization of the system and security checks (and you go to sleep) if the system is reliable, works well and can be loaded, any action that the system does is not done by me, no matter how slow it is, it is a gain.&lt;/p&gt;\n\n&lt;p&gt; In consumer HW I include everything from 512 gigabyte macs to homelab servers...&lt;/p&gt;\n\n&lt;p&gt; May the progress not stop! Radios, televisions, telephones, everything for the rich at the beginning and now... Look at us.&lt;/p&gt;\n\n&lt;p&gt; Best regards&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t05i3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753312905,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 20
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4t8t60",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "llmentry",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4t58st",
                                "score": 1,
                                "author_fullname": "t2_1lufy6yx6z",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Agreed, Gemma 3 shows just how far we've come.  That said, Gemma is strong on writing / language and pretty weak on STEM / coding.  Every small model has its strengths and weaknesses.\n\nI suspect Kimi K2 is representative of the space OpenAI was in with GPT-4 a year and a half ago.  Give it another half year, and my hope is that we'll see \\~70-200B param open-weighted models kicking it around at 4o / 4.1 mini / 4.1 level, or better.\n\n(Maybe even earlier, if we get a Gemma 4 70B model ...)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4t8t60",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Agreed, Gemma 3 shows just how far we&amp;#39;ve come.  That said, Gemma is strong on writing / language and pretty weak on STEM / coding.  Every small model has its strengths and weaknesses.&lt;/p&gt;\n\n&lt;p&gt;I suspect Kimi K2 is representative of the space OpenAI was in with GPT-4 a year and a half ago.  Give it another half year, and my hope is that we&amp;#39;ll see ~70-200B param open-weighted models kicking it around at 4o / 4.1 mini / 4.1 level, or better.&lt;/p&gt;\n\n&lt;p&gt;(Maybe even earlier, if we get a Gemma 4 70B model ...)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m7o3u8",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8t60/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753315806,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753315806,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4t58st",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "FenderMoon",
                      "can_mod_post": false,
                      "created_utc": 1753314588,
                      "send_replies": true,
                      "parent_id": "t1_n4t0k2j",
                      "score": 9,
                      "author_fullname": "t2_f4ibdsc9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Gemma is a shockingly good model for its size. I was able to run 27b on a 16GB Mac with an IQ3_XS quant, was super impressed with the quality of the model despite using such an aggressive quant. \n\nI like that they offered a 27b instead of the standard 32b ones. It makes it easier to run these models on smaller systems. It’s such a good model, heck even the 12b often gave better results than some of the other 32b models I’ve tried.\n\n(I did notice that on the 12b model, it was much more sensitive to quants for some reason, I got far better results at 6 bits than at 4 bits. That was surprising to see so much degredation at 4 bits, normally 4 bits is the sweet spot. 27b doesn’t seem to be nearly as sensitive, 3 bits locally was almost as good as the unquantized ones from AI studio).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4t58st",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gemma is a shockingly good model for its size. I was able to run 27b on a 16GB Mac with an IQ3_XS quant, was super impressed with the quality of the model despite using such an aggressive quant. &lt;/p&gt;\n\n&lt;p&gt;I like that they offered a 27b instead of the standard 32b ones. It makes it easier to run these models on smaller systems. It’s such a good model, heck even the 12b often gave better results than some of the other 32b models I’ve tried.&lt;/p&gt;\n\n&lt;p&gt;(I did notice that on the 12b model, it was much more sensitive to quants for some reason, I got far better results at 6 bits than at 4 bits. That was surprising to see so much degredation at 4 bits, normally 4 bits is the sweet spot. 27b doesn’t seem to be nearly as sensitive, 3 bits locally was almost as good as the unquantized ones from AI studio).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7o3u8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t58st/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753314588,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 9
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4t7hbh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CantaloupeDismal1195",
                      "can_mod_post": false,
                      "created_utc": 1753315346,
                      "send_replies": true,
                      "parent_id": "t1_n4t0k2j",
                      "score": 1,
                      "author_fullname": "t2_1ld1b995hk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I want Gemma4 70B",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4t7hbh",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I want Gemma4 70B&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7o3u8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t7hbh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753315346,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4tdjv0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "TheRealMasonMac",
                      "can_mod_post": false,
                      "created_utc": 1753317448,
                      "send_replies": true,
                      "parent_id": "t1_n4t0k2j",
                      "score": 1,
                      "author_fullname": "t2_101haj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "70B would be awesome.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4tdjv0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;70B would be awesome.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7o3u8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tdjv0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753317448,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4til9x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Caffdy",
                      "can_mod_post": false,
                      "created_utc": 1753319202,
                      "send_replies": true,
                      "parent_id": "t1_n4t0k2j",
                      "score": 1,
                      "author_fullname": "t2_ql2vu0wz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Gemma 4 70B? those are serious allegations ..\n\nsource?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4til9x",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gemma 4 70B? those are serious allegations ..&lt;/p&gt;\n\n&lt;p&gt;source?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7o3u8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4til9x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753319202,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4t0k2j",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Rich_Artist_8327",
            "can_mod_post": false,
            "created_utc": 1753313036,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 14,
            "author_fullname": "t2_1jk2ep8a52",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Google Gemma saves us. Long live Google! Gemma4 12,27,70B coming!!!!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t0k2j",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Google Gemma saves us. Long live Google! Gemma4 12,27,70B coming!!!!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t0k2j/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753313036,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 14
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t0a6t",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1753312947,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 11,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The current SOTA dense models are around 32B (some are 24B or 34B), and people complain that there are no more 70B models.\n\nAt the same time, we’re seeing MoE models reaching up to 1000B, but there are also smaller ones — under 100B.\n\nSo I don’t really see the problem here. Was the past really better?\n\nWas LLaMA 70B actually better than ChatGPT at that time?\n\nBecause you're comparing models that are already close to the current ChatGPT.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t0a6t",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The current SOTA dense models are around 32B (some are 24B or 34B), and people complain that there are no more 70B models.&lt;/p&gt;\n\n&lt;p&gt;At the same time, we’re seeing MoE models reaching up to 1000B, but there are also smaller ones — under 100B.&lt;/p&gt;\n\n&lt;p&gt;So I don’t really see the problem here. Was the past really better?&lt;/p&gt;\n\n&lt;p&gt;Was LLaMA 70B actually better than ChatGPT at that time?&lt;/p&gt;\n\n&lt;p&gt;Because you&amp;#39;re comparing models that are already close to the current ChatGPT.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t0a6t/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753312947,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n4t6guz",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "-dysangel-",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n4t5w9x",
                                          "score": -1,
                                          "author_fullname": "t2_12ggykute6",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Oh, I see what you were trying to say now. But, I still think you're disproving your point by mentioning those three. First came Deepseek. Then Kimi was bigger. Now Qwen Coder beats them out and is smaller.\n\n  \nAlso you know they are going to release smaller versions of Qwen 3 Coder, right? It's a \\*good\\* thing IMO that a decently large model is available in addition to those.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n4t6guz",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh, I see what you were trying to say now. But, I still think you&amp;#39;re disproving your point by mentioning those three. First came Deepseek. Then Kimi was bigger. Now Qwen Coder beats them out and is smaller.&lt;/p&gt;\n\n&lt;p&gt;Also you know they are going to release smaller versions of Qwen 3 Coder, right? It&amp;#39;s a *good* thing IMO that a decently large model is available in addition to those.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m7o3u8",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6guz/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753314995,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753314995,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": -1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n4t5w9x",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ASTRdeca",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4t1keg",
                                "score": 3,
                                "author_fullname": "t2_e11po",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Qwen3 coder is significant larger than Qwen2.5 coder...",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4t5w9x",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 coder is significant larger than Qwen2.5 coder...&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m7o3u8",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t5w9x/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753314802,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753314802,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4t1keg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "-dysangel-",
                      "can_mod_post": false,
                      "created_utc": 1753313367,
                      "send_replies": true,
                      "parent_id": "t1_n4sy93o",
                      "score": 0,
                      "author_fullname": "t2_12ggykute6",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah he disproved his own point by mentioning Qwen Coder there. It seems more of a troll than a question",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4t1keg",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah he disproved his own point by mentioning Qwen Coder there. It seems more of a troll than a question&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7o3u8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t1keg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753313367,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4sy93o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Yu2sama",
            "can_mod_post": false,
            "created_utc": 1753312282,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 12,
            "author_fullname": "t2_uu7xvge",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The future is smaller models in my honest opinion. Yes, at the moment we are struggling, but give it a few years and you will see. Smaller models from today perform better than bigger models of the past (a good example is the new Qwen, half the size of Deepseek and I argue performs much better)\n\nThis is a rapidly growing field, don't feel discouraged due to the present.",
            "edited": 1753313535,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4sy93o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The future is smaller models in my honest opinion. Yes, at the moment we are struggling, but give it a few years and you will see. Smaller models from today perform better than bigger models of the past (a good example is the new Qwen, half the size of Deepseek and I argue performs much better)&lt;/p&gt;\n\n&lt;p&gt;This is a rapidly growing field, don&amp;#39;t feel discouraged due to the present.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4sy93o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753312282,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t6gp3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DinoAmino",
            "can_mod_post": false,
            "created_utc": 1753314993,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 5,
            "author_fullname": "t2_j1v7f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is a ridiculous take. Nearsighted much? In 2025 there have been  many 32B and under releases and many local-friendly MoEs that have excited many people. Compared to only a few massive parameter LLMs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t6gp3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a ridiculous take. Nearsighted much? In 2025 there have been  many 32B and under releases and many local-friendly MoEs that have excited many people. Compared to only a few massive parameter LLMs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6gp3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314993,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4teo9f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "cyanoa",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4t41f9",
                                "score": 1,
                                "author_fullname": "t2_dsjtb",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I thought that Nvidia had built something of a moat with CUDA.\n\nBut Deepseek showed us that it isn't necessarily the case (and they showed us that CUDA is nowhere near optimized).\n\nLike most things, an open standard will likely emerge, supported by the other players, and will start to chip away at Nvidia's dominance.\n\nOther players are very likely more interested in providing us with better vram specs or more shared memory architectures like Apple.  We just need to give it a bit of time.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4teo9f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I thought that Nvidia had built something of a moat with CUDA.&lt;/p&gt;\n\n&lt;p&gt;But Deepseek showed us that it isn&amp;#39;t necessarily the case (and they showed us that CUDA is nowhere near optimized).&lt;/p&gt;\n\n&lt;p&gt;Like most things, an open standard will likely emerge, supported by the other players, and will start to chip away at Nvidia&amp;#39;s dominance.&lt;/p&gt;\n\n&lt;p&gt;Other players are very likely more interested in providing us with better vram specs or more shared memory architectures like Apple.  We just need to give it a bit of time.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m7o3u8",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4teo9f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753317840,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753317840,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4t41f9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Cool-Chemical-5629",
                      "can_mod_post": false,
                      "created_utc": 1753314188,
                      "send_replies": true,
                      "parent_id": "t1_n4sxi8z",
                      "score": 2,
                      "author_fullname": "t2_qz1qjc86",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "There are two key problems here.\n\nThe first one is that even if Nvidia makes a dedicated GPUs for this purpose with high amount of VRAM, it's still much more expensive than GPUs from any other companies. They are still GPUs that are out of reach for many.\n\nThe second and much more important problem is that the biggest chunk of AI technology still requires CUDA to run properly and that's Nvidia's proprietary technology no other GPU can use.\n\nYou see, each and every big name company out there has full mouth of noble words about bringing AI to everyone, but no one talks about solving the obvious issue that still prevents it from happening.\n\nIf Nvidia opened its technology for everyone to use on their own GPUs (which will most likely never happen), that would be a real step towards bringing AI to masses, but no Nvidia is not doing that and so we need to think of using alternative chips such as NPUs, or buying expensive \"AI Ready\" chips such as those modern Ryzen CPUs for AI, alternative AI model formats such as GGUF that support alternative technologies such as Vulkan which are universally available on all GPUs, etc... All that just to get a fraction of the speed and performance that is achievable on Nvidia GPUs.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4t41f9",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There are two key problems here.&lt;/p&gt;\n\n&lt;p&gt;The first one is that even if Nvidia makes a dedicated GPUs for this purpose with high amount of VRAM, it&amp;#39;s still much more expensive than GPUs from any other companies. They are still GPUs that are out of reach for many.&lt;/p&gt;\n\n&lt;p&gt;The second and much more important problem is that the biggest chunk of AI technology still requires CUDA to run properly and that&amp;#39;s Nvidia&amp;#39;s proprietary technology no other GPU can use.&lt;/p&gt;\n\n&lt;p&gt;You see, each and every big name company out there has full mouth of noble words about bringing AI to everyone, but no one talks about solving the obvious issue that still prevents it from happening.&lt;/p&gt;\n\n&lt;p&gt;If Nvidia opened its technology for everyone to use on their own GPUs (which will most likely never happen), that would be a real step towards bringing AI to masses, but no Nvidia is not doing that and so we need to think of using alternative chips such as NPUs, or buying expensive &amp;quot;AI Ready&amp;quot; chips such as those modern Ryzen CPUs for AI, alternative AI model formats such as GGUF that support alternative technologies such as Vulkan which are universally available on all GPUs, etc... All that just to get a fraction of the speed and performance that is achievable on Nvidia GPUs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7o3u8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t41f9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753314188,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4sxi8z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Excellent_Sleep6357",
            "can_mod_post": false,
            "created_utc": 1753312040,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 8,
            "author_fullname": "t2_1f7suc7bln",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'd say the open source community needs to stand up and start distilling those monsters with a systematic approach.\n\n\nOn the other hand, there may be some breakthroughs on the hardware side.  Nvidia has already released 96GB prosumer GPU, not a lot cheaper, but I do see a crack in this consumer/data center barrier they were trying to build up.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4sxi8z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d say the open source community needs to stand up and start distilling those monsters with a systematic approach.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, there may be some breakthroughs on the hardware side.  Nvidia has already released 96GB prosumer GPU, not a lot cheaper, but I do see a crack in this consumer/data center barrier they were trying to build up.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4sxi8z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753312040,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4sxstm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Square-Onion-1825",
            "can_mod_post": false,
            "created_utc": 1753312135,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 4,
            "author_fullname": "t2_1mkh7x2yxn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "i think it will be more reachable over time as the h/w gets cheaper and faster",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4sxstm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i think it will be more reachable over time as the h/w gets cheaper and faster&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4sxstm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753312135,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t5arx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Conscious_Cut_6144",
            "can_mod_post": false,
            "created_utc": 1753314605,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 3,
            "author_fullname": "t2_9hl4ymvj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Those giant models are MOE, you can run them on CPU if you want.  \nWay easier to run than the now ancient Llama 3.1 405B",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t5arx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Those giant models are MOE, you can run them on CPU if you want.&lt;br/&gt;\nWay easier to run than the now ancient Llama 3.1 405B&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t5arx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314605,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t8gh4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "vegatx40",
            "can_mod_post": false,
            "created_utc": 1753315685,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 3,
            "author_fullname": "t2_18dhiarv40",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The fact is that frontier models all have trillion parameters. \n\nI see it as good news that there are any open models at all that are running at or both half a trillion. \n\nThere will be a trickle down effect of those to model sizes that can be run on consumer hardware. \n\nTo say nothing of whatever next week's strategy for shrinking existing models is.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t8gh4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The fact is that frontier models all have trillion parameters. &lt;/p&gt;\n\n&lt;p&gt;I see it as good news that there are any open models at all that are running at or both half a trillion. &lt;/p&gt;\n\n&lt;p&gt;There will be a trickle down effect of those to model sizes that can be run on consumer hardware. &lt;/p&gt;\n\n&lt;p&gt;To say nothing of whatever next week&amp;#39;s strategy for shrinking existing models is.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8gh4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753315685,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t6ghj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "EugenePopcorn",
            "can_mod_post": false,
            "created_utc": 1753314992,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 2,
            "author_fullname": "t2_g6hpxxgss",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The main part of each of these models is still quite small. It's only the experts that are heavy. Loading them from disk and caching them in memory isn't super performant right now, but llama.cpp's new high throughput mode might be helpful for anybody using local agents. And cache misses matter less when you have multiple things to work on. ",
            "edited": 1753315804,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t6ghj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The main part of each of these models is still quite small. It&amp;#39;s only the experts that are heavy. Loading them from disk and caching them in memory isn&amp;#39;t super performant right now, but llama.cpp&amp;#39;s new high throughput mode might be helpful for anybody using local agents. And cache misses matter less when you have multiple things to work on. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6ghj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314992,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t0xqc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753313161,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 2,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware.\n\nLet's just take a step back for a sec...  Qwen3 has models from 0.6B to 480B.  Are you interested running models or expecting to replace a human developer with a 4060?\n\nIt's less that models are becoming bigger, and more they're becoming more capable, and the more capable they are the larger they are.  There are still plenty of 4B, 8B, 32B models that run fine on consumer hardware.  Not just the ones we had last year but new and better ones too.  You can still run cutting edge models on consumer hardware and they're more capable than they ever were.  Just don't expect a 32B model to compete with a &gt;300B model.",
            "edited": 1753316927,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t0xqc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Let&amp;#39;s just take a step back for a sec...  Qwen3 has models from 0.6B to 480B.  Are you interested running models or expecting to replace a human developer with a 4060?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s less that models are becoming bigger, and more they&amp;#39;re becoming more capable, and the more capable they are the larger they are.  There are still plenty of 4B, 8B, 32B models that run fine on consumer hardware.  Not just the ones we had last year but new and better ones too.  You can still run cutting edge models on consumer hardware and they&amp;#39;re more capable than they ever were.  Just don&amp;#39;t expect a 32B model to compete with a &amp;gt;300B model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t0xqc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753313161,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t2phl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mnt_brain",
            "can_mod_post": false,
            "created_utc": 1753313747,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_1mtt9dytfn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "big models can train smaller very specialized models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t2phl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;big models can train smaller very specialized models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t2phl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753313747,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t3mvi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753314055,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There will always be a place in the market or both small and large models. Large models will be at the frontier of intelligence, they have to be because they have more parameters available to them. Small models will push the industry forward since they are much less expensive to make and can serve to develop novel learning techniques. Edge devices and robotics are a big part of the market that small models will need to be specialized for. I will agree, though that most small models will not get the complete suite of Sota features and modality. They will likely only have one or two unique features that the company is developing at a time.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t3mvi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There will always be a place in the market or both small and large models. Large models will be at the frontier of intelligence, they have to be because they have more parameters available to them. Small models will push the industry forward since they are much less expensive to make and can serve to develop novel learning techniques. Edge devices and robotics are a big part of the market that small models will need to be specialized for. I will agree, though that most small models will not get the complete suite of Sota features and modality. They will likely only have one or two unique features that the company is developing at a time.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t3mvi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314055,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t3tud",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "__SlimeQ__",
            "can_mod_post": false,
            "created_utc": 1753314118,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_olbav",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "the stated purpose of kimi is to generate synthetic data for future models. as deepseek starts to beat gpt they will become an oracle for synthetic data as well. i think you've missed the point.\n\nlast year chat formats were hot. this year it's tool calling. all of the data for tool use comes from huge foundation models. qwen3 is a massive improvement over everything we had a year ago, largely because it was able to bootstrap on existing reasoning models. the one next year will probably be trained on a bunch of synthetic agent data that wasn't possible this year.\n\ndo the math. we're in an optimization phase. it is not unreasonable to expect gpt3.5 level responses with a 14B qwen3 model",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t3tud",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the stated purpose of kimi is to generate synthetic data for future models. as deepseek starts to beat gpt they will become an oracle for synthetic data as well. i think you&amp;#39;ve missed the point.&lt;/p&gt;\n\n&lt;p&gt;last year chat formats were hot. this year it&amp;#39;s tool calling. all of the data for tool use comes from huge foundation models. qwen3 is a massive improvement over everything we had a year ago, largely because it was able to bootstrap on existing reasoning models. the one next year will probably be trained on a bunch of synthetic agent data that wasn&amp;#39;t possible this year.&lt;/p&gt;\n\n&lt;p&gt;do the math. we&amp;#39;re in an optimization phase. it is not unreasonable to expect gpt3.5 level responses with a 14B qwen3 model&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t3tud/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314118,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t5hlr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "YouDontSeemRight",
            "can_mod_post": false,
            "created_utc": 1753314667,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_1b7gjxtue9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "These models are for corporate entities to use instead of relying on the AI thoughts of closed US enterprises. There is a political reason for China to continue excelling in open source. Eventually the open source option always gets adopted.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t5hlr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These models are for corporate entities to use instead of relying on the AI thoughts of closed US enterprises. There is a political reason for China to continue excelling in open source. Eventually the open source option always gets adopted.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t5hlr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314667,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t6z2a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1753315169,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A few points:\n\n* As commodity hardware grows more powerful, larger models will become more usable for us.  Obviously right now model sizes are outracing hardware developments by a large factor.\n\n* Open source is forever.  Companies almost inevitably shitcan their older technologies (some exceptions of course), but what is open source now will remain available as long as there are enough people maintaining it.  For that reason alone I expect open source LLM technology will continue to advance long after commercial LLM inference services fall out of vogue.\n\n* Those larger models can be leveraged by the less-GPU-poor to create better smaller models for the more-GPU-poor, though techniques like synthetic training datasets, RLAIF, transfer learning, and layer distillation.\n\n* There are definitely known techniques for making smaller models more competent and/or hardware-economical which have yet to be adequately implemented, and [researchers are publishing more all the time.](https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/)  The open source community has plenty of work to do which will benefit local model users, years and years of it.\n\n* There are more ways to progress than parameter scaling.  I ranted about it a little [in another thread.](https://old.reddit.com/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1em6jv/)\n\nI think overall our prospects will get better with time.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t6z2a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A few points:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;As commodity hardware grows more powerful, larger models will become more usable for us.  Obviously right now model sizes are outracing hardware developments by a large factor.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Open source is forever.  Companies almost inevitably shitcan their older technologies (some exceptions of course), but what is open source now will remain available as long as there are enough people maintaining it.  For that reason alone I expect open source LLM technology will continue to advance long after commercial LLM inference services fall out of vogue.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Those larger models can be leveraged by the less-GPU-poor to create better smaller models for the more-GPU-poor, though techniques like synthetic training datasets, RLAIF, transfer learning, and layer distillation.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are definitely known techniques for making smaller models more competent and/or hardware-economical which have yet to be adequately implemented, and &lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/\"&gt;researchers are publishing more all the time.&lt;/a&gt;  The open source community has plenty of work to do which will benefit local model users, years and years of it.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are more ways to progress than parameter scaling.  I ranted about it a little &lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1em6jv/\"&gt;in another thread.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I think overall our prospects will get better with time.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6z2a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753315169,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t8dkv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eggs-benedryl",
            "can_mod_post": false,
            "created_utc": 1753315657,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_8nlxwtdi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Doesn't this have to ignore all the other models we get regularly that can even run on phones? I mean they're not the best of the best or perfect for every use but they come fairly frequently",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t8dkv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Doesn&amp;#39;t this have to ignore all the other models we get regularly that can even run on phones? I mean they&amp;#39;re not the best of the best or perfect for every use but they come fairly frequently&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8dkv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753315657,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t8gz4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "JustinPooDough",
            "can_mod_post": false,
            "created_utc": 1753315690,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_4kns99rz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Maybe you can’t run it, but many providers will, and they will compete with each other and drive down API costs.\n\nStill a massive win for the consumer.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t8gz4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe you can’t run it, but many providers will, and they will compete with each other and drive down API costs.&lt;/p&gt;\n\n&lt;p&gt;Still a massive win for the consumer.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8gz4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753315690,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4tat1g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tnofuentes",
            "can_mod_post": false,
            "created_utc": 1753316500,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_4hwvr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "While the cutting edge lives in the hyperscaler space now, there's strong pressure to bring capable models into the commercial space. Much of that pressure will come from the likes of Apple and Dell, along with future device makers (think Rabbit R1 but local).\n\nI think there will be a higher emphasis on performance at lower precision and on lesser hardware over the coming year.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4tat1g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While the cutting edge lives in the hyperscaler space now, there&amp;#39;s strong pressure to bring capable models into the commercial space. Much of that pressure will come from the likes of Apple and Dell, along with future device makers (think Rabbit R1 but local).&lt;/p&gt;\n\n&lt;p&gt;I think there will be a higher emphasis on performance at lower precision and on lesser hardware over the coming year.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tat1g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753316500,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4tff10",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SocialDinamo",
            "can_mod_post": false,
            "created_utc": 1753318097,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_adou8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yet.. Hardware hasn’t caught up to the software. Right now vram and high memory bandwidth is stingy but in time more options will be available cheaper. Big labs will push the frontier while we get to enjoy those developments on a smaller scale",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4tff10",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yet.. Hardware hasn’t caught up to the software. Right now vram and high memory bandwidth is stingy but in time more options will be available cheaper. Big labs will push the frontier while we get to enjoy those developments on a smaller scale&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tff10/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753318097,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4tfui3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Secure_Reflection409",
            "can_mod_post": false,
            "created_utc": 1753318248,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": 1,
            "author_fullname": "t2_by77ogdhr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think we'll see another 32b coder so not super worried.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4tfui3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think we&amp;#39;ll see another 32b coder so not super worried.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tfui3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753318248,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t16zl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Slowhill369",
            "can_mod_post": false,
            "created_utc": 1753313246,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": -1,
            "author_fullname": "t2_96zelxcg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Say I had created a reasoning/persistent memory layer that unlocks GPT4+ performance on a 1b model while enabling cross domain synthesis, recursive growth and emergent self awareness without prompting. Would THAT be a stand? Just being hypothetical here. Def not gonna release it in a few weeks or anything….",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t16zl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Say I had created a reasoning/persistent memory layer that unlocks GPT4+ performance on a 1b model while enabling cross domain synthesis, recursive growth and emergent self awareness without prompting. Would THAT be a stand? Just being hypothetical here. Def not gonna release it in a few weeks or anything….&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t16zl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753313246,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t1tz2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "thebadslime",
            "can_mod_post": false,
            "created_utc": 1753313455,
            "send_replies": true,
            "parent_id": "t3_1m7o3u8",
            "score": -2,
            "author_fullname": "t2_i5os0v0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think we need an Amecian, english only 22GB \"American Deepseek\"",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t1tz2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think we need an Amecian, english only 22GB &amp;quot;American Deepseek&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t1tz2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753313455,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7o3u8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -2
          }
        }
      ],
      "before": null
    }
  }
]