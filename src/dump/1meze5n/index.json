[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I recently got into open source LLMs,I have now used a lot of models under 4b on my mobile and it runs gemma 2b (4bit medium) or llama 3.2 3b (4b med) reliably on pocketpal app\n\nTotal cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads\n\n1.do less cpu threads degrade the output quality?\n\n2.does it increase the hallucination rate? Most of the time,I m not really looking for longer context than 2k\n\n3.what do lower cpu threads enabled help in?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Question about cpu threads (beginner here)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1meze5n",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_63nhk1l7",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754061823,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got into open source LLMs,I have now used a lot of models under 4b on my mobile and it runs gemma 2b (4bit medium) or llama 3.2 3b (4b med) reliably on pocketpal app&lt;/p&gt;\n\n&lt;p&gt;Total cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads&lt;/p&gt;\n\n&lt;p&gt;1.do less cpu threads degrade the output quality?&lt;/p&gt;\n\n&lt;p&gt;2.does it increase the hallucination rate? Most of the time,I m not really looking for longer context than 2k&lt;/p&gt;\n\n&lt;p&gt;3.what do lower cpu threads enabled help in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1meze5n",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Gold_Bar_4072",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/",
            "subreddit_subscribers": 508772,
            "created_utc": 1754061823,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6dvj24",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Red_Redditor_Reddit",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6dq3n0",
                                                    "score": 1,
                                                    "author_fullname": "t2_8eelmfjg",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "A cheap CPU won't bottleneck I don't think.  Not unless the ram is faster than the CPU I think.\n\n&gt;Enterprise workplaces are all about Microsoft sadly\n\nI don't know how anybody can stand windows... or anything consumerist for that matter.  Back in the 90's and the 2000's, people complained but it at least got the job done.  Like I don't think anything else had anywhere near the backwards compatibility and relative user interface that windows did.\n\nNow from what I hear it's gotten so bad that even the gamers are jumping ship.  I tried windows 11 the other day and even the solitaire game has ads and wants the user to get a subscription.  They try and force you to have an online account and store all your data \"privately\" in the cloud.  Then there's this orwellian recall thing that's just like WTF?  If I had to run windows I probably wouldn't have a computer or just have some junky thing I found for the few times I have to use the internet for something.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6dvj24",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A cheap CPU won&amp;#39;t bottleneck I don&amp;#39;t think.  Not unless the ram is faster than the CPU I think.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Enterprise workplaces are all about Microsoft sadly&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I don&amp;#39;t know how anybody can stand windows... or anything consumerist for that matter.  Back in the 90&amp;#39;s and the 2000&amp;#39;s, people complained but it at least got the job done.  Like I don&amp;#39;t think anything else had anywhere near the backwards compatibility and relative user interface that windows did.&lt;/p&gt;\n\n&lt;p&gt;Now from what I hear it&amp;#39;s gotten so bad that even the gamers are jumping ship.  I tried windows 11 the other day and even the solitaire game has ads and wants the user to get a subscription.  They try and force you to have an online account and store all your data &amp;quot;privately&amp;quot; in the cloud.  Then there&amp;#39;s this orwellian recall thing that&amp;#39;s just like WTF?  If I had to run windows I probably wouldn&amp;#39;t have a computer or just have some junky thing I found for the few times I have to use the internet for something.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1meze5n",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6dvj24/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754069278,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754069278,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6dq3n0",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "AdamDhahabi",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6dnv88",
                                          "score": 1,
                                          "author_fullname": "t2_x5lnbc2",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Enterprise workplaces are all about Microsoft sadly, IT pro's need to handle such systems all day long. Anything server I go for Linux of course.\n\nI did some tests on a 10-core/16-thread consumer CPU i5 13400F and loaded Qwen 235b with heavy DDR5 usage. There is almost no speed gain between 6 threads and 10 threads when using llama.cpp. It makes me think that even a cheap CPU is not bottlenecking but I could be wrong.",
                                          "edited": 1754069692,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6dq3n0",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Enterprise workplaces are all about Microsoft sadly, IT pro&amp;#39;s need to handle such systems all day long. Anything server I go for Linux of course.&lt;/p&gt;\n\n&lt;p&gt;I did some tests on a 10-core/16-thread consumer CPU i5 13400F and loaded Qwen 235b with heavy DDR5 usage. There is almost no speed gain between 6 threads and 10 threads when using llama.cpp. It makes me think that even a cheap CPU is not bottlenecking but I could be wrong.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1meze5n",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6dq3n0/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754067755,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754067755,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6dnv88",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Red_Redditor_Reddit",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6dnauh",
                                "score": 2,
                                "author_fullname": "t2_8eelmfjg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I don't know.  I honestly haven't experimented with the threads since moe models came out.  I probably should since it doesn't take very long.\n\n&gt;I think we need to ignore Windows task manager\n\nLOL It's been twenty years since I've used windows in any meaningful way.  I don't even know how.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6dnv88",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know.  I honestly haven&amp;#39;t experimented with the threads since moe models came out.  I probably should since it doesn&amp;#39;t take very long.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I think we need to ignore Windows task manager&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;LOL It&amp;#39;s been twenty years since I&amp;#39;ve used windows in any meaningful way.  I don&amp;#39;t even know how.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1meze5n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6dnv88/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754067109,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754067109,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6dnauh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AdamDhahabi",
                      "can_mod_post": false,
                      "created_utc": 1754066948,
                      "send_replies": true,
                      "parent_id": "t1_n6d976i",
                      "score": 1,
                      "author_fullname": "t2_x5lnbc2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I was always wondering if that is also the case when heavily using system RAM for fitting large MoE models. Let's say 90 GB (including KV cache) spread over 32GB VRAM + 64 GB DDR5, how busy will your 14900k CPU really be? I think we need to ignore Windows task manager because it will give wrong indication. What do you say? This is important to know so that we spend our money on GPUs instead of expensive CPUs.",
                      "edited": 1754067258,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6dnauh",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was always wondering if that is also the case when heavily using system RAM for fitting large MoE models. Let&amp;#39;s say 90 GB (including KV cache) spread over 32GB VRAM + 64 GB DDR5, how busy will your 14900k CPU really be? I think we need to ignore Windows task manager because it will give wrong indication. What do you say? This is important to know so that we spend our money on GPUs instead of expensive CPUs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meze5n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6dnauh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754066948,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6d976i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Red_Redditor_Reddit",
            "can_mod_post": false,
            "created_utc": 1754062888,
            "send_replies": true,
            "parent_id": "t3_1meze5n",
            "score": 2,
            "author_fullname": "t2_8eelmfjg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Total threads don't reduce quality but they can reduce speed.  More does not equal better, especially if you're using two threads on one core.  Like at home I have a 14900k but I only use maybe eight threads on eight cores.  Anything more and the speed drops drastically.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6d976i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Total threads don&amp;#39;t reduce quality but they can reduce speed.  More does not equal better, especially if you&amp;#39;re using two threads on one core.  Like at home I have a 14900k but I only use maybe eight threads on eight cores.  Anything more and the speed drops drastically.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6d976i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754062888,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meze5n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6hi3ex",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Gold_Bar_4072",
                      "can_mod_post": false,
                      "created_utc": 1754115891,
                      "send_replies": true,
                      "parent_id": "t1_n6dl1t6",
                      "score": 1,
                      "author_fullname": "t2_63nhk1l7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Wow thank you!,I see the models work best at 3 threads enabled and not really any problem with output quality\n\n\nI was wonder why gemma 2b was so slow on my phone,but at 3 three threads,it's now around 5.5 tk/s.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6hi3ex",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow thank you!,I see the models work best at 3 threads enabled and not really any problem with output quality&lt;/p&gt;\n\n&lt;p&gt;I was wonder why gemma 2b was so slow on my phone,but at 3 three threads,it&amp;#39;s now around 5.5 tk/s.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meze5n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6hi3ex/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754115891,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6dl1t6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754066293,
            "send_replies": true,
            "parent_id": "t3_1meze5n",
            "score": 2,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; Total cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads\n\nThat's because you only really have 4 cores.  The \"8 threads\" is really to help things like user inteerfaces or network connections where a thread isn't doing a lot of work and it's mostly performing basic math or moving memory around. LLM inference, however, is a lot of work and relies on 'expensive' parts of the core that are more limited so you basically have 6 threads fighting for 4 compute cores.\n\nI've found llama.cpp (and derivatives) are quite sensitive to one thread being delayed, even if that thread is one of its own.  If you use 3 threads (to leave a core for the OS) then you'll probably run ~2x faster than one thread.  YMMV if you need to use something like `--cpu-mask` to prevent the OS from putting two threads on the same core - it shouldn't, but they aren't always super smart.\n\n&gt; 1.do less cpu threads degrade the output quality?\n&gt;\n&gt; 2.does it increase the hallucination rate?\n\nModels are just a bunch of math, and while the results can change _very_ slightly based on how you split and order operations, it's hyper unlikely that it'll create any noticeable effect.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dl1t6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Total cpu threads on my device is 8 (4 core),when I enable 1 cpu thread the 2b model generates around 3 times faster tk/s than at 6 cpu threads&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That&amp;#39;s because you only really have 4 cores.  The &amp;quot;8 threads&amp;quot; is really to help things like user inteerfaces or network connections where a thread isn&amp;#39;t doing a lot of work and it&amp;#39;s mostly performing basic math or moving memory around. LLM inference, however, is a lot of work and relies on &amp;#39;expensive&amp;#39; parts of the core that are more limited so you basically have 6 threads fighting for 4 compute cores.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found llama.cpp (and derivatives) are quite sensitive to one thread being delayed, even if that thread is one of its own.  If you use 3 threads (to leave a core for the OS) then you&amp;#39;ll probably run ~2x faster than one thread.  YMMV if you need to use something like &lt;code&gt;--cpu-mask&lt;/code&gt; to prevent the OS from putting two threads on the same core - it shouldn&amp;#39;t, but they aren&amp;#39;t always super smart.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;1.do less cpu threads degrade the output quality?&lt;/p&gt;\n\n&lt;p&gt;2.does it increase the hallucination rate?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Models are just a bunch of math, and while the results can change &lt;em&gt;very&lt;/em&gt; slightly based on how you split and order operations, it&amp;#39;s hyper unlikely that it&amp;#39;ll create any noticeable effect.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6dl1t6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754066293,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meze5n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6f4j40",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754082510,
            "send_replies": true,
            "parent_id": "t3_1meze5n",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "1. No.\n\n2. No.\n\n3. Not using all of your cores for inference helps you use your computer for other things, because other programs can use the cores not being used for inference.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6f4j40",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;No.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;No.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Not using all of your cores for inference helps you use your computer for other things, because other programs can use the cores not being used for inference.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meze5n/question_about_cpu_threads_beginner_here/n6f4j40/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754082510,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1meze5n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]