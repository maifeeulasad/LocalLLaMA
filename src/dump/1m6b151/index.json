[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "A while back I posted some [Strix Halo LLM performance testing](https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/) benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).\n\nThe biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.\n\nThis is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp \n\nAll the full data and latest info is available in the Github repo: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench) but here are the topline stats below:\n\n# Strix Halo LLM Benchmark Results\n\nAll testing was done on pre-production [Framework Desktop](https://frame.work/desktop) systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)\n\nExact testing/system details are in the results folders, but roughly these are running:\n\n* Close to production BIOS/EC\n* Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1\n* Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels\n* Recent llama.cpp builds (eg b5863 from 2005-07-10)\n\nJust to get a ballpark on the hardware:\n\n* \\~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)\n* theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is *much* lower\n\n# Results\n\n# Prompt Processing (pp) Performance\n\nhttps://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan||998.0|46.5|4237|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|HIP|hipBLASLt|906.1|40.8|4720|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|HIP|hipBLASLt|878.2|37.2|5308|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|fa=1|604.8|66.3|17527|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|HIP|hipBLASLt|316.9|13.6|14638|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1|270.5|17.1|68785|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|HIP|hipBLASLt|264.1|17.2|59720|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|HIP rocWMMA||94.7|4.5|41522|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n\n# Text Generation (tg) Performance\n\nhttps://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|b=256|591.1|72.0|17377|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|Vulkan|fa=1|620.9|47.9|4463|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan|fa=1|1014.1|45.8|4219|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|Vulkan|fa=1|614.2|42.0|5333|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|Vulkan|fa=1 b=256|146.1|19.3|59917|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1 b=256|223.9|17.1|68608|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|Vulkan|fa=1|119.6|14.3|14540|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|Vulkan|fa=1|26.4|5.0|41456|\n\n# Testing Notes\n\nThe best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.\n\nThere's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).\n\nOne thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.\n\nUnlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).\n\nFor testing, the HIP backend, I highly recommend trying `ROCBLAS_USE_HIPBLASLT=1` as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) `HSA_OVERRIDE_GFX_VERSION=11.0.0` \\- in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... ðŸ¤”",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 92,
            "top_awarded_type": null,
            "hide_score": true,
            "media_metadata": {
              "mjr2d31ujeef1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 71,
                    "x": 108,
                    "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80836e3346fcd0e6847fcb3f1d33c5f2ac3c12e3"
                  },
                  {
                    "y": 143,
                    "x": 216,
                    "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cd34de136a42ad65ac9facd37455912c1a13410"
                  },
                  {
                    "y": 212,
                    "x": 320,
                    "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=675c17530bfdbbd8ae2830c3b694e07b5163a1b0"
                  },
                  {
                    "y": 424,
                    "x": 640,
                    "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bdc9a2d7260d9fd3acbab23e12917b54e651493"
                  },
                  {
                    "y": 636,
                    "x": 960,
                    "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e051180cc70357e18ffe4725601fb9811ba1193"
                  },
                  {
                    "y": 715,
                    "x": 1080,
                    "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75e938dac2457cc4191363a5cd8cbcaf777049ed"
                  }
                ],
                "s": {
                  "y": 1181,
                  "x": 1782,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"
                },
                "id": "mjr2d31ujeef1"
              },
              "7y0pdbqujeef1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 71,
                    "x": 108,
                    "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=493fe8a11d9ee1d7a485e1b7233ca7e945637599"
                  },
                  {
                    "y": 143,
                    "x": 216,
                    "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c43022989f8b24ef64216d0b17642e3f80013763"
                  },
                  {
                    "y": 212,
                    "x": 320,
                    "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7f683ec0155e0b8f196a71bccaf428b44550399"
                  },
                  {
                    "y": 424,
                    "x": 640,
                    "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb65c24153d597ae592f5012aabff2a638b88357"
                  },
                  {
                    "y": 636,
                    "x": 960,
                    "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f57d68c01ba672fba3a5062a2ec12ac45a621fac"
                  },
                  {
                    "y": 715,
                    "x": 1080,
                    "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59b563c8992d2568941d7c73829b8f7ebc4f5585"
                  }
                ],
                "s": {
                  "y": 1181,
                  "x": 1782,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"
                },
                "id": "7y0pdbqujeef1"
              }
            },
            "name": "t3_1m6b151",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 19,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_eztox",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 19,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753182004,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while back I posted some &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/\"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I&amp;#39;m back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt;\n\n&lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt;\n\n&lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt;\n\n&lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href=\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt;\n\n&lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt;\n\n&lt;p&gt;All testing was done on pre-production &lt;a href=\"https://frame.work/desktop\"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt;\n\n&lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Close to production BIOS/EC&lt;/li&gt;\n&lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt;\n&lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt;\n&lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt;\n&lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Results&lt;/h1&gt;\n\n&lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;998.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;4237&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;906.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;40.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;878.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;5308&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;66.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;17527&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;316.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14638&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;270.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68785&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;264.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;59720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP rocWMMA&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;94.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;41522&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;591.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;72.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17377&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;620.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;47.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4463&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1014.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;45.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4219&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;614.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;5333&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;146.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;59917&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;223.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68608&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;119.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;14540&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26.4&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;41456&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Testing Notes&lt;/h1&gt;\n\n&lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#&amp;#39;s might be a bit much).&lt;/p&gt;\n\n&lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it&amp;#39;s now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt;\n\n&lt;p&gt;Unlike last time, I won&amp;#39;t be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt;\n\n&lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I&amp;#39;ve found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... ðŸ¤”&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1m6b151",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "randomfoo2",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
            "subreddit_subscribers": 502720,
            "created_utc": 1753182004,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4i8c2h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "randomfoo2",
            "can_mod_post": false,
            "created_utc": 1753183291,
            "send_replies": true,
            "parent_id": "t3_1m6b151",
            "score": 3,
            "author_fullname": "t2_eztox",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For those interested in tracking gfx1100 vs gfx1151 kernel performance regressions: [https://github.com/ROCm/ROCm/issues/4748](https://github.com/ROCm/ROCm/issues/4748)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4i8c2h",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For those interested in tracking gfx1100 vs gfx1151 kernel performance regressions: &lt;a href=\"https://github.com/ROCm/ROCm/issues/4748\"&gt;https://github.com/ROCm/ROCm/issues/4748&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i8c2h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753183291,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6b151",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4ifz2c",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AdamDhahabi",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4ib5lo",
                                "score": 1,
                                "author_fullname": "t2_x5lnbc2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Thanks for the link, 128GB barebone is indeed quoted \\~2000$. In euro currency 2500â‚¬ (including cheapest NVMe) which is 2900$. I guess because of EU VAT.",
                                "edited": 1753186688,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4ifz2c",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the link, 128GB barebone is indeed quoted ~2000$. In euro currency 2500â‚¬ (including cheapest NVMe) which is 2900$. I guess because of EU VAT.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m6b151",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ifz2c/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753186391,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753186391,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4ib5lo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "uti24",
                      "can_mod_post": false,
                      "created_utc": 1753184487,
                      "send_replies": true,
                      "parent_id": "t1_n4i8iu2",
                      "score": 2,
                      "author_fullname": "t2_13hbro",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "All Ryzen AI Max+ 395 computers has more or less same prise, because you can not change CPU or RAM\n\n128GB ram setup cost \\~2000$\n\n[https://frame.work/products/desktop-diy-amd-aimax300/configuration/new](https://frame.work/products/desktop-diy-amd-aimax300/configuration/new)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4ib5lo",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;All Ryzen AI Max+ 395 computers has more or less same prise, because you can not change CPU or RAM&lt;/p&gt;\n\n&lt;p&gt;128GB ram setup cost ~2000$&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://frame.work/products/desktop-diy-amd-aimax300/configuration/new\"&gt;https://frame.work/products/desktop-diy-amd-aimax300/configuration/new&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m6b151",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ib5lo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753184487,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4i8iu2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1753183372,
            "send_replies": true,
            "parent_id": "t3_1m6b151",
            "score": 3,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That's quite good, how much dollars would such a setup cost?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4i8iu2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s quite good, how much dollars would such a setup cost?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i8iu2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753183372,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6b151",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4i6rta",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "randomfoo2",
                      "can_mod_post": false,
                      "created_utc": 1753182597,
                      "send_replies": true,
                      "parent_id": "t1_n4i6b3m",
                      "score": 4,
                      "author_fullname": "t2_eztox",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "There are no issues w/ different sized quants, but Q3/Q4 XLs are just IMO the sweet spot for perf (accuracy/speed). As you can see, your tg is closely tied to your weight size, so you can just divide by 2 or 4 if you want an idea of how fast a Q8 or FP16 will inference.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4i6rta",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There are no issues w/ different sized quants, but Q3/Q4 XLs are just IMO the sweet spot for perf (accuracy/speed). As you can see, your tg is closely tied to your weight size, so you can just divide by 2 or 4 if you want an idea of how fast a Q8 or FP16 will inference.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m6b151",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i6rta/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753182597,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4i6b3m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Jotschi",
            "can_mod_post": false,
            "created_utc": 1753182386,
            "send_replies": true,
            "parent_id": "t3_1m6b151",
            "score": 2,
            "author_fullname": "t2_xsvaa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thanks for listing exact version info you used. Side question: is there are reason why so many q4 were used? Does q8 or fp16 cause issues?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4i6b3m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for listing exact version info you used. Side question: is there are reason why so many q4 were used? Does q8 or fp16 cause issues?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i6b3m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753182386,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6b151",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4ierbl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "spaceman_",
            "can_mod_post": false,
            "created_utc": 1753185920,
            "send_replies": true,
            "parent_id": "t3_1m6b151",
            "score": 2,
            "author_fullname": "t2_9neub",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thanks for this! I'm currently running the 395 w/64GB memory using llama.cpp and the Vulkan backend, and I'm eager to get this better performance. Are there any instructions on how to install rocm 7 nightlies anywhere I can follow?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4ierbl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for this! I&amp;#39;m currently running the 395 w/64GB memory using llama.cpp and the Vulkan backend, and I&amp;#39;m eager to get this better performance. Are there any instructions on how to install rocm 7 nightlies anywhere I can follow?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ierbl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753185920,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6b151",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4ibzvg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "uti24",
            "can_mod_post": false,
            "created_utc": 1753184826,
            "send_replies": true,
            "parent_id": "t3_1m6b151",
            "score": 1,
            "author_fullname": "t2_13hbro",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thank you for the detailed benchmarks.  \nIt actually looks pretty reasonable. So, for a budget build, you either tinker with multiple used 3090s or just take this.  \nBy the way, can this system support something like OcuLink or USB4 for an external GPU? People say you can improve MOE speed like 2 times with just a single GPU.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4ibzvg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for the detailed benchmarks.&lt;br/&gt;\nIt actually looks pretty reasonable. So, for a budget build, you either tinker with multiple used 3090s or just take this.&lt;br/&gt;\nBy the way, can this system support something like OcuLink or USB4 for an external GPU? People say you can improve MOE speed like 2 times with just a single GPU.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ibzvg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753184826,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6b151",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4iecuh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Zyguard7777777",
            "can_mod_post": false,
            "created_utc": 1753185760,
            "send_replies": true,
            "parent_id": "t3_1m6b151",
            "score": 1,
            "author_fullname": "t2_zo1h5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I look forward to the hybrid pp using both igpu and npu, should increase pp significantly",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4iecuh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I look forward to the hybrid pp using both igpu and npu, should increase pp significantly&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iecuh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753185760,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6b151",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]