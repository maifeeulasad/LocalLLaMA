[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I am very attracted to the idea of using server hardware for llms, since 16 channel ddr4 memory will give 400gb/s worth of bandwidth.\n\nHowever, one thing that keeps popping up when researching is pcie bandwidth being an issue\n\nLogically, it does make sense, since pcie 4.0x16 gives 32gb/s, way too little for llms, not to mention the latency.\n\nBut when I look up actual results, this doesn’t seem to be the case at all\n\nI am so confused on this matter, how does the pcie bandwidth affect the use of system ram, and a secondary gpu?\n\nIn this context, at least one gpu is being used",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "System Ram Speed Importance when using GPU",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mb7gxu",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rn6co7q5m",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753679731,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753679084,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am very attracted to the idea of using server hardware for llms, since 16 channel ddr4 memory will give 400gb/s worth of bandwidth.&lt;/p&gt;\n\n&lt;p&gt;However, one thing that keeps popping up when researching is pcie bandwidth being an issue&lt;/p&gt;\n\n&lt;p&gt;Logically, it does make sense, since pcie 4.0x16 gives 32gb/s, way too little for llms, not to mention the latency.&lt;/p&gt;\n\n&lt;p&gt;But when I look up actual results, this doesn’t seem to be the case at all&lt;/p&gt;\n\n&lt;p&gt;I am so confused on this matter, how does the pcie bandwidth affect the use of system ram, and a secondary gpu?&lt;/p&gt;\n\n&lt;p&gt;In this context, at least one gpu is being used&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mb7gxu",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "opoot_",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/",
            "subreddit_subscribers": 505879,
            "created_utc": 1753679084,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5k52it",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "randomqhacker",
            "can_mod_post": false,
            "created_utc": 1753679759,
            "send_replies": true,
            "parent_id": "t3_1mb7gxu",
            "score": 6,
            "author_fullname": "t2_4nw3v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If your GPU were set to use system RAM for overflow, sure, it would slow you down majorly. But for llama.cpp splitting a model between GPU and CPU, each does computation with its own local VRAM or RAM. The weights don't have to cross the PCIe bus, just the intermediate state which is relatively small.\n\n\nSystem RAM speed is still important if you have weights in RAM, so having a server motherboard might still be useful as a host if it gives you more RAM channels.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k52it",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If your GPU were set to use system RAM for overflow, sure, it would slow you down majorly. But for llama.cpp splitting a model between GPU and CPU, each does computation with its own local VRAM or RAM. The weights don&amp;#39;t have to cross the PCIe bus, just the intermediate state which is relatively small.&lt;/p&gt;\n\n&lt;p&gt;System RAM speed is still important if you have weights in RAM, so having a server motherboard might still be useful as a host if it gives you more RAM channels.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5k52it/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753679759,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb7gxu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5kh3t7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Threatening-Silence-",
                      "can_mod_post": false,
                      "created_utc": 1753686281,
                      "send_replies": true,
                      "parent_id": "t1_n5kefa0",
                      "score": 1,
                      "author_fullname": "t2_15wqsifdjf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;dual socket underperforms versus single socket.\n\nThis is because llama-cpp initialises memory on the main thread and inadvertently pins all buffers to the main thread numa.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5kh3t7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;dual socket underperforms versus single socket.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is because llama-cpp initialises memory on the main thread and inadvertently pins all buffers to the main thread numa.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb7gxu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5kh3t7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753686281,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5klesv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "opoot_",
                      "can_mod_post": false,
                      "created_utc": 1753688769,
                      "send_replies": true,
                      "parent_id": "t1_n5kefa0",
                      "score": 1,
                      "author_fullname": "t2_rn6co7q5m",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ah I meant dual cpu systems that would give 2 8 channel things, I didn’t know there was a difference.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5klesv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah I meant dual cpu systems that would give 2 8 channel things, I didn’t know there was a difference.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb7gxu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5klesv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753688769,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5kefa0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753684811,
            "send_replies": true,
            "parent_id": "t3_1mb7gxu",
            "score": 3,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Imagine you have a vector of size 10k.  You multiply it by a 10k x 10k matrix (100 million values!) and get another size 10k vector.  You need a lot of bandwidth to read the matrix, but the vector is of little matter.  This is a super simplified version of what happens in an LLM.  You need a lot of memory bandwidth to read all the matrices from the model, but the actual state you operate on is comparatively small.  The important thing to note is that the matrices are _constant_.\n\nSo, if you're running the whole thing on the GPU, then you only need to transfer the GBs of data to the GPU once at the start, and then it's in the GPU's memory and the PCIe bandwidth doesn't matter - just the bandwidth of the GPU memory to the GPU itself.  The CPU basically just sends it some small state and gets back a small state for each token generated while all the high bandwidth work happens on the GPU. \n\nThe reason you might want more memory bandwidth is if you _don't_ (entirely) use a GPU and instead use the CPU to run the model.  Now you need to read all those large matrices from RAM to the CPU to process, much like the GPU needs to read the matrices from VRAM to process.  Here memory bandwidth is critical, but again PCIe bandwidth is not... At most you transfer a small state between the CPU and GPU, but nothing on the order of a GB or anything close.  I guess to be clear: the processing in this 'mode' happens on the CPU.  CPUs are plenty adequate at math, and while a GPU would be faster the PCIe link would indeed make sreaming the model to the GPU for processing terribly slow.\n\n&gt; since 16 channel ddr4 memory will give 400gb/s worth of bandwidth.\n\nI'll call out that there is no 16ch DDR4 CPU, just dual 8ch/socket systems and dual socket underperforms versus single socket.  I'm not sure what the current numbers are since people are working on fixing it somewhat, but I'd say expect the second socket to only improve CPU inference by about 50% rather than doubling it.\n\nP.S. PCIe bandwidth can matter a lot in model training because there the state is _also_ giant matrices.  However, that's not going to matter for you.",
            "edited": 1753685108,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kefa0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Imagine you have a vector of size 10k.  You multiply it by a 10k x 10k matrix (100 million values!) and get another size 10k vector.  You need a lot of bandwidth to read the matrix, but the vector is of little matter.  This is a super simplified version of what happens in an LLM.  You need a lot of memory bandwidth to read all the matrices from the model, but the actual state you operate on is comparatively small.  The important thing to note is that the matrices are &lt;em&gt;constant&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;So, if you&amp;#39;re running the whole thing on the GPU, then you only need to transfer the GBs of data to the GPU once at the start, and then it&amp;#39;s in the GPU&amp;#39;s memory and the PCIe bandwidth doesn&amp;#39;t matter - just the bandwidth of the GPU memory to the GPU itself.  The CPU basically just sends it some small state and gets back a small state for each token generated while all the high bandwidth work happens on the GPU. &lt;/p&gt;\n\n&lt;p&gt;The reason you might want more memory bandwidth is if you &lt;em&gt;don&amp;#39;t&lt;/em&gt; (entirely) use a GPU and instead use the CPU to run the model.  Now you need to read all those large matrices from RAM to the CPU to process, much like the GPU needs to read the matrices from VRAM to process.  Here memory bandwidth is critical, but again PCIe bandwidth is not... At most you transfer a small state between the CPU and GPU, but nothing on the order of a GB or anything close.  I guess to be clear: the processing in this &amp;#39;mode&amp;#39; happens on the CPU.  CPUs are plenty adequate at math, and while a GPU would be faster the PCIe link would indeed make sreaming the model to the GPU for processing terribly slow.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;since 16 channel ddr4 memory will give 400gb/s worth of bandwidth.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;ll call out that there is no 16ch DDR4 CPU, just dual 8ch/socket systems and dual socket underperforms versus single socket.  I&amp;#39;m not sure what the current numbers are since people are working on fixing it somewhat, but I&amp;#39;d say expect the second socket to only improve CPU inference by about 50% rather than doubling it.&lt;/p&gt;\n\n&lt;p&gt;P.S. PCIe bandwidth can matter a lot in model training because there the state is &lt;em&gt;also&lt;/em&gt; giant matrices.  However, that&amp;#39;s not going to matter for you.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5kefa0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753684811,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb7gxu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5kczkp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "randomqhacker",
                      "can_mod_post": false,
                      "created_utc": 1753684019,
                      "send_replies": true,
                      "parent_id": "t1_n5kbex4",
                      "score": 1,
                      "author_fullname": "t2_4nw3v",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Isn't the low GPU utilization due to the GPU waiting on the CPU before it can go on to the next token?  How will you optimize that?  (Aside from a draft model.)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5kczkp",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t the low GPU utilization due to the GPU waiting on the CPU before it can go on to the next token?  How will you optimize that?  (Aside from a draft model.)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb7gxu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5kczkp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753684019,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5l7wg0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MelodicRecognition7",
                      "can_mod_post": false,
                      "created_utc": 1753701227,
                      "send_replies": true,
                      "parent_id": "t1_n5kbex4",
                      "score": 1,
                      "author_fullname": "t2_1eex9ug5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;  after fixing llama.cpp's processor affinity code which is broken.\n\ncould you share the patch please?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5l7wg0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;after fixing llama.cpp&amp;#39;s processor affinity code which is broken.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;could you share the patch please?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb7gxu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5l7wg0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753701227,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5kbex4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Guilty-History-9249",
            "can_mod_post": false,
            "created_utc": 1753683146,
            "send_replies": true,
            "parent_id": "t3_1mb7gxu",
            "score": 2,
            "author_fullname": "t2_rxk6hx4t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have an 8 channel 7985WX with 256 GB's of DDR5-6000 memory.  I just tested a 32B model at Q8 and got 8.3 tokens/sec after fixing llama.cpp's processor affinity code which is broken.  \nI also have dual 5090's in it and splitting this 32B model across both GPU gets me 37.5 tokens/sec.  I have some optimizations still to do given I'm getting less that 50% GPU utilization.\n\nI still need to pick one of the 200+ B MOE models to try.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kbex4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have an 8 channel 7985WX with 256 GB&amp;#39;s of DDR5-6000 memory.  I just tested a 32B model at Q8 and got 8.3 tokens/sec after fixing llama.cpp&amp;#39;s processor affinity code which is broken.&lt;br/&gt;\nI also have dual 5090&amp;#39;s in it and splitting this 32B model across both GPU gets me 37.5 tokens/sec.  I have some optimizations still to do given I&amp;#39;m getting less that 50% GPU utilization.&lt;/p&gt;\n\n&lt;p&gt;I still need to pick one of the 200+ B MOE models to try.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5kbex4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753683146,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb7gxu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5khk9u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BoxedInn",
            "can_mod_post": false,
            "created_utc": 1753686533,
            "send_replies": true,
            "parent_id": "t3_1mb7gxu",
            "score": 2,
            "author_fullname": "t2_2i5iysmz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thank you for this",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5khk9u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for this&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5khk9u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753686533,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb7gxu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5kpq8i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GregoryfromtheHood",
            "can_mod_post": false,
            "created_utc": 1753691300,
            "send_replies": true,
            "parent_id": "t3_1mb7gxu",
            "score": 1,
            "author_fullname": "t2_g0qor",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "PCIE bandwidth doesn't really matter for inference. I run a 3090 on PCIE 3.0 x4 and it does just fine.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kpq8i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;PCIE bandwidth doesn&amp;#39;t really matter for inference. I run a 3090 on PCIE 3.0 x4 and it does just fine.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5kpq8i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753691300,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb7gxu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5kq99r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1753691614,
            "send_replies": true,
            "parent_id": "t3_1mb7gxu",
            "score": 1,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "RAM speed is only important when your VRAM is too small.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kq99r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;RAM speed is only important when your VRAM is too small.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb7gxu/system_ram_speed_importance_when_using_gpu/n5kq99r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753691614,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mb7gxu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]