[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It’s been an eternity since is early preview… and now I wonder if it is doomed. Not just because it’s slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Is EXL3 doomed?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 70,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgl1qz",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "ups": 25,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_dissgzyl",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 25,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=989197fdb000ae3849fd70a2dd3363c36e2714e5",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754232324,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "github.com",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was very excited for the release of EXL3 because of its increased performance and revised design to support new models easier. It’s been an eternity since is early preview… and now I wonder if it is doomed. Not just because it’s slow to release, but because models are moving towards large MoEs that all but require they spill over into RAM for most of us. Still, we are getting models around 32b. So what do you think? Or what do you know? Is it on its way? Will it still be helpful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://github.com/turboderp-org/exllamav3",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?auto=webp&amp;s=3ecca6d81c2fa96f4b86230a6c6416f82eb4d520",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f93311c0ef06e6cbd0115381f5af07f2cf7c6763",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4328d8b4f520fbe8256ab167da238bdf76b974d",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ee0597232a29c03129624c370183e8c020fce3e",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=778afd8299a6ccb54136a78390cc8473e58bebed",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bad6d74304971cdf123c7601152d5c744927cbd",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9314e4d7ebbf51c941f82bcfbff6bd6a8d85a358",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "FXRrg4PULA4HvXvU8478ZHA7JbZ5sZwNQBeL67rZtHI"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mgl1qz",
            "is_robot_indexable": true,
            "num_duplicates": 2,
            "report_reasons": null,
            "author": "silenceimpaired",
            "discussion_type": null,
            "num_comments": 41,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/",
            "stickied": false,
            "url": "https://github.com/turboderp-org/exllamav3",
            "subreddit_subscribers": 509625,
            "created_utc": 1754232324,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6r3wcu",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "DungeonMasterSupreme",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qxik4",
                                "score": 3,
                                "author_fullname": "t2_5nzsbef2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Pretty sure it's mostly just [TabbyAPI](https://github.com/theroyallab/tabbyAPI).",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6r3wcu",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pretty sure it&amp;#39;s mostly just &lt;a href=\"https://github.com/theroyallab/tabbyAPI\"&gt;TabbyAPI&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r3wcu/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754252317,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754252317,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6qxik4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "c-rious",
                      "can_mod_post": false,
                      "created_utc": 1754250332,
                      "send_replies": true,
                      "parent_id": "t1_n6pphzd",
                      "score": 5,
                      "author_fullname": "t2_mjqap",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Been out of the loop for a while - care to share which backends allow for easy self hosting an openai compatible server with exl3?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6qxik4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Been out of the loop for a while - care to share which backends allow for easy self hosting an openai compatible server with exl3?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6qxik4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754250332,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6pphzd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "MoodyPurples",
            "can_mod_post": false,
            "created_utc": 1754236860,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 20,
            "author_fullname": "t2_z9r1dr6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Exllama3 is already the main way I run models up to the 235B Qwen models, aka 95% of what I run. It’s just so much faster that I think it will have a place regardless of the fact that llama.cpp is more popular. I have both setup through llama-swap so it’s also not like you actually need to just stick with one.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pphzd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Exllama3 is already the main way I run models up to the 235B Qwen models, aka 95% of what I run. It’s just so much faster that I think it will have a place regardless of the fact that llama.cpp is more popular. I have both setup through llama-swap so it’s also not like you actually need to just stick with one.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pphzd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754236860,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 20
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6r4lep",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "VoidAlchemy",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6prrgh",
                                "score": 2,
                                "author_fullname": "t2_n321yfw5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You can use TabbyAPI as the front end for exllamav3, but yeah not quite as easy for the masses as kcpp / ollama.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6r4lep",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can use TabbyAPI as the front end for exllamav3, but yeah not quite as easy for the masses as kcpp / ollama.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r4lep/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754252525,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754252525,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6prrgh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "silenceimpaired",
                      "can_mod_post": false,
                      "created_utc": 1754237557,
                      "send_replies": true,
                      "parent_id": "t1_n6pqx5o",
                      "score": 4,
                      "author_fullname": "t2_dissgzyl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That has also been on my mind. It feels less accessible. Less noticed these days. Hopefully EXL3 brings good tools to convert easily for those who aren’t very technically minded. I also wish there was a front end as easy to run as KoboldCPP or Ollama.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6prrgh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That has also been on my mind. It feels less accessible. Less noticed these days. Hopefully EXL3 brings good tools to convert easily for those who aren’t very technically minded. I also wish there was a front end as easy to run as KoboldCPP or Ollama.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6prrgh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754237557,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6qz4ac",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "randomanoni",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6quh1a",
                                "score": 2,
                                "author_fullname": "t2_tmyziykn",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Skill issue.\nNo but seriously check out the example scripts and post your specs and benchmark results.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6qz4ac",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Skill issue.\nNo but seriously check out the example scripts and post your specs and benchmark results.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6qz4ac/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754250839,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754250839,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6quh1a",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Writer_IT",
                      "can_mod_post": false,
                      "created_utc": 1754249354,
                      "send_replies": true,
                      "parent_id": "t1_n6pqx5o",
                      "score": 4,
                      "author_fullname": "t2_7cfzip6e",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I had religiously used only exl2 from its implementation (It was WAY faster than gguf when either was loaded to vram) till exl3. Then i went to exl3 but for some reasons felt.. wonky. Slower than It should, caused me some bugs with oobabooga, many time i downloaded exl3 quants that simply didn't work, with no idea why.\nAnd never felt the promised intelligence boost for quantization.\n\nThen i tried gguf again after a lot of time, and It was blazingly fast, no particolar issues, easy vision implementation with koboldcpp.\n\nI don't know what specifically, but i think something went REALLY wrong in the exl3 implementation, unfortunately. Still hope It can become again faster than gguf.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6quh1a",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I had religiously used only exl2 from its implementation (It was WAY faster than gguf when either was loaded to vram) till exl3. Then i went to exl3 but for some reasons felt.. wonky. Slower than It should, caused me some bugs with oobabooga, many time i downloaded exl3 quants that simply didn&amp;#39;t work, with no idea why.\nAnd never felt the promised intelligence boost for quantization.&lt;/p&gt;\n\n&lt;p&gt;Then i tried gguf again after a lot of time, and It was blazingly fast, no particolar issues, easy vision implementation with koboldcpp.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what specifically, but i think something went REALLY wrong in the exl3 implementation, unfortunately. Still hope It can become again faster than gguf.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6quh1a/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754249354,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6r4oo2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "VoidAlchemy",
                      "can_mod_post": false,
                      "created_utc": 1754252552,
                      "send_replies": true,
                      "parent_id": "t1_n6pqx5o",
                      "score": 1,
                      "author_fullname": "t2_n321yfw5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "There are some quant cookers releasing EXL3 like [https://huggingface.co/ArtusDev](https://huggingface.co/ArtusDev)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6r4oo2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There are some quant cookers releasing EXL3 like &lt;a href=\"https://huggingface.co/ArtusDev\"&gt;https://huggingface.co/ArtusDev&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r4oo2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754252552,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6q867y",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "FieldProgrammable",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6pvxpi",
                                "score": 6,
                                "author_fullname": "t2_moet0t",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "In terms of awq, definitely, these are pretty similar to GPTQ in their limitations. The introduction of imatrix ggufs allowed it to surpass exl2 in quality at the lower bits per weight. The same can't be currently said for exl3 which as shown by [turboderp's tests](https://github.com/turboderp-org/exllamav3/blob/master/doc/exl3.md) is generally superior to gguf.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6q867y",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In terms of awq, definitely, these are pretty similar to GPTQ in their limitations. The introduction of imatrix ggufs allowed it to surpass exl2 in quality at the lower bits per weight. The same can&amp;#39;t be currently said for exl3 which as shown by &lt;a href=\"https://github.com/turboderp-org/exllamav3/blob/master/doc/exl3.md\"&gt;turboderp&amp;#39;s tests&lt;/a&gt; is generally superior to gguf.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6q867y/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754242514,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754242514,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6r5g82",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "VoidAlchemy",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6pvxpi",
                                "score": 3,
                                "author_fullname": "t2_n321yfw5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "GGUF is simply a file format which is able to hold a variety of quantization types. For example, ik\\_llama.cpp offers KT quants in GGUF format which are similar to EXL3 in they are based on the QTIP Trellis style quantization. These KT quants can run on CPU as well, but token generation then becomes CPU limited instead of memory bandwidth limited generally due to the overhead of calculating trellis on CPU. ik\\_llama.cpp has other newer SOTA quantization types as well, the IQ4\\_KSS is quite nice and I have released the recent Qwen models with that size available and perplexity graphs showing performance vs size.\n\nSo its not all or nothing and exllamav3, turoboderp, and folks working on those repos effect each other and cross pollinate ideas which help the whole community and push that pareto curve downwards so we can run better models in less VRAM/RAM.\n\nWild times!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6r5g82",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GGUF is simply a file format which is able to hold a variety of quantization types. For example, ik_llama.cpp offers KT quants in GGUF format which are similar to EXL3 in they are based on the QTIP Trellis style quantization. These KT quants can run on CPU as well, but token generation then becomes CPU limited instead of memory bandwidth limited generally due to the overhead of calculating trellis on CPU. ik_llama.cpp has other newer SOTA quantization types as well, the IQ4_KSS is quite nice and I have released the recent Qwen models with that size available and perplexity graphs showing performance vs size.&lt;/p&gt;\n\n&lt;p&gt;So its not all or nothing and exllamav3, turoboderp, and folks working on those repos effect each other and cross pollinate ideas which help the whole community and push that pareto curve downwards so we can run better models in less VRAM/RAM.&lt;/p&gt;\n\n&lt;p&gt;Wild times!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r5g82/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754252780,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754252780,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6pvxpi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Secure_Reflection409",
                      "can_mod_post": false,
                      "created_utc": 1754238840,
                      "send_replies": true,
                      "parent_id": "t1_n6pqx5o",
                      "score": 1,
                      "author_fullname": "t2_by77ogdhr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is it fair to suggest gguf quants used to be super basic but have caught up with exl / awq, etc?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6pvxpi",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is it fair to suggest gguf quants used to be super basic but have caught up with exl / awq, etc?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pvxpi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754238840,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6pqx5o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1754237300,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 13,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For some reason all models are converted to gguf by community but I don't see exl2 or exl3 formats used on HF",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pqx5o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For some reason all models are converted to gguf by community but I don&amp;#39;t see exl2 or exl3 formats used on HF&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pqx5o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754237300,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 13
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6r6r8q",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "silenceimpaired",
                      "can_mod_post": false,
                      "created_utc": 1754253171,
                      "send_replies": true,
                      "parent_id": "t1_n6qkr6h",
                      "score": 1,
                      "author_fullname": "t2_dissgzyl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I know! I absolutely love EXL2 and want to see EXL3 succeed. It’s frustrating so few front ends support it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6r6r8q",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I know! I absolutely love EXL2 and want to see EXL3 succeed. It’s frustrating so few front ends support it.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r6r8q/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754253171,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6qkr6h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1754246323,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 8,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's sad because you don't realize how terrible the performance of llama.cpp and gguf is until you try exllamav3 or vllm. Literal 10x the speed sometimes. Llama.cpp is good to run single queries in your desktop/notebook and that's it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qkr6h",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s sad because you don&amp;#39;t realize how terrible the performance of llama.cpp and gguf is until you try exllamav3 or vllm. Literal 10x the speed sometimes. Llama.cpp is good to run single queries in your desktop/notebook and that&amp;#39;s it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6qkr6h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754246323,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6rpjou",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "a_slay_nub",
                      "can_mod_post": false,
                      "created_utc": 1754259098,
                      "send_replies": true,
                      "parent_id": "t1_n6pi0e4",
                      "score": 1,
                      "author_fullname": "t2_u8o4d",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "There is a huge graveyard of projects from the early days of Llama 1 that just fell off the map. As an aside, how the hell is Aphrodite engine still alive?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6rpjou",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is a huge graveyard of projects from the early days of Llama 1 that just fell off the map. As an aside, how the hell is Aphrodite engine still alive?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6rpjou/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754259098,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6pnndi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "silenceimpaired",
                      "can_mod_post": false,
                      "created_utc": 1754236294,
                      "send_replies": true,
                      "parent_id": "t1_n6pi0e4",
                      "score": 0,
                      "author_fullname": "t2_dissgzyl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I hope not EXL3… but adoption seems threatened by MoEs.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6pnndi",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I hope not EXL3… but adoption seems threatened by MoEs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pnndi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754236294,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6pi0e4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1754234572,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 12,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Not EXL3 specific, but 99% of early projects/products in any new field rarely survive long term. History is full of early projects/products that seemed very big or very important in they hay day, only to be quickly rendered obsolete by new entrants or major shifts/changes as the field starts to mature. Again, nothing against EXL3, but history is choke full of such examples.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pi0e4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not EXL3 specific, but 99% of early projects/products in any new field rarely survive long term. History is full of early projects/products that seemed very big or very important in they hay day, only to be quickly rendered obsolete by new entrants or major shifts/changes as the field starts to mature. Again, nothing against EXL3, but history is choke full of such examples.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pi0e4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754234572,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "7d921414-5177-11ee-b947-e27b363b98d5",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6savw7",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ReturningTarzan",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6png86",
                                "score": 3,
                                "author_fullname": "t2_4dru3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I just added GLM 4.5 to the dev branch, incidentally. Some quants [here](https://huggingface.co/turboderp/GLM-4.5-Air-exl3)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6savw7",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "ExLlama Developer"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just added GLM 4.5 to the dev branch, incidentally. Some quants &lt;a href=\"https://huggingface.co/turboderp/GLM-4.5-Air-exl3\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6savw7/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754266448,
                                "author_flair_text": "ExLlama Developer",
                                "treatment_tags": [],
                                "created_utc": 1754266448,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#5a74cc",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6png86",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "silenceimpaired",
                      "can_mod_post": false,
                      "created_utc": 1754236234,
                      "send_replies": true,
                      "parent_id": "t1_n6phoaq",
                      "score": 1,
                      "author_fullname": "t2_dissgzyl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah, that’s actually what made me think of it. Llama.cpp still hasn’t implemented GLM 4.5… and In the past EXL sometimes had a new model faster.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6png86",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, that’s actually what made me think of it. Llama.cpp still hasn’t implemented GLM 4.5… and In the past EXL sometimes had a new model faster.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6png86/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754236234,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6phoaq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1754234468,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 16,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Splitting models to ram is kinda cope. You're saying vllm and sglang are doomed too. \n\nIn exl3 I can fit qwen 235b in gpu too. Stuff like hunyuan, dots, etc. It may not be *great* but who knows for the future. Plus it has good VLM support.\n\nNothing stops TD from adding CPU support either. Think VLLM has it. We are more doomed if all we have left is llama.cpp for backeds. Single point of failure.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6phoaq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Splitting models to ram is kinda cope. You&amp;#39;re saying vllm and sglang are doomed too. &lt;/p&gt;\n\n&lt;p&gt;In exl3 I can fit qwen 235b in gpu too. Stuff like hunyuan, dots, etc. It may not be &lt;em&gt;great&lt;/em&gt; but who knows for the future. Plus it has good VLM support.&lt;/p&gt;\n\n&lt;p&gt;Nothing stops TD from adding CPU support either. Think VLLM has it. We are more doomed if all we have left is llama.cpp for backeds. Single point of failure.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6phoaq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754234468,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 16
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6r8brx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DrExample",
            "can_mod_post": false,
            "created_utc": 1754253641,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 3,
            "author_fullname": "t2_u0ajv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's literally the best engine you get for speed + quality if you have the VRAM. Plus fresh addition of TP to the dev branch and ease of access via TabbyAPI makes it my main go-to.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6r8brx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s literally the best engine you get for speed + quality if you have the VRAM. Plus fresh addition of TP to the dev branch and ease of access via TabbyAPI makes it my main go-to.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r8brx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754253641,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6r0139",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "randomanoni",
                      "can_mod_post": false,
                      "created_utc": 1754251126,
                      "send_replies": true,
                      "parent_id": "t1_n6pdgtu",
                      "score": 3,
                      "author_fullname": "t2_tmyziykn",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "He sure did! There's a draft PR on TabbyAPI too and it mostly works. I see performance gains on dense models. Really good stuff.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6r0139",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;He sure did! There&amp;#39;s a draft PR on TabbyAPI too and it mostly works. I see performance gains on dense models. Really good stuff.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r0139/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754251126,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n6pns3j",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "silenceimpaired",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n6pnjc1",
                                                              "score": 1,
                                                              "author_fullname": "t2_dissgzyl",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Very true. Perhaps my concern is unwarranted.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n6pns3j",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Very true. Perhaps my concern is unwarranted.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mgl1qz",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pns3j/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754236335,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754236335,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6pnjc1",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "kiselsa",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6pj8u1",
                                                    "score": 3,
                                                    "author_fullname": "t2_7vw78tdp",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "\\&gt; I am only expressing concern that people will not value a VRAM only solution in this day and age.\n\nThat's just not true. \n\nVRAM = perfomance. People need fast prompt processing to be able to code and also need parallel perfomance to serve model for multiple users.\n\nYou can't get that with CPU offloading.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6pnjc1",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; I am only expressing concern that people will not value a VRAM only solution in this day and age.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s just not true. &lt;/p&gt;\n\n&lt;p&gt;VRAM = perfomance. People need fast prompt processing to be able to code and also need parallel perfomance to serve model for multiple users.&lt;/p&gt;\n\n&lt;p&gt;You can&amp;#39;t get that with CPU offloading.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mgl1qz",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pnjc1/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754236260,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754236260,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                },
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6q8i55",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "QueasyEntrance6269",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6pj8u1",
                                                    "score": 3,
                                                    "author_fullname": "t2_y465nl54h",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Hobbyists only use llama.cpp, no one uses it in production. VRAM is cheap for businesses. ExllamaV3 is exciting because it can be potentially be used as a backend for VLLM",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6q8i55",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hobbyists only use llama.cpp, no one uses it in production. VRAM is cheap for businesses. ExllamaV3 is exciting because it can be potentially be used as a backend for VLLM&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mgl1qz",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6q8i55/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754242613,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754242613,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 3
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6pj8u1",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "silenceimpaired",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6pflcs",
                                          "score": 1,
                                          "author_fullname": "t2_dissgzyl",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I’m not arguing for that. I am only expressing concern that people will not value a VRAM only solution in this day and age.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6pj8u1",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m not arguing for that. I am only expressing concern that people will not value a VRAM only solution in this day and age.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mgl1qz",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pj8u1/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754234953,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754234953,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6pflcs",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "kiselsa",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6pe3u2",
                                "score": 8,
                                "author_fullname": "t2_7vw78tdp",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Why do you need another llama.cpp? If you want CPU offload, just use llama.cpp/ik\\_llama.cpp\n\nIf you want extreme perfomance with multiple users and high throughput processing on NVIDIA gpus, use exllamav3. It was the same way with exllamav2.\n\nThere is literally no reason for exllamav3 to transform into worse llama.cpp alternative, when it can focus on gpus perfomance instead, as it always was and be much better that llama.cpp in that area.\n\nThough, turboderp mentioned somewhere that he was thinking about adding cpu inference for MoE.\n\nBut, anyways, just use llama.cpp or forks if you need it. It's useless for multiple users or enterprise usecases anyways since prompt processing or parallel requests are very bad, where exllamav2/3 focuses on perfomance and scaling on consumer-grade GPUs with efficient context.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6pflcs",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why do you need another llama.cpp? If you want CPU offload, just use llama.cpp/ik_llama.cpp&lt;/p&gt;\n\n&lt;p&gt;If you want extreme perfomance with multiple users and high throughput processing on NVIDIA gpus, use exllamav3. It was the same way with exllamav2.&lt;/p&gt;\n\n&lt;p&gt;There is literally no reason for exllamav3 to transform into worse llama.cpp alternative, when it can focus on gpus perfomance instead, as it always was and be much better that llama.cpp in that area.&lt;/p&gt;\n\n&lt;p&gt;Though, turboderp mentioned somewhere that he was thinking about adding cpu inference for MoE.&lt;/p&gt;\n\n&lt;p&gt;But, anyways, just use llama.cpp or forks if you need it. It&amp;#39;s useless for multiple users or enterprise usecases anyways since prompt processing or parallel requests are very bad, where exllamav2/3 focuses on perfomance and scaling on consumer-grade GPUs with efficient context.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pflcs/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754233836,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754233836,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 8
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6pe3u2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "silenceimpaired",
                      "can_mod_post": false,
                      "created_utc": 1754233382,
                      "send_replies": true,
                      "parent_id": "t1_n6pdgtu",
                      "score": -2,
                      "author_fullname": "t2_dissgzyl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It is exciting to see movement, and I hope it finishes up and has a place. I just worry that pure VRAM solutions won’t get much adoption by the various platforms… but I suppose if it is backwards compatible existing implementations will adopt it.\n\nEDIT: I’m not advocating that EXL3 becomes another llama like solution.",
                      "edited": 1754234983,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6pe3u2",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is exciting to see movement, and I hope it finishes up and has a place. I just worry that pure VRAM solutions won’t get much adoption by the various platforms… but I suppose if it is backwards compatible existing implementations will adopt it.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I’m not advocating that EXL3 becomes another llama like solution.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pe3u2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754233382,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6pdgtu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "bullerwins",
            "can_mod_post": false,
            "created_utc": 1754233185,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 14,
            "author_fullname": "t2_d3wk5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think turboderp just added tensor parallel on the dev branch. So I don’t think it’s dead. Just that it’s a single dev, llama cpp has many more contributors. \nBut in terms of quant size/quality I think it uses sota techniques similar to ik_llama.cpp so it definitely has its use.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pdgtu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think turboderp just added tensor parallel on the dev branch. So I don’t think it’s dead. Just that it’s a single dev, llama cpp has many more contributors. \nBut in terms of quant size/quality I think it uses sota techniques similar to ik_llama.cpp so it definitely has its use.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pdgtu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754233185,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 14
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6rc7we",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "FieldProgrammable",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6r6gbc",
                                "score": 0,
                                "author_fullname": "t2_moet0t",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "As has been said by others, running entirely on GPU is not a problem for a lot of users and if they need decent speeds they are not going to be using RAM inference. indeed they will more likely prefer specialised CUDA/Rocm implementations like exllama that do offer significant speed advantages especially in prompt processing.\n\nThose running multi user local servers are also less likely to use GGUF as the dequantization becomes a bottleneck when you are compute rather than VRAM limited (which is the case for large workstations and LLM servers). They would more likely favour FP8 or a GPTQ derivative. [See here for an example of relative speeds on Aphrodite engine](https://www.reddit.com/r/KoboldAI/s/SupGlJ287j).",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6rc7we",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As has been said by others, running entirely on GPU is not a problem for a lot of users and if they need decent speeds they are not going to be using RAM inference. indeed they will more likely prefer specialised CUDA/Rocm implementations like exllama that do offer significant speed advantages especially in prompt processing.&lt;/p&gt;\n\n&lt;p&gt;Those running multi user local servers are also less likely to use GGUF as the dequantization becomes a bottleneck when you are compute rather than VRAM limited (which is the case for large workstations and LLM servers). They would more likely favour FP8 or a GPTQ derivative. &lt;a href=\"https://www.reddit.com/r/KoboldAI/s/SupGlJ287j\"&gt;See here for an example of relative speeds on Aphrodite engine&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6rc7we/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754254817,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754254817,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6r6gbc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "silenceimpaired",
                      "can_mod_post": false,
                      "created_utc": 1754253081,
                      "send_replies": true,
                      "parent_id": "t1_n6qag5c",
                      "score": 1,
                      "author_fullname": "t2_dissgzyl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I think the rigid requirements of VRAM only impacts adoption of this over llama.cpp based products. Anything can run on RAM only… you just get a speed drop. Still I think if the creator or someone else creates a front end that makes conversion straight forward and model selection for your hardware limits easy it could grow in popularity… especially if the creator leans into finding ways to speed up and compress larger models and/or make smaller models perform better (deep think type sampling)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6r6gbc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think the rigid requirements of VRAM only impacts adoption of this over llama.cpp based products. Anything can run on RAM only… you just get a speed drop. Still I think if the creator or someone else creates a front end that makes conversion straight forward and model selection for your hardware limits easy it could grow in popularity… especially if the creator leans into finding ways to speed up and compress larger models and/or make smaller models perform better (deep think type sampling)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r6gbc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754253081,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6s0dnz",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FrostyContribution35",
                      "can_mod_post": false,
                      "created_utc": 1754262784,
                      "send_replies": true,
                      "parent_id": "t1_n6qag5c",
                      "score": 1,
                      "author_fullname": "t2_60t745im",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Exllama isn’t quite as “click and run” as Ollama or LM studio, but it isn’t too far off.\n\nTabbyAPI offers an OpenAI compatible API, all you gotta do is change 2 strings and it should run on anything",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6s0dnz",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Exllama isn’t quite as “click and run” as Ollama or LM studio, but it isn’t too far off.&lt;/p&gt;\n\n&lt;p&gt;TabbyAPI offers an OpenAI compatible API, all you gotta do is change 2 strings and it should run on anything&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6s0dnz/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754262784,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6qag5c",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FieldProgrammable",
            "can_mod_post": false,
            "created_utc": 1754243191,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 3,
            "author_fullname": "t2_moet0t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think the lack of backend support is what has really kept exl2 and exl3 from being widely adopted. If you compare the capabilities, ease of installation and general compatibility of backends like ollama and LM studio to those that support exl3, its really night and day.\n\nOne can point to the poorer selection of quants on HF but that's more a symptom of poor demand that it's underlying cause.\n\nOne prominent example is that many VS code apps that support local models will recognise ollama or LM studio out of the box, whereas I haven't found any exllama compatible backed that will work with them. Coding is a much, much larger part of the LLM user base now than it was a couple of years ago. I'm convinced this is a factor killing exl3.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qag5c",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think the lack of backend support is what has really kept exl2 and exl3 from being widely adopted. If you compare the capabilities, ease of installation and general compatibility of backends like ollama and LM studio to those that support exl3, its really night and day.&lt;/p&gt;\n\n&lt;p&gt;One can point to the poorer selection of quants on HF but that&amp;#39;s more a symptom of poor demand that it&amp;#39;s underlying cause.&lt;/p&gt;\n\n&lt;p&gt;One prominent example is that many VS code apps that support local models will recognise ollama or LM studio out of the box, whereas I haven&amp;#39;t found any exllama compatible backed that will work with them. Coding is a much, much larger part of the LLM user base now than it was a couple of years ago. I&amp;#39;m convinced this is a factor killing exl3.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6qag5c/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754243191,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6praay",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Aaaaaaaaaeeeee",
            "can_mod_post": false,
            "created_utc": 1754237411,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 2,
            "author_fullname": "t2_el5pibmej",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Maybe not all of the model layers with the differences in bitwidth need to be gpu decode optimized. Model could be split, just two models with different decoding complexities so that the CPU can have the throughout strength for tensor parallel operations.\nEach of these engines have their strengths and it's important to see. \n\nSome of the optimization baselines like speculative decoding and tensor parallel, or kvcache are much more compelling. On existing exl2 its capable of the same level of tensor parallel speedup when scaling GPUs as VLLM. I'm certain it goes to 400% MBU with midrange 300-600GB/s gpus, sweet spots when you scale the GPUs to 8 with PCIE3X16.\nMaybe Llama.cpp can do that too, but that is not in their focus yet. Even though they say inference at the edge in practice, they still need to maintain their library to avoid being overwhelmed and they are already overwhelmed by all these new models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6praay",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe not all of the model layers with the differences in bitwidth need to be gpu decode optimized. Model could be split, just two models with different decoding complexities so that the CPU can have the throughout strength for tensor parallel operations.\nEach of these engines have their strengths and it&amp;#39;s important to see. &lt;/p&gt;\n\n&lt;p&gt;Some of the optimization baselines like speculative decoding and tensor parallel, or kvcache are much more compelling. On existing exl2 its capable of the same level of tensor parallel speedup when scaling GPUs as VLLM. I&amp;#39;m certain it goes to 400% MBU with midrange 300-600GB/s gpus, sweet spots when you scale the GPUs to 8 with PCIE3X16.\nMaybe Llama.cpp can do that too, but that is not in their focus yet. Even though they say inference at the edge in practice, they still need to maintain their library to avoid being overwhelmed and they are already overwhelmed by all these new models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6praay/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754237411,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6r9ft2",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Marksta",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6r5drg",
                                "score": 2,
                                "author_fullname": "t2_559a1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'm a dreamer, can't help it 😂 yeah some hueristics to do next expert prediction. Maybe no CPU inference but I feel like in that direction, some smart RAM off-swapping MoE algorithm could work. In pipeline parralelism almost all of the pcie lanes' bandwidth is left unused. Intelligently aggregate the bandwidth of 4+ gen4x16 lanes to keep the experts swapping in just in time for use. The bandwidth and predictions hit rate would translate to some number of usable above vram capacity use.\n\nOr maybe every expert gets used 10 times per second and my swapping thought is totally useless 🤣",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6r9ft2",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a dreamer, can&amp;#39;t help it 😂 yeah some hueristics to do next expert prediction. Maybe no CPU inference but I feel like in that direction, some smart RAM off-swapping MoE algorithm could work. In pipeline parralelism almost all of the pcie lanes&amp;#39; bandwidth is left unused. Intelligently aggregate the bandwidth of 4+ gen4x16 lanes to keep the experts swapping in just in time for use. The bandwidth and predictions hit rate would translate to some number of usable above vram capacity use.&lt;/p&gt;\n\n&lt;p&gt;Or maybe every expert gets used 10 times per second and my swapping thought is totally useless 🤣&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgl1qz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r9ft2/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754253975,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754253975,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6r5drg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "silenceimpaired",
                      "can_mod_post": false,
                      "created_utc": 1754252760,
                      "send_replies": true,
                      "parent_id": "t1_n6q5b4n",
                      "score": 1,
                      "author_fullname": "t2_dissgzyl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "32b models will still benefit from this architecture.\n\nNo use dreaming how the creator might address MoEs for VRAM restricted use cases as it isn’t very in line with the vision of it.\n\nStill, I wonder if someone could modify MoE routers to favor experts in VRAM already, and to prioritize loading experts that tend to be selected more. In other words the architecture automatic picks the most efficient loading for MoE models on a system for maximum speed with minor accuracy impacts for faster speed.",
                      "edited": 1754254509,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6r5drg",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;32b models will still benefit from this architecture.&lt;/p&gt;\n\n&lt;p&gt;No use dreaming how the creator might address MoEs for VRAM restricted use cases as it isn’t very in line with the vision of it.&lt;/p&gt;\n\n&lt;p&gt;Still, I wonder if someone could modify MoE routers to favor experts in VRAM already, and to prioritize loading experts that tend to be selected more. In other words the architecture automatic picks the most efficient loading for MoE models on a system for maximum speed with minor accuracy impacts for faster speed.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgl1qz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6r5drg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754252760,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6q5b4n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1754241663,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 2,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;models are moving towards large MoEs that all but require they spill over into RAM \n\nYeah I think this is kind of key. Unless 70B-100B class takes off again, I don't see huge purpose. The 32B that a lot of people can run just can't compete with the 500B-1T of extra MoE params.\n\nMaybe on a long term, if this meta holds up the dev can do some cool pivot melding the speed of dense layers in VRAM he already has and the experts to CPU. Yes, the way we're all now running llama.cpp but I feel like this work flow accidented its way into existence. So maybe with a calculated architecture of only MoE experts ever running in cpu he could come up with a unique offering that fits his architecture.\n\nEither way, hope he can keep at it!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6q5b4n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;models are moving towards large MoEs that all but require they spill over into RAM &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yeah I think this is kind of key. Unless 70B-100B class takes off again, I don&amp;#39;t see huge purpose. The 32B that a lot of people can run just can&amp;#39;t compete with the 500B-1T of extra MoE params.&lt;/p&gt;\n\n&lt;p&gt;Maybe on a long term, if this meta holds up the dev can do some cool pivot melding the speed of dense layers in VRAM he already has and the experts to CPU. Yes, the way we&amp;#39;re all now running llama.cpp but I feel like this work flow accidented its way into existence. So maybe with a calculated architecture of only MoE experts ever running in cpu he could come up with a unique offering that fits his architecture.&lt;/p&gt;\n\n&lt;p&gt;Either way, hope he can keep at it!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6q5b4n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754241663,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6pbhig",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "silenceimpaired",
            "can_mod_post": false,
            "created_utc": 1754232580,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 1,
            "author_fullname": "t2_dissgzyl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I thought of EXL because Llama.cpp still hasn’t implemented GLM 4.5 and EXL often beat Llama.cpp with support.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pbhig",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I thought of EXL because Llama.cpp still hasn’t implemented GLM 4.5 and EXL often beat Llama.cpp with support.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6pbhig/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754232580,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6qs9bd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1754248651,
            "send_replies": true,
            "parent_id": "t3_1mgl1qz",
            "score": 0,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "O",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qs9bd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;O&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgl1qz/is_exl3_doomed/n6qs9bd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754248651,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgl1qz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]