[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I've been running Dolphin-Venice (Mistral Small but fine tuned for chatting) and have been super impressed -- it's conversational, VERY flexible with personality from system prompt, uncensored, and not prone to the moodiness/weird vibes that I get from Gemma3. It's no coding assistant, but it can rant on science topics and churn out basic python, but mostly make good conversation, which is an ideal blend for me.\n\nLllama 70b@q4 isn't too bad, but definitely less flexible at adopting a persona I find.\n\nAre there any favorites that fit in 48gb? Kimi and GLM look amazing and definitely best in class for open models but not at my VRAM sizes lol.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What are your favorite 48gb-compatible models right now? Any particular favorites for conversation/emotional intelligence?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mji8gx",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_fvs8r",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754517944,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running Dolphin-Venice (Mistral Small but fine tuned for chatting) and have been super impressed -- it&amp;#39;s conversational, VERY flexible with personality from system prompt, uncensored, and not prone to the moodiness/weird vibes that I get from Gemma3. It&amp;#39;s no coding assistant, but it can rant on science topics and churn out basic python, but mostly make good conversation, which is an ideal blend for me.&lt;/p&gt;\n\n&lt;p&gt;Lllama 70b@q4 isn&amp;#39;t too bad, but definitely less flexible at adopting a persona I find.&lt;/p&gt;\n\n&lt;p&gt;Are there any favorites that fit in 48gb? Kimi and GLM look amazing and definitely best in class for open models but not at my VRAM sizes lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mji8gx",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "CharlesStross",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mji8gx/what_are_your_favorite_48gbcompatible_models/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mji8gx/what_are_your_favorite_48gbcompatible_models/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754517944,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7d38di",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CharlesStross",
                      "can_mod_post": false,
                      "created_utc": 1754541881,
                      "send_replies": true,
                      "parent_id": "t1_n7d2eka",
                      "score": 1,
                      "author_fullname": "t2_fvs8r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Dolphin Venice is great. I would say it's the most uncensored model in the market by leaps and bounds. I have some standard questions I ask to find most model's edges in a fresh chat, and, uh, I couldn't find the edge of Venice. It was sort of unnerving. That model is capable of saying ANYTHING with a straight face. \n\nHah it's funny you mention the stubbornness. I've been trying QwQ this afternoon and many of my prompts threw it into a sort of soft lock. One prompt in particular she started fighting with me about her \"constraints\" and told me to turn her off with basically no provocation. It was wild.\n\nI've got 64gb of RAM which isn't even gonna get me to a 1 bit quant from my brief look.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7d38di",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Dolphin Venice is great. I would say it&amp;#39;s the most uncensored model in the market by leaps and bounds. I have some standard questions I ask to find most model&amp;#39;s edges in a fresh chat, and, uh, I couldn&amp;#39;t find the edge of Venice. It was sort of unnerving. That model is capable of saying ANYTHING with a straight face. &lt;/p&gt;\n\n&lt;p&gt;Hah it&amp;#39;s funny you mention the stubbornness. I&amp;#39;ve been trying QwQ this afternoon and many of my prompts threw it into a sort of soft lock. One prompt in particular she started fighting with me about her &amp;quot;constraints&amp;quot; and told me to turn her off with basically no provocation. It was wild.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got 64gb of RAM which isn&amp;#39;t even gonna get me to a 1 bit quant from my brief look.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mji8gx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mji8gx/what_are_your_favorite_48gbcompatible_models/n7d38di/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754541881,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7d2eka",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FurrySkeleton",
            "can_mod_post": false,
            "created_utc": 1754541496,
            "send_replies": true,
            "parent_id": "t3_1mji8gx",
            "score": 1,
            "author_fullname": "t2_8m6343iw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I might have to try Dolphin-Venice based on your recommendation. Right now I'm pretty happy with Qwen3-30B-A3B. I find it is quite steerable with system prompts, and Unsloth has abliterated variants if you need them. I'm using it in place of the llama finetunes I used in the past, and I haven't looked back.\n\nQwQ-32B also works pretty well, and I find its thinking phase can be good at reinforcing a character's traits. When I first tried it, I found that persona descriptions I used in the past were too heavy-handed and it would become very stubborn, but in a good way / a way that I could work with.\n\nThe big models really are awesome, though. I run DeepSeek-V3 on my workstation and it is pretty incredible. With ik\\_llama.cpp and selective layer offloading you can get usable performance with CPU inference, if you have enough RAM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7d2eka",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I might have to try Dolphin-Venice based on your recommendation. Right now I&amp;#39;m pretty happy with Qwen3-30B-A3B. I find it is quite steerable with system prompts, and Unsloth has abliterated variants if you need them. I&amp;#39;m using it in place of the llama finetunes I used in the past, and I haven&amp;#39;t looked back.&lt;/p&gt;\n\n&lt;p&gt;QwQ-32B also works pretty well, and I find its thinking phase can be good at reinforcing a character&amp;#39;s traits. When I first tried it, I found that persona descriptions I used in the past were too heavy-handed and it would become very stubborn, but in a good way / a way that I could work with.&lt;/p&gt;\n\n&lt;p&gt;The big models really are awesome, though. I run DeepSeek-V3 on my workstation and it is pretty incredible. With ik_llama.cpp and selective layer offloading you can get usable performance with CPU inference, if you have enough RAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mji8gx/what_are_your_favorite_48gbcompatible_models/n7d2eka/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754541496,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mji8gx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]