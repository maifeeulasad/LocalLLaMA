[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey all, I've been experimenting with various LLM apps and have an idea for a small open-source project to address a frustration I'm hitting repeatedly. But before I dive deep, I wanted to quickly check if it already exists (fingers crossed)!\n\n**My Pain Point:**  \nI'm tired of being stuck with linear conversations. When exploring complex problems, like debugging or research, I often want to:\n\n* Ask side-questions without polluting the main conversation\n* Explore multiple paths (e.g., testing two possible solutions simultaneously)\n\nRight now, these side explorations clutter my main context, inflate token usage/costs, and make responses less relevant.\n\n**My Idea (OS)**: Small self-hosted micro-service + API that lets you:\n\n1. Branch a conversation\n2. Toggle past messages (i.e. ability to pick and choose which message are included in the context to minimize tokens and boost relevance)\n3. Get an optimized JSON context output, which you then feed into your existing LLM connector or custom client (thinking it makes the most sense to avoid direct complexity of sending messages directly to Local LLM, OpenAI, Anthropic, etc.)\n\n**Does something like this already exist?**  \nDoes this bother anyone else, is it just me, or am I missing something obvious?\n\nThanks so much for any candid feedback!\n\n**TLDR: Sick of linear LLM chats causing wasted tokens and cluttered context. Considering making an open-source tool/service for branching conversations + explicit message toggling, returning optimized JSON contexts for easy integration. Does this exist? Good idea, bad idea?**",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Tool for chat branching &amp; selective-context control exist?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhlxe1",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_7xaxpi27",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754334058,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754332985,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been experimenting with various LLM apps and have an idea for a small open-source project to address a frustration I&amp;#39;m hitting repeatedly. But before I dive deep, I wanted to quickly check if it already exists (fingers crossed)!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Pain Point:&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m tired of being stuck with linear conversations. When exploring complex problems, like debugging or research, I often want to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ask side-questions without polluting the main conversation&lt;/li&gt;\n&lt;li&gt;Explore multiple paths (e.g., testing two possible solutions simultaneously)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now, these side explorations clutter my main context, inflate token usage/costs, and make responses less relevant.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Idea (OS)&lt;/strong&gt;: Small self-hosted micro-service + API that lets you:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Branch a conversation&lt;/li&gt;\n&lt;li&gt;Toggle past messages (i.e. ability to pick and choose which message are included in the context to minimize tokens and boost relevance)&lt;/li&gt;\n&lt;li&gt;Get an optimized JSON context output, which you then feed into your existing LLM connector or custom client (thinking it makes the most sense to avoid direct complexity of sending messages directly to Local LLM, OpenAI, Anthropic, etc.)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Does something like this already exist?&lt;/strong&gt;&lt;br/&gt;\nDoes this bother anyone else, is it just me, or am I missing something obvious?&lt;/p&gt;\n\n&lt;p&gt;Thanks so much for any candid feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR: Sick of linear LLM chats causing wasted tokens and cluttered context. Considering making an open-source tool/service for branching conversations + explicit message toggling, returning optimized JSON contexts for easy integration. Does this exist? Good idea, bad idea?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mhlxe1",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "IsWired",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754332985,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6xi9o5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "IsWired",
                      "can_mod_post": false,
                      "created_utc": 1754337743,
                      "send_replies": true,
                      "parent_id": "t1_n6x7o6h",
                      "score": 1,
                      "author_fullname": "t2_7xaxpi27",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hmm, your project sounds pretty interesting- definitely would be interested in checking it out when you're done.\n\n  \nOn my third point, I’m not married to every detail yet, but my thinking is to keep this more of a **middleware layer** than a full client. The goal is to make it simple to use and easy to drop into existing setups.\n\nSo for example:\n\n* If you’re already using OpenAI’s SDK or LangChain, you’d normally pass your entire conversation history to the LLM on every request (or manage it with some custom logic). \n* With this tool, instead of doing that yourself, you’d call its API, and it would return **only the selected or branched messages in a JSON array,** already trimmed and ordered. \n* You’d then pass that array straight to your local LLM, OpenAI, or whatever connector you’re using.\n\nSo the idea is: the tool doesn’t touch API keys, billing, or vendor-specific integrations. It’s just a **“context optimizer”** that handles history and branching without forcing you to change how you interact with your LLM/API of choice.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6xi9o5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hmm, your project sounds pretty interesting- definitely would be interested in checking it out when you&amp;#39;re done.&lt;/p&gt;\n\n&lt;p&gt;On my third point, I’m not married to every detail yet, but my thinking is to keep this more of a &lt;strong&gt;middleware layer&lt;/strong&gt; than a full client. The goal is to make it simple to use and easy to drop into existing setups.&lt;/p&gt;\n\n&lt;p&gt;So for example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If you’re already using OpenAI’s SDK or LangChain, you’d normally pass your entire conversation history to the LLM on every request (or manage it with some custom logic). &lt;/li&gt;\n&lt;li&gt;With this tool, instead of doing that yourself, you’d call its API, and it would return &lt;strong&gt;only the selected or branched messages in a JSON array,&lt;/strong&gt; already trimmed and ordered. &lt;/li&gt;\n&lt;li&gt;You’d then pass that array straight to your local LLM, OpenAI, or whatever connector you’re using.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So the idea is: the tool doesn’t touch API keys, billing, or vendor-specific integrations. It’s just a &lt;strong&gt;“context optimizer”&lt;/strong&gt; that handles history and branching without forcing you to change how you interact with your LLM/API of choice.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlxe1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/n6xi9o5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754337743,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6x7o6h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "smahs9",
            "can_mod_post": false,
            "created_utc": 1754334635,
            "send_replies": true,
            "parent_id": "t3_1mhlxe1",
            "score": 2,
            "author_fullname": "t2_neyagc1uz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I haven't published it yet, but I am writing a tool that may fit the bill here. It was born out of similar frustration and the realization that not every use case is a chat, as I often experiment with different prompts/models to see how the outputs change (text or JSON). Its a local-only, frontend-only (no model runtime, so no large downloads or installs) tool built on indexeddb all the way, which makes the development a bit slow. As you go down the rabbit hole, it gets quite complex as you hit the UX problem for this use case, as you need some kind of text storage, search, versioning and integration with an editor etc. Anyway, I hope to post an intro soon.\n\nI am not sure if I understand your third point - would you mind explaining it with an example?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6x7o6h",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t published it yet, but I am writing a tool that may fit the bill here. It was born out of similar frustration and the realization that not every use case is a chat, as I often experiment with different prompts/models to see how the outputs change (text or JSON). Its a local-only, frontend-only (no model runtime, so no large downloads or installs) tool built on indexeddb all the way, which makes the development a bit slow. As you go down the rabbit hole, it gets quite complex as you hit the UX problem for this use case, as you need some kind of text storage, search, versioning and integration with an editor etc. Anyway, I hope to post an intro soon.&lt;/p&gt;\n\n&lt;p&gt;I am not sure if I understand your third point - would you mind explaining it with an example?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/n6x7o6h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754334635,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlxe1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xadhy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MindOrbits",
            "can_mod_post": false,
            "created_utc": 1754335412,
            "send_replies": true,
            "parent_id": "t3_1mhlxe1",
            "score": 1,
            "author_fullname": "t2_v9v8eio1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Generative Wiki please, bonus points for a Diffusion LLM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xadhy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Generative Wiki please, bonus points for a Diffusion LLM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/n6xadhy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754335412,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlxe1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6xc0ct",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DorphinPack",
                      "can_mod_post": false,
                      "created_utc": 1754335884,
                      "send_replies": true,
                      "parent_id": "t1_n6xbwwh",
                      "score": 2,
                      "author_fullname": "t2_zebuyjw9s",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Wow that doesn’t sound like me am I buying into the hype??",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6xc0ct",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow that doesn’t sound like me am I buying into the hype??&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlxe1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/n6xc0ct/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754335884,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6xjddp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "IsWired",
                      "can_mod_post": false,
                      "created_utc": 1754338070,
                      "send_replies": true,
                      "parent_id": "t1_n6xbwwh",
                      "score": 1,
                      "author_fullname": "t2_7xaxpi27",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I could see it. Given that, I have to assume people are already working on solutions? Know of anything worth looking into before I go re-inventing the wheel?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6xjddp",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I could see it. Given that, I have to assume people are already working on solutions? Know of anything worth looking into before I go re-inventing the wheel?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlxe1",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/n6xjddp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754338070,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xbwwh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DorphinPack",
            "can_mod_post": false,
            "created_utc": 1754335856,
            "send_replies": true,
            "parent_id": "t3_1mhlxe1",
            "score": 2,
            "author_fullname": "t2_zebuyjw9s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "These are the wide open problems.\n\nContext engineering is gonna be HUGE.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xbwwh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These are the wide open problems.&lt;/p&gt;\n\n&lt;p&gt;Context engineering is gonna be HUGE.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlxe1/tool_for_chat_branching_selectivecontext_control/n6xbwwh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754335856,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlxe1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]