[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "With the [release of DGX spark later this month](https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/), I was wondering how a new-ish homebrew system would compare.\n\nAll 5000-series NVIDIA cards are equipped with PCIE Gen 5, which puts the upper limit for cross-bus bandwidth at 128GB/s.  Dual channel DDR5 is capable of ~96GB/s and quad channel doubles that to ~192GB/s (bottlenecked to 128GB/s over PCIe).  Resizable BAR should allow for transfers to have minimal overhead.\n\n[HuggingFace accelerate](https://huggingface.co/docs/accelerate/v1.8.1/index) hierarchically distributes PyTorch models between the memory of GPU(s) and the CPU memory, and copies the layers to the VRAM during inference so only the GPU performs computation.\n\nThis is compared to:\n\n* [llama.cpp](https://github.com/ggml-org/llama.cpp) which splits the model between VRAM and CPU memory, where the GPU computes the layers stored in VRAM and the CPU computes the layers stored in CPU memory.\n\n* [vllm](https://github.com/vllm-project/vllm) which splits the model between multiple GPUs' VRAM and uses tensor parallelism to pipeline the layers between GPUs.\n\n\nMy expectation is that the 128GB/s bandwidth of PCIe 5.0 x16 would allow accelerate to utilize system memory at nearly maximum speed.  128GB/s bandwidth doesn't quite match DGX spark, but a powerful GPU and lots of DDR5 (in quad channel?) could beat the spark for batch inference.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How fast is inference when utilizing DDR5 and PCIe 5.0x16?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lv5je7",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_uei1i14w",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752022394,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the &lt;a href=\"https://wccftech.com/nvidia-mini-supercomputer-the-dgx-spark-launches-this-month/\"&gt;release of DGX spark later this month&lt;/a&gt;, I was wondering how a new-ish homebrew system would compare.&lt;/p&gt;\n\n&lt;p&gt;All 5000-series NVIDIA cards are equipped with PCIE Gen 5, which puts the upper limit for cross-bus bandwidth at 128GB/s.  Dual channel DDR5 is capable of ~96GB/s and quad channel doubles that to ~192GB/s (bottlenecked to 128GB/s over PCIe).  Resizable BAR should allow for transfers to have minimal overhead.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/docs/accelerate/v1.8.1/index\"&gt;HuggingFace accelerate&lt;/a&gt; hierarchically distributes PyTorch models between the memory of GPU(s) and the CPU memory, and copies the layers to the VRAM during inference so only the GPU performs computation.&lt;/p&gt;\n\n&lt;p&gt;This is compared to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp\"&gt;llama.cpp&lt;/a&gt; which splits the model between VRAM and CPU memory, where the GPU computes the layers stored in VRAM and the CPU computes the layers stored in CPU memory.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/vllm-project/vllm\"&gt;vllm&lt;/a&gt; which splits the model between multiple GPUs&amp;#39; VRAM and uses tensor parallelism to pipeline the layers between GPUs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My expectation is that the 128GB/s bandwidth of PCIe 5.0 x16 would allow accelerate to utilize system memory at nearly maximum speed.  128GB/s bandwidth doesn&amp;#39;t quite match DGX spark, but a powerful GPU and lots of DDR5 (in quad channel?) could beat the spark for batch inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?auto=webp&amp;s=2c3905dab01b88b0dbab01fcb0b574d9f1e512b5",
                    "width": 1920,
                    "height": 1080
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd25657d3fce734d4025693e620867a7cf866fd1",
                      "width": 108,
                      "height": 60
                    },
                    {
                      "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed12cf3ccefd6b6e923ec4d43579ac26f2d80130",
                      "width": 216,
                      "height": 121
                    },
                    {
                      "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf58c691ecc4a68a7e65aa0c0d4e284c08c9845a",
                      "width": 320,
                      "height": 180
                    },
                    {
                      "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8436d2033ab2a873dac41641dd69093f14dcb51c",
                      "width": 640,
                      "height": 360
                    },
                    {
                      "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0957457f9a703a8e5f4750384c880b16a2c80648",
                      "width": 960,
                      "height": 540
                    },
                    {
                      "url": "https://external-preview.redd.it/0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6ffcb9da830480ee39d159a6310bc89df79861f",
                      "width": 1080,
                      "height": 607
                    }
                  ],
                  "variants": {},
                  "id": "0pU0OZQ3jKyRpVTXegSNFV4uVFdUj2o4hXpi85CuSUA"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lv5je7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ButThatsMyRamSlot",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/",
            "subreddit_subscribers": 496591,
            "created_utc": 1752022394,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n23cvj6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Lossu",
                      "can_mod_post": false,
                      "created_utc": 1752023137,
                      "send_replies": true,
                      "parent_id": "t1_n23bhcn",
                      "score": 1,
                      "author_fullname": "t2_9zoum8go",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Unironically the best use of it for us GPU poors.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n23cvj6",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unironically the best use of it for us GPU poors.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lv5je7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/n23cvj6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752023137,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n23bhcn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ieatdownvotes4food",
            "can_mod_post": false,
            "created_utc": 1752022669,
            "send_replies": true,
            "parent_id": "t3_1lv5je7",
            "score": 2,
            "author_fullname": "t2_12fv96rb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "U gotta work hard to take advantage of PCI 5.0x16 with a 5090. Better use case is splitting x8 with two cards.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n23bhcn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;U gotta work hard to take advantage of PCI 5.0x16 with a 5090. Better use case is splitting x8 with two cards.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/n23bhcn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752022669,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lv5je7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n24asuw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MrTacoSauces",
            "can_mod_post": false,
            "created_utc": 1752035646,
            "send_replies": true,
            "parent_id": "t3_1lv5je7",
            "score": 1,
            "author_fullname": "t2_u7qxotulc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "So you have to very much think about the latency. AI compute is so weird. It's a constantly teetering scale. Do you go small scale dense models? Medium dense? MoE? These all change the use case.\n\nFor dense models a pcie unit is going to be borderline useless you are likely introducing more latency by the cpu translating those PCIE signals into useable buffers that by the time that entire bus is doing real work it's doing more damage than helping. For MoE maybe depending on support possibly maybe but PCIE ram is going to still be far more unperformanant and unhelpful than it's implementation cost. Shoehorning ram into a pcie 5 x 16 rail isn't just going to automatically be able to populate all of that theoretical bandwidth. System level ram doesn't even populate that much bandwidth in the best systems...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n24asuw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So you have to very much think about the latency. AI compute is so weird. It&amp;#39;s a constantly teetering scale. Do you go small scale dense models? Medium dense? MoE? These all change the use case.&lt;/p&gt;\n\n&lt;p&gt;For dense models a pcie unit is going to be borderline useless you are likely introducing more latency by the cpu translating those PCIE signals into useable buffers that by the time that entire bus is doing real work it&amp;#39;s doing more damage than helping. For MoE maybe depending on support possibly maybe but PCIE ram is going to still be far more unperformanant and unhelpful than it&amp;#39;s implementation cost. Shoehorning ram into a pcie 5 x 16 rail isn&amp;#39;t just going to automatically be able to populate all of that theoretical bandwidth. System level ram doesn&amp;#39;t even populate that much bandwidth in the best systems...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/n24asuw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752035646,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lv5je7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n26yurg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bick_nyers",
            "can_mod_post": false,
            "created_utc": 1752075557,
            "send_replies": true,
            "parent_id": "t3_1lv5je7",
            "score": 1,
            "author_fullname": "t2_6nwld4d3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Newest Intel supports MRDIMM on consumer motherboards if I remember correctly. Could be a way to get higher memory bandwidth on dual channel motherboards. Zen 6 EPYC is rumored to have MRDIMM compatibility as well, so that might trickle down to AM5.\n\n\nMoE models tend to prefer certain experts during a generation, so in a perfect world HF accelerate would swap out at the expert level (as opposed to layer level) and keep the \"high probability experts\" loaded to minimize transfer. Some form of prediction (even if simple) of what experts are needed next would be good too in order to fully utilize the bandwidth. Not sure what the current implementation looks like.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n26yurg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Newest Intel supports MRDIMM on consumer motherboards if I remember correctly. Could be a way to get higher memory bandwidth on dual channel motherboards. Zen 6 EPYC is rumored to have MRDIMM compatibility as well, so that might trickle down to AM5.&lt;/p&gt;\n\n&lt;p&gt;MoE models tend to prefer certain experts during a generation, so in a perfect world HF accelerate would swap out at the expert level (as opposed to layer level) and keep the &amp;quot;high probability experts&amp;quot; loaded to minimize transfer. Some form of prediction (even if simple) of what experts are needed next would be good too in order to fully utilize the bandwidth. Not sure what the current implementation looks like.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lv5je7/how_fast_is_inference_when_utilizing_ddr5_and/n26yurg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752075557,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lv5je7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]