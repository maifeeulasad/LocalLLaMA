[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Qwen has introduced a new technique called **GSPO** (Group Sequence Policy Optimization)\n\nPut simply:\n\n* It's a new method for training large language models\n* Instead of focusing on individual words like older methods, it optimizes entire sentences or passages as a whole — which is more logical and leads to better performance\n* This approach makes training more **stable** and less prone to crashes or errors, especially when used with large, modular models like **MoE (Mixture of Experts)**\n* The training process is **simpler** and doesn’t rely on complex tricks used in the past, making it cleaner and easier to manage\n* The more compute you throw at it, the better the model becomes — it **scales efficiently**.\n* The latest **Qwen3 models** (like those that can code or follow instructions) were trained using this method\n* Compared to the older **GRPO** method, GSPO leads to **faster convergence** (the model learns faster) and uses **fewer resources**\n\nPaper: [https://huggingface.co/papers/2507.18071](https://huggingface.co/papers/2507.18071)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Qwen GSPO (Group Sequence Policy Optimization)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Other"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1man0hu",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.98,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 58,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1heeqeidfc",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Other",
            "can_mod_post": false,
            "score": 58,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753624825,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen has introduced a new technique called &lt;strong&gt;GSPO&lt;/strong&gt; (Group Sequence Policy Optimization)&lt;/p&gt;\n\n&lt;p&gt;Put simply:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It&amp;#39;s a new method for training large language models&lt;/li&gt;\n&lt;li&gt;Instead of focusing on individual words like older methods, it optimizes entire sentences or passages as a whole — which is more logical and leads to better performance&lt;/li&gt;\n&lt;li&gt;This approach makes training more &lt;strong&gt;stable&lt;/strong&gt; and less prone to crashes or errors, especially when used with large, modular models like &lt;strong&gt;MoE (Mixture of Experts)&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;The training process is &lt;strong&gt;simpler&lt;/strong&gt; and doesn’t rely on complex tricks used in the past, making it cleaner and easier to manage&lt;/li&gt;\n&lt;li&gt;The more compute you throw at it, the better the model becomes — it &lt;strong&gt;scales efficiently&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;The latest &lt;strong&gt;Qwen3 models&lt;/strong&gt; (like those that can code or follow instructions) were trained using this method&lt;/li&gt;\n&lt;li&gt;Compared to the older &lt;strong&gt;GRPO&lt;/strong&gt; method, GSPO leads to &lt;strong&gt;faster convergence&lt;/strong&gt; (the model learns faster) and uses &lt;strong&gt;fewer resources&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://huggingface.co/papers/2507.18071\"&gt;https://huggingface.co/papers/2507.18071&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?auto=webp&amp;s=b7678a8af1d1d28f96c34fbaeb2656718573d56c",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=204816acf3c4a486bb403207785321d33214adc7",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=81cb7197b22ed427b6c24ae43d3f69dc4cb2730d",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=329be156f229f90b7be0f62070e92a848fedc1f2",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=82297d64dd06513853c691039970c2747e099d87",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=03df11e5919855e527dbf686542a55fb52fd228c",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d08b01c22320c544d58ffd0a85b1d12a04e7402",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "kpkVEAiwNd6D_mfl3tEdDni1cD692QYRZ9sC2FzlBz4"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#94e044",
            "id": "1man0hu",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "koc_Z3",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/",
            "subreddit_subscribers": 505617,
            "created_utc": 1753624825,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fwrar",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "bihungba1101",
            "can_mod_post": false,
            "created_utc": 1753628012,
            "send_replies": true,
            "parent_id": "t3_1man0hu",
            "score": 7,
            "author_fullname": "t2_5krbhd7o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is the advancements that we need!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fwrar",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is the advancements that we need!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/n5fwrar/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753628012,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1man0hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hg0c8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753644391,
            "send_replies": true,
            "parent_id": "t3_1man0hu",
            "score": 2,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is this not analogous to methods talked about in RLOO and Cohere's \"Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs\"?\n\nI know they applied them to GRPO so it's new and shiny, but my suspicion is the techniques are roughly equivalent to what was used there.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hg0c8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is this not analogous to methods talked about in RLOO and Cohere&amp;#39;s &amp;quot;Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;I know they applied them to GRPO so it&amp;#39;s new and shiny, but my suspicion is the techniques are roughly equivalent to what was used there.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/n5hg0c8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753644391,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1man0hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hjv9s",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1753645577,
            "send_replies": true,
            "parent_id": "t3_1man0hu",
            "score": 1,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "isn't that similar to CISPO used for minimax? (I mean, the aspect of not focusing on specific words)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hjv9s",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;isn&amp;#39;t that similar to CISPO used for minimax? (I mean, the aspect of not focusing on specific words)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1man0hu/qwen_gspo_group_sequence_policy_optimization/n5hjv9s/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753645577,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1man0hu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]