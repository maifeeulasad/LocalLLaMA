[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm going to download GLM 4.5. But since I'm VRAM poor, I can only run a small quant. What's better at around the same size in GB, Q2_K_XL or IQ3_XXS?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What's better Q2_K_XL or IQ3_XXS?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjef0p",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_o65i6kx",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754509097,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going to download GLM 4.5. But since I&amp;#39;m VRAM poor, I can only run a small quant. What&amp;#39;s better at around the same size in GB, Q2_K_XL or IQ3_XXS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjef0p",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "fallingdowndizzyvr",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/",
            "subreddit_subscribers": 512426,
            "created_utc": 1754509097,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7bseef",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ParaboloidalCrest",
            "can_mod_post": false,
            "created_utc": 1754524364,
            "send_replies": true,
            "parent_id": "t3_1mjef0p",
            "score": 2,
            "author_fullname": "t2_nc2u4f7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "One of those days I'm gonna ditch all those weird quants and use the stupid honest-to-god Q4\\_0! All those different configurations are getting ridiculous and perhaps no one can tell if they're empirically any better.",
            "edited": 1754524564,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bseef",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;One of those days I&amp;#39;m gonna ditch all those weird quants and use the stupid honest-to-god Q4_0! All those different configurations are getting ridiculous and perhaps no one can tell if they&amp;#39;re empirically any better.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7bseef/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754524364,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjef0p",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7az02m",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "fallingdowndizzyvr",
                      "can_mod_post": false,
                      "created_utc": 1754514971,
                      "send_replies": true,
                      "parent_id": "t1_n7aydvz",
                      "score": 1,
                      "author_fullname": "t2_o65i6kx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; So broadly I would say if you have surplus compute (vs memory bandwidth) the IQ3_XXS will be better but if you don't the Q2_K_XL might be better.\n\nThanks for that super helpful post. In fact, I do have more compute than the memory bandwidth to use it. So IQ3_XXS seems to be the better fit.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7az02m",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;So broadly I would say if you have surplus compute (vs memory bandwidth) the IQ3_XXS will be better but if you don&amp;#39;t the Q2_K_XL might be better.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Thanks for that super helpful post. In fact, I do have more compute than the memory bandwidth to use it. So IQ3_XXS seems to be the better fit.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjef0p",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7az02m/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754514971,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7aydvz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754514789,
            "send_replies": true,
            "parent_id": "t3_1mjef0p",
            "score": 1,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Quants are weird.  Honestly?  Maybe try both and see since ultimately it's difficult to predict how quality is affected.\n\nTechnically there isn't like a singular answer since any quant of any model is going to be a mix of types, both due to user designs and tensors layout (quants are done in blocks so if you have 20 parameters you can't use a 16 block size quant).  Here's a breakdown: \n\n| Tensor                       | IQ3_XXS  | Q2_K_XL |\n| ---------------------------- | -------- | ------- |\n| blk.5.attn_k.weight          | IQ4_XS   |  Q4_K   |\n| blk.5.attn_q.weight          | IQ4_XS   |  Q4_K   |\n| blk.5.attn_v.weight          | IQ4_XS   |  Q4_K   |\n| blk.5.ffn_down_exps.weight   | IQ3_S    |  Q3_K   |\n| blk.5.ffn_down_shexp.weight  | Q5_K     |  Q4_K   |\n| blk.5.ffn_gate_exps.weight   | IQ3_XXS  |  Q2_K   |\n| blk.5.ffn_gate_shexp.weight  | IQ4_XS   |  Q4_K   |\n| blk.5.ffn_up_exps.weight     | IQ3_XXS  |  Q2_K   |\n| blk.5.ffn_up_shexp.weight    | IQ4_XS   |  Q4_K   |\n| blk.5.attn_output.weight     | IQ4_XS   |  Q4_K   |\n\nNote that the Q2_K_XL is ironically mostly not Q2_K (just the experts which are the bulk of the model TBF).  This is because the `K_XL` is an unsloth custom, IIRC, that sets tensors like `attn_*` to Q4_K where as a standard `K_M`would use Q2_K.  This is a big difference in quality but also has a big affect on behavior.  The `attn_*` tenors are always active (unlike the experts) and make up like 1/4 of the model's active parameters, meaning doubling their size would increase memory bandwidth usage by 5/4 even though the size of disk barely changes (I explain/benchmark this a little better with GPT-OSS quants [here](https://old.reddit.com/r/LocalLLaMA/comments/1miuluj/why_unsloth_gptoss_quatizations_reduces_so_little/n76abo1/)).  However, they are also what ends up on the GPU when you run `-ngl 99 -ot exps=CPU` so aren't going to be a major problem with mixed CPU+GPU inference.\n\nBeyond that, mostly what you may notice here is that they aren't _super_ different and the biggest difference is the IQ# vs Q#.  The IQ# are generally considered better than the equivalent Q4_K, but at the expense of requiring more compute.  So broadly I would say if you have surplus compute (vs memory bandwidth) the IQ3_XXS will be better but if you don't the Q2_K_XL might be better.  In terms of quality, I suspect IQ3_XXS is a little a bit better but I doubt you'll see any practical effect (but it's almost certainly better than a Q2_K since I think heavily quantized attn can hurt a lot).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7aydvz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Quants are weird.  Honestly?  Maybe try both and see since ultimately it&amp;#39;s difficult to predict how quality is affected.&lt;/p&gt;\n\n&lt;p&gt;Technically there isn&amp;#39;t like a singular answer since any quant of any model is going to be a mix of types, both due to user designs and tensors layout (quants are done in blocks so if you have 20 parameters you can&amp;#39;t use a 16 block size quant).  Here&amp;#39;s a breakdown: &lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Tensor&lt;/th&gt;\n&lt;th&gt;IQ3_XXS&lt;/th&gt;\n&lt;th&gt;Q2_K_XL&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.attn_k.weight&lt;/td&gt;\n&lt;td&gt;IQ4_XS&lt;/td&gt;\n&lt;td&gt;Q4_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.attn_q.weight&lt;/td&gt;\n&lt;td&gt;IQ4_XS&lt;/td&gt;\n&lt;td&gt;Q4_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.attn_v.weight&lt;/td&gt;\n&lt;td&gt;IQ4_XS&lt;/td&gt;\n&lt;td&gt;Q4_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.ffn_down_exps.weight&lt;/td&gt;\n&lt;td&gt;IQ3_S&lt;/td&gt;\n&lt;td&gt;Q3_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.ffn_down_shexp.weight&lt;/td&gt;\n&lt;td&gt;Q5_K&lt;/td&gt;\n&lt;td&gt;Q4_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.ffn_gate_exps.weight&lt;/td&gt;\n&lt;td&gt;IQ3_XXS&lt;/td&gt;\n&lt;td&gt;Q2_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.ffn_gate_shexp.weight&lt;/td&gt;\n&lt;td&gt;IQ4_XS&lt;/td&gt;\n&lt;td&gt;Q4_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.ffn_up_exps.weight&lt;/td&gt;\n&lt;td&gt;IQ3_XXS&lt;/td&gt;\n&lt;td&gt;Q2_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.ffn_up_shexp.weight&lt;/td&gt;\n&lt;td&gt;IQ4_XS&lt;/td&gt;\n&lt;td&gt;Q4_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;blk.5.attn_output.weight&lt;/td&gt;\n&lt;td&gt;IQ4_XS&lt;/td&gt;\n&lt;td&gt;Q4_K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Note that the Q2_K_XL is ironically mostly not Q2_K (just the experts which are the bulk of the model TBF).  This is because the &lt;code&gt;K_XL&lt;/code&gt; is an unsloth custom, IIRC, that sets tensors like &lt;code&gt;attn_*&lt;/code&gt; to Q4_K where as a standard &lt;code&gt;K_M&lt;/code&gt;would use Q2_K.  This is a big difference in quality but also has a big affect on behavior.  The &lt;code&gt;attn_*&lt;/code&gt; tenors are always active (unlike the experts) and make up like 1/4 of the model&amp;#39;s active parameters, meaning doubling their size would increase memory bandwidth usage by 5/4 even though the size of disk barely changes (I explain/benchmark this a little better with GPT-OSS quants &lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1miuluj/why_unsloth_gptoss_quatizations_reduces_so_little/n76abo1/\"&gt;here&lt;/a&gt;).  However, they are also what ends up on the GPU when you run &lt;code&gt;-ngl 99 -ot exps=CPU&lt;/code&gt; so aren&amp;#39;t going to be a major problem with mixed CPU+GPU inference.&lt;/p&gt;\n\n&lt;p&gt;Beyond that, mostly what you may notice here is that they aren&amp;#39;t &lt;em&gt;super&lt;/em&gt; different and the biggest difference is the IQ# vs Q#.  The IQ# are generally considered better than the equivalent Q4_K, but at the expense of requiring more compute.  So broadly I would say if you have surplus compute (vs memory bandwidth) the IQ3_XXS will be better but if you don&amp;#39;t the Q2_K_XL might be better.  In terms of quality, I suspect IQ3_XXS is a little a bit better but I doubt you&amp;#39;ll see any practical effect (but it&amp;#39;s almost certainly better than a Q2_K since I think heavily quantized attn can hurt a lot).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7aydvz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754514789,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjef0p",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7auo5x",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "fallingdowndizzyvr",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7akt6u",
                                          "score": 1,
                                          "author_fullname": "t2_o65i6kx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Hm... I might give it a try. I always thought ik_llama.cpp benefited CPU inference. Which I won't be doing. It'll all be GPU.\n\nI wish there was an Q3 though. I might just wait for that.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7auo5x",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hm... I might give it a try. I always thought ik_llama.cpp benefited CPU inference. Which I won&amp;#39;t be doing. It&amp;#39;ll all be GPU.&lt;/p&gt;\n\n&lt;p&gt;I wish there was an Q3 though. I might just wait for that.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mjef0p",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7auo5x/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754513698,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754513698,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7akt6u",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Willing_Landscape_61",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7ak1ew",
                                "score": 1,
                                "author_fullname": "t2_8lvrytgw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "ik_llama.cpp is faster in my experience and u/VoidAlchemy makes very good quants for it.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7akt6u",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ik_llama.cpp is faster in my experience and &lt;a href=\"/u/VoidAlchemy\"&gt;u/VoidAlchemy&lt;/a&gt; makes very good quants for it.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjef0p",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7akt6u/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754510928,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754510928,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ak1ew",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "fallingdowndizzyvr",
                      "can_mod_post": false,
                      "created_utc": 1754510710,
                      "send_replies": true,
                      "parent_id": "t1_n7ahc44",
                      "score": 1,
                      "author_fullname": "t2_o65i6kx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What would the advantage of that over Q2_K_XL?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ak1ew",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What would the advantage of that over Q2_K_XL?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjef0p",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7ak1ew/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754510710,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7ahc44",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1754509947,
            "send_replies": true,
            "parent_id": "t3_1mjef0p",
            "score": 0,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'd go withÂ https://huggingface.co/ubergarm/GLM-4.5-GGUF#iq2_kl-127746-gib-3062-bpw on ik_llama.cpp",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ahc44",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d go withÂ &lt;a href=\"https://huggingface.co/ubergarm/GLM-4.5-GGUF#iq2_kl-127746-gib-3062-bpw\"&gt;https://huggingface.co/ubergarm/GLM-4.5-GGUF#iq2_kl-127746-gib-3062-bpw&lt;/a&gt; on ik_llama.cpp&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7ahc44/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754509947,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjef0p",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7b22ut",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "disillusioned_okapi",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7aubaz",
                                                    "score": 0,
                                                    "author_fullname": "t2_wy3w8",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Deep down all we want is a 1TB of VRAM, and only then we shall feel VRAM rich ðŸ˜…",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7b22ut",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Deep down all we want is a 1TB of VRAM, and only then we shall feel VRAM rich ðŸ˜…&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mjef0p",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7b22ut/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754515891,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754515891,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 0
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7aubaz",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "fallingdowndizzyvr",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7aoa9m",
                                          "score": 1,
                                          "author_fullname": "t2_o65i6kx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "&gt; 10gb is a sizable difference! that's more than 20%!\n\nIt's only about 7-8%.\n\n&gt; and how can you say that you are vram poor if you can fit the enitre model into vram?!\n\nBut I can't fit the entire model. That would be like 800GB. I can only fit a heavily quantized version of the model. Thus I'm VRAM poor.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7aubaz",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;10gb is a sizable difference! that&amp;#39;s more than 20%!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s only about 7-8%.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;and how can you say that you are vram poor if you can fit the enitre model into vram?!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;But I can&amp;#39;t fit the entire model. That would be like 800GB. I can only fit a heavily quantized version of the model. Thus I&amp;#39;m VRAM poor.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mjef0p",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7aubaz/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754513599,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754513599,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7aoa9m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LagOps91",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7ak839",
                                "score": 0,
                                "author_fullname": "t2_3wi6j7vwh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "10gb is a sizable difference! that's more than 20%!\n\nand how can you say that you are vram poor if you can fit the enitre model into vram?! that's crazy talk! 10gb isn't noise either... it's more than what many people have.\n\nanway, run the IQ3\\_XXS quant for higher quality. since you are entirely on vram, speed shouldn't factor into the equation since it should be quite fast.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7aoa9m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;10gb is a sizable difference! that&amp;#39;s more than 20%!&lt;/p&gt;\n\n&lt;p&gt;and how can you say that you are vram poor if you can fit the enitre model into vram?! that&amp;#39;s crazy talk! 10gb isn&amp;#39;t noise either... it&amp;#39;s more than what many people have.&lt;/p&gt;\n\n&lt;p&gt;anway, run the IQ3_XXS quant for higher quality. since you are entirely on vram, speed shouldn&amp;#39;t factor into the equation since it should be quite fast.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjef0p",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7aoa9m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754511897,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754511897,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ak839",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "fallingdowndizzyvr",
                      "can_mod_post": false,
                      "created_utc": 1754510763,
                      "send_replies": true,
                      "parent_id": "t1_n7ahgah",
                      "score": 1,
                      "author_fullname": "t2_o65i6kx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; i was answering under the impression that Q2_K_XL is smaller than IQ3_XXS.\n\nIQ3 is about 10GB bigger. But 10GB in the scheme of things is noise. I can fit both in VRAM.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ak839",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;i was answering under the impression that Q2_K_XL is smaller than IQ3_XXS.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;IQ3 is about 10GB bigger. But 10GB in the scheme of things is noise. I can fit both in VRAM.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjef0p",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7ak839/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754510763,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7ahgah",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1754509980,
            "send_replies": true,
            "parent_id": "t3_1mjef0p",
            "score": -1,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "IQ3\\_XXS should have a better perplexity, but Q2\\_K\\_XLÂ allows you to fit more context if you are limited in terms of ram+vram (it sounds like you are).  There should also be a small speed difference as well.\n\nI am running the Q2\\_K\\_XLÂ and the model works well for me.\n\nEDIT: i was answering under the impression that Q2\\_K\\_XLÂ is smaller than IQ3\\_XXS. if they are about the same size then forget about what i was saying about context. I'm unsure what quant actually gives better perplexity.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ahgah",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;IQ3_XXS should have a better perplexity, but Q2_K_XLÂ allows you to fit more context if you are limited in terms of ram+vram (it sounds like you are).  There should also be a small speed difference as well.&lt;/p&gt;\n\n&lt;p&gt;I am running the Q2_K_XLÂ and the model works well for me.&lt;/p&gt;\n\n&lt;p&gt;EDIT: i was answering under the impression that Q2_K_XLÂ is smaller than IQ3_XXS. if they are about the same size then forget about what i was saying about context. I&amp;#39;m unsure what quant actually gives better perplexity.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7ahgah/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754509980,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjef0p",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7agicr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Sad_Comfortable1819",
            "can_mod_post": false,
            "created_utc": 1754509712,
            "send_replies": true,
            "parent_id": "t3_1mjef0p",
            "score": -2,
            "author_fullname": "t2_j5ibijl8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Q2\\_K\\_XL gives 11.1 tok/s on my 96 GB box",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7agicr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Q2_K_XL gives 11.1 tok/s on my 96 GB box&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/n7agicr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754509712,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjef0p",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -2
          }
        }
      ],
      "before": null
    }
  }
]