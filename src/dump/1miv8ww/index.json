[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’ve recently started exploring Local LLMs, with a focus on building a Retrieval-Augmented Generation (RAG) system. My background is in data analysis and data science, primarily using Python and SQL. I’m not familiar with other programming languages.\n\nTo improve my skills, I’m currently building a local RAG system (in Python) that queries a local version of the English Wikipedia. I’m using the `wiki40b/en` dataset, and my current hardware setup includes an RTX 2070 (8GB VRAM) and 16GB of RAM.\n\nSo far, I’ve successfully embedded the dataset using FAISS. I processed 19 Parquet files into 57 FAISS index files totaling around 20GB. I also have a retrieval script that works with these FAISS indexes, using the `Gemma 3 4B-it-QAT-Q4` model as the LLM backend.\n\nWhen I load a small subset of the FAISS indexes (2–3 files at a time), the retrieval is fast and RAG produces very good results, as long as the relevant information exists within the subset. However, when I try to query across the full set of FAISS files, the retrieval process becomes extremely slow and eventually crashes due to running out of memory.\n\nI’ve optimized the script to only load 3 FAISS files into the GPU at any one time, but this doesn’t seem to be enough to handle the full dataset efficiently.\n\n# My Questions:\n\n1. Is there any way to make this setup work without upgrading my hardware? I’ve already optimized the loading to limit GPU usage, but it still runs out of memory.\n2. If handling the full wiki40b dataset isn’t feasible on my current setup, are there any smaller datasets that are good for practicing RAG techniques locally?\n3. If I upgrade to an RTX 5090 (32GB VRAM) and 256GB RAM, would that solve the issue? My assumption is that with this hardware, I can load the full FAISS index and the Gemma 3 4B model into GPU memory, which should make the process much faster. I’m considering this upgrade and would like to use this as justification for getting a new machine. \n   * The 256 GB RAM is primarily intended to support larger or MoE (Mixture of Experts) models that activate multiple experts or require large context windows. I also anticipate scaling to more demanding workloads in the near future.\n\nAny advice on managing large-scale RAG workloads locally or selecting appropriate hardware would be greatly appreciated.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Question Regarding RAG Implementation and Hardware Limitations",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1miv8ww",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_bkb0tcya",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754454722,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve recently started exploring Local LLMs, with a focus on building a Retrieval-Augmented Generation (RAG) system. My background is in data analysis and data science, primarily using Python and SQL. I’m not familiar with other programming languages.&lt;/p&gt;\n\n&lt;p&gt;To improve my skills, I’m currently building a local RAG system (in Python) that queries a local version of the English Wikipedia. I’m using the &lt;code&gt;wiki40b/en&lt;/code&gt; dataset, and my current hardware setup includes an RTX 2070 (8GB VRAM) and 16GB of RAM.&lt;/p&gt;\n\n&lt;p&gt;So far, I’ve successfully embedded the dataset using FAISS. I processed 19 Parquet files into 57 FAISS index files totaling around 20GB. I also have a retrieval script that works with these FAISS indexes, using the &lt;code&gt;Gemma 3 4B-it-QAT-Q4&lt;/code&gt; model as the LLM backend.&lt;/p&gt;\n\n&lt;p&gt;When I load a small subset of the FAISS indexes (2–3 files at a time), the retrieval is fast and RAG produces very good results, as long as the relevant information exists within the subset. However, when I try to query across the full set of FAISS files, the retrieval process becomes extremely slow and eventually crashes due to running out of memory.&lt;/p&gt;\n\n&lt;p&gt;I’ve optimized the script to only load 3 FAISS files into the GPU at any one time, but this doesn’t seem to be enough to handle the full dataset efficiently.&lt;/p&gt;\n\n&lt;h1&gt;My Questions:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is there any way to make this setup work without upgrading my hardware? I’ve already optimized the loading to limit GPU usage, but it still runs out of memory.&lt;/li&gt;\n&lt;li&gt;If handling the full wiki40b dataset isn’t feasible on my current setup, are there any smaller datasets that are good for practicing RAG techniques locally?&lt;/li&gt;\n&lt;li&gt;If I upgrade to an RTX 5090 (32GB VRAM) and 256GB RAM, would that solve the issue? My assumption is that with this hardware, I can load the full FAISS index and the Gemma 3 4B model into GPU memory, which should make the process much faster. I’m considering this upgrade and would like to use this as justification for getting a new machine. \n\n&lt;ul&gt;\n&lt;li&gt;The 256 GB RAM is primarily intended to support larger or MoE (Mixture of Experts) models that activate multiple experts or require large context windows. I also anticipate scaling to more demanding workloads in the near future.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any advice on managing large-scale RAG workloads locally or selecting appropriate hardware would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1miv8ww",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "Saruphon",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1miv8ww/question_regarding_rag_implementation_and/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1miv8ww/question_regarding_rag_implementation_and/",
            "subreddit_subscribers": 511887,
            "created_utc": 1754454722,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n76iw9m",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Saruphon",
                      "can_mod_post": false,
                      "created_utc": 1754457568,
                      "send_replies": true,
                      "parent_id": "t1_n76gyjw",
                      "score": 0,
                      "author_fullname": "t2_bkb0tcya",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I am pretty sure it is suitable since it work when i have only 3-4 files. No longer work properly when i try to look at all files.\n\nHowever, as I am still learning about RAG, will also look into other types of vector dbs.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n76iw9m",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am pretty sure it is suitable since it work when i have only 3-4 files. No longer work properly when i try to look at all files.&lt;/p&gt;\n\n&lt;p&gt;However, as I am still learning about RAG, will also look into other types of vector dbs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miv8ww",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miv8ww/question_regarding_rag_implementation_and/n76iw9m/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754457568,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n76gyjw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Leflakk",
            "can_mod_post": false,
            "created_utc": 1754456637,
            "send_replies": true,
            "parent_id": "t3_1miv8ww",
            "score": 2,
            "author_fullname": "t2_udr659irv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Not a dev here but are you sure FAISS is suitable with so many indexes (fiiles) in parallel? Before upgrading, I would check for others type of vector dbs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76gyjw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not a dev here but are you sure FAISS is suitable with so many indexes (fiiles) in parallel? Before upgrading, I would check for others type of vector dbs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miv8ww/question_regarding_rag_implementation_and/n76gyjw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754456637,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miv8ww",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n76iqcb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Saruphon",
                      "can_mod_post": false,
                      "created_utc": 1754457489,
                      "send_replies": true,
                      "parent_id": "t1_n76i1j1",
                      "score": 1,
                      "author_fullname": "t2_bkb0tcya",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thank you for your advise on this.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n76iqcb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for your advise on this.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miv8ww",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miv8ww/question_regarding_rag_implementation_and/n76iqcb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754457489,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n76i1j1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "balianone",
            "can_mod_post": false,
            "created_utc": 1754457155,
            "send_replies": true,
            "parent_id": "t3_1miv8ww",
            "score": 2,
            "author_fullname": "t2_8pgou3uq9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Your 20GB index is too big for your 16GB RAM; use a memory-mapped FAISS index to query from disk, or yes, upgrading your hardware will solve the problem by letting you load everything into memory for much faster performance.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76i1j1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your 20GB index is too big for your 16GB RAM; use a memory-mapped FAISS index to query from disk, or yes, upgrading your hardware will solve the problem by letting you load everything into memory for much faster performance.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miv8ww/question_regarding_rag_implementation_and/n76i1j1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754457155,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miv8ww",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]