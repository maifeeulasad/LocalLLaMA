[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "üß† Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I‚Äôve implemented after Gemma üöÄ. This was a major milestone for me, especially since there‚Äôs no open-source implementation of Qwen 2 available online (at least none I could find).\n\nWhat makes this build special:\n‚úÖ Implemented without access to source code\nüìñ Based entirely on the Qwen 1 &amp; Qwen 2 research papers\nüß± Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n‚ö†Ô∏è Does not support Mixture of Experts (MoE) yet\n\nThis project pushed my understanding of transformer architectures even further, and I‚Äôm excited to keep going.\nIf you're into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!\n\nSource code: https://github.com/introlix/Swiftlet\nKaggle: https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Reimplemention of Qwen 2 from scratch",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgpb8t",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.92,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 71,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_6qpq9avr5",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 71,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754242723,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üß† Just Finished: Implementing Qwen 2 (1.5B) from Scratch\nA few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I‚Äôve implemented after Gemma üöÄ. This was a major milestone for me, especially since there‚Äôs no open-source implementation of Qwen 2 available online (at least none I could find).&lt;/p&gt;\n\n&lt;p&gt;What makes this build special:\n‚úÖ Implemented without access to source code\nüìñ Based entirely on the Qwen 1 &amp;amp; Qwen 2 research papers\nüß± Supports Qwen 2-1.5B architecture (more sizes coming soon!)\n‚ö†Ô∏è Does not support Mixture of Experts (MoE) yet&lt;/p&gt;\n\n&lt;p&gt;This project pushed my understanding of transformer architectures even further, and I‚Äôm excited to keep going.\nIf you&amp;#39;re into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/introlix/Swiftlet\"&gt;https://github.com/introlix/Swiftlet&lt;/a&gt;\nKaggle: &lt;a href=\"https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet\"&gt;https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?auto=webp&amp;s=519a96c79f13a619e42513cc0f904d9b36729fa1",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09e4e357ff6f03ec51f0d4d875169c2822efb899",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a804dcd73acb0eafcbf3c98a337b04b0330590",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6460444e14680b3ebc1e71bebe27f5eaaab08b28",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=142de401ddac2c07588c3df4af6a9a88a8c43665",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=730308dc3a494b5e493c8cc6e298f0435aba1498",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=798ec017daa864fb62d31aa0cd92c8316b840da9",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "qSeAasESDn-vQm932F6I_C6FnJQShX4HO9nyyxSZWlY"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mgpb8t",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "CodingWithSatyam",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/",
            "subreddit_subscribers": 509625,
            "created_utc": 1754242723,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6qc4ls",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1754243686,
            "send_replies": true,
            "parent_id": "t3_1mgpb8t",
            "score": 8,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Congratulations !",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qc4ls",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Congratulations !&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/n6qc4ls/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754243686,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpb8t",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6qc9g1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "thisismylastaccount_",
            "can_mod_post": false,
            "created_utc": 1754243726,
            "send_replies": true,
            "parent_id": "t3_1mgpb8t",
            "score": 8,
            "author_fullname": "t2_r0nzujad",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Good work, but how is it different from the HF Transformers implementation of Qwen2?  Is this a pedagogical effort?\n\nedit: I just saw that this is the 1.5B params version. Are there any significant arch differences from the 7B one?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qc9g1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Good work, but how is it different from the HF Transformers implementation of Qwen2?  Is this a pedagogical effort?&lt;/p&gt;\n\n&lt;p&gt;edit: I just saw that this is the 1.5B params version. Are there any significant arch differences from the 7B one?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/n6qc9g1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754243726,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpb8t",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6r1pl0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Technical-General578",
            "can_mod_post": false,
            "created_utc": 1754251649,
            "send_replies": true,
            "parent_id": "t3_1mgpb8t",
            "score": 0,
            "author_fullname": "t2_78o7po6p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How is this different from the transformers code in their repo ?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6r1pl0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How is this different from the transformers code in their repo ?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/n6r1pl0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754251649,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgpb8t",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "total_awards_received": 0,
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "ups": 0,
            "removal_reason": null,
            "link_id": "t3_1mgpb8t",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6r5xzu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "[deleted]",
            "can_mod_post": false,
            "send_replies": true,
            "parent_id": "t3_1mgpb8t",
            "score": 0,
            "approved_by": null,
            "report_reasons": null,
            "all_awardings": [],
            "subreddit_id": "t5_81eyvm",
            "body": "Thanks but I can't even install \"bitsandbytes\" (a dependency) because I don't want closed source CUDA installed on my machine. This applies to other packages as well (that require CUDA) and the problem cascades, limiting what I can do (but not as severely as you might think.) I wish all open source programmers would reject this closed source blob nonsense, it's antithetical and counter-productive. I guess I'm less pragmatic than I am idealistic, but these are my two cents.",
            "edited": false,
            "downs": 0,
            "author_flair_css_class": null,
            "collapsed": false,
            "is_submitter": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks but I can&amp;#39;t even install &amp;quot;bitsandbytes&amp;quot; (a dependency) because I don&amp;#39;t want closed source CUDA installed on my machine. This applies to other packages as well (that require CUDA) and the problem cascades, limiting what I can do (but not as severely as you might think.) I wish all open source programmers would reject this closed source blob nonsense, it&amp;#39;s antithetical and counter-productive. I guess I&amp;#39;m less pragmatic than I am idealistic, but these are my two cents.&lt;/p&gt;\n&lt;/div&gt;",
            "gildings": {},
            "collapsed_reason": null,
            "associated_award": null,
            "stickied": false,
            "subreddit_type": "public",
            "can_gild": false,
            "top_awarded_type": null,
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/n6r5xzu/",
            "num_reports": null,
            "locked": false,
            "name": "t1_n6r5xzu",
            "created": 1754252930,
            "subreddit": "LocalLLaMA",
            "author_flair_text": null,
            "treatment_tags": [],
            "created_utc": 1754252930,
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "mod_note": null,
            "distinguished": null
          }
        }
      ],
      "before": null
    }
  }
]