[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.\n\nIf I already know that I want to do something like creative writing, why can’t I just have just the creative writing expert so I only need to load that?\n\nWouldn’t this help with the required ram/vram amount?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Can you just have one expert from an MOE model",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8qmd7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.71,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rn6co7q5m",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753420347,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From what I understand, an MOE model contains many experts, and when you give it a prompt, it chooses one expert to answer your query.&lt;/p&gt;\n\n&lt;p&gt;If I already know that I want to do something like creative writing, why can’t I just have just the creative writing expert so I only need to load that?&lt;/p&gt;\n\n&lt;p&gt;Wouldn’t this help with the required ram/vram amount?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m8qmd7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "opoot_",
            "discussion_type": null,
            "num_comments": 18,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/",
            "subreddit_subscribers": 504253,
            "created_utc": 1753420347,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n51aq0s",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "rawrmaan",
            "can_mod_post": false,
            "created_utc": 1753420778,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 30,
            "author_fullname": "t2_7u686",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This won’t work for multiple reasons, but one of the reasons is that the experts aren’t experts in defined subjects. Their expertise is extremely abstract and emerges during training.\n\nSo in an MoE model, the “creative writing expertise” could be spread amongst many experts, and it would be quite involved to determine which experts those are with analysis of activation patterns etc.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51aq0s",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This won’t work for multiple reasons, but one of the reasons is that the experts aren’t experts in defined subjects. Their expertise is extremely abstract and emerges during training.&lt;/p&gt;\n\n&lt;p&gt;So in an MoE model, the “creative writing expertise” could be spread amongst many experts, and it would be quite involved to determine which experts those are with analysis of activation patterns etc.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51aq0s/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753420778,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 30
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n51ziol",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "blankboy2022",
                      "can_mod_post": false,
                      "created_utc": 1753434301,
                      "send_replies": true,
                      "parent_id": "t1_n51cjwm",
                      "score": 2,
                      "author_fullname": "t2_42ba8hq5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Cool explanation!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n51ziol",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Cool explanation!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8qmd7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51ziol/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753434301,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n51cjwm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "IKeepForgetting",
            "can_mod_post": false,
            "created_utc": 1753421680,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 23,
            "author_fullname": "t2_3fqrk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think calling it \"experts\" was bad marketing... let's call them legos.   \n  \nWhen training it was like \"ok you have to learn how to solve each of these problems using **exactly** 5 legos, but you have these 50 legos to choose from\". So it learned how to solve **every** problem with exactly 5 legos/\"experts\". It's just in practice we see \"oh, for creative writing it tends to choose this lego here\"",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51cjwm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think calling it &amp;quot;experts&amp;quot; was bad marketing... let&amp;#39;s call them legos.   &lt;/p&gt;\n\n&lt;p&gt;When training it was like &amp;quot;ok you have to learn how to solve each of these problems using &lt;strong&gt;exactly&lt;/strong&gt; 5 legos, but you have these 50 legos to choose from&amp;quot;. So it learned how to solve &lt;strong&gt;every&lt;/strong&gt; problem with exactly 5 legos/&amp;quot;experts&amp;quot;. It&amp;#39;s just in practice we see &amp;quot;oh, for creative writing it tends to choose this lego here&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51cjwm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753421680,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 23
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n51z1ur",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "boringcynicism",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n51p8rj",
                                "score": 2,
                                "author_fullname": "t2_1hz0lz0k5i",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "They are trained that way though, but it's a good point each expert has to be able to interpret another's context.\n\nEdit: This isn't really true, as the router can select sequences where it knows they understand their predecessors. They also have a to interpret the previous layer no matter what expert was there. So representation may be very shared among experts.",
                                "edited": 1753434265,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n51z1ur",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They are trained that way though, but it&amp;#39;s a good point each expert has to be able to interpret another&amp;#39;s context.&lt;/p&gt;\n\n&lt;p&gt;Edit: This isn&amp;#39;t really true, as the router can select sequences where it knows they understand their predecessors. They also have a to interpret the previous layer no matter what expert was there. So representation may be very shared among experts.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8qmd7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51z1ur/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753434027,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753434027,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n51p8rj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Fetlocks_Glistening",
                      "can_mod_post": false,
                      "created_utc": 1753428403,
                      "send_replies": true,
                      "parent_id": "t1_n51dda7",
                      "score": 4,
                      "author_fullname": "t2_lq9vco2g",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Doesn't that granularity totally break context, and mean none of the experts processes a contextually-relevant interpretation of their token?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n51p8rj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Doesn&amp;#39;t that granularity totally break context, and mean none of the experts processes a contextually-relevant interpretation of their token?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8qmd7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51p8rj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753428403,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n51zd72",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AuspiciousApple",
                      "can_mod_post": false,
                      "created_utc": 1753434211,
                      "send_replies": true,
                      "parent_id": "t1_n51dda7",
                      "score": 3,
                      "author_fullname": "t2_bndbg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Man, imagine being the \"press tab\" expert",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n51zd72",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Man, imagine being the &amp;quot;press tab&amp;quot; expert&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8qmd7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51zd72/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753434211,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n52b4sw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "boringcynicism",
                      "can_mod_post": false,
                      "created_utc": 1753440533,
                      "send_replies": true,
                      "parent_id": "t1_n51dda7",
                      "score": 1,
                      "author_fullname": "t2_1hz0lz0k5i",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I guess you could change the training to bias staying on the same expert.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52b4sw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I guess you could change the training to bias staying on the same expert.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8qmd7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n52b4sw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753440533,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n51dda7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Threatening-Silence-",
            "can_mod_post": false,
            "created_utc": 1753422091,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 16,
            "author_fullname": "t2_15wqsifdjf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://preview.redd.it/kjfzuc14kyef1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=7628757c10dc563bc182202b7a37ff9294062602\n\nExpert choice can change with every token.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51dda7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/kjfzuc14kyef1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7628757c10dc563bc182202b7a37ff9294062602\"&gt;https://preview.redd.it/kjfzuc14kyef1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7628757c10dc563bc182202b7a37ff9294062602&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Expert choice can change with every token.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51dda7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753422091,
            "media_metadata": {
              "kjfzuc14kyef1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 78,
                    "x": 108,
                    "u": "https://preview.redd.it/kjfzuc14kyef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2fd3e1cb001de1644b17d18e2b4362e4fbcb6de3"
                  },
                  {
                    "y": 156,
                    "x": 216,
                    "u": "https://preview.redd.it/kjfzuc14kyef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d38e5785cbc4df932c3fdcfa9fba9fdb4a1c491"
                  },
                  {
                    "y": 231,
                    "x": 320,
                    "u": "https://preview.redd.it/kjfzuc14kyef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c8a206f9191eaf0046e91d4a8b727a1f684e055"
                  },
                  {
                    "y": 463,
                    "x": 640,
                    "u": "https://preview.redd.it/kjfzuc14kyef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99f9f5dd217c4e921edbed7dc0459a70c7dc75de"
                  },
                  {
                    "y": 695,
                    "x": 960,
                    "u": "https://preview.redd.it/kjfzuc14kyef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ef7410ee87dc3672f5853f02180edea965400bba"
                  },
                  {
                    "y": 782,
                    "x": 1080,
                    "u": "https://preview.redd.it/kjfzuc14kyef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=098acd6c7d5ba63f6e234842be95ce32075fa60c"
                  }
                ],
                "s": {
                  "y": 782,
                  "x": 1080,
                  "u": "https://preview.redd.it/kjfzuc14kyef1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=7628757c10dc563bc182202b7a37ff9294062602"
                },
                "id": "kjfzuc14kyef1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 16
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n51njkp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CV514",
                      "can_mod_post": false,
                      "created_utc": 1753427448,
                      "send_replies": true,
                      "parent_id": "t1_n51chlp",
                      "score": 1,
                      "author_fullname": "t2_n4zvv",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "And for creative writing specifically, this is still a very solid option.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n51njkp",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And for creative writing specifically, this is still a very solid option.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8qmd7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51njkp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753427448,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n51chlp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Former-Ad-5757",
            "can_mod_post": false,
            "created_utc": 1753421648,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 9,
            "author_fullname": "t2_ihsdiwk6k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "And the expert is not chosen per prompt, but per token. If an expert was chosen per prompt then you would just have a simple 12b model",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51chlp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And the expert is not chosen per prompt, but per token. If an expert was chosen per prompt then you would just have a simple 12b model&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51chlp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753421648,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n51ojh4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "anarchos",
            "can_mod_post": false,
            "created_utc": 1753428008,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 5,
            "author_fullname": "t2_44chy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Unfortunately it doesn't work like that.  They're not really experts like we'd think, there's no \"writing expert\", no \"math expert\" and no \"coding expert\".  Think of it more like they split the model into chunks.  Each new token generated might use any chunk, because the \"knowledge\" for that token could happen to be contained in any chunk.\n\nSince any token might need any chunk, they all have to be available in ram/vram.  The only thing MOE helps with is speed, since sending the context through a smaller chunk to get the next token is much faster than sending it through a monolithic (or bigger) chunk.\n\nWhen generating anything other than a simple \"Hello!\" response I'd hazard to guess every chunk (expert) in a MOE model was activated at least once.  I don't actually know that for sure but it's what my intuition is telling me.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51ojh4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately it doesn&amp;#39;t work like that.  They&amp;#39;re not really experts like we&amp;#39;d think, there&amp;#39;s no &amp;quot;writing expert&amp;quot;, no &amp;quot;math expert&amp;quot; and no &amp;quot;coding expert&amp;quot;.  Think of it more like they split the model into chunks.  Each new token generated might use any chunk, because the &amp;quot;knowledge&amp;quot; for that token could happen to be contained in any chunk.&lt;/p&gt;\n\n&lt;p&gt;Since any token might need any chunk, they all have to be available in ram/vram.  The only thing MOE helps with is speed, since sending the context through a smaller chunk to get the next token is much faster than sending it through a monolithic (or bigger) chunk.&lt;/p&gt;\n\n&lt;p&gt;When generating anything other than a simple &amp;quot;Hello!&amp;quot; response I&amp;#39;d hazard to guess every chunk (expert) in a MOE model was activated at least once.  I don&amp;#39;t actually know that for sure but it&amp;#39;s what my intuition is telling me.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51ojh4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753428008,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n51nkhe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Gusanidas",
            "can_mod_post": false,
            "created_utc": 1753427462,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 3,
            "author_fullname": "t2_8pkku1t0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Other comments have mentioned that expert choice depends on each token. It also varies per layer. Each expert is simply an MLP (not a complete model), and at every layer, the routing mechanism selects one or more experts to process each token. Given the vast number of possible expert combinations across all layers, it's entirely possible—even likely—that certain prompts will trigger routing patterns that have never occurred during training (or ever).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51nkhe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Other comments have mentioned that expert choice depends on each token. It also varies per layer. Each expert is simply an MLP (not a complete model), and at every layer, the routing mechanism selects one or more experts to process each token. Given the vast number of possible expert combinations across all layers, it&amp;#39;s entirely possible—even likely—that certain prompts will trigger routing patterns that have never occurred during training (or ever).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51nkhe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753427462,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n51yn51",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "opoot_",
                      "can_mod_post": false,
                      "created_utc": 1753433789,
                      "send_replies": true,
                      "parent_id": "t1_n51iibu",
                      "score": 1,
                      "author_fullname": "t2_rn6co7q5m",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I assumed from the name and people talking about it being way faster than one big model.\n\nBecause like it says a smaller parameter after the big parameter that’s the whole model.\n\nSo I thought “Oh, it uses one of the smaller parameters after choosing which one to use”",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n51yn51",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I assumed from the name and people talking about it being way faster than one big model.&lt;/p&gt;\n\n&lt;p&gt;Because like it says a smaller parameter after the big parameter that’s the whole model.&lt;/p&gt;\n\n&lt;p&gt;So I thought “Oh, it uses one of the smaller parameters after choosing which one to use”&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8qmd7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51yn51/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753433789,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n51iibu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1753424734,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;From what I understand...\n\nSo, did somebody suggest this to you or write that it worked like this? 🤔",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51iibu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;From what I understand...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;So, did somebody suggest this to you or write that it worked like this? 🤔&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51iibu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753424734,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n51lvuo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Wooden-Potential2226",
            "can_mod_post": false,
            "created_utc": 1753426545,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 1,
            "author_fullname": "t2_9mo23y823",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "AFAIK Exllama2 had some cli options for adding or removing layers from MoE models during inference",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51lvuo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;AFAIK Exllama2 had some cli options for adding or removing layers from MoE models during inference&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51lvuo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753426545,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n51tixp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Impressive_Half_2819",
            "can_mod_post": false,
            "created_utc": 1753430835,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 1,
            "author_fullname": "t2_1fpbfjtnc5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I don’t think it works that way.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51tixp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don’t think it works that way.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n51tixp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753430835,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n520lpv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AFruitShopOwner",
            "can_mod_post": false,
            "created_utc": 1753434926,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 1,
            "author_fullname": "t2_h15f0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "2024\t“Not All Experts Are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.”\t\n\nPost-training algorithm to drop entire experts (task-agnostic or task-specific) and a dynamic “expert-skipping” scheme at inference, cutting memory and latency with &lt;3 pt average task loss.\tLu et al., ACL 2024\nhttps://arxiv.org/abs/2402.14800\n\n\n2024\t“Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs.”\n\nGradient-free evolutionary strategy (EEP) that prunes up to 75 % of Mixtral-8×7B experts; sometimes improves downstream accuracy.\tLiu et al., arXiv Jul 2024\nhttps://arxiv.org/html/2402.14800v1?\n\n2024\t“MoE-Pruner: Pruning Mixture-of-Experts Large Language Models Using the Hints from Its Router.”\n\nOne-shot pruning guided by the router’s routing weights × activations; no retraining needed, preserves 99 % of Mixtral-8×7B after pruning 50 % of weights plus expert-wise KD.\tXie et al., arXiv Oct 2024\nhttps://arxiv.org/html/2410.12013v1?utm_source=chatgpt.com",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n520lpv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;2024    “Not All Experts Are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.”    &lt;/p&gt;\n\n&lt;p&gt;Post-training algorithm to drop entire experts (task-agnostic or task-specific) and a dynamic “expert-skipping” scheme at inference, cutting memory and latency with &amp;lt;3 pt average task loss.   Lu et al., ACL 2024\n&lt;a href=\"https://arxiv.org/abs/2402.14800\"&gt;https://arxiv.org/abs/2402.14800&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;2024    “Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs.”&lt;/p&gt;\n\n&lt;p&gt;Gradient-free evolutionary strategy (EEP) that prunes up to 75 % of Mixtral-8×7B experts; sometimes improves downstream accuracy.  Liu et al., arXiv Jul 2024\n&lt;a href=\"https://arxiv.org/html/2402.14800v1\"&gt;https://arxiv.org/html/2402.14800v1&lt;/a&gt;?&lt;/p&gt;\n\n&lt;p&gt;2024    “MoE-Pruner: Pruning Mixture-of-Experts Large Language Models Using the Hints from Its Router.”&lt;/p&gt;\n\n&lt;p&gt;One-shot pruning guided by the router’s routing weights × activations; no retraining needed, preserves 99 % of Mixtral-8×7B after pruning 50 % of weights plus expert-wise KD.  Xie et al., arXiv Oct 2024\n&lt;a href=\"https://arxiv.org/html/2410.12013v1?utm_source=chatgpt.com\"&gt;https://arxiv.org/html/2410.12013v1?utm_source=chatgpt.com&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n520lpv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753434926,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n524i08",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Herr_Drosselmeyer",
            "can_mod_post": false,
            "created_utc": 1753437120,
            "send_replies": true,
            "parent_id": "t3_1m8qmd7",
            "score": 1,
            "author_fullname": "t2_1zr9gwsn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;it chooses one expert to answer your query.\n\nNope.\n\nExperts are chosen per token and per layer and they're not experts in the sense that one is better at maths, another one at biology etc. \n\nInstead, they're 'experts' at handling certain types of tokens, like say one is better at punctuation, one is better at numbers, etc. (that's still simplified but you get the idea).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n524i08",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;it chooses one expert to answer your query.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Nope.&lt;/p&gt;\n\n&lt;p&gt;Experts are chosen per token and per layer and they&amp;#39;re not experts in the sense that one is better at maths, another one at biology etc. &lt;/p&gt;\n\n&lt;p&gt;Instead, they&amp;#39;re &amp;#39;experts&amp;#39; at handling certain types of tokens, like say one is better at punctuation, one is better at numbers, etc. (that&amp;#39;s still simplified but you get the idea).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8qmd7/can_you_just_have_one_expert_from_an_moe_model/n524i08/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753437120,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8qmd7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]