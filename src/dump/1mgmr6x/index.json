[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.  \nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How many of you actually know by heart the general structure of the transformer architecture?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgmr6x",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rgy8m2w",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754236507,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mean we all know the concepts, but how many can actually say they memorized at least the high level architecture of the transformer.&lt;br/&gt;\nwhich architectures/knowledge do you consider a must for scaling and fine tuning models? (GPT? BERT? what ever deepseek did with their articles?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgmr6x",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "CptKrupnik",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/",
            "subreddit_subscribers": 509916,
            "created_utc": 1754236507,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6pqyu9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "viag",
            "can_mod_post": false,
            "created_utc": 1754237314,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 11,
            "author_fullname": "t2_i3e2x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I do, but it's my job :p\n\nHonestly, more than the architecture of the model itself I think the most important part of model fine-tuning will always be to make sure your experimental setup is sound, and that you have a reliable way to evaluate the baseline &amp; your models. Also, spend some time to make sure your data is clean! :) (it's probably not)\n\nNot very sexy but this gets you 80% there.\n\nThen you can experiment with instruction tuning, preference tuning, RLVR or anything you'd like. Training models is often a very hands-on approach, you have to try stuff!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pqyu9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I do, but it&amp;#39;s my job :p&lt;/p&gt;\n\n&lt;p&gt;Honestly, more than the architecture of the model itself I think the most important part of model fine-tuning will always be to make sure your experimental setup is sound, and that you have a reliable way to evaluate the baseline &amp;amp; your models. Also, spend some time to make sure your data is clean! :) (it&amp;#39;s probably not)&lt;/p&gt;\n\n&lt;p&gt;Not very sexy but this gets you 80% there.&lt;/p&gt;\n\n&lt;p&gt;Then you can experiment with instruction tuning, preference tuning, RLVR or anything you&amp;#39;d like. Training models is often a very hands-on approach, you have to try stuff!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6pqyu9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754237314,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6pqh1p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "walagoth",
            "can_mod_post": false,
            "created_utc": 1754237163,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 9,
            "author_fullname": "t2_ckz6ct5c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm sure there are technical purists out there that claim this is needed, but most of us aren't implementing new architectures. Unless you are a top1% researcher with resources to do pretraining, it just isn't needed. Maybe go through the karpathy videos so you know what you are missing and then get on with something more tangible.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pqh1p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure there are technical purists out there that claim this is needed, but most of us aren&amp;#39;t implementing new architectures. Unless you are a top1% researcher with resources to do pretraining, it just isn&amp;#39;t needed. Maybe go through the karpathy videos so you know what you are missing and then get on with something more tangible.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6pqh1p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754237163,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6q6b67",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BigRepresentative731",
            "can_mod_post": false,
            "created_utc": 1754241961,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 3,
            "author_fullname": "t2_8a78x7h6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Well I know that we have an input sequence of tokens up to a fixed context length, I know that each token is turned into an embedding and combined with a positional embedding, I know that each transformer layer has a self-attention block and a feed-forward block, and that the self-attention uses three learned matrices to compute query, key, and value vectors from the token embeddings.\n\nI know that we take the dot product of each query with every key to get attention scores, scale them, apply softmax to get attention weights, and use those weights to take a weighted sum of the value vectors, producing a new representation for each token.\n\nI know that this output goes through a residual connection and layer norm, then through the feed-forward block, which is just a small MLP, then another residual and norm. I know that this whole process is repeated across layers, and in decoder models like GPT, we use causal masking so tokens can't see future ones.\n\nI know that at the end, we project back to vocabulary space for predictions.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6q6b67",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well I know that we have an input sequence of tokens up to a fixed context length, I know that each token is turned into an embedding and combined with a positional embedding, I know that each transformer layer has a self-attention block and a feed-forward block, and that the self-attention uses three learned matrices to compute query, key, and value vectors from the token embeddings.&lt;/p&gt;\n\n&lt;p&gt;I know that we take the dot product of each query with every key to get attention scores, scale them, apply softmax to get attention weights, and use those weights to take a weighted sum of the value vectors, producing a new representation for each token.&lt;/p&gt;\n\n&lt;p&gt;I know that this output goes through a residual connection and layer norm, then through the feed-forward block, which is just a small MLP, then another residual and norm. I know that this whole process is repeated across layers, and in decoder models like GPT, we use causal masking so tokens can&amp;#39;t see future ones.&lt;/p&gt;\n\n&lt;p&gt;I know that at the end, we project back to vocabulary space for predictions.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6q6b67/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754241961,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6pz2uo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "hi87",
            "can_mod_post": false,
            "created_utc": 1754239806,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 2,
            "author_fullname": "t2_etrdb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Building LLM From Scratch is a great book for an in depth understanding of the architechture.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pz2uo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Building LLM From Scratch is a great book for an in depth understanding of the architechture.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6pz2uo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754239806,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6pr6kj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "101m4n",
            "can_mod_post": false,
            "created_utc": 1754237380,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 1,
            "author_fullname": "t2_p7nc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The basic structure of transformer models isn't all that complicated, if you understand matrix multiplication then you can learn the broad strokes in an afternoon.\n\nThere are variations on it of course, sliding window attention (mistral), grouped query attention (llama), multi-head latent attention (deepseek). \n\nThere are also ancillary concepts like embedding and vector search, RAG, quantisation, perplexity etc.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pr6kj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The basic structure of transformer models isn&amp;#39;t all that complicated, if you understand matrix multiplication then you can learn the broad strokes in an afternoon.&lt;/p&gt;\n\n&lt;p&gt;There are variations on it of course, sliding window attention (mistral), grouped query attention (llama), multi-head latent attention (deepseek). &lt;/p&gt;\n\n&lt;p&gt;There are also ancillary concepts like embedding and vector search, RAG, quantisation, perplexity etc.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6pr6kj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754237380,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6puw8z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DeltaSqueezer",
            "can_mod_post": false,
            "created_utc": 1754238522,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 1,
            "author_fullname": "t2_8jqx3m14",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I just have a hazy overview. I could draw a block diagram from memory, but if I had to sit down and implement it in code from scratch from memory, I would quickly discover the many holes in my understanding. I just learned enough for what I need.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6puw8z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just have a hazy overview. I could draw a block diagram from memory, but if I had to sit down and implement it in code from scratch from memory, I would quickly discover the many holes in my understanding. I just learned enough for what I need.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6puw8z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754238522,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6q7bq7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "no_witty_username",
            "can_mod_post": false,
            "created_utc": 1754242264,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 1,
            "author_fullname": "t2_4j2nc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I doubt too many people do, but this level of knowledge is not needed to make significant progress in the area IMO. I think building a robust scaffolding around LLM's is the most important area that will yield best value for your time.  There is still A LOT of low hanging fruit left in the field.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6q7bq7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I doubt too many people do, but this level of knowledge is not needed to make significant progress in the area IMO. I think building a robust scaffolding around LLM&amp;#39;s is the most important area that will yield best value for your time.  There is still A LOT of low hanging fruit left in the field.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6q7bq7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754242264,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6sydir",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wfgy_engine",
            "can_mod_post": false,
            "created_utc": 1754274939,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 1,
            "author_fullname": "t2_1tgp8l87vk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "honestly, the real architecture most ppl *should* memorize isn’t the transformer stack — it’s where the reasoning fails quietly.\n\nlike:\n\n* model gives you a confident answer… but it's structurally off\n* logic chain resets halfway and no one notices\n* chunk looks semantically fine but breaks downstream\n* attention replays past paths without realizing it\n\nwe ended up mapping these as 16 failure patterns in actual production — not theory, just stuff that kept breaking when you scale up LLM use.  \nMIT-licensed, open source. even got a ⭐ from the tesseract.js author (he found it useful too lol)\n\nif you're curious, happy to send over the map. just let me know.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6sydir",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;honestly, the real architecture most ppl &lt;em&gt;should&lt;/em&gt; memorize isn’t the transformer stack — it’s where the reasoning fails quietly.&lt;/p&gt;\n\n&lt;p&gt;like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;model gives you a confident answer… but it&amp;#39;s structurally off&lt;/li&gt;\n&lt;li&gt;logic chain resets halfway and no one notices&lt;/li&gt;\n&lt;li&gt;chunk looks semantically fine but breaks downstream&lt;/li&gt;\n&lt;li&gt;attention replays past paths without realizing it&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;we ended up mapping these as 16 failure patterns in actual production — not theory, just stuff that kept breaking when you scale up LLM use.&lt;br/&gt;\nMIT-licensed, open source. even got a ⭐ from the tesseract.js author (he found it useful too lol)&lt;/p&gt;\n\n&lt;p&gt;if you&amp;#39;re curious, happy to send over the map. just let me know.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6sydir/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754274939,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ti2kk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dapper_Extent_7474",
            "can_mod_post": false,
            "created_utc": 1754283372,
            "send_replies": true,
            "parent_id": "t3_1mgmr6x",
            "score": 1,
            "author_fullname": "t2_1pxvlldm8v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "feedforward, self attention, normalization, some kind of positional encoding (like rotary), just some things off the top of my head. The self attention is truly what makes a transformer a transformer though.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ti2kk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;feedforward, self attention, normalization, some kind of positional encoding (like rotary), just some things off the top of my head. The self attention is truly what makes a transformer a transformer though.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgmr6x/how_many_of_you_actually_know_by_heart_the/n6ti2kk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754283372,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgmr6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]