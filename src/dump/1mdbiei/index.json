[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'd like to make a video game that utilizes AI to have some conversation with users.  It doesn't need to win an IMO but it should be able to carry normal every day conversations.  And preferably it would be able to do text to speech.  But I don't think normal computers are powerful enough for this?  Am I mistaken?  Can a local llama of some type be run on an average PC to understand and speak?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "AI for normal PCs?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdbiei",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_u5j388982",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753893603,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to make a video game that utilizes AI to have some conversation with users.  It doesn&amp;#39;t need to win an IMO but it should be able to carry normal every day conversations.  And preferably it would be able to do text to speech.  But I don&amp;#39;t think normal computers are powerful enough for this?  Am I mistaken?  Can a local llama of some type be run on an average PC to understand and speak?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdbiei",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ShardsOfSalt",
            "discussion_type": null,
            "num_comments": 18,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/",
            "subreddit_subscribers": 507275,
            "created_utc": 1753893603,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n60ietg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Xamanthas",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n60etzn",
                                "score": 9,
                                "author_fullname": "t2_e6bnx",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Learn to crawl and walk before running.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n60ietg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Learn to crawl and walk before running.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdbiei",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60ietg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753896401,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753896401,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 9
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62mmq9",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "ashirviskas",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n60ogno",
                                          "score": 1,
                                          "author_fullname": "t2_9pixf",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Just force JSON schema? No need for bigger models",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62mmq9",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just force JSON schema? No need for bigger models&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdbiei",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n62mmq9/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753918636,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753918636,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n60ogno",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "General_Service_8209",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n60etzn",
                                "score": 3,
                                "author_fullname": "t2_jkfuj8nc",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I think you are underestimating dialogue managers.\nThere’s a game called Event 0, which uses them for pretty much what you envision: The player can talk to an AI, completely in natural language and without any constraints, and the AI can make sense of it and give the player relevant answers, as well as interact with the game‘s systems.\n\nhttps://store.steampowered.com/app/470260/Event0/\n\nThat game is from 2016, before LLMs even existed.\n\nThe problem you’re running into with LLMs is that you don’t just want your AI to keep a coherent conversation. It also has to incorporate knowledge about the game world and reasoning into its reply, has to double check what the player tells it (e.g. not give in if the player insists they have a key they don’t have), not make up or hallucinate stuff, and communicate back to the rest of the game, which requires output in a highly specific format to name sure it can be interpreted by a fixed program. All of these make the task harder.\n\nThere are local LLMs that can do all this, but they require much more resources to run. I would roughly guess at least 8-10GB of vram if you’re really lucky and can deal with some unreliability, snd realistically, more like 16GB. That’s more than the majority of Steam users has.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n60ogno",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think you are underestimating dialogue managers.\nThere’s a game called Event 0, which uses them for pretty much what you envision: The player can talk to an AI, completely in natural language and without any constraints, and the AI can make sense of it and give the player relevant answers, as well as interact with the game‘s systems.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://store.steampowered.com/app/470260/Event0/\"&gt;https://store.steampowered.com/app/470260/Event0/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;That game is from 2016, before LLMs even existed.&lt;/p&gt;\n\n&lt;p&gt;The problem you’re running into with LLMs is that you don’t just want your AI to keep a coherent conversation. It also has to incorporate knowledge about the game world and reasoning into its reply, has to double check what the player tells it (e.g. not give in if the player insists they have a key they don’t have), not make up or hallucinate stuff, and communicate back to the rest of the game, which requires output in a highly specific format to name sure it can be interpreted by a fixed program. All of these make the task harder.&lt;/p&gt;\n\n&lt;p&gt;There are local LLMs that can do all this, but they require much more resources to run. I would roughly guess at least 8-10GB of vram if you’re really lucky and can deal with some unreliability, snd realistically, more like 16GB. That’s more than the majority of Steam users has.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdbiei",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60ogno/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753898017,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753898017,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n60etzn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ShardsOfSalt",
                      "can_mod_post": false,
                      "created_utc": 1753895446,
                      "send_replies": true,
                      "parent_id": "t1_n60b6b5",
                      "score": 1,
                      "author_fullname": "t2_u5j388982",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "For running the game, there should be relatively low overhead there.  The game I want to make is similar in graphical style to point and click adventure games so not a lot of graphics processing.  The real issue is for the play I want I need speech to text, text to speech, and an LLM that can understand what's being asked to translate to game functionality and also respond with reasons why the requested input isn't possible.  Like if someone says \"please use the key to open the door\" or \"open the door with the key\" or something it should be able to figure out if that's possible plus respond something like \"there is no key hole for this door\" if it's not possible.  I don't think other dialogue systems besides an LLM could handle it because I want it to handle multiple languages.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n60etzn",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For running the game, there should be relatively low overhead there.  The game I want to make is similar in graphical style to point and click adventure games so not a lot of graphics processing.  The real issue is for the play I want I need speech to text, text to speech, and an LLM that can understand what&amp;#39;s being asked to translate to game functionality and also respond with reasons why the requested input isn&amp;#39;t possible.  Like if someone says &amp;quot;please use the key to open the door&amp;quot; or &amp;quot;open the door with the key&amp;quot; or something it should be able to figure out if that&amp;#39;s possible plus respond something like &amp;quot;there is no key hole for this door&amp;quot; if it&amp;#39;s not possible.  I don&amp;#39;t think other dialogue systems besides an LLM could handle it because I want it to handle multiple languages.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdbiei",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60etzn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753895446,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n60b6b5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "General_Service_8209",
            "can_mod_post": false,
            "created_utc": 1753894463,
            "send_replies": true,
            "parent_id": "t3_1mdbiei",
            "score": 5,
            "author_fullname": "t2_jkfuj8nc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It’s definitely possible, but you will have to make a compromise between minimum system requirements and the quality you get.\n\nFor what you are intending, a fairly small, modern LLM should be enough, something like 1.5B parameters maybe. You can run this on a modern, low to mid-end gaming GPU without issue, and still have enough headroom for the game run at the same time.\n\nThe problem is going to be older PCs, and anything without a discrete GPU, both in terms of performance and software support.\n\nIf you want those systems to be able to run your game as well, you will have to look into older dialogue management systems rather than LLMs. As long as you can outline what your chatbot is supposed to do, rather than it doing everything, these are surprisingly powerful, and can run on pretty much everything. You‘ll need to put more coding work into it though.\n\nAs for TTS, compared to the LLM part, that isn’t going to be a major problem. Most TTS models are so small they can be run purely on the CPU, and they don’t use that much resources there either.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60b6b5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s definitely possible, but you will have to make a compromise between minimum system requirements and the quality you get.&lt;/p&gt;\n\n&lt;p&gt;For what you are intending, a fairly small, modern LLM should be enough, something like 1.5B parameters maybe. You can run this on a modern, low to mid-end gaming GPU without issue, and still have enough headroom for the game run at the same time.&lt;/p&gt;\n\n&lt;p&gt;The problem is going to be older PCs, and anything without a discrete GPU, both in terms of performance and software support.&lt;/p&gt;\n\n&lt;p&gt;If you want those systems to be able to run your game as well, you will have to look into older dialogue management systems rather than LLMs. As long as you can outline what your chatbot is supposed to do, rather than it doing everything, these are surprisingly powerful, and can run on pretty much everything. You‘ll need to put more coding work into it though.&lt;/p&gt;\n\n&lt;p&gt;As for TTS, compared to the LLM part, that isn’t going to be a major problem. Most TTS models are so small they can be run purely on the CPU, and they don’t use that much resources there either.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60b6b5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753894463,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdbiei",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n60ma65",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ShardsOfSalt",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n60fivw",
                                "score": 2,
                                "author_fullname": "t2_u5j388982",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For the game I want to build it's not really that immersive.  It's more like one of those choose your own adventure games (text based) but with graphics and I want the LLM to be able to support many languages and respond things like \"you can't use an axe on the door the door is too thick\" or \"it doesn't make sense to combine those things.\"",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n60ma65",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For the game I want to build it&amp;#39;s not really that immersive.  It&amp;#39;s more like one of those choose your own adventure games (text based) but with graphics and I want the LLM to be able to support many languages and respond things like &amp;quot;you can&amp;#39;t use an axe on the door the door is too thick&amp;quot; or &amp;quot;it doesn&amp;#39;t make sense to combine those things.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdbiei",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60ma65/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753897433,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753897433,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n60fivw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Double_Cause4609",
                      "can_mod_post": false,
                      "created_utc": 1753895631,
                      "send_replies": true,
                      "parent_id": "t1_n60fif9",
                      "score": 2,
                      "author_fullname": "t2_1kubzxt2ww",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Also, there's support beyond just drivers. You need to think about the actual software backend that actually runs the model. LlamaCPP has advantages, vLLM...Might not be the best for consumer applications, TabbyAPI is hard to package (unless the user can be expected to know pip, etc), and so on so forth.\n\nYou can technically package the model for the user in something like Onnx or Executorch but that gets into the territory of dedicated DevOPs engineers, and sometimes requires patching graph breaks because someone upstream implemented a model with a Python \"if\" statement instead of a branch-less solution, and you might have to implement some forward methods or some functionality needed for your game specifically on your own.\n\nFor text to speech, we have a lot of fragmentation in the field and nothing's really standardized yet. There's a lot of asymmetric capabilities in available TTS systems (ie: model 1 can do ABC, model 2 can do ADE, model 3 can do CDF, and so on), and they all have pretty severe limitations that make them hard to use in a real time open ended context. They can absolutely work, especially in limited situations (like pre-computing a bunch of dialogue to use the player's character name, or doing a bunch of dialogue ahead of time to customize a few quests to the player's actions or something), but there's not really a magic system where you just flick a lever and it works out of the box.\n\nAlso, you have to think about what working with AI in an open narrative context really means. It's a completely different type of programming, almost as dramatic as the shift from assembly to object oriented programming. You have to start thinking about semantic programming, systems, context engineering, LLM functions, and so on. There's actually a lot involved to make it a seamless experience.\n\nTl;DR: Yes, kind of. A lot of modern computers (enough for an indie game to have an audience) can run AI but it's early days, and there's really jagged software support for this kind of thing. It takes a lot of thought right now, and you're going to be building a lot of stuff from scratch to make an immersive experience.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n60fivw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also, there&amp;#39;s support beyond just drivers. You need to think about the actual software backend that actually runs the model. LlamaCPP has advantages, vLLM...Might not be the best for consumer applications, TabbyAPI is hard to package (unless the user can be expected to know pip, etc), and so on so forth.&lt;/p&gt;\n\n&lt;p&gt;You can technically package the model for the user in something like Onnx or Executorch but that gets into the territory of dedicated DevOPs engineers, and sometimes requires patching graph breaks because someone upstream implemented a model with a Python &amp;quot;if&amp;quot; statement instead of a branch-less solution, and you might have to implement some forward methods or some functionality needed for your game specifically on your own.&lt;/p&gt;\n\n&lt;p&gt;For text to speech, we have a lot of fragmentation in the field and nothing&amp;#39;s really standardized yet. There&amp;#39;s a lot of asymmetric capabilities in available TTS systems (ie: model 1 can do ABC, model 2 can do ADE, model 3 can do CDF, and so on), and they all have pretty severe limitations that make them hard to use in a real time open ended context. They can absolutely work, especially in limited situations (like pre-computing a bunch of dialogue to use the player&amp;#39;s character name, or doing a bunch of dialogue ahead of time to customize a few quests to the player&amp;#39;s actions or something), but there&amp;#39;s not really a magic system where you just flick a lever and it works out of the box.&lt;/p&gt;\n\n&lt;p&gt;Also, you have to think about what working with AI in an open narrative context really means. It&amp;#39;s a completely different type of programming, almost as dramatic as the shift from assembly to object oriented programming. You have to start thinking about semantic programming, systems, context engineering, LLM functions, and so on. There&amp;#39;s actually a lot involved to make it a seamless experience.&lt;/p&gt;\n\n&lt;p&gt;Tl;DR: Yes, kind of. A lot of modern computers (enough for an indie game to have an audience) can run AI but it&amp;#39;s early days, and there&amp;#39;s really jagged software support for this kind of thing. It takes a lot of thought right now, and you&amp;#39;re going to be building a lot of stuff from scratch to make an immersive experience.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdbiei",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60fivw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753895631,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n60my0m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Double_Cause4609",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n60l656",
                                "score": 1,
                                "author_fullname": "t2_1kubzxt2ww",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You could try a demo with a small model (1 to 3B parameters, or an MoE like I mentioned above) and run it on CPU, I suppose.\n\nYou could chain it to Kokoro TTS if you really need a TTS system (DMOSpeech 2 looks promising for speed but it's not as mature in support yet), and you'd probably hit the latency you're looking at.\n\nOn my system (with middle of the road modern memory speeds), on CPU hits around 20-70 T/s on the models I mentioned, which is around 10 to 40 words per second.\n\nKokoro TTS is also fairly fast, but I generally do offline TTS if any at all so I wasn't super interested in it, personally. Altogether I think your latency targets are do-able, and you could also possibly set up streaming to lower the time to first token further (this really makes latency feel a lot lower to an end user).\n\nBut yeah, honestly, the big problem is just hardware and software fragmentation. It's hard to make a single complete recipe that just works on all systems, all setups, and all situations right now. I'm guessing things will get more standardized into 2026 though, so if a person's making a pretty big game today (and is closer to the start of development) it can be assumed that everything will be ready by the time it's finished.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n60my0m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You could try a demo with a small model (1 to 3B parameters, or an MoE like I mentioned above) and run it on CPU, I suppose.&lt;/p&gt;\n\n&lt;p&gt;You could chain it to Kokoro TTS if you really need a TTS system (DMOSpeech 2 looks promising for speed but it&amp;#39;s not as mature in support yet), and you&amp;#39;d probably hit the latency you&amp;#39;re looking at.&lt;/p&gt;\n\n&lt;p&gt;On my system (with middle of the road modern memory speeds), on CPU hits around 20-70 T/s on the models I mentioned, which is around 10 to 40 words per second.&lt;/p&gt;\n\n&lt;p&gt;Kokoro TTS is also fairly fast, but I generally do offline TTS if any at all so I wasn&amp;#39;t super interested in it, personally. Altogether I think your latency targets are do-able, and you could also possibly set up streaming to lower the time to first token further (this really makes latency feel a lot lower to an end user).&lt;/p&gt;\n\n&lt;p&gt;But yeah, honestly, the big problem is just hardware and software fragmentation. It&amp;#39;s hard to make a single complete recipe that just works on all systems, all setups, and all situations right now. I&amp;#39;m guessing things will get more standardized into 2026 though, so if a person&amp;#39;s making a pretty big game today (and is closer to the start of development) it can be assumed that everything will be ready by the time it&amp;#39;s finished.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdbiei",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60my0m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753897611,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753897611,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n60l656",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ShardsOfSalt",
                      "can_mod_post": false,
                      "created_utc": 1753897137,
                      "send_replies": true,
                      "parent_id": "t1_n60fif9",
                      "score": 1,
                      "author_fullname": "t2_u5j388982",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "The game I want would probably be best with a less than 2 second response rate for sentences of maybe at most 10 words at a time.  Longer wait times for longer sentences would be fine.   I didn't realize there was so much discrepancy for hardware and other things, the demo tools I've used so far seem to just know what hardware I'm using and move forward.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n60l656",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The game I want would probably be best with a less than 2 second response rate for sentences of maybe at most 10 words at a time.  Longer wait times for longer sentences would be fine.   I didn&amp;#39;t realize there was so much discrepancy for hardware and other things, the demo tools I&amp;#39;ve used so far seem to just know what hardware I&amp;#39;m using and move forward.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdbiei",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60l656/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753897137,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n60fif9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753895628,
            "send_replies": true,
            "parent_id": "t3_1mdbiei",
            "score": 2,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The big problem isn't necessarily \"can a significant number of computers run local AI\"\n\nThe problem is \"how do they run local AI?\"\n\nCompute stacks are pretty complicated and depend on specialized compute drivers that an end-user can't necessarily be expected to be able to install and configure, and it's extremely difficult to package them in a way that lets a user run an AI application with zero-effort.\n\nIt's especially hard when you factor in the variety of hardware available. Sure, if someone has a fairly recent Nvidia GPU, their Vulkan implementation is actually really good for ML (surprisingly), and a similar story for AMD GPUs, but what if someone's on Intel? Or heaven forbid a Qualcomm SoC?\n\nOkay, what about DirectML? Gamers are generally on Windows, maybe DirectML has better support, right?\n\nWell, a major issue is that a lot of nerds who like to mess around with compute drivers etc tend to be on Linux, so community adoption of DirectML is a lot lower. This means it's primarily driven by corporate interests who tend to have different priorities to end users, and there can be a lot of weird \"well, yeah, it works, but you have to have this specific hardware and you have to use it as we intend it to be used\".\n\nOkay, maybe we should be using dedicated AI hardware, like NPUs. We just need to address them...How...? With Onnx runtime? With this one random package on Github?\n\nOkay, no, maybe not that way. Maybe we should just stick to raw CPU. CPU compilers are generic, mature, and well supported across vendors. We can use something like Highway to make portable code that works on all operating systems and instruction sets...But then you're implementing forward methods yourself, which is a PITA, so maybe you compile something like LlamaCPP into builds for the user, and just ship that with your game. To be totally fair: This does work. Not only that, but it can be expected that there are models which will run competently on a fairly recent CPU, too! MoE models nowadays have fairly few active parameters, so something like Moonlight, Qwen 3 30B MoE, Ling Lite MoE, OlmoE, Granite 3 3B, etc can all be expected to run competently on a CPU. Any of them can be finagled to provide high quality narratives in a pure text modality given appropriate prompting...But it's still not going to be an instant response, it's a lot of overhead, and it's probably more suitable to a turn based game than a real time open world.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60fif9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The big problem isn&amp;#39;t necessarily &amp;quot;can a significant number of computers run local AI&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The problem is &amp;quot;how do they run local AI?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Compute stacks are pretty complicated and depend on specialized compute drivers that an end-user can&amp;#39;t necessarily be expected to be able to install and configure, and it&amp;#39;s extremely difficult to package them in a way that lets a user run an AI application with zero-effort.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s especially hard when you factor in the variety of hardware available. Sure, if someone has a fairly recent Nvidia GPU, their Vulkan implementation is actually really good for ML (surprisingly), and a similar story for AMD GPUs, but what if someone&amp;#39;s on Intel? Or heaven forbid a Qualcomm SoC?&lt;/p&gt;\n\n&lt;p&gt;Okay, what about DirectML? Gamers are generally on Windows, maybe DirectML has better support, right?&lt;/p&gt;\n\n&lt;p&gt;Well, a major issue is that a lot of nerds who like to mess around with compute drivers etc tend to be on Linux, so community adoption of DirectML is a lot lower. This means it&amp;#39;s primarily driven by corporate interests who tend to have different priorities to end users, and there can be a lot of weird &amp;quot;well, yeah, it works, but you have to have this specific hardware and you have to use it as we intend it to be used&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Okay, maybe we should be using dedicated AI hardware, like NPUs. We just need to address them...How...? With Onnx runtime? With this one random package on Github?&lt;/p&gt;\n\n&lt;p&gt;Okay, no, maybe not that way. Maybe we should just stick to raw CPU. CPU compilers are generic, mature, and well supported across vendors. We can use something like Highway to make portable code that works on all operating systems and instruction sets...But then you&amp;#39;re implementing forward methods yourself, which is a PITA, so maybe you compile something like LlamaCPP into builds for the user, and just ship that with your game. To be totally fair: This does work. Not only that, but it can be expected that there are models which will run competently on a fairly recent CPU, too! MoE models nowadays have fairly few active parameters, so something like Moonlight, Qwen 3 30B MoE, Ling Lite MoE, OlmoE, Granite 3 3B, etc can all be expected to run competently on a CPU. Any of them can be finagled to provide high quality narratives in a pure text modality given appropriate prompting...But it&amp;#39;s still not going to be an instant response, it&amp;#39;s a lot of overhead, and it&amp;#39;s probably more suitable to a turn based game than a real time open world.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60fif9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753895628,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdbiei",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n60a967",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArtisticHamster",
            "can_mod_post": false,
            "created_utc": 1753894213,
            "send_replies": true,
            "parent_id": "t3_1mdbiei",
            "score": 0,
            "author_fullname": "t2_2t2xbyfm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I believe, it's possible to do what you describe on top of the line Macs due to large amount of unified RAM with pretty high bandwidth (I used to run Qwen3 on my M4 Max with 128Gb with pretty good speed even in thinking mode, Studio has options of up-to 512Gb, though definitely pricier). Concerning text to speech, my believe is that it requires much less resources than LLMs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60a967",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I believe, it&amp;#39;s possible to do what you describe on top of the line Macs due to large amount of unified RAM with pretty high bandwidth (I used to run Qwen3 on my M4 Max with 128Gb with pretty good speed even in thinking mode, Studio has options of up-to 512Gb, though definitely pricier). Concerning text to speech, my believe is that it requires much less resources than LLMs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60a967/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753894213,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdbiei",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n60jiuv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753896697,
            "send_replies": true,
            "parent_id": "t3_1mdbiei",
            "score": 1,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Normal PC?  Who knows, but for a game you could start with the [Steam Hardware Survey](https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam) which indicates most people have 8GB (6-12) VRAM and maybe RTX 2000+.  This would limit you to relatively small models but there are options, e.g. [gemma-3n-E2B-it](https://huggingface.co/google/gemma-3n-E2B-it) (5.5B with 2B active so should be fast even on poor hardware) though IDK is the license is okay for you.\n\nThat said, nothing will replace your judgement.  It's on you, the developer, to try different models and see if their performance (in terms of speed and capability) meets your needs and decide want your minimum hardware spec to be.  Technically you can release a game that requires a RTX PRO 6000 ;).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60jiuv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Normal PC?  Who knows, but for a game you could start with the &lt;a href=\"https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam\"&gt;Steam Hardware Survey&lt;/a&gt; which indicates most people have 8GB (6-12) VRAM and maybe RTX 2000+.  This would limit you to relatively small models but there are options, e.g. &lt;a href=\"https://huggingface.co/google/gemma-3n-E2B-it\"&gt;gemma-3n-E2B-it&lt;/a&gt; (5.5B with 2B active so should be fast even on poor hardware) though IDK is the license is okay for you.&lt;/p&gt;\n\n&lt;p&gt;That said, nothing will replace your judgement.  It&amp;#39;s on you, the developer, to try different models and see if their performance (in terms of speed and capability) meets your needs and decide want your minimum hardware spec to be.  Technically you can release a game that requires a RTX PRO 6000 ;).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60jiuv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753896697,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdbiei",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n60us6n",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ShardsOfSalt",
                      "can_mod_post": false,
                      "created_utc": 1753899752,
                      "send_replies": true,
                      "parent_id": "t1_n60t6ka",
                      "score": 1,
                      "author_fullname": "t2_u5j388982",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I don't think most of my users will be able to utilize an API like that but it's certainly worth it to include that as an option.  Unfortunately the people I want to target with the game also aren't your typical \"I own a gaming rig\" type people either.\n\nYea latency is mainly a user experience thing.  The LLM is their interface for the game if the interface is slow then they won't enjoy the experience.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n60us6n",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think most of my users will be able to utilize an API like that but it&amp;#39;s certainly worth it to include that as an option.  Unfortunately the people I want to target with the game also aren&amp;#39;t your typical &amp;quot;I own a gaming rig&amp;quot; type people either.&lt;/p&gt;\n\n&lt;p&gt;Yea latency is mainly a user experience thing.  The LLM is their interface for the game if the interface is slow then they won&amp;#39;t enjoy the experience.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdbiei",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60us6n/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753899752,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n60t6ka",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SM8085",
            "can_mod_post": false,
            "created_utc": 1753899308,
            "send_replies": true,
            "parent_id": "t3_1mdbiei",
            "score": 1,
            "author_fullname": "t2_14vikjao97",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;But I don't think normal computers are powerful enough for this? Am I mistaken?\n\nIn my opinion you shouldn't worry about this as a dev.\n\nIf you do everything for the LLM over the API then it's modular and I can point it toward my LLM rig on my LAN.\n\nWhatever bot you test your program with as you build it can be the one you suggest to others.  \"Tested with &lt;brand&gt; &lt;parameter value&gt;B.\"  Whether it's a Gemma3, Qwen3, Mistral, etc.\n\n&gt;The game I want would probably be best with a less than 2 second response rate for sentences of maybe at most 10 words at a time.\n\nWhy does latency matter?  Is there a reason beyond user experience?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60t6ka",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;But I don&amp;#39;t think normal computers are powerful enough for this? Am I mistaken?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;In my opinion you shouldn&amp;#39;t worry about this as a dev.&lt;/p&gt;\n\n&lt;p&gt;If you do everything for the LLM over the API then it&amp;#39;s modular and I can point it toward my LLM rig on my LAN.&lt;/p&gt;\n\n&lt;p&gt;Whatever bot you test your program with as you build it can be the one you suggest to others.  &amp;quot;Tested with &amp;lt;brand&amp;gt; &amp;lt;parameter value&amp;gt;B.&amp;quot;  Whether it&amp;#39;s a Gemma3, Qwen3, Mistral, etc.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The game I want would probably be best with a less than 2 second response rate for sentences of maybe at most 10 words at a time.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Why does latency matter?  Is there a reason beyond user experience?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n60t6ka/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753899308,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdbiei",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n61o8cb",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "No-Yak4416",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n61ngqg",
                                "score": 1,
                                "author_fullname": "t2_1efi4dcf6i",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I was honestly surprised at how well my laptop with no dedicated gpu handles the little llms. Obv not every computer will handle them, but every “gaming” computer should. Just make sure to have a setting to change which model is being used so people with nicer systems can get better convo results",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n61o8cb",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was honestly surprised at how well my laptop with no dedicated gpu handles the little llms. Obv not every computer will handle them, but every “gaming” computer should. Just make sure to have a setting to change which model is being used so people with nicer systems can get better convo results&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdbiei",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n61o8cb/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753908098,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753908098,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n61ngqg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ShardsOfSalt",
                      "can_mod_post": false,
                      "created_utc": 1753907885,
                      "send_replies": true,
                      "parent_id": "t1_n61myf1",
                      "score": 1,
                      "author_fullname": "t2_u5j388982",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks.  I'm going to give it a shot.  I really feel like it won't work on most people's machines but maybe I can offer compatibility with LLM APIs if speech to text and text to speech are at least doable.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n61ngqg",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks.  I&amp;#39;m going to give it a shot.  I really feel like it won&amp;#39;t work on most people&amp;#39;s machines but maybe I can offer compatibility with LLM APIs if speech to text and text to speech are at least doable.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdbiei",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n61ngqg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753907885,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61myf1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No-Yak4416",
            "can_mod_post": false,
            "created_utc": 1753907744,
            "send_replies": true,
            "parent_id": "t3_1mdbiei",
            "score": 1,
            "author_fullname": "t2_1efi4dcf6i",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I’m not the best person to answer this probably, op, but I have been running 3-7b models on my i5 1135g7 laptop with no gpu and no issue. I would say the 7b models are slightly slower than talking speed but the smaller ones work perfectly fine! If you have a gpu with enough vram to handle the llm and the game, ( and maybe the tts model) then you should have no problem!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61myf1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m not the best person to answer this probably, op, but I have been running 3-7b models on my i5 1135g7 laptop with no gpu and no issue. I would say the 7b models are slightly slower than talking speed but the smaller ones work perfectly fine! If you have a gpu with enough vram to handle the llm and the game, ( and maybe the tts model) then you should have no problem!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n61myf1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753907744,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdbiei",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61un67",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "o5mfiHTNsH748KVq",
            "can_mod_post": false,
            "created_utc": 1753909892,
            "send_replies": true,
            "parent_id": "t3_1mdbiei",
            "score": 1,
            "author_fullname": "t2_e11zi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think anybody with a modern GPU can run a Gemma-3 quant and get really high quality results. Like, my 3 year old phone can run gemma-3n on CPU and it only takes about 1s to start spitting text back at a decent rate.",
            "edited": 1753910091,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61un67",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think anybody with a modern GPU can run a Gemma-3 quant and get really high quality results. Like, my 3 year old phone can run gemma-3n on CPU and it only takes about 1s to start spitting text back at a decent rate.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdbiei/ai_for_normal_pcs/n61un67/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753909892,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdbiei",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]