[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "trying to optimize how i load relevant context into new chats (mostly claude api). currently have hundreds of structured documents/notes but manual selection is getting inefficient.\n\ncurrent workflow: manually pick relevant docs &gt; paste into new conversation &gt; often end up with redundant context or miss relevant stuff &gt; high token costs ($300-500/month)\n\nas the document library grows, this is becoming unsustainable. anyone solved similar problems?\n\nideally looking for:\n- semantic search to auto-suggest relevant docs before i paste context\n- local/offline solution (don't want docs going to cloud)\nminimal technical setup\n- something that learns document relationships over time\n\nthinking RAG type solution but most seem geared toward developers, but preferably easy to setup. \n\nanyone found user friendly tools for this that can run without a super powerful GPU?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best way to manage context/notes locally for API usage while optimizing token costs?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9nwk7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1sm1o6jilv",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753517320,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;trying to optimize how i load relevant context into new chats (mostly claude api). currently have hundreds of structured documents/notes but manual selection is getting inefficient.&lt;/p&gt;\n\n&lt;p&gt;current workflow: manually pick relevant docs &amp;gt; paste into new conversation &amp;gt; often end up with redundant context or miss relevant stuff &amp;gt; high token costs ($300-500/month)&lt;/p&gt;\n\n&lt;p&gt;as the document library grows, this is becoming unsustainable. anyone solved similar problems?&lt;/p&gt;\n\n&lt;p&gt;ideally looking for:\n- semantic search to auto-suggest relevant docs before i paste context\n- local/offline solution (don&amp;#39;t want docs going to cloud)\nminimal technical setup\n- something that learns document relationships over time&lt;/p&gt;\n\n&lt;p&gt;thinking RAG type solution but most seem geared toward developers, but preferably easy to setup. &lt;/p&gt;\n\n&lt;p&gt;anyone found user friendly tools for this that can run without a super powerful GPU?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m9nwk7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "boomerdaycare",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9nwk7/best_way_to_manage_contextnotes_locally_for_api/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9nwk7/best_way_to_manage_contextnotes_locally_for_api/",
            "subreddit_subscribers": 504974,
            "created_utc": 1753517320,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5cac71",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kissgeri96",
            "can_mod_post": false,
            "created_utc": 1753569814,
            "send_replies": true,
            "parent_id": "t3_1m9nwk7",
            "score": 1,
            "author_fullname": "t2_47eqehtw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You might like arkhon-memory — it's local-first, tracks reuse + time decay, and surfaces only what matters.\nSuper lightweight, no GPU needed.\nI use it to trim token bloat — might need slight tweaking for your case, but works great as a mini-RAG layer.\n\nhttps://www.reddit.com/r/LocalLLaMA/s/HSAakncgIx",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5cac71",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You might like arkhon-memory — it&amp;#39;s local-first, tracks reuse + time decay, and surfaces only what matters.\nSuper lightweight, no GPU needed.\nI use it to trim token bloat — might need slight tweaking for your case, but works great as a mini-RAG layer.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/HSAakncgIx\"&gt;https://www.reddit.com/r/LocalLLaMA/s/HSAakncgIx&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9nwk7/best_way_to_manage_contextnotes_locally_for_api/n5cac71/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753569814,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9nwk7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]