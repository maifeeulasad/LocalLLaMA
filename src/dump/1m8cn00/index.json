[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I want to do work with longer texts using local models (think going through an entire book with each sentence being it's own chat request/response).   \nI've been using LM Studio and Ollama for awhile now.   \nAnd more recently I've been building agents (for working with my Obsidian notes primarily) using PydanticAI.   \nBut I find myself wanting to experiment with long running agents and, knowing that I'm not that original or creative, wanted to hear about what you've been doing to make this work. \n\nWhat is your process?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Do you have a batch/background LLM task processing setup working locally?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8cn00",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1azds5ogui",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753382583,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to do work with longer texts using local models (think going through an entire book with each sentence being it&amp;#39;s own chat request/response).&lt;br/&gt;\nI&amp;#39;ve been using LM Studio and Ollama for awhile now.&lt;br/&gt;\nAnd more recently I&amp;#39;ve been building agents (for working with my Obsidian notes primarily) using PydanticAI.&lt;br/&gt;\nBut I find myself wanting to experiment with long running agents and, knowing that I&amp;#39;m not that original or creative, wanted to hear about what you&amp;#39;ve been doing to make this work. &lt;/p&gt;\n\n&lt;p&gt;What is your process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m8cn00",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "This_Conclusion9402",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/",
            "subreddit_subscribers": 504254,
            "created_utc": 1753382583,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n517r8d",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "crantob",
                      "can_mod_post": false,
                      "created_utc": 1753419361,
                      "send_replies": true,
                      "parent_id": "t1_n4ytdjk",
                      "score": 2,
                      "author_fullname": "t2_gyg8tngx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is just using the tools that were invented 50 years ago to do the same jobs kids do with $whizbang now.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n517r8d",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is just using the tools that were invented 50 years ago to do the same jobs kids do with $whizbang now.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8cn00",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n517r8d/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753419361,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4ytdjk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "1000_Spiders",
            "can_mod_post": false,
            "created_utc": 1753389122,
            "send_replies": true,
            "parent_id": "t3_1m8cn00",
            "score": 6,
            "author_fullname": "t2_1qhn7duj36",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I might be stupid, but i literally just have cron jobs setup to run python scripts to do batch/background processing. \n\nThese run independently from the main api and do things like compound summaries, update documentation, run tests, and write/execute sql against the backend db.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4ytdjk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I might be stupid, but i literally just have cron jobs setup to run python scripts to do batch/background processing. &lt;/p&gt;\n\n&lt;p&gt;These run independently from the main api and do things like compound summaries, update documentation, run tests, and write/execute sql against the backend db.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4ytdjk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753389122,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8cn00",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4y6x7g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1753382791,
            "send_replies": true,
            "parent_id": "t3_1m8cn00",
            "score": 5,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I just use llama-cli (from llama.cpp) on the bash command line.  It's trivial to have it process in batches with standard unixy utilities like find(1) (when prompt content is in files) or even bash for-loops.\n\n**Edited to add:** For example, to get Gemma3-27B to explain every python source file in a repo:\n\n&gt; $ find -name '*.py' -exec bin/g3 \"Explain this Python code in detail:\\n\\n\\`cat {}\\`\" \\;\n\n.. where `g3` is my Gemma3 wrapper script: http://ciar.org/h/g3",
            "edited": 1753383541,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4y6x7g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just use llama-cli (from llama.cpp) on the bash command line.  It&amp;#39;s trivial to have it process in batches with standard unixy utilities like find(1) (when prompt content is in files) or even bash for-loops.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edited to add:&lt;/strong&gt; For example, to get Gemma3-27B to explain every python source file in a repo:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;$ find -name &amp;#39;*.py&amp;#39; -exec bin/g3 &amp;quot;Explain this Python code in detail:\\n\\n`cat {}`&amp;quot; \\;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;.. where &lt;code&gt;g3&lt;/code&gt; is my Gemma3 wrapper script: &lt;a href=\"http://ciar.org/h/g3\"&gt;http://ciar.org/h/g3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4y6x7g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753382791,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m8cn00",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4yniuj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "This_Conclusion9402",
                      "can_mod_post": false,
                      "created_utc": 1753387493,
                      "send_replies": true,
                      "parent_id": "t1_n4yh33n",
                      "score": 2,
                      "author_fullname": "t2_1azds5ogui",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ah, ok, so you have a script template and then you use that as the baseline to create a one-off task script.  \nI like that.  \nThat's similar to something I did with an MCP server for creating tools.  \nBut I had not considered it for this.\n\nHave you found fully self contained scripts to be more manageable than a main script that you then add tasks to?\n\n*Edited last sentence*",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4yniuj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah, ok, so you have a script template and then you use that as the baseline to create a one-off task script.&lt;br/&gt;\nI like that.&lt;br/&gt;\nThat&amp;#39;s similar to something I did with an MCP server for creating tools.&lt;br/&gt;\nBut I had not considered it for this.&lt;/p&gt;\n\n&lt;p&gt;Have you found fully self contained scripts to be more manageable than a main script that you then add tasks to?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Edited last sentence&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8cn00",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4yniuj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753387493,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4yh33n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SM8085",
            "can_mod_post": false,
            "created_utc": 1753385697,
            "send_replies": true,
            "parent_id": "t3_1m8cn00",
            "score": 3,
            "author_fullname": "t2_14vikjao97",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;What is your process?\n\nI feed a bot a basic Python script like [llm-python-file.py](https://github.com/Jay4242/llm-scripts/blob/main/llm-python-file.py) which has been my go-to format for sending files to the bot and I tell it to use that format but do whatever I'm trying to do.\n\nSo, I would feed it that and say, \"But make it so that it chops up the text by sentence and loops through sending each sentence to the LLM backend wrapped with my commands instead of sending the entire text file.\"\n\nI have vision examples for sending images, like[ llm-python-vision-ollama.py](https://github.com/Jay4242/llm-scripts/blob/main/llm-python-vision-ollama.py).\n\nThen anything you &amp; the bot can figure out in python is possible.  Other languages are possible but then you have to parse the JSON response without the openai package.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4yh33n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;What is your process?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I feed a bot a basic Python script like &lt;a href=\"https://github.com/Jay4242/llm-scripts/blob/main/llm-python-file.py\"&gt;llm-python-file.py&lt;/a&gt; which has been my go-to format for sending files to the bot and I tell it to use that format but do whatever I&amp;#39;m trying to do.&lt;/p&gt;\n\n&lt;p&gt;So, I would feed it that and say, &amp;quot;But make it so that it chops up the text by sentence and loops through sending each sentence to the LLM backend wrapped with my commands instead of sending the entire text file.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I have vision examples for sending images, like&lt;a href=\"https://github.com/Jay4242/llm-scripts/blob/main/llm-python-vision-ollama.py\"&gt; llm-python-vision-ollama.py&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Then anything you &amp;amp; the bot can figure out in python is possible.  Other languages are possible but then you have to parse the JSON response without the openai package.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4yh33n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753385697,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8cn00",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4ywvi9",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "HypnoDaddy4You",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4ynxou",
                                "score": 1,
                                "author_fullname": "t2_lb2n7mbsw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "No, I keep promising the next time I rewrite it I'll add generic task handling, but there's a lot of data models and ui to build to gonalong with that. This is my 3rd rewrite in 2 years lol\n\nAnd the first one that actually wrote a mostly self consistent novella. Ok like 30 of them and counting but scale is the while point\n\nThe current system implements a state machine per endeavor to track status. Basically, each table in the database has a completed flag that gets set after the task is successful.  Each time it's queue is empty, it picks the most-done endeavor with uncompleted tasks, and does the next one.",
                                "edited": 1753392482,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4ywvi9",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, I keep promising the next time I rewrite it I&amp;#39;ll add generic task handling, but there&amp;#39;s a lot of data models and ui to build to gonalong with that. This is my 3rd rewrite in 2 years lol&lt;/p&gt;\n\n&lt;p&gt;And the first one that actually wrote a mostly self consistent novella. Ok like 30 of them and counting but scale is the while point&lt;/p&gt;\n\n&lt;p&gt;The current system implements a state machine per endeavor to track status. Basically, each table in the database has a completed flag that gets set after the task is successful.  Each time it&amp;#39;s queue is empty, it picks the most-done endeavor with uncompleted tasks, and does the next one.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8cn00",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4ywvi9/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753390102,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753390102,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4ynxou",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "This_Conclusion9402",
                      "can_mod_post": false,
                      "created_utc": 1753387607,
                      "send_replies": true,
                      "parent_id": "t1_n4yfz4m",
                      "score": 1,
                      "author_fullname": "t2_1azds5ogui",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah not my domain for sure.   \nCurious though, what format are you using to store the tasks in your queue?  \nHave you found something broadly applicable?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4ynxou",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah not my domain for sure.&lt;br/&gt;\nCurious though, what format are you using to store the tasks in your queue?&lt;br/&gt;\nHave you found something broadly applicable?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8cn00",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4ynxou/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753387607,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4yfz4m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HypnoDaddy4You",
            "can_mod_post": false,
            "created_utc": 1753385379,
            "send_replies": true,
            "parent_id": "t3_1m8cn00",
            "score": 2,
            "author_fullname": "t2_lb2n7mbsw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I wrote my own processing system with .net\n\nWhich is probably not going to be easy to reproduce if you don't have .net skills lol\n\nBasically, a database of in progress work and a backgroundworkerthread to submit them and update the database.\n\nAnyone who wants details, feel free to dm me",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4yfz4m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wrote my own processing system with .net&lt;/p&gt;\n\n&lt;p&gt;Which is probably not going to be easy to reproduce if you don&amp;#39;t have .net skills lol&lt;/p&gt;\n\n&lt;p&gt;Basically, a database of in progress work and a backgroundworkerthread to submit them and update the database.&lt;/p&gt;\n\n&lt;p&gt;Anyone who wants details, feel free to dm me&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4yfz4m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753385379,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8cn00",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4z9bzx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753393735,
            "send_replies": true,
            "parent_id": "t3_1m8cn00",
            "score": 1,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "the closest I have to this so far is just an agent that queries the memory vector db, one memory at a time, and checks if memories are too trivial to keep, or very similar to existing ones and can be deleted or consolidated.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4z9bzx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the closest I have to this so far is just an agent that queries the memory vector db, one memory at a time, and checks if memories are too trivial to keep, or very similar to existing ones and can be deleted or consolidated.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n4z9bzx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753393735,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1m8cn00",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n50rr24",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753412562,
            "send_replies": true,
            "parent_id": "t3_1m8cn00",
            "score": 1,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For this kind of operation I personally do vLLM; it's just so much faster for concurrent requests.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n50rr24",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For this kind of operation I personally do vLLM; it&amp;#39;s just so much faster for concurrent requests.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8cn00/do_you_have_a_batchbackground_llm_task_processing/n50rr24/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753412562,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8cn00",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]