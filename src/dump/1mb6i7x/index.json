[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I remember when vLLM was just a narrowly specialized tool which almost nobody used. Everyone was using Ollama (basically a wrapper for llama.cpp which turns it into an OpenAI-capable API and adds some easy tools for downloading models), or using llama.cpp directly.\n\nBut I've been seeing more and more people using vLLM everywhere now, and have been hearing that they have a very efficient architecture that increases processing speed, has more efficient parallel processing, better response time, efficient batching that runs multiple requests at the same time, multi-GPU support, supports LoRAs without bloating memory usage, has way lower VRAM usage when using long contexts, etc.\n\nAnd it also implements the OpenAI API.\n\nSo my question is: Should I just uninstall Ollama/llama.cpp and switch to vLLM full-time? Seems like that's where it's at now.\n\n\\---\n\nEdit: Okay here's a summary:\n\n* vLLM: Extremely well optimized code. Made for enterprise, where latency and throughput is the highest importance. Only loads a single model per instance. Uses a lot of modern GPU features for speedup, so it doesn't work on older GPUs. It has great multi-GPU support (spreading model weights across the GPUs and acting as if they're one GPU with combined VRAM). Uses very fast caching techniques (its major innovation being a paged KV cache which massively reduces VRAM usage for long prompt contexts). It pre-allocates 90% of your VRAM to itself for speed regardless of how small the model is. It does NOT support VRAM offloading or CPU-split inference. It's designed to keep the ENTIRE model in VRAM. So if you are able to fit the models in your VRAM, then vLLM is better, but since it was made for dedicated enterprise servers it has the downside that you have to restart vLLM if you want to change model.\n* Ollama: Can change models on the fly and automatically unloads the old model and loads the new one. It works on pretty much any GPU. It's able to do split inference and RAM offloading so that models which don't fit on the GPU will use offloading and still be able to run even if you have too little VRAM. And it's also very easy for beginners.\n\nSo for casual users, Ollama is a big winner. Just start and go. Whereas vLLM only sounds worth it if you mostly use one model, and you're able to fit it in VRAM, and you really wanna push its performance higher.\n\nWith this in mind, I'll stay on Ollama and only consider vLLM if I see a model that I really want to optimize and use a lot. So I'll use Ollama for general model testing and multi-model swapping, and will only use vLLM if there's something I end up using a lot and think it's worth the extra hassle of using vLLM to speed it up a bit.\n\nAs for answering my own original topic question: No. vLLM has not \"made Ollama redundant now\". vLLM has actually \\*always\\* made Ollama redundant from day 1. Because they serve two totally different purposes. Ollama is way better and way more convenient for most home users. And vLLM is way better for servers and people who have tons of VRAM and want the fastest inference. That's it. Two totally different user groups. I'm personally mostly in the Ollama group with my 24 GB VRAM and hobbyist setup.\n\n\\---\n\nEdit: To put some actual numbers on it, I found a nice post where someone did a detailed benchmark of vLLM vs Ollama. The result was simple: **vLLM was up to 3.23x faster than Ollama in an inference throughput/concurrency test:** [https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd](https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd)\n\nBut for home users, Ollama is better at pretty much everything else that an average home user needs.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Has vLLM made Ollama and llama.cpp redundant?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mb6i7x",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.26,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_4a13s1mr",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753743196,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753675804,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember when vLLM was just a narrowly specialized tool which almost nobody used. Everyone was using Ollama (basically a wrapper for llama.cpp which turns it into an OpenAI-capable API and adds some easy tools for downloading models), or using llama.cpp directly.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;ve been seeing more and more people using vLLM everywhere now, and have been hearing that they have a very efficient architecture that increases processing speed, has more efficient parallel processing, better response time, efficient batching that runs multiple requests at the same time, multi-GPU support, supports LoRAs without bloating memory usage, has way lower VRAM usage when using long contexts, etc.&lt;/p&gt;\n\n&lt;p&gt;And it also implements the OpenAI API.&lt;/p&gt;\n\n&lt;p&gt;So my question is: Should I just uninstall Ollama/llama.cpp and switch to vLLM full-time? Seems like that&amp;#39;s where it&amp;#39;s at now.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Edit: Okay here&amp;#39;s a summary:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;vLLM: Extremely well optimized code. Made for enterprise, where latency and throughput is the highest importance. Only loads a single model per instance. Uses a lot of modern GPU features for speedup, so it doesn&amp;#39;t work on older GPUs. It has great multi-GPU support (spreading model weights across the GPUs and acting as if they&amp;#39;re one GPU with combined VRAM). Uses very fast caching techniques (its major innovation being a paged KV cache which massively reduces VRAM usage for long prompt contexts). It pre-allocates 90% of your VRAM to itself for speed regardless of how small the model is. It does NOT support VRAM offloading or CPU-split inference. It&amp;#39;s designed to keep the ENTIRE model in VRAM. So if you are able to fit the models in your VRAM, then vLLM is better, but since it was made for dedicated enterprise servers it has the downside that you have to restart vLLM if you want to change model.&lt;/li&gt;\n&lt;li&gt;Ollama: Can change models on the fly and automatically unloads the old model and loads the new one. It works on pretty much any GPU. It&amp;#39;s able to do split inference and RAM offloading so that models which don&amp;#39;t fit on the GPU will use offloading and still be able to run even if you have too little VRAM. And it&amp;#39;s also very easy for beginners.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So for casual users, Ollama is a big winner. Just start and go. Whereas vLLM only sounds worth it if you mostly use one model, and you&amp;#39;re able to fit it in VRAM, and you really wanna push its performance higher.&lt;/p&gt;\n\n&lt;p&gt;With this in mind, I&amp;#39;ll stay on Ollama and only consider vLLM if I see a model that I really want to optimize and use a lot. So I&amp;#39;ll use Ollama for general model testing and multi-model swapping, and will only use vLLM if there&amp;#39;s something I end up using a lot and think it&amp;#39;s worth the extra hassle of using vLLM to speed it up a bit.&lt;/p&gt;\n\n&lt;p&gt;As for answering my own original topic question: No. vLLM has not &amp;quot;made Ollama redundant now&amp;quot;. vLLM has actually *always* made Ollama redundant from day 1. Because they serve two totally different purposes. Ollama is way better and way more convenient for most home users. And vLLM is way better for servers and people who have tons of VRAM and want the fastest inference. That&amp;#39;s it. Two totally different user groups. I&amp;#39;m personally mostly in the Ollama group with my 24 GB VRAM and hobbyist setup.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Edit: To put some actual numbers on it, I found a nice post where someone did a detailed benchmark of vLLM vs Ollama. The result was simple: &lt;strong&gt;vLLM was up to 3.23x faster than Ollama in an inference throughput/concurrency test:&lt;/strong&gt; &lt;a href=\"https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd\"&gt;https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But for home users, Ollama is better at pretty much everything else that an average home user needs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA.png?auto=webp&amp;s=7d022357376ba2db038569e6922d4e4e26f51d03",
                    "width": 1200,
                    "height": 795
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9651e112d40e457f68e29ea968b21c203638d6d5",
                      "width": 108,
                      "height": 71
                    },
                    {
                      "url": "https://external-preview.redd.it/gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=016e64e5b67498b7a594d25a73dfe4297f621f09",
                      "width": 216,
                      "height": 143
                    },
                    {
                      "url": "https://external-preview.redd.it/gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba2837f3912a397745289ce29afb5971df48ff78",
                      "width": 320,
                      "height": 212
                    },
                    {
                      "url": "https://external-preview.redd.it/gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=96f819e5e4dc47d01b5a15d4ba30dedbb6708dad",
                      "width": 640,
                      "height": 424
                    },
                    {
                      "url": "https://external-preview.redd.it/gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f11c71b9f2e32368f70242faf6ec0990c9899d40",
                      "width": 960,
                      "height": 636
                    },
                    {
                      "url": "https://external-preview.redd.it/gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=218f10e612d89f98d1591ab6ccee84db83f0da9c",
                      "width": 1080,
                      "height": 715
                    }
                  ],
                  "variants": {},
                  "id": "gcraJ-_ZjGA3RGGP329KqzVct1E6PhewGRbEc8JjoCA"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mb6i7x",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "pilkyton",
            "discussion_type": null,
            "num_comments": 20,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/",
            "subreddit_subscribers": 506191,
            "created_utc": 1753675804,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5jywk7",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "pilkyton",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5jy8ck",
                                          "score": 0,
                                          "author_fullname": "t2_4a13s1mr",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Ahhhh, thanks a lot for that info. So vLLM probably uses a bunch of optimizations via APIs that only exist on newer GPUs. Which would give more speed but locks out older GPUs.\n\nI'll have to look if 3090 is supported. But I ran a vision model on vLLM a year or so ago... so I hope it will be possible to move all my LLMs to it too. Would be nice to keep everything on 1 platform.\n\nEdit: Okay the biggest difference is that vLLM is truly made for dedicated servers. It loads 1 model. It must fit in VRAM. And it cannot swap models. It's made to serve and to be super fast. That's it. Whereas Ollama is for home users who frequently have low VRAM and constantly change models, so Ollama supports all of those home-user friendly features.\n\n**I'll stay with Ollama for now.**",
                                          "edited": 1753678704,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5jywk7",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ahhhh, thanks a lot for that info. So vLLM probably uses a bunch of optimizations via APIs that only exist on newer GPUs. Which would give more speed but locks out older GPUs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll have to look if 3090 is supported. But I ran a vision model on vLLM a year or so ago... so I hope it will be possible to move all my LLMs to it too. Would be nice to keep everything on 1 platform.&lt;/p&gt;\n\n&lt;p&gt;Edit: Okay the biggest difference is that vLLM is truly made for dedicated servers. It loads 1 model. It must fit in VRAM. And it cannot swap models. It&amp;#39;s made to serve and to be super fast. That&amp;#39;s it. Whereas Ollama is for home users who frequently have low VRAM and constantly change models, so Ollama supports all of those home-user friendly features.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I&amp;#39;ll stay with Ollama for now.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mb6i7x",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jywk7/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753676749,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753676749,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 0
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5jy8ck",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "grubnenah",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5jxi77",
                                "score": 7,
                                "author_fullname": "t2_ityn8",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "vLLM doesn't support as old of GPU generation either, so it's basically just llama.cpp for me. Not too long ago there were a ton of people buying up P40s to get a bunch of VRAM for cheap, which are also unsupported by vLLM.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5jy8ck",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vLLM doesn&amp;#39;t support as old of GPU generation either, so it&amp;#39;s basically just llama.cpp for me. Not too long ago there were a ton of people buying up P40s to get a bunch of VRAM for cheap, which are also unsupported by vLLM.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mb6i7x",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jy8ck/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753676432,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753676432,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 7
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5jxi77",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pilkyton",
                      "can_mod_post": false,
                      "created_utc": 1753676098,
                      "send_replies": true,
                      "parent_id": "t1_n5jxbnq",
                      "score": -1,
                      "author_fullname": "t2_4a13s1mr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Are you saying that vLLM is tuned for multi-GPU enterprise and Ollama is tuned for single-GPU home users?\n\nWouldn't vLLM's optimizations help for home users too? A lot of things that vLLM does to shave off seconds of processing time for enterprise would have some benefit for home users too.\n\nOr do you just mean how easy it is to start up Ollama with one command? I guess that's a benefit. I've used vLLM once (to host the API for a vision model), and it took some time to learn how to set it up. But I don't really care about setup time, I just want the optimal inference time.\n\n\\---\n\nSpeaking of home users: One of the *seemingly* \"nice\" things about Ollama is that it makes it very easy to download models. Until you realize that most of them are incorrectly configured and are missing the required system prompt, making you have to dig up the official model repository and rebuild the correct system prompt yourself anyway.\n\nI've been seeing that issue with most of the important and popular models I've tried with Ollama, so I am not impressed with the \"user friendliness\". Having to download the model files myself (which is easy with huggingface's CLI tool) for vLLM is basically no problem since I have to go dig up official repos anyway to fix Ollama's empty system prompts.\n\nWe're talking about stuff like completely missing the correct prompt formatting that the model was trained on, such as the important query formatting like \"{start\\_system} you are blabla {end\\_system} {start\\_user\\_query} (your prompt) {end\\_user\\_query} {start\\_response} ...\" and also missing the stop markers like \"stop model when the model outputs {end\\_response}\" etc for things like chat-models/instruct-models, where that's super important since all training used that format...",
                      "edited": 1753677582,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5jxi77",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you saying that vLLM is tuned for multi-GPU enterprise and Ollama is tuned for single-GPU home users?&lt;/p&gt;\n\n&lt;p&gt;Wouldn&amp;#39;t vLLM&amp;#39;s optimizations help for home users too? A lot of things that vLLM does to shave off seconds of processing time for enterprise would have some benefit for home users too.&lt;/p&gt;\n\n&lt;p&gt;Or do you just mean how easy it is to start up Ollama with one command? I guess that&amp;#39;s a benefit. I&amp;#39;ve used vLLM once (to host the API for a vision model), and it took some time to learn how to set it up. But I don&amp;#39;t really care about setup time, I just want the optimal inference time.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Speaking of home users: One of the &lt;em&gt;seemingly&lt;/em&gt; &amp;quot;nice&amp;quot; things about Ollama is that it makes it very easy to download models. Until you realize that most of them are incorrectly configured and are missing the required system prompt, making you have to dig up the official model repository and rebuild the correct system prompt yourself anyway.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been seeing that issue with most of the important and popular models I&amp;#39;ve tried with Ollama, so I am not impressed with the &amp;quot;user friendliness&amp;quot;. Having to download the model files myself (which is easy with huggingface&amp;#39;s CLI tool) for vLLM is basically no problem since I have to go dig up official repos anyway to fix Ollama&amp;#39;s empty system prompts.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re talking about stuff like completely missing the correct prompt formatting that the model was trained on, such as the important query formatting like &amp;quot;{start_system} you are blabla {end_system} {start_user_query} (your prompt) {end_user_query} {start_response} ...&amp;quot; and also missing the stop markers like &amp;quot;stop model when the model outputs {end_response}&amp;quot; etc for things like chat-models/instruct-models, where that&amp;#39;s super important since all training used that format...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb6i7x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jxi77/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753676098,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5jxbnq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "fp4guru",
            "can_mod_post": false,
            "created_utc": 1753676017,
            "send_replies": true,
            "parent_id": "t3_1mb6i7x",
            "score": 16,
            "author_fullname": "t2_1tp8zldw5g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ollama people don't like your description. Vllm is for the GPU rich. Everything else is for the GPU poor.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5jxbnq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama people don&amp;#39;t like your description. Vllm is for the GPU rich. Everything else is for the GPU poor.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jxbnq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753676017,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb6i7x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 16
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5k1vqh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pilkyton",
                      "can_mod_post": false,
                      "created_utc": 1753678174,
                      "send_replies": true,
                      "parent_id": "t1_n5k198d",
                      "score": 1,
                      "author_fullname": "t2_4a13s1mr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks! That is a very important piece of knowledge:\n\n&gt;No CPU fallback\n\n&gt;No layer-swapping between RAM and VRAM\n\n&gt;No support for inference with partially-loaded models\n\n&gt;This is by design - vLLM's architecture is optimized for maximum throughput and minimal latency, not maximum compatibility with low VRAM setups.\n\n&gt;So if your model is 24 GB and your GPU has only 16 GB free, vLLM cannot run it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5k1vqh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks! That is a very important piece of knowledge:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;No CPU fallback&lt;/p&gt;\n\n&lt;p&gt;No layer-swapping between RAM and VRAM&lt;/p&gt;\n\n&lt;p&gt;No support for inference with partially-loaded models&lt;/p&gt;\n\n&lt;p&gt;This is by design - vLLM&amp;#39;s architecture is optimized for maximum throughput and minimal latency, not maximum compatibility with low VRAM setups.&lt;/p&gt;\n\n&lt;p&gt;So if your model is 24 GB and your GPU has only 16 GB free, vLLM cannot run it.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb6i7x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5k1vqh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753678174,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5k198d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Betadoggo_",
            "can_mod_post": false,
            "created_utc": 1753677873,
            "send_replies": true,
            "parent_id": "t3_1mb6i7x",
            "score": 7,
            "author_fullname": "t2_a177eog2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "llamacpp is ideal for systems running with layers on both cpu and gpu or just cpu. vllm is ideal when you're running the whole model on gpus. llamacpp has also supported an openai compatible api for quite a while.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k198d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llamacpp is ideal for systems running with layers on both cpu and gpu or just cpu. vllm is ideal when you&amp;#39;re running the whole model on gpus. llamacpp has also supported an openai compatible api for quite a while.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5k198d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753677873,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb6i7x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ku5uj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "NNN_Throwaway2",
            "can_mod_post": false,
            "created_utc": 1753693923,
            "send_replies": true,
            "parent_id": "t3_1mb6i7x",
            "score": 5,
            "author_fullname": "t2_8rrihts9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "ollama is already redundant",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ku5uj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ollama is already redundant&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5ku5uj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753693923,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb6i7x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": {
                                                                "kind": "Listing",
                                                                "data": {
                                                                  "after": null,
                                                                  "dist": null,
                                                                  "modhash": "",
                                                                  "geo_filter": "",
                                                                  "children": [
                                                                    {
                                                                      "kind": "t1",
                                                                      "data": {
                                                                        "subreddit_id": "t5_81eyvm",
                                                                        "approved_at_utc": null,
                                                                        "author_is_blocked": false,
                                                                        "comment_type": null,
                                                                        "awarders": [],
                                                                        "mod_reason_by": null,
                                                                        "banned_by": null,
                                                                        "author_flair_type": "text",
                                                                        "total_awards_received": 0,
                                                                        "subreddit": "LocalLLaMA",
                                                                        "author_flair_template_id": null,
                                                                        "distinguished": null,
                                                                        "likes": null,
                                                                        "replies": "",
                                                                        "user_reports": [],
                                                                        "saved": false,
                                                                        "id": "n5mofvm",
                                                                        "banned_at_utc": null,
                                                                        "mod_reason_title": null,
                                                                        "gilded": 0,
                                                                        "archived": false,
                                                                        "collapsed_reason_code": null,
                                                                        "no_follow": true,
                                                                        "author": "chibop1",
                                                                        "can_mod_post": false,
                                                                        "send_replies": true,
                                                                        "parent_id": "t1_n5k3jm9",
                                                                        "score": 3,
                                                                        "author_fullname": "t2_e9jh97s",
                                                                        "approved_by": null,
                                                                        "mod_note": null,
                                                                        "all_awardings": [],
                                                                        "collapsed": false,
                                                                        "body": "Also look at Github stars.\n\n* [Ollama:](https://github.com/ollama/ollama) 147787\n* [Llama.cpp:](https://github.com/ggml-org/llama.cpp) 83570\n* [Vllm:](https://github.com/vllm-project/vllm) 53389\n\nSome people have popularity complex. lol",
                                                                        "edited": 1753719004,
                                                                        "gildings": {},
                                                                        "author_flair_css_class": null,
                                                                        "name": "t1_n5mofvm",
                                                                        "is_submitter": false,
                                                                        "downs": 0,
                                                                        "author_flair_richtext": [],
                                                                        "author_patreon_flair": false,
                                                                        "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also look at Github stars.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ollama/ollama\"&gt;Ollama:&lt;/a&gt; 147787&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp\"&gt;Llama.cpp:&lt;/a&gt; 83570&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/vllm-project/vllm\"&gt;Vllm:&lt;/a&gt; 53389&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Some people have popularity complex. lol&lt;/p&gt;\n&lt;/div&gt;",
                                                                        "removal_reason": null,
                                                                        "collapsed_reason": null,
                                                                        "link_id": "t3_1mb6i7x",
                                                                        "associated_award": null,
                                                                        "stickied": false,
                                                                        "author_premium": false,
                                                                        "can_gild": false,
                                                                        "top_awarded_type": null,
                                                                        "unrepliable_reason": null,
                                                                        "author_flair_text_color": null,
                                                                        "score_hidden": false,
                                                                        "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5mofvm/",
                                                                        "subreddit_type": "public",
                                                                        "locked": false,
                                                                        "report_reasons": null,
                                                                        "created": 1753718631,
                                                                        "author_flair_text": null,
                                                                        "treatment_tags": [],
                                                                        "created_utc": 1753718631,
                                                                        "subreddit_name_prefixed": "r/LocalLLaMA",
                                                                        "controversiality": 0,
                                                                        "depth": 6,
                                                                        "author_flair_background_color": null,
                                                                        "collapsed_because_crowd_control": null,
                                                                        "mod_reports": [],
                                                                        "num_reports": null,
                                                                        "ups": 3
                                                                      }
                                                                    }
                                                                  ],
                                                                  "before": null
                                                                }
                                                              },
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n5k3jm9",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "pilkyton",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n5k2uko",
                                                              "score": 1,
                                                              "author_fullname": "t2_4a13s1mr",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Why are you so triggered by the objective fact that Ollama is vastly more popular? Just look at Google's search trends. Ollama is hovering between 5-10x more popular.\n\nIt doesn't matter \\*why\\* it's more popular. It \\*IS\\* more popular. That was my only statement: More people use, talk about and search for content about Ollama.\n\nThat is an objective fact. Which the rude idiot above took issue with for some braindead reason. And now you're piling on with the same idiocy. Stop wasting my time.\n\nYes, huge amounts of \"noobs\" as you call them are using Ollama with their 6 GB GPUs running GGUFs. That's obvious since it's super easy to set up and spreads like wildfire among hobbyists.\n\nI'll repeat it one more time for the very slow people in the back: It doesn't matter \\*why\\* it's more popular. It \\*IS\\* more popular. **That was my ONLY statement:** More people use, talk about and search for content about Ollama.\n\nIt shouldn't surprise anyone that the backend made for home computers is more popular than the one with high hardware requirements.\n\nThat is an **objectively correct statement,** which you're angry about for some dumb reason. It doesn't matter if vLLM is superior and that all the pros use finely-tuned vLLM servers at home. I am already aware that vLLM is better optimized (literally just read my original post, dude).\n\nAll that matters regarding our argument is that Ollama is objectively more popular, which is an objectively correct statement which you seem unable to accept - but vLLM is steadily rising, which is why I am interested in it and wanted to hear if it's worth switching.\n\nI am putting an end to this waste of time now by blocking both of you. I don't need rude idiots who pettily argue against the most basic facts and keep shifting the goalposts.\n\nCome on, **think for a moment about what you are saying.** You're arguing against Ollama's popularity by saying \"yeah Ollama IS vastly more popular because every noob uses it, BUT vLLM is better\". That's a total non-sequiteur in an argument about Ollama's **\\*popularity\\*. Sigh. So tiring!**\n\nPlease stop wasting time with dishonest arguments on the internet. Anyone else who tries it is getting immediately blocked.\n\nPS: I've already set up vLLM in the past. It wasn't particularly hard and only took like five minutes. I was merely asking if I should switch to it full-time. Don't waste any more of my time.",
                                                              "edited": 1753721550,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n5k3jm9",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why are you so triggered by the objective fact that Ollama is vastly more popular? Just look at Google&amp;#39;s search trends. Ollama is hovering between 5-10x more popular.&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t matter *why* it&amp;#39;s more popular. It *IS* more popular. That was my only statement: More people use, talk about and search for content about Ollama.&lt;/p&gt;\n\n&lt;p&gt;That is an objective fact. Which the rude idiot above took issue with for some braindead reason. And now you&amp;#39;re piling on with the same idiocy. Stop wasting my time.&lt;/p&gt;\n\n&lt;p&gt;Yes, huge amounts of &amp;quot;noobs&amp;quot; as you call them are using Ollama with their 6 GB GPUs running GGUFs. That&amp;#39;s obvious since it&amp;#39;s super easy to set up and spreads like wildfire among hobbyists.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll repeat it one more time for the very slow people in the back: It doesn&amp;#39;t matter *why* it&amp;#39;s more popular. It *IS* more popular. &lt;strong&gt;That was my ONLY statement:&lt;/strong&gt; More people use, talk about and search for content about Ollama.&lt;/p&gt;\n\n&lt;p&gt;It shouldn&amp;#39;t surprise anyone that the backend made for home computers is more popular than the one with high hardware requirements.&lt;/p&gt;\n\n&lt;p&gt;That is an &lt;strong&gt;objectively correct statement,&lt;/strong&gt; which you&amp;#39;re angry about for some dumb reason. It doesn&amp;#39;t matter if vLLM is superior and that all the pros use finely-tuned vLLM servers at home. I am already aware that vLLM is better optimized (literally just read my original post, dude).&lt;/p&gt;\n\n&lt;p&gt;All that matters regarding our argument is that Ollama is objectively more popular, which is an objectively correct statement which you seem unable to accept - but vLLM is steadily rising, which is why I am interested in it and wanted to hear if it&amp;#39;s worth switching.&lt;/p&gt;\n\n&lt;p&gt;I am putting an end to this waste of time now by blocking both of you. I don&amp;#39;t need rude idiots who pettily argue against the most basic facts and keep shifting the goalposts.&lt;/p&gt;\n\n&lt;p&gt;Come on, &lt;strong&gt;think for a moment about what you are saying.&lt;/strong&gt; You&amp;#39;re arguing against Ollama&amp;#39;s popularity by saying &amp;quot;yeah Ollama IS vastly more popular because every noob uses it, BUT vLLM is better&amp;quot;. That&amp;#39;s a total non-sequiteur in an argument about Ollama&amp;#39;s &lt;strong&gt;*popularity*. Sigh. So tiring!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Please stop wasting time with dishonest arguments on the internet. Anyone else who tries it is getting immediately blocked.&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;ve already set up vLLM in the past. It wasn&amp;#39;t particularly hard and only took like five minutes. I was merely asking if I should switch to it full-time. Don&amp;#39;t waste any more of my time.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mb6i7x",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5k3jm9/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753678991,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753678991,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5k2uko",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "DinoAmino",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5k1e40",
                                                    "score": 0,
                                                    "author_fullname": "t2_j1v7f",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "So a lot of noobs that heard about DeepSeek GGUFs running on Ollama from some YouTubers searched a ton for \"how to install Ollama\". Meanwhile vLLM  and llama.cpp users who had their shit together didn't need to search about their setup. Ok, you got me I guess.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5k2uko",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So a lot of noobs that heard about DeepSeek GGUFs running on Ollama from some YouTubers searched a ton for &amp;quot;how to install Ollama&amp;quot;. Meanwhile vLLM  and llama.cpp users who had their shit together didn&amp;#39;t need to search about their setup. Ok, you got me I guess.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mb6i7x",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5k2uko/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753678644,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753678644,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 0
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5k1e40",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "pilkyton",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5k10kk",
                                          "score": 0,
                                          "author_fullname": "t2_4a13s1mr",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yes, it shows what's on the mind of everyone in the world at the time by extrapolating from the world's most popular search engine (\\~90% of all searches in the world go via Google).\n\nThe numbers are scaled relative to the highest search volume ever recorded (that's the \"100\" peak for Ollama). The data points are on a weekly basis and gathers all the search volume for that week.\n\nSo you can hover any data point and see for example: vLLM = 2, Ollama = 17. Meaning that people searched for \"ollama\" 17/2 = 8.5x more that week.\n\n\\---\n\nOllama is consistently \\*vastly\\* more popular among people.\n\nNot sure why that objective and easily verifiable fact triggers some people.\n\nPS: It's funny that this thread has both Ollama haters and vLLM haters depending on which comment chain you read, haha. Welcome to the vLLM chain. Have a cup of tea. There's biscuits on the table.",
                                          "edited": 1753678551,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5k1e40",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, it shows what&amp;#39;s on the mind of everyone in the world at the time by extrapolating from the world&amp;#39;s most popular search engine (~90% of all searches in the world go via Google).&lt;/p&gt;\n\n&lt;p&gt;The numbers are scaled relative to the highest search volume ever recorded (that&amp;#39;s the &amp;quot;100&amp;quot; peak for Ollama). The data points are on a weekly basis and gathers all the search volume for that week.&lt;/p&gt;\n\n&lt;p&gt;So you can hover any data point and see for example: vLLM = 2, Ollama = 17. Meaning that people searched for &amp;quot;ollama&amp;quot; 17/2 = 8.5x more that week.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Ollama is consistently *vastly* more popular among people.&lt;/p&gt;\n\n&lt;p&gt;Not sure why that objective and easily verifiable fact triggers some people.&lt;/p&gt;\n\n&lt;p&gt;PS: It&amp;#39;s funny that this thread has both Ollama haters and vLLM haters depending on which comment chain you read, haha. Welcome to the vLLM chain. Have a cup of tea. There&amp;#39;s biscuits on the table.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mb6i7x",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5k1e40/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753677939,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753677939,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 0
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5k10kk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DinoAmino",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5jzfzq",
                                "score": 1,
                                "author_fullname": "t2_j1v7f",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Are you objectively correct using Google search frequency to make that kind of assumption?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5k10kk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you objectively correct using Google search frequency to make that kind of assumption?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mb6i7x",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5k10kk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753677755,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753677755,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5jzfzq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pilkyton",
                      "can_mod_post": false,
                      "created_utc": 1753677003,
                      "send_replies": true,
                      "parent_id": "t1_n5jytl7",
                      "score": -2,
                      "author_fullname": "t2_4a13s1mr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Well you are objectively wrong. Like I said, **vLLM: Almost nobody used it.**\n\nBut Ollama has shrunk from a 9x lead to a 4.5x lead:\n\n[https://trends.google.com/trends/explore?date=2023-07-28%202025-07-28&amp;q=vllm,ollama&amp;hl=en-GB](https://trends.google.com/trends/explore?date=2023-07-28%202025-07-28&amp;q=vllm,ollama&amp;hl=en-GB)\n\n\\---\n\nEdit: I'll also surface the information about **GitHub stars** brought up by someone else below.\n\n* [Ollama:](https://github.com/ollama/ollama)**147787**\n* [Llama.cpp:](https://github.com/ggml-org/llama.cpp)83570\n* [Vllm:](https://github.com/vllm-project/vllm)53389\n\n**So even among the most technical people - developers - Ollama is 3x more popular.** And among casual users, Ollama is vastly more popular because it **actually works** on low-power home computers.\n\nAnd yes, I blocked you because your comment was a dick move and just a total waste of time. I am not interested in seeing any more comments from you since this is your level of dishonest, rude discourse.",
                      "edited": 1753725171,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5jzfzq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well you are objectively wrong. Like I said, &lt;strong&gt;vLLM: Almost nobody used it.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;But Ollama has shrunk from a 9x lead to a 4.5x lead:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://trends.google.com/trends/explore?date=2023-07-28%202025-07-28&amp;amp;q=vllm,ollama&amp;amp;hl=en-GB\"&gt;https://trends.google.com/trends/explore?date=2023-07-28%202025-07-28&amp;amp;q=vllm,ollama&amp;amp;hl=en-GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;ll also surface the information about &lt;strong&gt;GitHub stars&lt;/strong&gt; brought up by someone else below.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ollama/ollama\"&gt;Ollama:&lt;/a&gt;&lt;strong&gt;147787&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp\"&gt;Llama.cpp:&lt;/a&gt;83570&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/vllm-project/vllm\"&gt;Vllm:&lt;/a&gt;53389&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;So even among the most technical people - developers - Ollama is 3x more popular.&lt;/strong&gt; And among casual users, Ollama is vastly more popular because it &lt;strong&gt;actually works&lt;/strong&gt; on low-power home computers.&lt;/p&gt;\n\n&lt;p&gt;And yes, I blocked you because your comment was a dick move and just a total waste of time. I am not interested in seeing any more comments from you since this is your level of dishonest, rude discourse.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb6i7x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jzfzq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753677003,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5jytl7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "entsnack",
            "can_mod_post": false,
            "created_utc": 1753676709,
            "send_replies": true,
            "parent_id": "t3_1mb6i7x",
            "score": 14,
            "author_fullname": "t2_1a48h7vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\\&gt; almost nobody used\n\nstopped reading here\n\nEdit: lmao the OP blocked me for this comment, tells you all you need to know",
            "edited": 1753721886,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5jytl7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; almost nobody used&lt;/p&gt;\n\n&lt;p&gt;stopped reading here&lt;/p&gt;\n\n&lt;p&gt;Edit: lmao the OP blocked me for this comment, tells you all you need to know&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jytl7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753676709,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "link_id": "t3_1mb6i7x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "transparent",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 14
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5jzzn7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pilkyton",
                      "can_mod_post": false,
                      "created_utc": 1753677260,
                      "send_replies": true,
                      "parent_id": "t1_n5jytok",
                      "score": 3,
                      "author_fullname": "t2_4a13s1mr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's a really good question and it seems the answer is no. I read something saying that vLLM is tightly optimized for high-throughput serving of a single model loaded into memory, using shared KV cache and GPU-accelerated paging (which allows it to support very long input contexts while reducing VRAM usage).\n\nAnd that it therefore loads 1 model per instance and cannot switch without restarting vLLM.\n\nThat is a major win for Ollama. Even if the throughput is worse, being able to switch models on the fly via OpenWebUI is a major benefit for me since I use different models for different tasks in different chat tabs/sessions.\n\nI guess I'll benchmark the two at some time and then decide if the speedup is enough to be worth the hassle of having to manually restart vLLM to change model. Most of the time, I only use one model, so it could be worth it sometimes. Heck, I could even use both. vLLM for specific models, and Ollama in general for multi-swapping.",
                      "edited": 1753724889,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5jzzn7",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s a really good question and it seems the answer is no. I read something saying that vLLM is tightly optimized for high-throughput serving of a single model loaded into memory, using shared KV cache and GPU-accelerated paging (which allows it to support very long input contexts while reducing VRAM usage).&lt;/p&gt;\n\n&lt;p&gt;And that it therefore loads 1 model per instance and cannot switch without restarting vLLM.&lt;/p&gt;\n\n&lt;p&gt;That is a major win for Ollama. Even if the throughput is worse, being able to switch models on the fly via OpenWebUI is a major benefit for me since I use different models for different tasks in different chat tabs/sessions.&lt;/p&gt;\n\n&lt;p&gt;I guess I&amp;#39;ll benchmark the two at some time and then decide if the speedup is enough to be worth the hassle of having to manually restart vLLM to change model. Most of the time, I only use one model, so it could be worth it sometimes. Heck, I could even use both. vLLM for specific models, and Ollama in general for multi-swapping.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb6i7x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jzzn7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753677260,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5jytok",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Apprehensive-Emu357",
            "can_mod_post": false,
            "created_utc": 1753676711,
            "send_replies": true,
            "parent_id": "t3_1mb6i7x",
            "score": 2,
            "author_fullname": "t2_m0b14m4v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Does vLLM support switching models on the fly yet? if not, thats a very important feature",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5jytok",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does vLLM support switching models on the fly yet? if not, thats a very important feature&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5jytok/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753676711,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb6i7x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5mumln",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pilkyton",
                      "can_mod_post": false,
                      "created_utc": 1753720399,
                      "send_replies": true,
                      "parent_id": "t1_n5kkzi2",
                      "score": 1,
                      "author_fullname": "t2_4a13s1mr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ollama is a llama.cpp inference engine frontend.\n\nvLLM is a vLLM inference engine frontend.\n\n**Comparing both means benchmarking the llama.cpp vs vLLM inference engines.**\n\nI am sure you know all this which is what makes your comment even dumber and an even bigger waste of time. Why did you waste your time and my time typing out that total waste of time for both of us?\n\nLet's not argue any more pointless semantics after this. I've had enough of people arguing pointless things dishonestly. There isn't enough time in the day for that.",
                      "edited": 1753720640,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5mumln",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama is a llama.cpp inference engine frontend.&lt;/p&gt;\n\n&lt;p&gt;vLLM is a vLLM inference engine frontend.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Comparing both means benchmarking the llama.cpp vs vLLM inference engines.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am sure you know all this which is what makes your comment even dumber and an even bigger waste of time. Why did you waste your time and my time typing out that total waste of time for both of us?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s not argue any more pointless semantics after this. I&amp;#39;ve had enough of people arguing pointless things dishonestly. There isn&amp;#39;t enough time in the day for that.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb6i7x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5mumln/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753720399,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5kkzi2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Sicarius_The_First",
            "can_mod_post": false,
            "created_utc": 1753688523,
            "send_replies": true,
            "parent_id": "t3_1mb6i7x",
            "score": 0,
            "author_fullname": "t2_ik8czvp65",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "vllm and ollama or not comparable.\n\nWhat's better, bacon or the color blue?\n\nollama is a frontend, vllm is an inference engine for scale.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kkzi2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vllm and ollama or not comparable.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s better, bacon or the color blue?&lt;/p&gt;\n\n&lt;p&gt;ollama is a frontend, vllm is an inference engine for scale.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/n5kkzi2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753688523,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb6i7x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]