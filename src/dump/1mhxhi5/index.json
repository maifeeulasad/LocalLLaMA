[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I saw the weights were released (they were supposed to be [a couple days ago](https://www.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/), but [the upload failed](https://huggingface.co/qihoo360/Light-IF-32B/discussions/1#68901831d6b49f34311d92d3)), but nobody made GGUFs, so I used GGUF-my-repo (no imatrix) and gave it a shot. [https://huggingface.co/DeProgrammer/Light-IF-32B-Q4\\_K\\_M-GGUF](https://huggingface.co/DeProgrammer/Light-IF-32B-Q4_K_M-GGUF)\n\nIt's based on Qwen3-32B, and they don't seem to have lobotomized it, but I couldn't tell a difference from trying two \"write a bunch of code based on this spec\" prompts because most recent models follow instructions pretty well. (This is a fine-tune focused on instruction following, if you didn't catch that.) For the third attempt, I used a prompt from my first LLM experiment last March, trying to generate informational text with a lot of constraints in the form of JavaScript.\n\nI ran it with temperature = 0, and it got stuck in a loop on one out of three prompts. I also think its overthinking is about on par with QwQ. This is about 9500 tokens of thinking: [Prompt](https://pastebin.com/XAau1rxk), [response](https://pastebin.com/xW6CLQtP)\n\nHow I'd rate the results on a per-requirement basis (again, keeping in mind this is Q4\\_K\\_M and no imatrix):\n\n|Result (out of 10)|Requirement|Notes|\n|:-|:-|:-|\n|9|Valid JavaScript|It didn't escape an example it gave using `&lt;` despite the inclusion of HTML nodes in my example; i.e., it didn't **infer** that it should escape HTML.|\n|7|you should try to structure the information with many levels of hierarchy and more details at each level|It stuck to strictly 3 levels. My example doesn't show an accordion in an accordion, but the instructions did indicate that it's allowed.|\n|4|Top-level categories don't have to make statements or suggestions, but others must do so to the maximum feasible extent.|The accordion nodes don't make any sort of statement or suggestion; they're just labels. Additionally, one item node said \"failing to\"--not a statement or suggestion.|\n|10|Place any lengthy reasoning (if it requires more than five words), more esoteric knowledge (language-, domain-, library-, or algorithm-specific), and examples in POP nodes.||\n|10|The KnowledgeNode constructor's first parameter can be 0 for all your responses||\n|6|Dig deep for obscure knowledge and tips; skip generally-obvious information|By design, I believe LLMs aren't particularly good at obscurity.|\n|8|write terse statements and imperative sentences instead of using wordy \"professional college textbook author\" language|I don't think the text could be *much* shorter.|\n|8|but also describing conflicting views|It recommended markdown, wikis, note-taking apps, *and* a dedicated tool.|\n|4|(unstated) Subtrees should not contain similar numbers of nodes just for the sake of consistency|This is another thing I noticed LLMs in general seem to love--3-4 points in every section even though it's unrealistic for there to be such a perfect split in real-world categories. It produced 3, 3, 3, 4, 4, 4, but at least there were 6 accordion nodes.|",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Light-IF-32B weights",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhxhi5",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.88,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_w4j8t",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754361785,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754361519,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw the weights were released (they were supposed to be &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/\"&gt;a couple days ago&lt;/a&gt;, but &lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B/discussions/1#68901831d6b49f34311d92d3\"&gt;the upload failed&lt;/a&gt;), but nobody made GGUFs, so I used GGUF-my-repo (no imatrix) and gave it a shot. &lt;a href=\"https://huggingface.co/DeProgrammer/Light-IF-32B-Q4_K_M-GGUF\"&gt;https://huggingface.co/DeProgrammer/Light-IF-32B-Q4_K_M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s based on Qwen3-32B, and they don&amp;#39;t seem to have lobotomized it, but I couldn&amp;#39;t tell a difference from trying two &amp;quot;write a bunch of code based on this spec&amp;quot; prompts because most recent models follow instructions pretty well. (This is a fine-tune focused on instruction following, if you didn&amp;#39;t catch that.) For the third attempt, I used a prompt from my first LLM experiment last March, trying to generate informational text with a lot of constraints in the form of JavaScript.&lt;/p&gt;\n\n&lt;p&gt;I ran it with temperature = 0, and it got stuck in a loop on one out of three prompts. I also think its overthinking is about on par with QwQ. This is about 9500 tokens of thinking: &lt;a href=\"https://pastebin.com/XAau1rxk\"&gt;Prompt&lt;/a&gt;, &lt;a href=\"https://pastebin.com/xW6CLQtP\"&gt;response&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;How I&amp;#39;d rate the results on a per-requirement basis (again, keeping in mind this is Q4_K_M and no imatrix):&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Result (out of 10)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Requirement&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notes&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;9&lt;/td&gt;\n&lt;td align=\"left\"&gt;Valid JavaScript&lt;/td&gt;\n&lt;td align=\"left\"&gt;It didn&amp;#39;t escape an example it gave using &lt;code&gt;&amp;lt;&lt;/code&gt; despite the inclusion of HTML nodes in my example; i.e., it didn&amp;#39;t &lt;strong&gt;infer&lt;/strong&gt; that it should escape HTML.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;you should try to structure the information with many levels of hierarchy and more details at each level&lt;/td&gt;\n&lt;td align=\"left\"&gt;It stuck to strictly 3 levels. My example doesn&amp;#39;t show an accordion in an accordion, but the instructions did indicate that it&amp;#39;s allowed.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;Top-level categories don&amp;#39;t have to make statements or suggestions, but others must do so to the maximum feasible extent.&lt;/td&gt;\n&lt;td align=\"left\"&gt;The accordion nodes don&amp;#39;t make any sort of statement or suggestion; they&amp;#39;re just labels. Additionally, one item node said &amp;quot;failing to&amp;quot;--not a statement or suggestion.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;Place any lengthy reasoning (if it requires more than five words), more esoteric knowledge (language-, domain-, library-, or algorithm-specific), and examples in POP nodes.&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;The KnowledgeNode constructor&amp;#39;s first parameter can be 0 for all your responses&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;Dig deep for obscure knowledge and tips; skip generally-obvious information&lt;/td&gt;\n&lt;td align=\"left\"&gt;By design, I believe LLMs aren&amp;#39;t particularly good at obscurity.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;write terse statements and imperative sentences instead of using wordy &amp;quot;professional college textbook author&amp;quot; language&lt;/td&gt;\n&lt;td align=\"left\"&gt;I don&amp;#39;t think the text could be &lt;em&gt;much&lt;/em&gt; shorter.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;but also describing conflicting views&lt;/td&gt;\n&lt;td align=\"left\"&gt;It recommended markdown, wikis, note-taking apps, &lt;em&gt;and&lt;/em&gt; a dedicated tool.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;(unstated) Subtrees should not contain similar numbers of nodes just for the sake of consistency&lt;/td&gt;\n&lt;td align=\"left\"&gt;This is another thing I noticed LLMs in general seem to love--3-4 points in every section even though it&amp;#39;s unrealistic for there to be such a perfect split in real-world categories. It produced 3, 3, 3, 4, 4, 4, but at least there were 6 accordion nodes.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?auto=webp&amp;s=d1d445d7e1c7e87311bd6343052f8b3d97c89755",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a54e357184df8fd6f26574e2c0b7b571a1e22ce",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ffe22b9b72abd7c913d9f976f6c446d16f1c478",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64af7db451b58e1345e62b5b21fb831dcae9c1ea",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=80887f38291adb0acf529d86a8f4b04f1953edb6",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8bd084a33806e89ad5f4d5bdfa76a5cac18c3db",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6a4ef870f8531e9522139c71b54b46b7308ec566",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "tI1sapCmqbZGukHzbMC9_a1hbFiuJ7D5e252D-gJnrc"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mhxhi5",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "DeProgrammer99",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhxhi5/lightif32b_weights/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhxhi5/lightif32b_weights/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754361519,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6zmfdx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1754362996,
            "send_replies": true,
            "parent_id": "t3_1mhxhi5",
            "score": 2,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen 3 based models, for example, can end up being fundamentally still fairly close to Qwen 3 base if their fine tune is only 1,000 to 10,000 response pairs of SFT without RL for example. There are organisations that would release that as a “new” “model” even though in the diffusion model image generation world that would be known as merely a “checkpoint”, which is more accurate. RL does immediately have more potential per training token to shift the model behaviour however, for example, proximal policy gradient methods have significant entropy regularisation via the KL-divergence term in the loss function and this limits how far the weights will move. In fact now that I check, they absolutely did use an entropy-adaptive method so this prediction will likely hold.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6zmfdx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen 3 based models, for example, can end up being fundamentally still fairly close to Qwen 3 base if their fine tune is only 1,000 to 10,000 response pairs of SFT without RL for example. There are organisations that would release that as a “new” “model” even though in the diffusion model image generation world that would be known as merely a “checkpoint”, which is more accurate. RL does immediately have more potential per training token to shift the model behaviour however, for example, proximal policy gradient methods have significant entropy regularisation via the KL-divergence term in the loss function and this limits how far the weights will move. In fact now that I check, they absolutely did use an entropy-adaptive method so this prediction will likely hold.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhxhi5/lightif32b_weights/n6zmfdx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754362996,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhxhi5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]