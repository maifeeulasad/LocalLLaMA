[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": " We're in the era now where open source releases are nipping at the heels of closed-source models in benchmarks. But it's all in text modality. \n\nAs far as I can tell, there hasn't been a really solid contender when it comes to both being a SOTA model, and also having native audio/image/video input and image/audio output which has been demonstrated by OpenAI and Google.\n\nI feel like this is a really big deal that is mostly overlooked when comparing open source to closed source. Programming benchmarks are cool and all, but for a truly useful assistant, you need a model you can speak to, show stuff to, and it can speak back and generate images to show you stuff as well.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why is open source so behind on multi-modalitty?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdruc9",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 77,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_66km3",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 77,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753935061,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re in the era now where open source releases are nipping at the heels of closed-source models in benchmarks. But it&amp;#39;s all in text modality. &lt;/p&gt;\n\n&lt;p&gt;As far as I can tell, there hasn&amp;#39;t been a really solid contender when it comes to both being a SOTA model, and also having native audio/image/video input and image/audio output which has been demonstrated by OpenAI and Google.&lt;/p&gt;\n\n&lt;p&gt;I feel like this is a really big deal that is mostly overlooked when comparing open source to closed source. Programming benchmarks are cool and all, but for a truly useful assistant, you need a model you can speak to, show stuff to, and it can speak back and generate images to show you stuff as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mdruc9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "AnticitizenPrime",
            "discussion_type": null,
            "num_comments": 51,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/",
            "subreddit_subscribers": 507935,
            "created_utc": 1753935061,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64x6mh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "XeNoGeaR52",
                      "can_mod_post": false,
                      "created_utc": 1753955069,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 19,
                      "author_fullname": "t2_esok7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "don't forget the hardware ! Not everyone has hyper powerful GPU farm at home",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64x6mh",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;don&amp;#39;t forget the hardware ! Not everyone has hyper powerful GPU farm at home&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64x6mh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753955069,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 19
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n66hmxi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "swagonflyyyy",
                      "can_mod_post": false,
                      "created_utc": 1753975354,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 3,
                      "author_fullname": "t2_iev1qh7k",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "True that. Not to mention building a framework around that is extremely hard to do from scratch.\n\nNot only do you have to grapple with dependency hell but you also need to make sure all these interconnected parts work together in harmony, which requires a lot of precision, optimization and timing to get right. \n\nAt least the hardware requirements are steadily dropping. Thats a huge barrier for most people. But yeah, the lack of support for multimodal is staggering.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n66hmxi",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;True that. Not to mention building a framework around that is extremely hard to do from scratch.&lt;/p&gt;\n\n&lt;p&gt;Not only do you have to grapple with dependency hell but you also need to make sure all these interconnected parts work together in harmony, which requires a lot of precision, optimization and timing to get right. &lt;/p&gt;\n\n&lt;p&gt;At least the hardware requirements are steadily dropping. Thats a huge barrier for most people. But yeah, the lack of support for multimodal is staggering.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n66hmxi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753975354,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n654eab",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Apprehensive_Rub2",
                      "can_mod_post": false,
                      "created_utc": 1753958779,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 2,
                      "author_fullname": "t2_6odjqu6o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "They do have support on llamacpp though? \nAt least Minicpm-o &amp; gemma I believe are available. It's just pretty recent and not super well documented. \n\n\nSo yeah your overall point is definitely accurate. I just thought I'd point out that support is getting a lot better. Through kobold especially it's pretty trivial to get these models working locally. \n\n\nAlso the context required for images is really high so high vram required. I can't run any semidecent model with more than 8k context on 8gb vram. Meaning like 4 images max. Even on 16gb cards it's a major limitation",
                      "edited": 1753962223,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n654eab",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They do have support on llamacpp though? \nAt least Minicpm-o &amp;amp; gemma I believe are available. It&amp;#39;s just pretty recent and not super well documented. &lt;/p&gt;\n\n&lt;p&gt;So yeah your overall point is definitely accurate. I just thought I&amp;#39;d point out that support is getting a lot better. Through kobold especially it&amp;#39;s pretty trivial to get these models working locally. &lt;/p&gt;\n\n&lt;p&gt;Also the context required for images is really high so high vram required. I can&amp;#39;t run any semidecent model with more than 8k context on 8gb vram. Meaning like 4 images max. Even on 16gb cards it&amp;#39;s a major limitation&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n654eab/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753958779,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n676nj2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "larrytheevilbunnie",
                      "can_mod_post": false,
                      "created_utc": 1753982338,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 2,
                      "author_fullname": "t2_2hco1k",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah it’s kinda bad, despite Boeing the focus of a hackathon, Gemma 3n multimodal still isn’t supported by ollama",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n676nj2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah it’s kinda bad, despite Boeing the focus of a hackathon, Gemma 3n multimodal still isn’t supported by ollama&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n676nj2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753982338,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "total_awards_received": 0,
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "ups": 3,
                      "removal_reason": null,
                      "link_id": "t3_1mdruc9",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n65f99g",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "FullOf_Bad_Ideas",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n64gd6x",
                                "score": 1,
                                "author_fullname": "t2_9s7pmakgx",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Nemotron 49B v1.5 works nicely in exllamav3 with tabbyAPI BTW. Quants are on HF.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n65f99g",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nemotron 49B v1.5 works nicely in exllamav3 with tabbyAPI BTW. Quants are on HF.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n65f99g/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753963396,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753963396,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n64gd6x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": "DELETED",
                      "no_follow": true,
                      "author": "[deleted]",
                      "can_mod_post": false,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 3,
                      "approved_by": null,
                      "report_reasons": null,
                      "all_awardings": [],
                      "subreddit_id": "t5_81eyvm",
                      "body": "[deleted]",
                      "edited": false,
                      "author_flair_css_class": null,
                      "collapsed": true,
                      "downs": 0,
                      "is_submitter": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "associated_award": null,
                      "stickied": false,
                      "subreddit_type": "public",
                      "can_gild": false,
                      "top_awarded_type": null,
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64gd6x/",
                      "num_reports": null,
                      "locked": false,
                      "name": "t1_n64gd6x",
                      "created": 1753945511,
                      "subreddit": "LocalLLaMA",
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "created_utc": 1753945511,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "mod_note": null,
                      "distinguished": null
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n67pe09",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Accomplished_Mode170",
                      "can_mod_post": false,
                      "created_utc": 1753987513,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 1,
                      "author_fullname": "t2_4hfmiefj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "We need cross platform mlx",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n67pe09",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We need cross platform mlx&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n67pe09/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753987513,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n67s03l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Evening_Ad6637",
                      "can_mod_post": false,
                      "created_utc": 1753988248,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 1,
                      "author_fullname": "t2_p45er6oo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Qwen omni has llamacpp support. at least partially, but it’s the Most supported multimodal model in llamacpp.cpp (audio and Vision at same time)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n67s03l",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen omni has llamacpp support. at least partially, but it’s the Most supported multimodal model in llamacpp.cpp (audio and Vision at same time)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n67s03l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753988248,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n68c7r4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Leopold_Boom",
                      "can_mod_post": false,
                      "created_utc": 1753994067,
                      "send_replies": true,
                      "parent_id": "t1_n647837",
                      "score": 1,
                      "author_fullname": "t2_5weqo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "My big challenge was that you can't actually do multimodal inferencing using llama-cpp-python or any of the major python bindings. \n\nI ended up havint to cook up a Python bridge using cppyy and Cython (and some patches to llama.cpp) to enable support of head llama.cpp for multi-modal inferencing (i.e. you can call parts of llama-mtmd from python). It works well if you want to do something a little fancier than just llama-server calls. Lemme me know if it would be useful if I push it to a public Git repository. It's usable but not sure I can maintain / add features etc.",
                      "edited": 1753994384,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n68c7r4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My big challenge was that you can&amp;#39;t actually do multimodal inferencing using llama-cpp-python or any of the major python bindings. &lt;/p&gt;\n\n&lt;p&gt;I ended up havint to cook up a Python bridge using cppyy and Cython (and some patches to llama.cpp) to enable support of head llama.cpp for multi-modal inferencing (i.e. you can call parts of llama-mtmd from python). It works well if you want to do something a little fancier than just llama-server calls. Lemme me know if it would be useful if I push it to a public Git repository. It&amp;#39;s usable but not sure I can maintain / add features etc.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n68c7r4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753994067,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n647837",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Betadoggo_",
            "can_mod_post": false,
            "created_utc": 1753940630,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 68,
            "author_fullname": "t2_a177eog2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We already have a ton of them that aren't being used due to the lack of an accessible implementation. These days if a model doesn't have support in llamacpp within 2 weeks of release it's pretty much dead. Minicpm-o, Ernie-VL, phi-vision/multimodal, and qwen-omni are good examples of this. None of these models ever had a chance because the average person can't use them outside of huggingface demos.\n\nIt's not anyone's fault, it's just significantly harder to implement these models, and the pool of people with the knowledge, skill, and motivation to do so is very small.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n647837",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We already have a ton of them that aren&amp;#39;t being used due to the lack of an accessible implementation. These days if a model doesn&amp;#39;t have support in llamacpp within 2 weeks of release it&amp;#39;s pretty much dead. Minicpm-o, Ernie-VL, phi-vision/multimodal, and qwen-omni are good examples of this. None of these models ever had a chance because the average person can&amp;#39;t use them outside of huggingface demos.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not anyone&amp;#39;s fault, it&amp;#39;s just significantly harder to implement these models, and the pool of people with the knowledge, skill, and motivation to do so is very small.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n647837/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753940630,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 68
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64cakr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "No_Efficiency_1144",
                      "can_mod_post": false,
                      "created_utc": 1753943290,
                      "send_replies": true,
                      "parent_id": "t1_n6401r1",
                      "score": 7,
                      "author_fullname": "t2_1nkj9l14b0",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes they usually want to hold at least one thing back for closed source and at the moment multimodality is the thing that gets held back.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64cakr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes they usually want to hold at least one thing back for closed source and at the moment multimodality is the thing that gets held back.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64cakr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753943290,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64gff0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "RhubarbSimilar1683",
                      "can_mod_post": false,
                      "created_utc": 1753945544,
                      "send_replies": true,
                      "parent_id": "t1_n6401r1",
                      "score": 5,
                      "author_fullname": "t2_1k4sjdwzk2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I believe that at least qwen is not natively multimodal and multimodality is achieved by separately running OCR and a secondary AI model for image description.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64gff0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I believe that at least qwen is not natively multimodal and multimodality is achieved by separately running OCR and a secondary AI model for image description.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64gff0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753945544,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6401r1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753937111,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 25,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'd guess because multimodal is more of a product feature that sells subscriptions while text has a bunch of benchmarks for bragging rights.  Generating images is fun while solving hard math problems shows you're a serious \"AI\" company.  For example, [Qwen](https://chat.qwen.ai/) seems to offer a multimodal Qwen3-235B-A22B-2507 but the released model is text only.  Of course, it could be pseudo multimodal, but the visual part seems integrated at least.\n\nI suspect another part of it is data.  I think the release of Deepseek R1 was a real boon to the industry since it might not have been perfect but it enabled AI companies to generate and process huge amounts of data which they could feed back into their models to train.  Nvidia does this quite a bit with their Nemotron models, for example.  Labeled image data, however, is much less available and much more expensive.  This means that there's more motivation to keep it closed and make some money on it, as well as making it less likely that there will be much open competition to one-up.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6401r1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d guess because multimodal is more of a product feature that sells subscriptions while text has a bunch of benchmarks for bragging rights.  Generating images is fun while solving hard math problems shows you&amp;#39;re a serious &amp;quot;AI&amp;quot; company.  For example, &lt;a href=\"https://chat.qwen.ai/\"&gt;Qwen&lt;/a&gt; seems to offer a multimodal Qwen3-235B-A22B-2507 but the released model is text only.  Of course, it could be pseudo multimodal, but the visual part seems integrated at least.&lt;/p&gt;\n\n&lt;p&gt;I suspect another part of it is data.  I think the release of Deepseek R1 was a real boon to the industry since it might not have been perfect but it enabled AI companies to generate and process huge amounts of data which they could feed back into their models to train.  Nvidia does this quite a bit with their Nemotron models, for example.  Labeled image data, however, is much less available and much more expensive.  This means that there&amp;#39;s more motivation to keep it closed and make some money on it, as well as making it less likely that there will be much open competition to one-up.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n6401r1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753937111,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 25
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n64gb3r",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "RhubarbSimilar1683",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n63x9p9",
                                "score": 12,
                                "author_fullname": "t2_1k4sjdwzk2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "are we sure that those massive models are natively multimodal? they could be running OCR on your screenshots for all we know . maybe they could be running a separate LLM to describe the screenshot before sending it to the main model. edit Maybe it's a rag pipeline for images i don't really know.",
                                "edited": 1753961292,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n64gb3r",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;are we sure that those massive models are natively multimodal? they could be running OCR on your screenshots for all we know . maybe they could be running a separate LLM to describe the screenshot before sending it to the main model. edit Maybe it&amp;#39;s a rag pipeline for images i don&amp;#39;t really know.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64gb3r/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753945479,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753945479,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 12
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n63x9p9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AnticitizenPrime",
                      "can_mod_post": false,
                      "created_utc": 1753935824,
                      "send_replies": true,
                      "parent_id": "t1_n63w0rx",
                      "score": 14,
                      "author_fullname": "t2_66km3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Open source models are getting so very good, but I find myself still switching to Gemini or Claude or whatever to share a screenshot or something to speed up solving my problem du jour. And while I don't use speech/audio or image generation much or at all, it's kind of a big deal that doesn't really exist on the local scene (outside of a few experimental small models).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63x9p9",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Open source models are getting so very good, but I find myself still switching to Gemini or Claude or whatever to share a screenshot or something to speed up solving my problem du jour. And while I don&amp;#39;t use speech/audio or image generation much or at all, it&amp;#39;s kind of a big deal that doesn&amp;#39;t really exist on the local scene (outside of a few experimental small models).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n63x9p9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753935824,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 14
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64emek",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No_Efficiency_1144",
                      "can_mod_post": false,
                      "created_utc": 1753944569,
                      "send_replies": true,
                      "parent_id": "t1_n63w0rx",
                      "score": 2,
                      "author_fullname": "t2_1nkj9l14b0",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "A big thing is that reasoning training can be done turboMax speed by distilling deepseek chains of thought.\n\n\nEven without that, it is not a lot of steps of GRPO to add okay reasoning to an LLM sometimes.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64emek",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A big thing is that reasoning training can be done turboMax speed by distilling deepseek chains of thought.&lt;/p&gt;\n\n&lt;p&gt;Even without that, it is not a lot of steps of GRPO to add okay reasoning to an LLM sometimes.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64emek/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753944569,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n655vka",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "-dysangel-",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n64x7bv",
                                "score": 3,
                                "author_fullname": "t2_12ggykute6",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The scientific method is more important than the data it's applied to IMO. There are a lot of scientists out there who do not apply it well. Veritaseum did a good video on research papers in general [https://www.youtube.com/watch?v=42QuXLucH3Q](https://www.youtube.com/watch?v=42QuXLucH3Q) . Garbage in, garbage out. So IMO we need really high quality data which teaches the principles of logical thinking.\n\n\n\nI agree that having general knowledge is probably a very helpful guiding factor, but again - a smart agent with access to good RAG is IMO way more likely to solve real problems than a large agent that can spit out the whole of wikipedia, and every paper ever made token for token, but has not been taught how to think from first principles.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n655vka",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The scientific method is more important than the data it&amp;#39;s applied to IMO. There are a lot of scientists out there who do not apply it well. Veritaseum did a good video on research papers in general &lt;a href=\"https://www.youtube.com/watch?v=42QuXLucH3Q\"&gt;https://www.youtube.com/watch?v=42QuXLucH3Q&lt;/a&gt; . Garbage in, garbage out. So IMO we need really high quality data which teaches the principles of logical thinking.&lt;/p&gt;\n\n&lt;p&gt;I agree that having general knowledge is probably a very helpful guiding factor, but again - a smart agent with access to good RAG is IMO way more likely to solve real problems than a large agent that can spit out the whole of wikipedia, and every paper ever made token for token, but has not been taught how to think from first principles.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n655vka/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753959463,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753959463,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n64x7bv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "PurpleWinterDawn",
                      "can_mod_post": false,
                      "created_utc": 1753955079,
                      "send_replies": true,
                      "parent_id": "t1_n63w0rx",
                      "score": 1,
                      "author_fullname": "t2_dncq6xad",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Meanwhile this is also a thing [https://arxiv.org/abs/2411.04986](https://arxiv.org/abs/2411.04986)\n\nWhile focusing on improving \"thinking\" as a standalone item is a good idea, don't get me wrong, is it really the best thing to do when the data to think on is either incomplete or too coarse? Perhaps the diversification of data modalities is part of a large improvement in thinking capabilities that's yet to be fully explored.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64x7bv",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Meanwhile this is also a thing &lt;a href=\"https://arxiv.org/abs/2411.04986\"&gt;https://arxiv.org/abs/2411.04986&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While focusing on improving &amp;quot;thinking&amp;quot; as a standalone item is a good idea, don&amp;#39;t get me wrong, is it really the best thing to do when the data to think on is either incomplete or too coarse? Perhaps the diversification of data modalities is part of a large improvement in thinking capabilities that&amp;#39;s yet to be fully explored.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64x7bv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753955079,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n63w0rx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753935281,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 35,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "IMO it's because while those are going to be great use cases over time, reasoning ability is currently the \"killer app\" that needs to be figured out before we start throwing these things in embodied robots with vision and speech etc, so most people are focused on that",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63w0rx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;IMO it&amp;#39;s because while those are going to be great use cases over time, reasoning ability is currently the &amp;quot;killer app&amp;quot; that needs to be figured out before we start throwing these things in embodied robots with vision and speech etc, so most people are focused on that&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n63w0rx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753935281,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 35
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n64lskl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Blizado",
            "can_mod_post": false,
            "created_utc": 1753948579,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 5,
            "author_fullname": "t2_j0e2r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My hope lays a bit on MistralAI, they have now so much different multi modal models which can to different stuff that I think it is only a matter of time until they put all together in one model. Maybe it could be even possible that the community does it, since many models are all based on the same Mistral Small model and we know which layers are for vision/speech/etc.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64lskl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My hope lays a bit on MistralAI, they have now so much different multi modal models which can to different stuff that I think it is only a matter of time until they put all together in one model. Maybe it could be even possible that the community does it, since many models are all based on the same Mistral Small model and we know which layers are for vision/speech/etc.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64lskl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753948579,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n64bjhh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Eden1506",
            "can_mod_post": false,
            "created_utc": 1753942884,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 7,
            "author_fullname": "t2_2ezqqypt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What you are looking at when going to a website like chatgpt is a complete package consisting of multiple models: One model for TTS/ One model for STT/ one model for image generation...\n\n... those are not all one model.\n\nOpenAI/Google/anthropic all use multiple models for separate tasks and you can do the same\n\nThe easiest way to accomplish it would be something like koboldcpp:\n\nyou can add an llm for text generation and image recognition \n\nyou can add flux or sdxl for image generation and image editing \n\nyou can add whisper for Speech to Text and OutTTS  for text to speech\n\nyou can add websearch via settings \n\nOnly Rag and tool calling like a python environment is missing which you would need to create a custom solution for.\n\n\nThere does not exist one model that can do all those things at a SOTA level or even close to it as of right now.\n\nEven gemini which can take text, audio, video and images as an input cannot actually output in all those formats and uses a separate model called Imagen for image generation, Veo for video...",
            "edited": 1753944282,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64bjhh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What you are looking at when going to a website like chatgpt is a complete package consisting of multiple models: One model for TTS/ One model for STT/ one model for image generation...&lt;/p&gt;\n\n&lt;p&gt;... those are not all one model.&lt;/p&gt;\n\n&lt;p&gt;OpenAI/Google/anthropic all use multiple models for separate tasks and you can do the same&lt;/p&gt;\n\n&lt;p&gt;The easiest way to accomplish it would be something like koboldcpp:&lt;/p&gt;\n\n&lt;p&gt;you can add an llm for text generation and image recognition &lt;/p&gt;\n\n&lt;p&gt;you can add flux or sdxl for image generation and image editing &lt;/p&gt;\n\n&lt;p&gt;you can add whisper for Speech to Text and OutTTS  for text to speech&lt;/p&gt;\n\n&lt;p&gt;you can add websearch via settings &lt;/p&gt;\n\n&lt;p&gt;Only Rag and tool calling like a python environment is missing which you would need to create a custom solution for.&lt;/p&gt;\n\n&lt;p&gt;There does not exist one model that can do all those things at a SOTA level or even close to it as of right now.&lt;/p&gt;\n\n&lt;p&gt;Even gemini which can take text, audio, video and images as an input cannot actually output in all those formats and uses a separate model called Imagen for image generation, Veo for video...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64bjhh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753942884,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n63wxpk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AnticitizenPrime",
                      "can_mod_post": false,
                      "created_utc": 1753935676,
                      "send_replies": true,
                      "parent_id": "t1_n63w4qo",
                      "score": 8,
                      "author_fullname": "t2_66km3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "7B model right? Kinda hoping for something a bit beefier.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63wxpk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;7B model right? Kinda hoping for something a bit beefier.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n63wxpk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753935676,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 8
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n63w4qo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Agreeable-Market-692",
            "can_mod_post": false,
            "created_utc": 1753935328,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 6,
            "author_fullname": "t2_ajuhoi00",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You don't like Qwen2.5 Omni?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63w4qo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You don&amp;#39;t like Qwen2.5 Omni?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n63w4qo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753935328,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64z4p0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Healthy-Nebula-3603",
                      "can_mod_post": false,
                      "created_utc": 1753956112,
                      "send_replies": true,
                      "parent_id": "t1_n648p1y",
                      "score": 3,
                      "author_fullname": "t2_ogjj6ebj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Have you seen the newest Wan 2.2 ?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64z4p0",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Have you seen the newest Wan 2.2 ?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64z4p0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753956112,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64bq4j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FpRhGf",
                      "can_mod_post": false,
                      "created_utc": 1753942982,
                      "send_replies": true,
                      "parent_id": "t1_n648p1y",
                      "score": 2,
                      "author_fullname": "t2_3xxhqo7c",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Take chatgpt out of the equation too",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64bq4j",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Take chatgpt out of the equation too&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64bq4j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753942982,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n648p1y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "PromptAfraid4598",
            "can_mod_post": false,
            "created_utc": 1753941388,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 3,
            "author_fullname": "t2_9mrzll0d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Instead of pitting open-source against closed-source, it's really about open-source versus Google's Veo 3. If you take Google out of the equation, open-source actually holds its own against closed-source.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n648p1y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Instead of pitting open-source against closed-source, it&amp;#39;s really about open-source versus Google&amp;#39;s Veo 3. If you take Google out of the equation, open-source actually holds its own against closed-source.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n648p1y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753941388,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n65t7qu",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "EuphoricPenguin22",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n65gwq3",
                                "score": 1,
                                "author_fullname": "t2_bzm5pqm",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I think another one just came out as well, but the model weights are smaller in terms of parameter count, so I think its text capabilities are reduced in favor of image generation.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n65t7qu",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think another one just came out as well, but the model weights are smaller in terms of parameter count, so I think its text capabilities are reduced in favor of image generation.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n65t7qu/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753968227,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753968227,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n65gwq3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "cromagnone",
                      "can_mod_post": false,
                      "created_utc": 1753964011,
                      "send_replies": true,
                      "parent_id": "t1_n642aow",
                      "score": 3,
                      "author_fullname": "t2_2lu27",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I’m surprised it took so long for someone to mention Bagel. There’s a good [quant](https://huggingface.co/DFloat11/BAGEL-7B-MoT-DF11) that runs within 24Gb VRAM.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n65gwq3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m surprised it took so long for someone to mention Bagel. There’s a good &lt;a href=\"https://huggingface.co/DFloat11/BAGEL-7B-MoT-DF11\"&gt;quant&lt;/a&gt; that runs within 24Gb VRAM.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n65gwq3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753964011,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6498tw",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "sautdepage",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n643uoh",
                                "score": 8,
                                "author_fullname": "t2_6qdom3n2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "\\&gt; a SINGLE multimodal AI you could run that can do it all\n\nUntil a significantly better text-only model releases the next month..\n\nI see it as similar to open-source vs enterprise:  on one side we have a vibrant community and top notch tools in a more spread out ecosystem, but that diversity is also how/why it strives. On the other side we have proprietary turn-key solutions selling you a convenience package as a subscription - and checking more boxes means more lock-in and therefore money.\n\nI would not be surprised things stay that way, maybe until everything stabilizes and the best of everything gets commoditized even in open-source, likely not anytime soon.\n\nThat's okay. The benefits outweight the inconvenience. You can run small things on your older home server, etc.",
                                "edited": 1753942175,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6498tw",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; a SINGLE multimodal AI you could run that can do it all&lt;/p&gt;\n\n&lt;p&gt;Until a significantly better text-only model releases the next month..&lt;/p&gt;\n\n&lt;p&gt;I see it as similar to open-source vs enterprise:  on one side we have a vibrant community and top notch tools in a more spread out ecosystem, but that diversity is also how/why it strives. On the other side we have proprietary turn-key solutions selling you a convenience package as a subscription - and checking more boxes means more lock-in and therefore money.&lt;/p&gt;\n\n&lt;p&gt;I would not be surprised things stay that way, maybe until everything stabilizes and the best of everything gets commoditized even in open-source, likely not anytime soon.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s okay. The benefits outweight the inconvenience. You can run small things on your older home server, etc.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n6498tw/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753941673,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753941673,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 8
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6638cz",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Caffdy",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n643uoh",
                                "score": 1,
                                "author_fullname": "t2_ql2vu0wz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "&gt; Meanwhile there could be a SINGLE multimodal AI you could run that can do it all. We're seeing that it's possible right now, but no open source LLM is doing it the way the closed ones are\n\nyou don't know and no one knows how closed models do it; many people bet that their multimodal capabilities are an orchestra of systems under the hood instead of a single unified model. I don't doubt we will eventually get true multimodal ones, but those \"janky\" setups as you call them are the norm in any software stack/service, there is no miracle run-it-all program or solution",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6638cz",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Meanwhile there could be a SINGLE multimodal AI you could run that can do it all. We&amp;#39;re seeing that it&amp;#39;s possible right now, but no open source LLM is doing it the way the closed ones are&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;you don&amp;#39;t know and no one knows how closed models do it; many people bet that their multimodal capabilities are an orchestra of systems under the hood instead of a single unified model. I don&amp;#39;t doubt we will eventually get true multimodal ones, but those &amp;quot;janky&amp;quot; setups as you call them are the norm in any software stack/service, there is no miracle run-it-all program or solution&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n6638cz/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753971267,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753971267,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n643uoh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AnticitizenPrime",
                      "can_mod_post": false,
                      "created_utc": 1753938932,
                      "send_replies": true,
                      "parent_id": "t1_n642aow",
                      "score": 1,
                      "author_fullname": "t2_66km3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What you're describing is doable, but holy hell, it's a lot of configuration and setup to juggle all those models, and the workflow side of things... you'd have to remember what model is the best for the input, etc. Also, your chat LLM is probably being served by llama.cpp or Ollama or whatever, while the TTS/STT models are some janky Python thing you're running, maybe Docker involved, so you have a whole workstation setup with various implementations running side by side, all fighting for RAM or other resources...\n\nMeanwhile there could be a SINGLE multimodal AI you could run that can do it all. We're seeing that it's possible right now, but no open source LLM is doing it the way the closed ones are. It's the biggest discrepancy in ability IMO.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n643uoh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What you&amp;#39;re describing is doable, but holy hell, it&amp;#39;s a lot of configuration and setup to juggle all those models, and the workflow side of things... you&amp;#39;d have to remember what model is the best for the input, etc. Also, your chat LLM is probably being served by llama.cpp or Ollama or whatever, while the TTS/STT models are some janky Python thing you&amp;#39;re running, maybe Docker involved, so you have a whole workstation setup with various implementations running side by side, all fighting for RAM or other resources...&lt;/p&gt;\n\n&lt;p&gt;Meanwhile there could be a SINGLE multimodal AI you could run that can do it all. We&amp;#39;re seeing that it&amp;#39;s possible right now, but no open source LLM is doing it the way the closed ones are. It&amp;#39;s the biggest discrepancy in ability IMO.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n643uoh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753938932,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n642aow",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "EuphoricPenguin22",
            "can_mod_post": false,
            "created_utc": 1753938164,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 5,
            "author_fullname": "t2_bzm5pqm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think we have a pretty robust ecosystem of domain-specific models at this point that some gaps in multi-modal capabilities aren't as big of a deal. It honestly depends on what your use case is. I think the main benefit of multi-modal models with autoregressive image generation is creating things like sequential comic panels with consistent characters. [Bagel](https://github.com/ByteDance-Seed/Bagel) is probably the best open-weight example of that we have right now, but even it is a bit rough around the edges. In terms of multi-modal models for audio or domain-specific vision stuff, I don't really see the advantage over what you can achieve with dedicated models. [Chatterbox TTS](https://huggingface.co/ResembleAI/chatterbox) is fairly lightweight and really easy to customize with zero-shot voice cloning, plus it sounds quite good. The [Whisper](https://github.com/openai/whisper) model family is still the most popular option for STT, from what I gather, and they tend to perform quite well across the different model sizes. [PTA-1](https://huggingface.co/AskUI/PTA-1) crams a whole visual tagging model for text-prompted UI elements into a single gigabyte. I think we have a pretty good smattering of domain-specific models for most use cases if you're willing to piece a solution together. That isn't to say a solid all-in-one model wouldn't be awesome, but it definitely isn't a must-have.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n642aow",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think we have a pretty robust ecosystem of domain-specific models at this point that some gaps in multi-modal capabilities aren&amp;#39;t as big of a deal. It honestly depends on what your use case is. I think the main benefit of multi-modal models with autoregressive image generation is creating things like sequential comic panels with consistent characters. &lt;a href=\"https://github.com/ByteDance-Seed/Bagel\"&gt;Bagel&lt;/a&gt; is probably the best open-weight example of that we have right now, but even it is a bit rough around the edges. In terms of multi-modal models for audio or domain-specific vision stuff, I don&amp;#39;t really see the advantage over what you can achieve with dedicated models. &lt;a href=\"https://huggingface.co/ResembleAI/chatterbox\"&gt;Chatterbox TTS&lt;/a&gt; is fairly lightweight and really easy to customize with zero-shot voice cloning, plus it sounds quite good. The &lt;a href=\"https://github.com/openai/whisper\"&gt;Whisper&lt;/a&gt; model family is still the most popular option for STT, from what I gather, and they tend to perform quite well across the different model sizes. &lt;a href=\"https://huggingface.co/AskUI/PTA-1\"&gt;PTA-1&lt;/a&gt; crams a whole visual tagging model for text-prompted UI elements into a single gigabyte. I think we have a pretty good smattering of domain-specific models for most use cases if you&amp;#39;re willing to piece a solution together. That isn&amp;#39;t to say a solid all-in-one model wouldn&amp;#39;t be awesome, but it definitely isn&amp;#39;t a must-have.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n642aow/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753938164,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n648kum",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Scam_Altman",
            "can_mod_post": false,
            "created_utc": 1753941327,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 2,
            "author_fullname": "t2_1a0oiggi8d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I like this one, I'm trying to do a multimodal Gemini distillation with it:\n\nhttps://github.com/OpenBMB/MiniCPM-o",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n648kum",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I like this one, I&amp;#39;m trying to do a multimodal Gemini distillation with it:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/OpenBMB/MiniCPM-o\"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n648kum/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753941327,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n65h6mq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "HypnoDaddy4You",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n658vyf",
                                "score": 1,
                                "author_fullname": "t2_lb2n7mbsw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I have no doubt that fine tuning has been attempted by people with far more resources than I. The latent space of audio and text looks fundamentally different due to things like intonation, cadence, expression, etc",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n65h6mq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have no doubt that fine tuning has been attempted by people with far more resources than I. The latent space of audio and text looks fundamentally different due to things like intonation, cadence, expression, etc&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n65h6mq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753964113,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753964113,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n658vyf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CheatCodesOfLife",
                      "can_mod_post": false,
                      "created_utc": 1753960806,
                      "send_replies": true,
                      "parent_id": "t1_n6577dy",
                      "score": 1,
                      "author_fullname": "t2_32el727b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I reckon we can get there. We've got the mistral model that new can do audio -&gt; text response directly (and it's pretty good). Could probably finetune it to spit out descrete audio tokens instead of text. Then we just need something like snac-24k in front of it, and it's *almost* a full voice -&gt; voice chat model.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n658vyf",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I reckon we can get there. We&amp;#39;ve got the mistral model that new can do audio -&amp;gt; text response directly (and it&amp;#39;s pretty good). Could probably finetune it to spit out descrete audio tokens instead of text. Then we just need something like snac-24k in front of it, and it&amp;#39;s &lt;em&gt;almost&lt;/em&gt; a full voice -&amp;gt; voice chat model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n658vyf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753960806,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6577dy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HypnoDaddy4You",
            "can_mod_post": false,
            "created_utc": 1753960061,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 2,
            "author_fullname": "t2_lb2n7mbsw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Quick point of clarity: multimodal models can understand images. They might be able to understand audio, idk.\n\nBut they don't generate either. OpenAI and Gemini have additional models based on a quite different prediction strategy to generate pictures, and another type of model to convert text tokens to speech.\n\nThey just hide all that behind the one api call, making it look seamless.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6577dy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Quick point of clarity: multimodal models can understand images. They might be able to understand audio, idk.&lt;/p&gt;\n\n&lt;p&gt;But they don&amp;#39;t generate either. OpenAI and Gemini have additional models based on a quite different prediction strategy to generate pictures, and another type of model to convert text tokens to speech.&lt;/p&gt;\n\n&lt;p&gt;They just hide all that behind the one api call, making it look seamless.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n6577dy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753960061,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n64cqhz",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Sartorianby",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n63y2ep",
                                "score": 1,
                                "author_fullname": "t2_188uuttv7v",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I played around with MiMo 7B, it was alright for img-text. There's also Qwen 2.5 Omni. So we know they're developing them.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n64cqhz",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I played around with MiMo 7B, it was alright for img-text. There&amp;#39;s also Qwen 2.5 Omni. So we know they&amp;#39;re developing them.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdruc9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64cqhz/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753943530,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753943530,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n63y2ep",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AnticitizenPrime",
                      "can_mod_post": false,
                      "created_utc": 1753936186,
                      "send_replies": true,
                      "parent_id": "t1_n63wlr0",
                      "score": 3,
                      "author_fullname": "t2_66km3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "China is pumping out some tremendous open source models that are nipping at the heels of the closed source ones - not sure if cost is the issue (though maybe it is). To me it feels like a difference in end goals or methodology. It feels like none of the open source creators are working on native multimodality. \n\nI am aware that there are speech models, image models, etc being released all the time; what I'm taking about here are models with native multimodality baked into an LLM. I can show GPT, Claude, and Gemini pictures or audio and they natively 'get it', which is not something you can say for the latest and greatest Deepseek, Qwen, GLM, etc, even if they are very good on benchmarks and intelligence.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63y2ep",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;China is pumping out some tremendous open source models that are nipping at the heels of the closed source ones - not sure if cost is the issue (though maybe it is). To me it feels like a difference in end goals or methodology. It feels like none of the open source creators are working on native multimodality. &lt;/p&gt;\n\n&lt;p&gt;I am aware that there are speech models, image models, etc being released all the time; what I&amp;#39;m taking about here are models with native multimodality baked into an LLM. I can show GPT, Claude, and Gemini pictures or audio and they natively &amp;#39;get it&amp;#39;, which is not something you can say for the latest and greatest Deepseek, Qwen, GLM, etc, even if they are very good on benchmarks and intelligence.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n63y2ep/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753936186,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n63wlr0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Sartorianby",
            "can_mod_post": false,
            "created_utc": 1753935533,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 2,
            "author_fullname": "t2_188uuttv7v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The short answer is money. The longer answer is it takes a lot more computing power, more specific kinds of data, and different architecture than pure text models. So they're behind in the same way as how open sourced used to be far behind closed models but we'll get there.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63wlr0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The short answer is money. The longer answer is it takes a lot more computing power, more specific kinds of data, and different architecture than pure text models. So they&amp;#39;re behind in the same way as how open sourced used to be far behind closed models but we&amp;#39;ll get there.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n63wlr0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753935533,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n64gblg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Irisi11111",
            "can_mod_post": false,
            "created_utc": 1753945486,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_27amtukh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Visual tokens are pretty different from text tokens. Next token prediction is great with LLMs, but there's a noticeable gap when we talk about visual tokens. To fill that gap, you really need tons of data. Open source models can use synthetic data to tackle shortages in text tokens, but options for visual tokens are limited.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64gblg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Visual tokens are pretty different from text tokens. Next token prediction is great with LLMs, but there&amp;#39;s a noticeable gap when we talk about visual tokens. To fill that gap, you really need tons of data. Open source models can use synthetic data to tackle shortages in text tokens, but options for visual tokens are limited.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64gblg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753945486,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n65cs07",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jstanaway",
            "can_mod_post": false,
            "created_utc": 1753962437,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_16mw7v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Would love if this was the case also. I’m using Gemini to get structured data out of pdf files for an application I built. Works very well but it would be nice to have something like this in open source also. \n\nHow’s the structured output situation with open source models ? \n\nI mean the real structured output where I can provide a defined schema and it returns exactly that data and I don’t have to parse etc? ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n65cs07",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Would love if this was the case also. I’m using Gemini to get structured data out of pdf files for an application I built. Works very well but it would be nice to have something like this in open source also. &lt;/p&gt;\n\n&lt;p&gt;How’s the structured output situation with open source models ? &lt;/p&gt;\n\n&lt;p&gt;I mean the real structured output where I can provide a defined schema and it returns exactly that data and I don’t have to parse etc? &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n65cs07/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753962437,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n65csl4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1753962444,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Pixtral-large is the biggest example of a VLM with strong conversational skills like the cloud models. When I throw those images in there, they sure do take up a lot of context. If it did videos.. oh my.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n65csl4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pixtral-large is the biggest example of a VLM with strong conversational skills like the cloud models. When I throw those images in there, they sure do take up a lot of context. If it did videos.. oh my.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n65csl4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753962444,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n65fs0b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullOf_Bad_Ideas",
            "can_mod_post": false,
            "created_utc": 1753963591,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_9s7pmakgx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's not. Huggingface daily papers is mostly multimodal. Internvl3 78B and InternVL S1 are close to SOTA on images. There are many video understanding models too that are open source. There's a ton of open source multimodal research, you just need to read those papers and use them, they're not heavily discussed here or on Twitter but it's there. https://huggingface.co/papers/",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n65fs0b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not. Huggingface daily papers is mostly multimodal. Internvl3 78B and InternVL S1 are close to SOTA on images. There are many video understanding models too that are open source. There&amp;#39;s a ton of open source multimodal research, you just need to read those papers and use them, they&amp;#39;re not heavily discussed here or on Twitter but it&amp;#39;s there. &lt;a href=\"https://huggingface.co/papers/\"&gt;https://huggingface.co/papers/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n65fs0b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753963591,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n661dxv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Conversation9561",
            "can_mod_post": false,
            "created_utc": 1753970723,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_jqxb4pte",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ernie-VL looked promising but didn’t get support",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n661dxv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ernie-VL looked promising but didn’t get support&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n661dxv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753970723,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n661ijp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HilLiedTroopsDied",
            "can_mod_post": false,
            "created_utc": 1753970762,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_1snfn3ui",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How do we use multimodals in our workflows? Everyone uses different tools and has different goals; coding, story telling, chatting, agentic scripts, etc.\nWhat I think is lacking in the tools that we use is an easy handoff for configurable use model x to process image to txt &gt; handoff to Strong text gen LLM for user request etc. That seamless handoff for multiple models is what I hardly see in modern tools. It's the easiest, systems engineering solution to chaining multiple models together to get the best benefits from everything.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n661ijp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do we use multimodals in our workflows? Everyone uses different tools and has different goals; coding, story telling, chatting, agentic scripts, etc.\nWhat I think is lacking in the tools that we use is an easy handoff for configurable use model x to process image to txt &amp;gt; handoff to Strong text gen LLM for user request etc. That seamless handoff for multiple models is what I hardly see in modern tools. It&amp;#39;s the easiest, systems engineering solution to chaining multiple models together to get the best benefits from everything.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n661ijp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753970762,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n66bn72",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Sherbert3326",
            "can_mod_post": false,
            "created_utc": 1753973676,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_77r05pym",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Gemma 3 is pretty good in my opinion",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n66bn72",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gemma 3 is pretty good in my opinion&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n66bn72/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753973676,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n66d58j",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GabryIta",
            "can_mod_post": false,
            "created_utc": 1753974095,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_pv1nb9469",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just a few hours after this post, Command A and Step 3 were released 😳",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n66d58j",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just a few hours after this post, Command A and Step 3 were released 😳&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n66d58j/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753974095,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n66kfhc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ShinyAnkleBalls",
            "can_mod_post": false,
            "created_utc": 1753976138,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 2,
            "author_fullname": "t2_2m3au2xb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Because money?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n66kfhc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because money?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n66kfhc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753976138,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64z0l5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Healthy-Nebula-3603",
                      "can_mod_post": false,
                      "created_utc": 1753956049,
                      "send_replies": true,
                      "parent_id": "t1_n648kky",
                      "score": 1,
                      "author_fullname": "t2_ogjj6ebj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "We have a lot of multimodal open source models ...",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64z0l5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We have a lot of multimodal open source models ...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdruc9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64z0l5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753956049,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n648kky",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lightninghyped",
            "can_mod_post": false,
            "created_utc": 1753941323,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 1,
            "author_fullname": "t2_ehng03r1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It is too heavy, both data itself and tokenized ones.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n648kky",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is too heavy, both data itself and tokenized ones.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n648kky/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753941323,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n653ao0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "msp26",
            "can_mod_post": false,
            "created_utc": 1753958243,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": 0,
            "author_fullname": "t2_cr96i",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Also the lack of universal pan and scan support is a pain in the ass. If you input a large image it just gets scaled down rather than being split into tiles.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n653ao0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also the lack of universal pan and scan support is a pain in the ass. If you input a large image it just gets scaled down rather than being split into tiles.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n653ao0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753958243,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n64sg6r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Maleficent_Age1577",
            "can_mod_post": false,
            "created_utc": 1753952372,
            "send_replies": true,
            "parent_id": "t3_1mdruc9",
            "score": -1,
            "author_fullname": "t2_gxl5vlowd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "because there is a huge cap between 20gb model 10 000gb model. what would the purpose of multimodal small model which would say to most of the pictures you present that: I have no fucking idea what is in the picture?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64sg6r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;because there is a huge cap between 20gb model 10 000gb model. what would the purpose of multimodal small model which would say to most of the pictures you present that: I have no fucking idea what is in the picture?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/n64sg6r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753952372,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdruc9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        }
      ],
      "before": null
    }
  }
]