[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi everyone,\n\nI'm beginning my journey into working with LLMs, RAG pipelines, and local inference — and I’m facing a real-world challenge right off the bat.\n\nI have a large corpus of documents (thousands of them), mostly in PDF format, some exceeding 10,000 pages each. All files have already gone through OCR, so the text is extractable. The goal is to run qualitative analysis and extract specific information entities (e.g., names, dates, events, relationships, modus operandi) from these documents. Due to the sensitive nature of the data, everything must be processed fully offline, with no external API calls.\n\nHere’s my local setup:\n\nCPU: Intel i7-13700\n\nRAM: 128 GB DDR5\n\nGPU: RTX 4080 (16 GB VRAM)\n\nStorage: 2 TB SSD\n\nOS: Windows 11\n\nInstalled tools: Ollama, Python, and basic NLP libraries (spaCy, PyMuPDF, LangChain, etc.)\n\n\nWhat I’m looking for:\n\nBest practices for chunking extremely long PDFs for RAG-type pipelines\n\nLocal embedding + retrieval strategies (ChromaDB? FAISS?)\n\nRecommendations on which models (via Ollama or other means) can handle long-context reasoning locally (e.g., LLaMA 3 8B, Mistral, Phi-3, etc.)\n\nWhether I should pre-index and classify content into topics/entities beforehand, or rely on the LLM’s capabilities at runtime\n\nIdeas for combining structured outputs (e.g., JSON schemas) from unstructured data chunks\n\n\nAny workflows, architecture tips, or open-source projects/examples to look at would be incredibly appreciated. \n\nThanks a lot!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Running Local RAG on Thousands of OCR’d PDFs — Need Advice for Efficient Long-Doc Processing",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mejq45",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_edq2cb7d",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754012354,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m beginning my journey into working with LLMs, RAG pipelines, and local inference — and I’m facing a real-world challenge right off the bat.&lt;/p&gt;\n\n&lt;p&gt;I have a large corpus of documents (thousands of them), mostly in PDF format, some exceeding 10,000 pages each. All files have already gone through OCR, so the text is extractable. The goal is to run qualitative analysis and extract specific information entities (e.g., names, dates, events, relationships, modus operandi) from these documents. Due to the sensitive nature of the data, everything must be processed fully offline, with no external API calls.&lt;/p&gt;\n\n&lt;p&gt;Here’s my local setup:&lt;/p&gt;\n\n&lt;p&gt;CPU: Intel i7-13700&lt;/p&gt;\n\n&lt;p&gt;RAM: 128 GB DDR5&lt;/p&gt;\n\n&lt;p&gt;GPU: RTX 4080 (16 GB VRAM)&lt;/p&gt;\n\n&lt;p&gt;Storage: 2 TB SSD&lt;/p&gt;\n\n&lt;p&gt;OS: Windows 11&lt;/p&gt;\n\n&lt;p&gt;Installed tools: Ollama, Python, and basic NLP libraries (spaCy, PyMuPDF, LangChain, etc.)&lt;/p&gt;\n\n&lt;p&gt;What I’m looking for:&lt;/p&gt;\n\n&lt;p&gt;Best practices for chunking extremely long PDFs for RAG-type pipelines&lt;/p&gt;\n\n&lt;p&gt;Local embedding + retrieval strategies (ChromaDB? FAISS?)&lt;/p&gt;\n\n&lt;p&gt;Recommendations on which models (via Ollama or other means) can handle long-context reasoning locally (e.g., LLaMA 3 8B, Mistral, Phi-3, etc.)&lt;/p&gt;\n\n&lt;p&gt;Whether I should pre-index and classify content into topics/entities beforehand, or rely on the LLM’s capabilities at runtime&lt;/p&gt;\n\n&lt;p&gt;Ideas for combining structured outputs (e.g., JSON schemas) from unstructured data chunks&lt;/p&gt;\n\n&lt;p&gt;Any workflows, architecture tips, or open-source projects/examples to look at would be incredibly appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mejq45",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "NaturalInitial1025",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mejq45/running_local_rag_on_thousands_of_ocrd_pdfs_need/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mejq45/running_local_rag_on_thousands_of_ocrd_pdfs_need/",
            "subreddit_subscribers": 508191,
            "created_utc": 1754012354,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6a79n2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "solidsnakeblue",
            "can_mod_post": false,
            "created_utc": 1754016347,
            "send_replies": true,
            "parent_id": "t3_1mejq45",
            "score": 2,
            "author_fullname": "t2_7zh6fslk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I found this the other day, it may be what you are looking for:\n\n[https://github.com/google/langextract/](https://github.com/google/langextract/)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6a79n2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I found this the other day, it may be what you are looking for:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/google/langextract/\"&gt;https://github.com/google/langextract/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mejq45/running_local_rag_on_thousands_of_ocrd_pdfs_need/n6a79n2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754016347,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mejq45",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6a2mhp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "decentralizedbee",
            "can_mod_post": false,
            "created_utc": 1754014605,
            "send_replies": true,
            "parent_id": "t3_1mejq45",
            "score": 1,
            "author_fullname": "t2_hjqo50xu2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "we just built something very similar. happy to send you the process document/tools we used. Feel free to DM",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6a2mhp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;we just built something very similar. happy to send you the process document/tools we used. Feel free to DM&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mejq45/running_local_rag_on_thousands_of_ocrd_pdfs_need/n6a2mhp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754014605,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mejq45",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6c3vrc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754050016,
            "send_replies": true,
            "parent_id": "t3_1mejq45",
            "score": 1,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen3 235B basically built my pipeline. I can’t share, but can recommend an approach:\n\n1. Carefully and exactingly draft a prompt explaining the problem and the expected solution.\n\n2. Ask a SOTA model to re-draft your prompt to improve it for use by the final offline model. You’ll be amazed at the difference.\n\n3. Have your offline model do the vibe coding work of building the pipeline. If you have Claude then all the better, just have it build the pipeline and then run it offline. The cloud need never see your data, only your code.\n\n4. Rinse/repeat until it’s good.\n\n5. Good luck!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c3vrc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 235B basically built my pipeline. I can’t share, but can recommend an approach:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Carefully and exactingly draft a prompt explaining the problem and the expected solution.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ask a SOTA model to re-draft your prompt to improve it for use by the final offline model. You’ll be amazed at the difference.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have your offline model do the vibe coding work of building the pipeline. If you have Claude then all the better, just have it build the pipeline and then run it offline. The cloud need never see your data, only your code.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Rinse/repeat until it’s good.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Good luck!&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mejq45/running_local_rag_on_thousands_of_ocrd_pdfs_need/n6c3vrc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754050016,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mejq45",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]