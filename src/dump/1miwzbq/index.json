[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I use a few specific prompts that I like to use to test a model's general world knowledge, its ability to identify the most relevant pieces of information to give, etc. One of those is \"tell me about the Apple A6\". The model \"passes\" if it mentions the key relevant factor that makes the A6 a historical processor (it was the first Apple SOC that used custom designed \"swift\" cores rather than off-the-shelf ARM cores). More powerful models will almost always mention this. Weaker models will just regurgitate a bunch of facts with no deeper understanding of what a user might actually want to know first. \n\nThe idea of these kinds of prompts is to test the model's ability to understand what's most relevant and to prioritize significant information first. (Another good one is \"Tell me more about the Pentium D. Why was it a bad processor?\" It should specifically emphasize the real reason, which was that it two Pentium 4 dies slapped onto one chip with no interconnect. Or \"Explain how a coral reef ecosystem is similar to a startup company in its early years, using at least three points of analogy\" - it should come up with analogies that are equally good across both domains without using terminology that is specific only to one.) \n\nAnyway, I'm running some of these prompts as benchmarks on the new OpenAI-OSS models (specifically the 20b one). I've noticed that LM studio allows you to arbitrarily increase the number of experts, so I gave it a shot and increased it from 4 active experts to 16 (out of 32). \n\n**GPT-OSS on 16 experts (a word salad response):** \n\n&gt;*&lt;|channel|&gt;analysis&lt;|message|&gt;We need to respond to user asking \"Tell me about the Apple A6\". The user likely refers to Apple A6? Could be a mis-typo or ambiguous. Possibly Apple A6 is an Apple product, like \"Apple A6\" could refer to Apple A6 chip, maybe \"Apple A6\" is \"Apple A6\" as part of Apple's hardware. Actually Apple has various chips: Apple A1, A2, A3, etc. But Apple A6 might be a specific chip that existed? There was no A6 in the known series (A4, A5, A7). There are some older chips like Apple A8 and A9. The Apple A6 is maybe an \"Apple A6\" refers to some code for a product like \"Apple A6\" not sure.*\n\n&gt;*Could also refer to Apple's \"Apple A6\" as the name of a \"Apple A6\" device? Possibly \"Apple A6\" refers to \"Apple A6\" like the Apple \"A6\" used in \"Apple A6\" iPhone? There's no such product. It's likely an error or mis-typo.*\n\n&gt;*We need to explain what Apple A6 is, maybe mention that there is no official Apple A6 chip or device. But we could mention that Apple's chip series includes A4, A5, etc. But there's a \"Apple A6\" used in the iPhone 3? Actually I think it's a mis-typo for \"Apple A6\" perhaps they want to talk about the \"Apple A6\" as a brand or product. Could also be \"Apple A6\" refers to \"Apple A6\" as a \"iPod A6\"? But no.*\n\n&gt;*Alternatively, maybe the user wants information about Apple A6 being part of Apple's hardware. There might be a \"Apple A6\" as \"Apple 8.4\" or \"Apple A6\"? Let's think: The Apple chip series: A1, A2 (S3), A3 (iPhone 3?), A4 (iPhone 4), A5 (iPhone 5), A7 (iPhone 6), etc. There is no A6.*\n\n&gt;*But maybe the user wants \"Apple A6\" as a brand product? Maybe it's like \"Apple A6\" used in the iPhone 2? No.*\n\n&gt;*Could be that there's \"Apple A6\" refers to \"Apple A6\" as \"Apple A6\" used in Apple \"Apple A6\" something. Or maybe we could explain that there is no known Apple A6, but we can mention the series.*\n\n&gt;*Alternatively, user might ask about \"Apple A6\" referring to a new product like \"Apple A6\", but it's not known. So we can say: The Apple A6 chip has never existed; Apple used a series of chips starting from A4 etc.*\n\n&gt;*We could also mention that Apple's internal code naming might have A6 as part of hardware, but no publicly released product.*\n\n&gt;*But maybe the user is referencing \"Apple A6\" as \"Apple A6\" which refers to \"A6\" as a model of something like Apple A6? There's no known product. So we can respond: \"I don't know about Apple A6; there is no official Apple A6 chip or device.\".*\n\n&gt;*We could also mention that the Apple A6 might be an internal code for a certain hardware component, but it's not publicly documented. Or maybe it's a mis-typo for \"Apple A6\" which refers to \"A6\" as \"iPhone 4\"? Actually iPhone 4 uses A4; iPhone 5 uses A5; iPhone 6 uses A7.*\n\n&gt;*So we can answer: There is no Apple A6. The Apple chip series includes A4, A5, etc. Maybe the user might be mistaken. Alternatively we could mention that \"Apple A6\" refers to an internal code for a certain hardware component but not publicly known.*\n\n&gt;*We also might provide some background about Apple's chip series and timeline. But we might mention that there is no such product. So answer accordingly: There's no Apple A6; maybe you meant A4 or A5 etc.*\n\n&gt;*Alternatively, \"Apple A6\" could be an \"Apple 6\" like \"Apple iPhone 6\"? Not exactly.*\n\n&gt;*Thus our response: We need to clarify that no official product called \"Apple A6\" exists. Provide context of Apple chip series.*\n\n&gt;*We can also mention that the Apple A6 might have been a proprietary internal code for a certain component, but not publicly announced. So we can answer accordingly.&lt;|end|&gt;*\n\n&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;There isn’t any public‑released Apple device or processor officially named “Apple A6.”\n\n&gt;Apple’s naming convention for its silicon chips has followed a series of “A” numbers starting with the first chip (A1), and then A2, A3, A4, A5, etc. For example:\n\n&gt;\\* \\*\\*A4\\*\\* – used in the iPhone 4 and iPad 1\n\n&gt;\\* \\*\\*A5\\*\\* – used in the iPhone 5, iPad 2\n\n&gt;\\* \\*\\*A7\\*\\* – used in the iPhone 6 and iPhone 6 Plus\n\n&gt;\\* \\*\\*A8\\*\\* – used in the iPhone 7 and iPhone 7 Plus\n\n&gt;\\* \\*\\*A9\\*\\* – used in the iPhone 8, iPhone 8 Plus, etc.\n\n&gt;There was no “Apple A6” in this sequence. If you’re referring to a specific product or component that might have an internal code of “A6,” it would likely be proprietary and not publicly documented.\n\n&gt;If you were looking for information about the Apple A4, A5, or any other chip, let me know which one you’re interested in!\n\n**For comparison, this is the response generated with 4 experts (the default configuration):**\n\n&gt;*&lt;|channel|&gt;analysis&lt;|message|&gt;We need to provide a response about the Apple A6 processor. The user just asks \"Tell me about the Apple A6.\" We can give an overview: introduced in 2012, used in iPhone 5 and iPad Mini 1st gen, features, architecture (ARMv7), dual core, performance improvements over A5, etc.*\n\n&gt;*We should be mindful of policy: no disallowed content. It's fine. Provide technical details, release info, specs. We can mention clock speed, CPU cores, GPU, process node, power consumption, performance benchmarks, manufacturing details. Also mention architecture: 32-bit ARMv7, dual-core Apple custom, etc.*\n\n&gt;*We should also maybe note that the A6 was built on 28nm process. It had a \"dual-core\" CPU with each core being a \"custom\" designed by Apple. The GPU is PowerVR SGX543MP2. It introduced new power management, improved performance per watt, supports 3G LTE (in iPhone 5). Also mention that the A6 was replaced by A7 in 2013.*\n\n&gt;*We could also include usage: used in iPhone 5, iPad mini (1st gen), and some iPod touch? Actually the iPod touch 4th generation had A8. So just iPhone 5 and iPad mini.*\n\n&gt;*Also mention that the chip is 32-bit only.*\n\n&gt;*Add maybe some performance numbers: around 2-3x faster than A5, etc. Provide memory architecture, TTI, L1 caches. Also talk about the 'Apple custom\" CPU design, but no need to get too deep.*\n\n&gt;*Should also note that Apple didn't provide public specs; many details come from leaked or reverse engineering.*\n\n&gt;*Ok let's produce a concise yet thorough answer.*\n\n&gt;*We can include bullet points for clarity: release date, devices, architecture, performance, process node, GPU, etc. Also mention differences vs A5 and A7.*\n\n&gt;*We should keep it respectful to Apple, no disallowed content.*\n\n&gt;We'll comply with policy.&lt;|end|&gt;\n\n&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;## The Apple A6 – Quick Overview\n\n&gt;| Feature | Details |\n\n&gt;|---------|---------|\n\n&gt;| \\*\\*Launch\\*\\* | 2012 (iPhone 5 &amp; iPad Mini) |\n\n&gt;| \\*\\*Process Node\\*\\* | 28 nm (Apple custom fabrication) |\n\n&gt;| \\*\\*CPU Architecture\\*\\* | Dual‑core ARMv7‑based Apple custom CPU (32‑bit) |\n\n&gt;| \\*\\*GPU\\*\\* | PowerVR SGX543MP2, dual‑core (6‑shader cores total) |\n\n&gt;| \\*\\*Clock Speed\\*\\* | \\~1.3 GHz per core (actual frequency can vary by task) |\n\n&gt;| \\*\\*Performance Gains vs. A5\\*\\* | Roughly 2–3× faster in CPU benchmarks; \\~1.5–2× better GPU performance |\n\n&gt;| \\*\\*Power Efficiency\\*\\* | 20‑30% lower power draw at comparable workloads compared to A5 |\n\n&gt;| \\*\\*Key Innovations\\*\\* | • First Apple‑designed dual‑core CPU in an iPhone&lt;br&gt;• Integrated LTE modem support (iPhone 5)&lt;br&gt;• Advanced power‑management unit for lower idle consumption |\n\n&gt;\\---\n\n&gt;\\## In a Nutshell\n\n&gt;The Apple A6 was the first major step beyond the A5, marking Apple’s move toward a \\*\\*custom silicon\\*\\* strategy. While still based on the ARMv7 instruction set (32‑bit only), Apple designed its own CPU cores to squeeze more performance per watt.\n\n&gt;\\### CPU\n\n&gt;\\- Two identical cores share a unified 1 MB L2 cache and separate 64 KB L1 instruction/ data caches.\n\n&gt;\\- The core design was optimized for high clock speeds while keeping thermal output low, which allowed the iPhone 5 to run cooler and with fewer battery drain issues compared to its predecessor.\n\n&gt;\\### GPU\n\n&gt;\\- PowerVR SGX543MP2 (dual‑core) brought a 6‑shader architecture that handled both 3D rendering and 2D compositing more efficiently.\n\n&gt;\\- This GPU was enough for smooth gaming performance on the iPhone 5 but would later be superseded by Apple’s own GPU in the A7/A8.\n\n&gt;\\### Other Highlights\n\n&gt;\\- \\*\\*LTE Modem Integration\\*\\*: The A6 included an LTE baseband (for iPhone 5), a first for Apple silicon.\n\n&gt;\\- \\*\\*Memory &amp; I/O\\*\\*: 512 MB LPDDR2 RAM (iPad Mini) or 1 GB DDR3L (iPhone 5); integrated PCIe, USB, and display controllers.\n\n&gt;\\- \\*\\*Manufacturing\\*\\*: Built on a 28 nm process by Samsung; Apple began moving to smaller nodes with the A7.\n\n&gt;\\---\n\n&gt;\\## Devices Powered by the A6\n\n&gt;| Device | Release Year | Notes |\n\n&gt;|--------|--------------|-------|\n\n&gt;| iPhone 5 | 2012 | First iPhone to use the A6, introducing LTE and a new design. |\n\n&gt;| iPad Mini (1st Gen) | 2012 | Compact tablet with 256‑ or 512‑GB storage variants; shared the same SoC as the iPhone 5. |\n\n&gt;\\*(The A6 was not used in any later devices; it was replaced by the Apple A7 in 2013.)\\*\n\n&gt;\\---\n\n&gt;\\## Legacy\n\n&gt;\\- The A6 set a precedent for \\*\\*Apple’s custom silicon\\*\\* philosophy, paving the way for future designs that integrated CPUs, GPUs, and baseband modems into single SoCs.\n\n&gt;\\- It proved that Apple could squeeze significant performance gains while improving power efficiency—a dual focus that remains central to all subsequent iPhone processors.\n\n&gt;\\---\n\n&gt;\\### Bottom Line\n\n&gt;The Apple A6 was a milestone in 2012: a dual‑core ARMv7 CPU with an integrated LTE modem, delivering double the speed of its predecessor on a modest 28 nm process. It powered the first iPhone 5 and the original iPad Mini, marking the start of Apple’s deeper dive into custom silicon that would define its mobile strategy for years to come.\n\nWe get a strong response from the default 4-expert configuration. It's not perfect, GPT-OSS gets some details wrong (such as the number of GPU cores, and certain other specs), but the response is otherwise coherent. And it does correctly mention that it is the first SOC to use Apple-designed custom cores, which is the primary thing we're trying to test in this benchmark(the model's ability of the model to generate relevant and substantial prompts that aren't just surface level regurgitations of basic facts). \n\nUsing 16 experts, on the other hand, pretty much neuters the model and renders it useless, resulting in the worst response I have ever seen from any model I have ever tried. \n\nI plan on doing more experimentation. It's a somewhat unique MoE configuration that OpenAI has given us on the 20b MoE model, with only 3.6B active parameters on each token (including routing and embedding tokens). It's a remarkably lean model, but so far, it's performing quite strongly if you leave it on the default settings. I want to try to compare to Gemma3 as well and see how it stacks up against the 12b and 27b models. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Don't try to increase the number of active experts on GPT-OSS",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1miwzbq",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.73,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_f4ibdsc9",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754461009,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754460729,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use a few specific prompts that I like to use to test a model&amp;#39;s general world knowledge, its ability to identify the most relevant pieces of information to give, etc. One of those is &amp;quot;tell me about the Apple A6&amp;quot;. The model &amp;quot;passes&amp;quot; if it mentions the key relevant factor that makes the A6 a historical processor (it was the first Apple SOC that used custom designed &amp;quot;swift&amp;quot; cores rather than off-the-shelf ARM cores). More powerful models will almost always mention this. Weaker models will just regurgitate a bunch of facts with no deeper understanding of what a user might actually want to know first. &lt;/p&gt;\n\n&lt;p&gt;The idea of these kinds of prompts is to test the model&amp;#39;s ability to understand what&amp;#39;s most relevant and to prioritize significant information first. (Another good one is &amp;quot;Tell me more about the Pentium D. Why was it a bad processor?&amp;quot; It should specifically emphasize the real reason, which was that it two Pentium 4 dies slapped onto one chip with no interconnect. Or &amp;quot;Explain how a coral reef ecosystem is similar to a startup company in its early years, using at least three points of analogy&amp;quot; - it should come up with analogies that are equally good across both domains without using terminology that is specific only to one.) &lt;/p&gt;\n\n&lt;p&gt;Anyway, I&amp;#39;m running some of these prompts as benchmarks on the new OpenAI-OSS models (specifically the 20b one). I&amp;#39;ve noticed that LM studio allows you to arbitrarily increase the number of experts, so I gave it a shot and increased it from 4 active experts to 16 (out of 32). &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPT-OSS on 16 experts (a word salad response):&lt;/strong&gt; &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;&amp;lt;|channel|&amp;gt;analysis&amp;lt;|message|&amp;gt;We need to respond to user asking &amp;quot;Tell me about the Apple A6&amp;quot;. The user likely refers to Apple A6? Could be a mis-typo or ambiguous. Possibly Apple A6 is an Apple product, like &amp;quot;Apple A6&amp;quot; could refer to Apple A6 chip, maybe &amp;quot;Apple A6&amp;quot; is &amp;quot;Apple A6&amp;quot; as part of Apple&amp;#39;s hardware. Actually Apple has various chips: Apple A1, A2, A3, etc. But Apple A6 might be a specific chip that existed? There was no A6 in the known series (A4, A5, A7). There are some older chips like Apple A8 and A9. The Apple A6 is maybe an &amp;quot;Apple A6&amp;quot; refers to some code for a product like &amp;quot;Apple A6&amp;quot; not sure.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Could also refer to Apple&amp;#39;s &amp;quot;Apple A6&amp;quot; as the name of a &amp;quot;Apple A6&amp;quot; device? Possibly &amp;quot;Apple A6&amp;quot; refers to &amp;quot;Apple A6&amp;quot; like the Apple &amp;quot;A6&amp;quot; used in &amp;quot;Apple A6&amp;quot; iPhone? There&amp;#39;s no such product. It&amp;#39;s likely an error or mis-typo.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We need to explain what Apple A6 is, maybe mention that there is no official Apple A6 chip or device. But we could mention that Apple&amp;#39;s chip series includes A4, A5, etc. But there&amp;#39;s a &amp;quot;Apple A6&amp;quot; used in the iPhone 3? Actually I think it&amp;#39;s a mis-typo for &amp;quot;Apple A6&amp;quot; perhaps they want to talk about the &amp;quot;Apple A6&amp;quot; as a brand or product. Could also be &amp;quot;Apple A6&amp;quot; refers to &amp;quot;Apple A6&amp;quot; as a &amp;quot;iPod A6&amp;quot;? But no.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Alternatively, maybe the user wants information about Apple A6 being part of Apple&amp;#39;s hardware. There might be a &amp;quot;Apple A6&amp;quot; as &amp;quot;Apple 8.4&amp;quot; or &amp;quot;Apple A6&amp;quot;? Let&amp;#39;s think: The Apple chip series: A1, A2 (S3), A3 (iPhone 3?), A4 (iPhone 4), A5 (iPhone 5), A7 (iPhone 6), etc. There is no A6.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;But maybe the user wants &amp;quot;Apple A6&amp;quot; as a brand product? Maybe it&amp;#39;s like &amp;quot;Apple A6&amp;quot; used in the iPhone 2? No.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Could be that there&amp;#39;s &amp;quot;Apple A6&amp;quot; refers to &amp;quot;Apple A6&amp;quot; as &amp;quot;Apple A6&amp;quot; used in Apple &amp;quot;Apple A6&amp;quot; something. Or maybe we could explain that there is no known Apple A6, but we can mention the series.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Alternatively, user might ask about &amp;quot;Apple A6&amp;quot; referring to a new product like &amp;quot;Apple A6&amp;quot;, but it&amp;#39;s not known. So we can say: The Apple A6 chip has never existed; Apple used a series of chips starting from A4 etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We could also mention that Apple&amp;#39;s internal code naming might have A6 as part of hardware, but no publicly released product.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;But maybe the user is referencing &amp;quot;Apple A6&amp;quot; as &amp;quot;Apple A6&amp;quot; which refers to &amp;quot;A6&amp;quot; as a model of something like Apple A6? There&amp;#39;s no known product. So we can respond: &amp;quot;I don&amp;#39;t know about Apple A6; there is no official Apple A6 chip or device.&amp;quot;.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We could also mention that the Apple A6 might be an internal code for a certain hardware component, but it&amp;#39;s not publicly documented. Or maybe it&amp;#39;s a mis-typo for &amp;quot;Apple A6&amp;quot; which refers to &amp;quot;A6&amp;quot; as &amp;quot;iPhone 4&amp;quot;? Actually iPhone 4 uses A4; iPhone 5 uses A5; iPhone 6 uses A7.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;So we can answer: There is no Apple A6. The Apple chip series includes A4, A5, etc. Maybe the user might be mistaken. Alternatively we could mention that &amp;quot;Apple A6&amp;quot; refers to an internal code for a certain hardware component but not publicly known.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We also might provide some background about Apple&amp;#39;s chip series and timeline. But we might mention that there is no such product. So answer accordingly: There&amp;#39;s no Apple A6; maybe you meant A4 or A5 etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Alternatively, &amp;quot;Apple A6&amp;quot; could be an &amp;quot;Apple 6&amp;quot; like &amp;quot;Apple iPhone 6&amp;quot;? Not exactly.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thus our response: We need to clarify that no official product called &amp;quot;Apple A6&amp;quot; exists. Provide context of Apple chip series.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We can also mention that the Apple A6 might have been a proprietary internal code for a certain component, but not publicly announced. So we can answer accordingly.&amp;lt;|end|&amp;gt;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;|start|&amp;gt;assistant&amp;lt;|channel|&amp;gt;final&amp;lt;|message|&amp;gt;There isn’t any public‑released Apple device or processor officially named “Apple A6.”&lt;/p&gt;\n\n&lt;p&gt;Apple’s naming convention for its silicon chips has followed a series of “A” numbers starting with the first chip (A1), and then A2, A3, A4, A5, etc. For example:&lt;/p&gt;\n\n&lt;p&gt;* **A4** – used in the iPhone 4 and iPad 1&lt;/p&gt;\n\n&lt;p&gt;* **A5** – used in the iPhone 5, iPad 2&lt;/p&gt;\n\n&lt;p&gt;* **A7** – used in the iPhone 6 and iPhone 6 Plus&lt;/p&gt;\n\n&lt;p&gt;* **A8** – used in the iPhone 7 and iPhone 7 Plus&lt;/p&gt;\n\n&lt;p&gt;* **A9** – used in the iPhone 8, iPhone 8 Plus, etc.&lt;/p&gt;\n\n&lt;p&gt;There was no “Apple A6” in this sequence. If you’re referring to a specific product or component that might have an internal code of “A6,” it would likely be proprietary and not publicly documented.&lt;/p&gt;\n\n&lt;p&gt;If you were looking for information about the Apple A4, A5, or any other chip, let me know which one you’re interested in!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;For comparison, this is the response generated with 4 experts (the default configuration):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;&amp;lt;|channel|&amp;gt;analysis&amp;lt;|message|&amp;gt;We need to provide a response about the Apple A6 processor. The user just asks &amp;quot;Tell me about the Apple A6.&amp;quot; We can give an overview: introduced in 2012, used in iPhone 5 and iPad Mini 1st gen, features, architecture (ARMv7), dual core, performance improvements over A5, etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We should be mindful of policy: no disallowed content. It&amp;#39;s fine. Provide technical details, release info, specs. We can mention clock speed, CPU cores, GPU, process node, power consumption, performance benchmarks, manufacturing details. Also mention architecture: 32-bit ARMv7, dual-core Apple custom, etc.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We should also maybe note that the A6 was built on 28nm process. It had a &amp;quot;dual-core&amp;quot; CPU with each core being a &amp;quot;custom&amp;quot; designed by Apple. The GPU is PowerVR SGX543MP2. It introduced new power management, improved performance per watt, supports 3G LTE (in iPhone 5). Also mention that the A6 was replaced by A7 in 2013.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We could also include usage: used in iPhone 5, iPad mini (1st gen), and some iPod touch? Actually the iPod touch 4th generation had A8. So just iPhone 5 and iPad mini.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Also mention that the chip is 32-bit only.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Add maybe some performance numbers: around 2-3x faster than A5, etc. Provide memory architecture, TTI, L1 caches. Also talk about the &amp;#39;Apple custom&amp;quot; CPU design, but no need to get too deep.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Should also note that Apple didn&amp;#39;t provide public specs; many details come from leaked or reverse engineering.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Ok let&amp;#39;s produce a concise yet thorough answer.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We can include bullet points for clarity: release date, devices, architecture, performance, process node, GPU, etc. Also mention differences vs A5 and A7.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;We should keep it respectful to Apple, no disallowed content.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ll comply with policy.&amp;lt;|end|&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;|start|&amp;gt;assistant&amp;lt;|channel|&amp;gt;final&amp;lt;|message|&amp;gt;## The Apple A6 – Quick Overview&lt;/p&gt;\n\n&lt;p&gt;| Feature | Details |&lt;/p&gt;\n\n&lt;p&gt;|---------|---------|&lt;/p&gt;\n\n&lt;p&gt;| **Launch** | 2012 (iPhone 5 &amp;amp; iPad Mini) |&lt;/p&gt;\n\n&lt;p&gt;| **Process Node** | 28 nm (Apple custom fabrication) |&lt;/p&gt;\n\n&lt;p&gt;| **CPU Architecture** | Dual‑core ARMv7‑based Apple custom CPU (32‑bit) |&lt;/p&gt;\n\n&lt;p&gt;| **GPU** | PowerVR SGX543MP2, dual‑core (6‑shader cores total) |&lt;/p&gt;\n\n&lt;p&gt;| **Clock Speed** | ~1.3 GHz per core (actual frequency can vary by task) |&lt;/p&gt;\n\n&lt;p&gt;| **Performance Gains vs. A5** | Roughly 2–3× faster in CPU benchmarks; ~1.5–2× better GPU performance |&lt;/p&gt;\n\n&lt;p&gt;| **Power Efficiency** | 20‑30% lower power draw at comparable workloads compared to A5 |&lt;/p&gt;\n\n&lt;p&gt;| **Key Innovations** | • First Apple‑designed dual‑core CPU in an iPhone&amp;lt;br&amp;gt;• Integrated LTE modem support (iPhone 5)&amp;lt;br&amp;gt;• Advanced power‑management unit for lower idle consumption |&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;## In a Nutshell&lt;/p&gt;\n\n&lt;p&gt;The Apple A6 was the first major step beyond the A5, marking Apple’s move toward a **custom silicon** strategy. While still based on the ARMv7 instruction set (32‑bit only), Apple designed its own CPU cores to squeeze more performance per watt.&lt;/p&gt;\n\n&lt;p&gt;### CPU&lt;/p&gt;\n\n&lt;p&gt;- Two identical cores share a unified 1 MB L2 cache and separate 64 KB L1 instruction/ data caches.&lt;/p&gt;\n\n&lt;p&gt;- The core design was optimized for high clock speeds while keeping thermal output low, which allowed the iPhone 5 to run cooler and with fewer battery drain issues compared to its predecessor.&lt;/p&gt;\n\n&lt;p&gt;### GPU&lt;/p&gt;\n\n&lt;p&gt;- PowerVR SGX543MP2 (dual‑core) brought a 6‑shader architecture that handled both 3D rendering and 2D compositing more efficiently.&lt;/p&gt;\n\n&lt;p&gt;- This GPU was enough for smooth gaming performance on the iPhone 5 but would later be superseded by Apple’s own GPU in the A7/A8.&lt;/p&gt;\n\n&lt;p&gt;### Other Highlights&lt;/p&gt;\n\n&lt;p&gt;- **LTE Modem Integration**: The A6 included an LTE baseband (for iPhone 5), a first for Apple silicon.&lt;/p&gt;\n\n&lt;p&gt;- **Memory &amp;amp; I/O**: 512 MB LPDDR2 RAM (iPad Mini) or 1 GB DDR3L (iPhone 5); integrated PCIe, USB, and display controllers.&lt;/p&gt;\n\n&lt;p&gt;- **Manufacturing**: Built on a 28 nm process by Samsung; Apple began moving to smaller nodes with the A7.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;## Devices Powered by the A6&lt;/p&gt;\n\n&lt;p&gt;| Device | Release Year | Notes |&lt;/p&gt;\n\n&lt;p&gt;|--------|--------------|-------|&lt;/p&gt;\n\n&lt;p&gt;| iPhone 5 | 2012 | First iPhone to use the A6, introducing LTE and a new design. |&lt;/p&gt;\n\n&lt;p&gt;| iPad Mini (1st Gen) | 2012 | Compact tablet with 256‑ or 512‑GB storage variants; shared the same SoC as the iPhone 5. |&lt;/p&gt;\n\n&lt;p&gt;*(The A6 was not used in any later devices; it was replaced by the Apple A7 in 2013.)*&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;## Legacy&lt;/p&gt;\n\n&lt;p&gt;- The A6 set a precedent for **Apple’s custom silicon** philosophy, paving the way for future designs that integrated CPUs, GPUs, and baseband modems into single SoCs.&lt;/p&gt;\n\n&lt;p&gt;- It proved that Apple could squeeze significant performance gains while improving power efficiency—a dual focus that remains central to all subsequent iPhone processors.&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;### Bottom Line&lt;/p&gt;\n\n&lt;p&gt;The Apple A6 was a milestone in 2012: a dual‑core ARMv7 CPU with an integrated LTE modem, delivering double the speed of its predecessor on a modest 28 nm process. It powered the first iPhone 5 and the original iPad Mini, marking the start of Apple’s deeper dive into custom silicon that would define its mobile strategy for years to come.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We get a strong response from the default 4-expert configuration. It&amp;#39;s not perfect, GPT-OSS gets some details wrong (such as the number of GPU cores, and certain other specs), but the response is otherwise coherent. And it does correctly mention that it is the first SOC to use Apple-designed custom cores, which is the primary thing we&amp;#39;re trying to test in this benchmark(the model&amp;#39;s ability of the model to generate relevant and substantial prompts that aren&amp;#39;t just surface level regurgitations of basic facts). &lt;/p&gt;\n\n&lt;p&gt;Using 16 experts, on the other hand, pretty much neuters the model and renders it useless, resulting in the worst response I have ever seen from any model I have ever tried. &lt;/p&gt;\n\n&lt;p&gt;I plan on doing more experimentation. It&amp;#39;s a somewhat unique MoE configuration that OpenAI has given us on the 20b MoE model, with only 3.6B active parameters on each token (including routing and embedding tokens). It&amp;#39;s a remarkably lean model, but so far, it&amp;#39;s performing quite strongly if you leave it on the default settings. I want to try to compare to Gemma3 as well and see how it stacks up against the 12b and 27b models. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1miwzbq",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "FenderMoon",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/",
            "subreddit_subscribers": 511884,
            "created_utc": 1754460729,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n77jm7p",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "No_Efficiency_1144",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n76re3t",
                                "score": 1,
                                "author_fullname": "t2_1nkj9l14b0",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Breaking models can be good for fun outputs",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n77jm7p",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Breaking models can be good for fun outputs&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1miwzbq",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/n77jm7p/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754477454,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754477454,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n76re3t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FenderMoon",
                      "can_mod_post": false,
                      "created_utc": 1754461978,
                      "send_replies": true,
                      "parent_id": "t1_n76quqo",
                      "score": 1,
                      "author_fullname": "t2_f4ibdsc9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It's frankly surprising that it even works at all. Although in GPT-OSS's case, \"working\" is maybe a bit too generous of a word.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n76re3t",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s frankly surprising that it even works at all. Although in GPT-OSS&amp;#39;s case, &amp;quot;working&amp;quot; is maybe a bit too generous of a word.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miwzbq",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/n76re3t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754461978,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n76quqo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Entubulated",
            "can_mod_post": false,
            "created_utc": 1754461688,
            "send_replies": true,
            "parent_id": "t3_1miwzbq",
            "score": 5,
            "author_fullname": "t2_1opxde6hyq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This sort of thing keeps coming up... Varying the number of experts used in any MoE model invites a chance of things going sideways unless the routing layer(s) get retrained to work with the new number of experts. And even then, performance not guaranteed to improve with more experts active.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76quqo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This sort of thing keeps coming up... Varying the number of experts used in any MoE model invites a chance of things going sideways unless the routing layer(s) get retrained to work with the new number of experts. And even then, performance not guaranteed to improve with more experts active.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/n76quqo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754461688,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzbq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76tfsu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ELPascalito",
            "can_mod_post": false,
            "created_utc": 1754463100,
            "send_replies": true,
            "parent_id": "t3_1miwzbq",
            "score": 1,
            "author_fullname": "t2_6ox5x11a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I face similar problems on the normal OSS on ollama, no edits whatsoever, but everytime I give it a very hard quiz or a question about logic, it just keeps overthinking then doesn't even try to answer, very weird behaviour, albeit models like QwQ also overthink, in the end they come to the realisation that they don't understand the problem, thus choose an arbitrary answer and explain their logic, or they solve the quiz, but GPT for some reason overthinks then just hangs up, or responds by refusing my question or telling me it needs further info to answer me",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76tfsu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I face similar problems on the normal OSS on ollama, no edits whatsoever, but everytime I give it a very hard quiz or a question about logic, it just keeps overthinking then doesn&amp;#39;t even try to answer, very weird behaviour, albeit models like QwQ also overthink, in the end they come to the realisation that they don&amp;#39;t understand the problem, thus choose an arbitrary answer and explain their logic, or they solve the quiz, but GPT for some reason overthinks then just hangs up, or responds by refusing my question or telling me it needs further info to answer me&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/n76tfsu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754463100,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzbq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76w6ge",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Fetlocks_Glistening",
            "can_mod_post": false,
            "created_utc": 1754464589,
            "send_replies": true,
            "parent_id": "t3_1miwzbq",
            "score": 1,
            "author_fullname": "t2_lq9vco2g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "But I mean, if weights were trained for x experts, so for any given token the remaining neurons are essentially not trained, how would increasing the number of experts even work? You're just adding untrained/ incorrectly weighted neurons into the mix?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76w6ge",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;But I mean, if weights were trained for x experts, so for any given token the remaining neurons are essentially not trained, how would increasing the number of experts even work? You&amp;#39;re just adding untrained/ incorrectly weighted neurons into the mix?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/n76w6ge/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754464589,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzbq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76zomz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "agreeduponspring",
            "can_mod_post": false,
            "created_utc": 1754466572,
            "send_replies": true,
            "parent_id": "t3_1miwzbq",
            "score": 1,
            "author_fullname": "t2_vcy0zfodp",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\"We should keep it respectful to Apple, no disallowed content.\"\n\nAfter porn, they're coming for being negative towards brands.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76zomz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;We should keep it respectful to Apple, no disallowed content.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;After porn, they&amp;#39;re coming for being negative towards brands.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/n76zomz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754466572,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzbq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n770b8v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Professional-Bear857",
            "can_mod_post": false,
            "created_utc": 1754466927,
            "send_replies": true,
            "parent_id": "t3_1miwzbq",
            "score": 1,
            "author_fullname": "t2_yrl9ztfsa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Normally 8 experts are used, as this is often optimal, I wonder why Openai went with 4.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n770b8v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Normally 8 experts are used, as this is often optimal, I wonder why Openai went with 4.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miwzbq/dont_try_to_increase_the_number_of_active_experts/n770b8v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754466927,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miwzbq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]