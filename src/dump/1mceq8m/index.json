[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi everyone,\n\nI'm trying to optimize running larger MoE models like Qwen3-30B-A3B on a low-VRAM setup (4GB GPU) by using intelligent/manual offloading.\n\nThe goal is to keep the most relevant experts for a specific task (e.g., coding) permanently in VRAM for better performance, while offloading the less used ones to the CPU/RAM.\n\nThis obviously requires knowing which expert ID corresponds to which specialized function. Has anyone already done the legwork of profiling the model? For example, by feeding it pure code vs. pure prose and logging the expert activation frequency with tools like llama.cpp?\n\nI'm looking for any kind of data.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Has anyone profiled the expert specialization in MoE models like Qwen3-30B-A3B?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mceq8m",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.84,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_mhb0rkd4",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753803457,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to optimize running larger MoE models like Qwen3-30B-A3B on a low-VRAM setup (4GB GPU) by using intelligent/manual offloading.&lt;/p&gt;\n\n&lt;p&gt;The goal is to keep the most relevant experts for a specific task (e.g., coding) permanently in VRAM for better performance, while offloading the less used ones to the CPU/RAM.&lt;/p&gt;\n\n&lt;p&gt;This obviously requires knowing which expert ID corresponds to which specialized function. Has anyone already done the legwork of profiling the model? For example, by feeding it pure code vs. pure prose and logging the expert activation frequency with tools like llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for any kind of data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mceq8m",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Eden63",
            "discussion_type": null,
            "num_comments": 20,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/",
            "subreddit_subscribers": 506711,
            "created_utc": 1753803457,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5uw38l",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Accomplished_Mode170",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5ubseg",
                                          "score": 1,
                                          "author_fullname": "t2_4hfmiefj",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "The experts ARE the [emergent](https://arxiv.org/abs/2505.11471) [propensities](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) (us too?) towards the token probabilities based on parameters trained-in",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5uw38l",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The experts ARE the &lt;a href=\"https://arxiv.org/abs/2505.11471\"&gt;emergent&lt;/a&gt; &lt;a href=\"https://transformer-circuits.pub/2025/attribution-graphs/methods.html\"&gt;propensities&lt;/a&gt; (us too?) towards the token probabilities based on parameters trained-in&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mceq8m",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5uw38l/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753820655,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753820655,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ubseg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Waarheid",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5u9b0u",
                                "score": 2,
                                "author_fullname": "t2_amsk4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Naive question, but I wonder if some of those less-used experts are related to handling tokens of a different language, and the poster there only used English.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ubseg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Naive question, but I wonder if some of those less-used experts are related to handling tokens of a different language, and the poster there only used English.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mceq8m",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5ubseg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753814902,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753814902,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5u9b0u",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1753814212,
                      "send_replies": true,
                      "parent_id": "t1_n5t7no7",
                      "score": 4,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "While it's true that \"experts\" are just 'random' parts of a sparse layer, most MoE models _do_ have some reasonably strong biases towards a subset of experts.  [Here's someone looking at routing in Qwen-30B, layer 24](https://x.com/kalomaze/status/1918238263330148487): they find the most common expert was 5% and the least was 0%.\n\nSo in principle, one could offload the least common experts on CPU so the common cases could run on GPU.  However, I don't know how realistic that is since the overhead of routing fallback is probably fairly significant and the odds of hitting a least one of the common experts is higher than you probably want.  Also, these expert biases are basically a bug in the model training and there is ongoing research on how to eliminate them.  I don't think you could really justify the effort when the best new models will be endeavoring to make the optimization hack obsolete.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5u9b0u",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While it&amp;#39;s true that &amp;quot;experts&amp;quot; are just &amp;#39;random&amp;#39; parts of a sparse layer, most MoE models &lt;em&gt;do&lt;/em&gt; have some reasonably strong biases towards a subset of experts.  &lt;a href=\"https://x.com/kalomaze/status/1918238263330148487\"&gt;Here&amp;#39;s someone looking at routing in Qwen-30B, layer 24&lt;/a&gt;: they find the most common expert was 5% and the least was 0%.&lt;/p&gt;\n\n&lt;p&gt;So in principle, one could offload the least common experts on CPU so the common cases could run on GPU.  However, I don&amp;#39;t know how realistic that is since the overhead of routing fallback is probably fairly significant and the odds of hitting a least one of the common experts is higher than you probably want.  Also, these expert biases are basically a bug in the model training and there is ongoing research on how to eliminate them.  I don&amp;#39;t think you could really justify the effort when the best new models will be endeavoring to make the optimization hack obsolete.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mceq8m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5u9b0u/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753814212,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5uup9i",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Accomplished_Mode170",
                      "can_mod_post": false,
                      "created_utc": 1753820268,
                      "send_replies": true,
                      "parent_id": "t1_n5t7no7",
                      "score": 1,
                      "author_fullname": "t2_4hfmiefj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "A [coherent reasoning trace is analogous to a spline or circuit ](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)within the weight matrix\n\n[An ideal n-class ColBERT model can make them more coherent still…](https://arxiv.org/abs/2505.11471)\n\n✍️ edit: i.e. [Experts are shoggoth, but more understandable day by day](https://www.lesswrong.com/posts/yjzW7gxk2h7bBs2qr/the-meaning-of-shoggoth-ai-memes)\n\ne.g. [sample code](https://github.com/sigridjineth/crisp-py)\n\n📊 Fun times y’all 🤠",
                      "edited": 1753820741,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5uup9i",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A &lt;a href=\"https://transformer-circuits.pub/2025/attribution-graphs/methods.html\"&gt;coherent reasoning trace is analogous to a spline or circuit &lt;/a&gt;within the weight matrix&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2505.11471\"&gt;An ideal n-class ColBERT model can make them more coherent still…&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;✍️ edit: i.e. &lt;a href=\"https://www.lesswrong.com/posts/yjzW7gxk2h7bBs2qr/the-meaning-of-shoggoth-ai-memes\"&gt;Experts are shoggoth, but more understandable day by day&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;e.g. &lt;a href=\"https://github.com/sigridjineth/crisp-py\"&gt;sample code&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;📊 Fun times y’all 🤠&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mceq8m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5uup9i/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753820268,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5uautc",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Azuriteh",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5u7yrx",
                                          "score": 1,
                                          "author_fullname": "t2_wmv41",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "lmao I was half asleep I just re-read the post, you're right",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5uautc",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;lmao I was half asleep I just re-read the post, you&amp;#39;re right&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mceq8m",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5uautc/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753814644,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753814644,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5uvumo",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Eden63",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5u7yrx",
                                          "score": 1,
                                          "author_fullname": "t2_mhb0rkd4",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "That's what it's all about. With this, it could be possible to run even larger models efficiently for your requirements. With proper profiling and a good algorithm, you can improve the situation and load the model for your specific field—whether it's coding, writing, or whatever else you do.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5uvumo",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s what it&amp;#39;s all about. With this, it could be possible to run even larger models efficiently for your requirements. With proper profiling and a good algorithm, you can improve the situation and load the model for your specific field—whether it&amp;#39;s coding, writing, or whatever else you do.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mceq8m",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5uvumo/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753820588,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753820588,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5u7yrx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "segmond",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5tx305",
                                "score": 3,
                                "author_fullname": "t2_ah13x",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "he's not talking about removing experts, he's talking about keeping those in VRAM.  so say qwen3-30B-a3b has 30 layers and say 3 are active at once.   We can run inference multiple times and keep track of how many times each layer is activated, chances are you will not get even distribution.   It might be that layers 20-25 are most active, in which case you try to keep those in VRAM and the rest in CPU ram, you should get better performance.  I did ask this same question before in the llama.cpp github discussions.  I don't know, but it would be interesting to see and test out.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5u7yrx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;he&amp;#39;s not talking about removing experts, he&amp;#39;s talking about keeping those in VRAM.  so say qwen3-30B-a3b has 30 layers and say 3 are active at once.   We can run inference multiple times and keep track of how many times each layer is activated, chances are you will not get even distribution.   It might be that layers 20-25 are most active, in which case you try to keep those in VRAM and the rest in CPU ram, you should get better performance.  I did ask this same question before in the llama.cpp github discussions.  I don&amp;#39;t know, but it would be interesting to see and test out.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mceq8m",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5u7yrx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753813838,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753813838,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5tx305",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Azuriteh",
                      "can_mod_post": false,
                      "created_utc": 1753810851,
                      "send_replies": true,
                      "parent_id": "t1_n5t7no7",
                      "score": -2,
                      "author_fullname": "t2_wmv41",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Indeed. Although there's a possibility that some experts are much more specialized towards coding than other ones! Nonetheless, it's likely that even by removing an expert that's not completely related to coding will likely lobotomize the model, as the interaction between experts is also an important part in the architecture.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5tx305",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Indeed. Although there&amp;#39;s a possibility that some experts are much more specialized towards coding than other ones! Nonetheless, it&amp;#39;s likely that even by removing an expert that&amp;#39;s not completely related to coding will likely lobotomize the model, as the interaction between experts is also an important part in the architecture.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mceq8m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5tx305/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753810851,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5t7no7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "T2WIN",
            "can_mod_post": false,
            "created_utc": 1753803856,
            "send_replies": true,
            "parent_id": "t3_1mceq8m",
            "score": 16,
            "author_fullname": "t2_38ilynpn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think i doesn't work like that. I am no expert but from what i have seen from my own research, experts is a misleading name. Experts in MoE aren't specialized in something easily human understandable.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5t7no7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think i doesn&amp;#39;t work like that. I am no expert but from what i have seen from my own research, experts is a misleading name. Experts in MoE aren&amp;#39;t specialized in something easily human understandable.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5t7no7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753803856,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mceq8m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 16
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5uyns5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OfficialHashPanda",
                      "can_mod_post": false,
                      "created_utc": 1753821375,
                      "send_replies": true,
                      "parent_id": "t1_n5u8z1j",
                      "score": 1,
                      "author_fullname": "t2_8w6mm4hmo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; I did a few experiments, but didn't see much difference, so say I had 60 layers and could only load half in VRAM. I loaded 1-30, then next run 30-60 then next run 10-40, I was hoping one of them would run slightly better, but they all ran roughly the same for the same prompt. \n\n\nOkay, but we activate each layer for each token, so this is not particularly surprising, right? \n\n\nThe post talks about loading specific experts within each layer, which don't all activate for each token.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5uyns5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I did a few experiments, but didn&amp;#39;t see much difference, so say I had 60 layers and could only load half in VRAM. I loaded 1-30, then next run 30-60 then next run 10-40, I was hoping one of them would run slightly better, but they all ran roughly the same for the same prompt. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Okay, but we activate each layer for each token, so this is not particularly surprising, right? &lt;/p&gt;\n\n&lt;p&gt;The post talks about loading specific experts within each layer, which don&amp;#39;t all activate for each token.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mceq8m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5uyns5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753821375,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5u8z1j",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "segmond",
            "can_mod_post": false,
            "created_utc": 1753814120,
            "send_replies": true,
            "parent_id": "t3_1mceq8m",
            "score": 3,
            "author_fullname": "t2_ah13x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I don't think anyone has done so, but if you can hack llama.cpp and add some profiling, you can collect data on which layers are activated, do a bunch of run then look at the data.   the thing to bear in mind is that it will be specific to your quantization, so if you are running q4, that data will be different from q6.  I did a few experiments, but didn't see much difference, so say I had 60 layers and could only load half in VRAM.  I loaded 1-30, then next run 30-60 then next run 10-40, I was hoping one of them would run slightly better, but they all ran roughly the same for the same prompt.  I didn't pin the seed when I did my experiment tho.  A better approach will be to do a real profiling, use a fixed seed and see if our idea holds true.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5u8z1j",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think anyone has done so, but if you can hack llama.cpp and add some profiling, you can collect data on which layers are activated, do a bunch of run then look at the data.   the thing to bear in mind is that it will be specific to your quantization, so if you are running q4, that data will be different from q6.  I did a few experiments, but didn&amp;#39;t see much difference, so say I had 60 layers and could only load half in VRAM.  I loaded 1-30, then next run 30-60 then next run 10-40, I was hoping one of them would run slightly better, but they all ran roughly the same for the same prompt.  I didn&amp;#39;t pin the seed when I did my experiment tho.  A better approach will be to do a real profiling, use a fixed seed and see if our idea holds true.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5u8z1j/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753814120,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mceq8m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5u0ahg",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Double_Cause4609",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5txsyc",
                                                    "score": 2,
                                                    "author_fullname": "t2_1kubzxt2ww",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I'd argue it's the other way around.\n\nWith GUI tools settings are hard to find, sometimes are buggy, don't set properly, and in the end, they require the same steps and information as setting a flag when launching a program from a CLI anyway. I vastly prefer being able to go to the docs and ctrl-f the exact thing I'm looking for in a dense text document, possibly even being able to let an LLM put together a complete launch command if needed.\n\nTo each their own, though",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5u0ahg",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d argue it&amp;#39;s the other way around.&lt;/p&gt;\n\n&lt;p&gt;With GUI tools settings are hard to find, sometimes are buggy, don&amp;#39;t set properly, and in the end, they require the same steps and information as setting a flag when launching a program from a CLI anyway. I vastly prefer being able to go to the docs and ctrl-f the exact thing I&amp;#39;m looking for in a dense text document, possibly even being able to let an LLM put together a complete launch command if needed.&lt;/p&gt;\n\n&lt;p&gt;To each their own, though&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mceq8m",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5u0ahg/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753811710,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753811710,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5txsyc",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "prusswan",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5twm4e",
                                          "score": 1,
                                          "author_fullname": "t2_kegwk",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "\\&gt; but to my knowledge no software handles shared VRAM + system RAM gracefully, or at least none I'd care to use.\n\nyeah it's just a bit messy without some GUI tool to manage the settings for different models",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5txsyc",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; but to my knowledge no software handles shared VRAM + system RAM gracefully, or at least none I&amp;#39;d care to use.&lt;/p&gt;\n\n&lt;p&gt;yeah it&amp;#39;s just a bit messy without some GUI tool to manage the settings for different models&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mceq8m",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5txsyc/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753811044,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753811044,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5twm4e",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Double_Cause4609",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5tt29y",
                                "score": 3,
                                "author_fullname": "t2_1kubzxt2ww",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I don't know. I personally elect not to use Ollama as they've done a disservice to the work provided to them by upstream LlamaCPP by attempting to hide involvement as much as they are able.\n\nI believe there are some backends that handle some allocations of VRAM automatically (like TabbyAPI when doing multi-GPU), but to my knowledge no software handles shared VRAM + system RAM gracefully, or at least none I'd care to use.\n\nGiven that LCPP is about two minutes to figure out a good offloading setup when there's an interesting model I find it easier to just figure it out and then copy down the launch command in a text file, personally.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5twm4e",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know. I personally elect not to use Ollama as they&amp;#39;ve done a disservice to the work provided to them by upstream LlamaCPP by attempting to hide involvement as much as they are able.&lt;/p&gt;\n\n&lt;p&gt;I believe there are some backends that handle some allocations of VRAM automatically (like TabbyAPI when doing multi-GPU), but to my knowledge no software handles shared VRAM + system RAM gracefully, or at least none I&amp;#39;d care to use.&lt;/p&gt;\n\n&lt;p&gt;Given that LCPP is about two minutes to figure out a good offloading setup when there&amp;#39;s an interesting model I find it easier to just figure it out and then copy down the launch command in a text file, personally.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mceq8m",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5twm4e/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753810725,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753810725,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5tt29y",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "prusswan",
                      "can_mod_post": false,
                      "created_utc": 1753809767,
                      "send_replies": true,
                      "parent_id": "t1_n5tbm6l",
                      "score": 1,
                      "author_fullname": "t2_kegwk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Does ollama or any other software handles the offloading to system ram automatically? i.e. It will try to use up the VRAM followed by RAM",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5tt29y",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does ollama or any other software handles the offloading to system ram automatically? i.e. It will try to use up the VRAM followed by RAM&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mceq8m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5tt29y/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753809767,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ufmj8",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Double_Cause4609",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5ucts3",
                                "score": 2,
                                "author_fullname": "t2_1kubzxt2ww",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yup, that's intentional. In a lot of situations people have enough VRAM to run a shared expert and KV-Cache so I use that pattern as a sensible default that gives the best balance of speed and VRAM usage.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ufmj8",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup, that&amp;#39;s intentional. In a lot of situations people have enough VRAM to run a shared expert and KV-Cache so I use that pattern as a sensible default that gives the best balance of speed and VRAM usage.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mceq8m",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5ufmj8/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753815969,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753815969,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ucts3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1753815189,
                      "send_replies": true,
                      "parent_id": "t1_n5tbm6l",
                      "score": 1,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; I believe conditional experts are a single tensor; there's not really a clean way to extract individual experts out of there.\n\nCorrect.  If we look at Kimi K2 layer 1 tensors:\n\n    blk.1.exp_probs_b.bias      -  [  384,     1,     1,     1]\n    blk.1.ffn_down_exps.weight  -  [ 2048,  7168,   384,     1]\n    blk.1.ffn_down_shexp.weight -  [ 2048,  7168,     1,     1]\n    blk.1.ffn_gate_exps.weight  -  [ 7168,  2048,   384,     1]\n    blk.1.ffn_gate_inp.weight   -  [ 7168,   384,     1,     1]\n    blk.1.ffn_gate_shexp.weight -  [ 7168,  2048,     1,     1]\n    blk.1.ffn_norm.weight       -  [ 7168,     1,     1,     1]\n    blk.1.ffn_up_exps.weight    -  [ 7168,  2048,   384,     1]\n    blk.1.ffn_up_shexp.weight   -  [ 7168,  2048,     1,     1]\n\nYou can see the routed expert tensors are actually 3D matrices where the 3rd dim is the individual expert.  This is different from e.g. the hf safetensors where they are named like `model.layers.1.mlp.experts.125.gate_proj.weight`.  Also worth pointing out the shared expert is split off and called `shexp` so won't get caught by the `exps=CPU` pattern.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ucts3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I believe conditional experts are a single tensor; there&amp;#39;s not really a clean way to extract individual experts out of there.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Correct.  If we look at Kimi K2 layer 1 tensors:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;blk.1.exp_probs_b.bias      -  [  384,     1,     1,     1]\nblk.1.ffn_down_exps.weight  -  [ 2048,  7168,   384,     1]\nblk.1.ffn_down_shexp.weight -  [ 2048,  7168,     1,     1]\nblk.1.ffn_gate_exps.weight  -  [ 7168,  2048,   384,     1]\nblk.1.ffn_gate_inp.weight   -  [ 7168,   384,     1,     1]\nblk.1.ffn_gate_shexp.weight -  [ 7168,  2048,     1,     1]\nblk.1.ffn_norm.weight       -  [ 7168,     1,     1,     1]\nblk.1.ffn_up_exps.weight    -  [ 7168,  2048,   384,     1]\nblk.1.ffn_up_shexp.weight   -  [ 7168,  2048,     1,     1]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You can see the routed expert tensors are actually 3D matrices where the 3rd dim is the individual expert.  This is different from e.g. the hf safetensors where they are named like &lt;code&gt;model.layers.1.mlp.experts.125.gate_proj.weight&lt;/code&gt;.  Also worth pointing out the shared expert is split off and called &lt;code&gt;shexp&lt;/code&gt; so won&amp;#39;t get caught by the &lt;code&gt;exps=CPU&lt;/code&gt; pattern.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mceq8m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5ucts3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753815189,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5tbm6l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753804949,
            "send_replies": true,
            "parent_id": "t3_1mceq8m",
            "score": 6,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "With the GGUF ecosystem, I believe conditional experts are a single tensor; there's not really a clean way to extract individual experts out of there.\n\nA much better option is just to use the -ot flag to throw the Attention weights and KV cache onto your GPU. I'm not sure if it'll fit in 4GB of VRAM (that's quite tight), but you could do...\n\n\\--ngl 99 \\\\\n\n\\--ot \"exps=CPU\"\n\nTo offload all experts to your CPU. If you don't have enough VRAM, you can keep setting --ngl lower until it fits onto your GPU.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5tbm6l",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;With the GGUF ecosystem, I believe conditional experts are a single tensor; there&amp;#39;s not really a clean way to extract individual experts out of there.&lt;/p&gt;\n\n&lt;p&gt;A much better option is just to use the -ot flag to throw the Attention weights and KV cache onto your GPU. I&amp;#39;m not sure if it&amp;#39;ll fit in 4GB of VRAM (that&amp;#39;s quite tight), but you could do...&lt;/p&gt;\n\n&lt;p&gt;--ngl 99 \\&lt;/p&gt;\n\n&lt;p&gt;--ot &amp;quot;exps=CPU&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;To offload all experts to your CPU. If you don&amp;#39;t have enough VRAM, you can keep setting --ngl lower until it fits onto your GPU.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5tbm6l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753804949,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mceq8m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5t8bst",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kironlau",
            "can_mod_post": false,
            "created_utc": 1753804041,
            "send_replies": true,
            "parent_id": "t3_1mceq8m",
            "score": 2,
            "author_fullname": "t2_tb0dz2ds",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "just use ik\\_lamma.cpp\n\n\\`\\`\\`\n\n.\\\\ik\\_llama-bin-win-cuda-12.8-x64-avx2\\\\llama-server \\^\n\n\\--model \"G:\\\\lm-studio\\\\models\\\\ubergarm\\\\Qwen3-30B-A3B-GGUF\\\\Qwen3-30B-A3B-mix-IQ4\\_K.gguf\" \\^\n\n\\--alias Qwen/Qwen3-30B-A3B \\^\n\n\\-fa \\^\n\n\\-c 32768 \\^\n\n\\-ctk q8\\_0 -ctv q8\\_0 \\^\n\n\\-fmoe \\^\n\n\\-rtr \\^\n\n\\--no-mmap \\^\n\n\\-ot exps=CPU\\^\n\n\\-ngl 99\\^\n\n\\--threads 8 \\^\n\n\\--port 8080\n\n\\`\\`\\`\n\nit is MOE optimized, should use up 3gb vram, other offloading to CPU/RAM,\n\n\"\\^\" is for window, replace with  \"\\\\\" for linux",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5t8bst",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;just use ik_lamma.cpp&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;.\\ik_llama-bin-win-cuda-12.8-x64-avx2\\llama-server ^&lt;/p&gt;\n\n&lt;p&gt;--model &amp;quot;G:\\lm-studio\\models\\ubergarm\\Qwen3-30B-A3B-GGUF\\Qwen3-30B-A3B-mix-IQ4_K.gguf&amp;quot; ^&lt;/p&gt;\n\n&lt;p&gt;--alias Qwen/Qwen3-30B-A3B ^&lt;/p&gt;\n\n&lt;p&gt;-fa ^&lt;/p&gt;\n\n&lt;p&gt;-c 32768 ^&lt;/p&gt;\n\n&lt;p&gt;-ctk q8_0 -ctv q8_0 ^&lt;/p&gt;\n\n&lt;p&gt;-fmoe ^&lt;/p&gt;\n\n&lt;p&gt;-rtr ^&lt;/p&gt;\n\n&lt;p&gt;--no-mmap ^&lt;/p&gt;\n\n&lt;p&gt;-ot exps=CPU^&lt;/p&gt;\n\n&lt;p&gt;-ngl 99^&lt;/p&gt;\n\n&lt;p&gt;--threads 8 ^&lt;/p&gt;\n\n&lt;p&gt;--port 8080&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;it is MOE optimized, should use up 3gb vram, other offloading to CPU/RAM,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;^&amp;quot; is for window, replace with  &amp;quot;\\&amp;quot; for linux&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5t8bst/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753804041,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mceq8m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5t9ls0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mkengine",
            "can_mod_post": false,
            "created_utc": 1753804391,
            "send_replies": true,
            "parent_id": "t3_1mceq8m",
            "score": 0,
            "author_fullname": "t2_9p2xe",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Maybe this helps?\n\nhttps://reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5t9ls0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe this helps?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/\"&gt;https://reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mceq8m/has_anyone_profiled_the_expert_specialization_in/n5t9ls0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753804391,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mceq8m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]