[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "It should be really easy to make something like: \n\nJust MOE gatting network is  initially loaded into RAM ( or offloaded to the GPU ) and stays there\n\n\n\nActivation Process: When an input is received, the gating network evaluates it and determines which experts should be activated based on the input's characteristics.\n\n\n\nLoading Active Experts: Only the parameters of the selected experts are oflloaded to the GPU (or loaded into RAM, by choice)  for processing.\n\n\n\nFor the next prompt if  gatting network decides different  experts will be activated they are just replaced in RAM ( VRAM) .    \n  \nThere will be a little latency at the start but it is  nothing compared to present clumsiness and huge processing time  if not enough RAM or VRAM and memory swapping..  \n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why there is still no a proper or helpful inference for MOE models ?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8oz07",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.47,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1qychuraq9",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753414990,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It should be really easy to make something like: &lt;/p&gt;\n\n&lt;p&gt;Just MOE gatting network is  initially loaded into RAM ( or offloaded to the GPU ) and stays there&lt;/p&gt;\n\n&lt;p&gt;Activation Process: When an input is received, the gating network evaluates it and determines which experts should be activated based on the input&amp;#39;s characteristics.&lt;/p&gt;\n\n&lt;p&gt;Loading Active Experts: Only the parameters of the selected experts are oflloaded to the GPU (or loaded into RAM, by choice)  for processing.&lt;/p&gt;\n\n&lt;p&gt;For the next prompt if  gatting network decides different  experts will be activated they are just replaced in RAM ( VRAM) .    &lt;/p&gt;\n\n&lt;p&gt;There will be a little latency at the start but it is  nothing compared to present clumsiness and huge processing time  if not enough RAM or VRAM and memory swapping..  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m8oz07",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Highwaytothebeach",
            "discussion_type": null,
            "num_comments": 15,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/",
            "subreddit_subscribers": 504487,
            "created_utc": 1753414990,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n52f1sk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "ColorlessCrowfeet",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n51bpx2",
                                "score": 5,
                                "author_fullname": "t2_9g89e30o",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yes, they're selected per token *and* per layer. Very fine-grained. \"Experts\" are not a \"thing\", they're more like layer-fragments.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n52f1sk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, they&amp;#39;re selected per token &lt;em&gt;and&lt;/em&gt; per layer. Very fine-grained. &amp;quot;Experts&amp;quot; are not a &amp;quot;thing&amp;quot;, they&amp;#39;re more like layer-fragments.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8oz07",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n52f1sk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753442315,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753442315,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n51bpx2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "phree_radical",
                      "can_mod_post": false,
                      "created_utc": 1753421268,
                      "send_replies": true,
                      "parent_id": "t1_n50zrda",
                      "score": 10,
                      "author_fullname": "t2_44nkp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "per LAYER.  You don't know which \"experts\" to activate for layer 2 until you get the embedding from running layer 1",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n51bpx2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;per LAYER.  You don&amp;#39;t know which &amp;quot;experts&amp;quot; to activate for layer 2 until you get the embedding from running layer 1&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8oz07",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n51bpx2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753421268,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 10
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n512rpn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Conscious_Cut_6144",
                      "can_mod_post": false,
                      "created_utc": 1753417124,
                      "send_replies": true,
                      "parent_id": "t1_n50zrda",
                      "score": 3,
                      "author_fullname": "t2_9hl4ymvj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n512rpn",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8oz07",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n512rpn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753417124,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n51avkl",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Marksta",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n515pxj",
                                "score": 7,
                                "author_fullname": "t2_559a1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The way literally every massive MoE model card tells you to run the model.\n\n`-ngl 99 -ot \".*ffn_.*_exps.*=CPU\"`\n\nThere isn't any options for intelligent or even performant loading weights from SSD at this time. So the SSD example doesn't have argument support aside from running -ngl 0 and letting mmap do its thing. KV cache will be on the GPU. Weights will get flipped in and out of RAM as needed. Maybe they hold dense layers in RAM if possible but really don't think anyone snuck some optimized MoE layer loading via SSD algo into llama.cpp.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n51avkl",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The way literally every massive MoE model card tells you to run the model.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-ngl 99 -ot &amp;quot;.*ffn_.*_exps.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;There isn&amp;#39;t any options for intelligent or even performant loading weights from SSD at this time. So the SSD example doesn&amp;#39;t have argument support aside from running -ngl 0 and letting mmap do its thing. KV cache will be on the GPU. Weights will get flipped in and out of RAM as needed. Maybe they hold dense layers in RAM if possible but really don&amp;#39;t think anyone snuck some optimized MoE layer loading via SSD algo into llama.cpp.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8oz07",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n51avkl/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753420853,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753420853,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 7
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n515pxj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Highwaytothebeach",
                      "can_mod_post": false,
                      "created_utc": 1753418428,
                      "send_replies": true,
                      "parent_id": "t1_n50zrda",
                      "score": 2,
                      "author_fullname": "t2_1qychuraq9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Sure. How to offload shared experts and all the other important bits to the GPU and LOCK it there, and for comparison how to offload shared experts and all the other important bits to the RAM (CPU)  and LOCK it there while everything else stays on ssd?",
                      "edited": 1753418876,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n515pxj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sure. How to offload shared experts and all the other important bits to the GPU and LOCK it there, and for comparison how to offload shared experts and all the other important bits to the RAM (CPU)  and LOCK it there while everything else stays on ssd?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8oz07",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n515pxj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753418428,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n50zrda",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "homak666",
            "can_mod_post": false,
            "created_utc": 1753415840,
            "send_replies": true,
            "parent_id": "t3_1m8oz07",
            "score": 33,
            "author_fullname": "t2_16tybc6k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Experts are activated per token, not per prompt. That's why the proper practice is to offload experts to RAM and keep shared experts and all the other important bits in VRAM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n50zrda",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Experts are activated per token, not per prompt. That&amp;#39;s why the proper practice is to offload experts to RAM and keep shared experts and all the other important bits in VRAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n50zrda/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753415840,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8oz07",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 33
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n510dt6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753416103,
            "send_replies": true,
            "parent_id": "t3_1m8oz07",
            "score": 9,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "In short, they don't work like that.  The \"mixture of experts\" name is pretty misleading, it's just a sparse model, meaning only a fraction gets activated at a time.  That fraction doesn't just change per prompt, or token, but per layer per token.\n\nNow, it's true that some 'experts' get used more than others, but realistically that's more of a deficiency of the training and new models are trying to correct that.  Even then, IIRC it's something like the most used expert is like ~4% activation max (IIRC, I can't find a report in a quick search), so it's not like you can offload the most common ones in a limited VRAM situation.\n\nThat said, it _is_ common practice to offload all non-expert tensors.  After all, MoE models are still LLMs and have more than just the expert tensors.  Those common tensors make up (very roughly, depends on architecture) 1/3 or the weights activated for a given token.  Thus you'll see people running `llama.cpp -ngl 99 -op exps=CPU` which says to put everything on GPU except the tensors with \"exps\" in the name (i.e. the experts).  This gives a solid speedup and doesn't require any fancy logic.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n510dt6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In short, they don&amp;#39;t work like that.  The &amp;quot;mixture of experts&amp;quot; name is pretty misleading, it&amp;#39;s just a sparse model, meaning only a fraction gets activated at a time.  That fraction doesn&amp;#39;t just change per prompt, or token, but per layer per token.&lt;/p&gt;\n\n&lt;p&gt;Now, it&amp;#39;s true that some &amp;#39;experts&amp;#39; get used more than others, but realistically that&amp;#39;s more of a deficiency of the training and new models are trying to correct that.  Even then, IIRC it&amp;#39;s something like the most used expert is like ~4% activation max (IIRC, I can&amp;#39;t find a report in a quick search), so it&amp;#39;s not like you can offload the most common ones in a limited VRAM situation.&lt;/p&gt;\n\n&lt;p&gt;That said, it &lt;em&gt;is&lt;/em&gt; common practice to offload all non-expert tensors.  After all, MoE models are still LLMs and have more than just the expert tensors.  Those common tensors make up (very roughly, depends on architecture) 1/3 or the weights activated for a given token.  Thus you&amp;#39;ll see people running &lt;code&gt;llama.cpp -ngl 99 -op exps=CPU&lt;/code&gt; which says to put everything on GPU except the tensors with &amp;quot;exps&amp;quot; in the name (i.e. the experts).  This gives a solid speedup and doesn&amp;#39;t require any fancy logic.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n510dt6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753416103,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8oz07",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n50zdwn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fp4guru",
            "can_mod_post": false,
            "created_utc": 1753415680,
            "send_replies": true,
            "parent_id": "t3_1m8oz07",
            "score": 3,
            "author_fullname": "t2_1tp8zldw5g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We have been using llamacpp for years.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n50zdwn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We have been using llamacpp for years.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n50zdwn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753415680,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8oz07",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n510tb2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Threatening-Silence-",
                      "can_mod_post": false,
                      "created_utc": 1753416285,
                      "send_replies": true,
                      "parent_id": "t1_n510q8r",
                      "score": 3,
                      "author_fullname": "t2_15wqsifdjf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "https://preview.redd.it/akknwcju2yef1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=1d15b7c94b6bbff5c0807ac7f2fd80298ebb63fa",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n510tb2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/akknwcju2yef1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d15b7c94b6bbff5c0807ac7f2fd80298ebb63fa\"&gt;https://preview.redd.it/akknwcju2yef1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d15b7c94b6bbff5c0807ac7f2fd80298ebb63fa&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8oz07",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n510tb2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753416285,
                      "media_metadata": {
                        "akknwcju2yef1": {
                          "status": "valid",
                          "e": "Image",
                          "m": "image/png",
                          "p": [
                            {
                              "y": 78,
                              "x": 108,
                              "u": "https://preview.redd.it/akknwcju2yef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=28eeedfb258152b5348034a8896ae2a6ac223331"
                            },
                            {
                              "y": 156,
                              "x": 216,
                              "u": "https://preview.redd.it/akknwcju2yef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16f6d676979ee94124429cf50c8749b32d3988c7"
                            },
                            {
                              "y": 231,
                              "x": 320,
                              "u": "https://preview.redd.it/akknwcju2yef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e5f1a7bfd1fc3b56aeb615a190f9892d6dee1d1"
                            },
                            {
                              "y": 463,
                              "x": 640,
                              "u": "https://preview.redd.it/akknwcju2yef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0b0962ca7d71659fed553d7e86c84e649155241"
                            },
                            {
                              "y": 695,
                              "x": 960,
                              "u": "https://preview.redd.it/akknwcju2yef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa52665a7432360d58aa518b14c99deacfbc1f20"
                            },
                            {
                              "y": 782,
                              "x": 1080,
                              "u": "https://preview.redd.it/akknwcju2yef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=068e13e396a61ac11913439b9d206e050b998798"
                            }
                          ],
                          "s": {
                            "y": 782,
                            "x": 1080,
                            "u": "https://preview.redd.it/akknwcju2yef1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=1d15b7c94b6bbff5c0807ac7f2fd80298ebb63fa"
                          },
                          "id": "akknwcju2yef1"
                        }
                      },
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n510q8r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Threatening-Silence-",
            "can_mod_post": false,
            "created_utc": 1753416249,
            "send_replies": true,
            "parent_id": "t3_1m8oz07",
            "score": 3,
            "author_fullname": "t2_15wqsifdjf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The \"selected experts\" can and do change from token to token. \n\nAre you going to swap the experts in and out of VRAM every token? What do you think the latency on that would be over PCIE? Lol.\n\nhttps://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n510q8r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The &amp;quot;selected experts&amp;quot; can and do change from token to token. &lt;/p&gt;\n\n&lt;p&gt;Are you going to swap the experts in and out of VRAM every token? What do you think the latency on that would be over PCIE? Lol.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts\"&gt;https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n510q8r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753416249,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8oz07",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n51a6v0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kevin_1994",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n512r15",
                                "score": 2,
                                "author_fullname": "t2_o015g",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "https://github.com/k-koehler/gguf-tensor-overrider\n\nI wrote a tool for this which seems to work pretty well. It should automatically allocate the optimal tensors for you. \n\nIm realizing (after using it for a couple months) I have to tweak it a little bit, but it currently works very well in 99% of circumstances",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n51a6v0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/k-koehler/gguf-tensor-overrider\"&gt;https://github.com/k-koehler/gguf-tensor-overrider&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wrote a tool for this which seems to work pretty well. It should automatically allocate the optimal tensors for you. &lt;/p&gt;\n\n&lt;p&gt;Im realizing (after using it for a couple months) I have to tweak it a little bit, but it currently works very well in 99% of circumstances&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": true,
                                "can_gild": false,
                                "link_id": "t3_1m8oz07",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n51a6v0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753420518,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753420518,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n51atsw",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Highwaytothebeach",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n518f01",
                                          "score": 1,
                                          "author_fullname": "t2_1qychuraq9",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I appreciate it, but IMO still you have very partial control of it. What I was thinking it should be possible to do something selectively offloading to GPU and CPU till it is almost full and LOCK it there. everything else that doesn't fit it stays out on ssd possibly covered) with mmap and doesn't mess with anything (the most important stuff) already LOCKED  in RAM and GPU...\n\nsomething like :\n\n\\-ngl 0\n\n\\-ot whatever =GPU\"   and LOCK it there  \n\\-ot whatever =CPU\" and LOCK it there",
                                          "edited": 1753421391,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n51atsw",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I appreciate it, but IMO still you have very partial control of it. What I was thinking it should be possible to do something selectively offloading to GPU and CPU till it is almost full and LOCK it there. everything else that doesn&amp;#39;t fit it stays out on ssd possibly covered) with mmap and doesn&amp;#39;t mess with anything (the most important stuff) already LOCKED  in RAM and GPU...&lt;/p&gt;\n\n&lt;p&gt;something like :&lt;/p&gt;\n\n&lt;p&gt;-ngl 0&lt;/p&gt;\n\n&lt;p&gt;-ot whatever =GPU&amp;quot;   and LOCK it there&lt;br/&gt;\n-ot whatever =CPU&amp;quot; and LOCK it there&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m8oz07",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n51atsw/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753420830,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753420830,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n518f01",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Double_Cause4609",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n512r15",
                                "score": 1,
                                "author_fullname": "t2_1kubzxt2ww",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "search up -ot and --tensor-override regex.\n\nThere's a few examples on the internet for a variety of setups. The most common is just to do something like\n\n\\-ngl 99 \\\\  \n\\-ot \"(\\[0-9\\]\\[0-9\\]).ffn\\_.\\*\\_exps.=CPU\"\n\nThe above works for Scout and Deepseek style MoEs to do the offloading pattern I described above (offload shared expert, Attention, and KV Cache to GPU, all conditional experts to CPU) which by default is what you want to do.\n\nFor Qwen 3 235B, I manually figured out how many layers I could offload to GPU, and reversed that to get the number of FFN I needed to throw on CPU with\n\n\\-ot \"(5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78|79|80|81|82|83|84|85|86|87)+.ffn\\_.\\*\\_exps.=CPU\" \n\nFor example. There are other regexes floating around for specific things. If you want, you can launch a model with the verbose flag in LlamaCPP and it'll show you the names of various tensors for use with tensor override.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n518f01",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;search up -ot and --tensor-override regex.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a few examples on the internet for a variety of setups. The most common is just to do something like&lt;/p&gt;\n\n&lt;p&gt;-ngl 99 \\&lt;br/&gt;\n-ot &amp;quot;([0-9][0-9]).ffn_.*_exps.=CPU&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The above works for Scout and Deepseek style MoEs to do the offloading pattern I described above (offload shared expert, Attention, and KV Cache to GPU, all conditional experts to CPU) which by default is what you want to do.&lt;/p&gt;\n\n&lt;p&gt;For Qwen 3 235B, I manually figured out how many layers I could offload to GPU, and reversed that to get the number of FFN I needed to throw on CPU with&lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;(5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78|79|80|81|82|83|84|85|86|87)+.ffn_.*_exps.=CPU&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;For example. There are other regexes floating around for specific things. If you want, you can launch a model with the verbose flag in LlamaCPP and it&amp;#39;ll show you the names of various tensors for use with tensor override.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8oz07",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n518f01/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753419673,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753419673,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n512r15",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Highwaytothebeach",
                      "can_mod_post": false,
                      "created_utc": 1753417116,
                      "send_replies": true,
                      "parent_id": "t1_n51000x",
                      "score": 2,
                      "author_fullname": "t2_1qychuraq9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "There seems to be very vague or Insufficient documentation for overriding tensors. For example, I would like to  load the Attention and KV cache onto GPU, and shared expert (active for every token) in ram ( CPU), while everything else stays just  on ssd,  just for start. and experimenting.  How to do that with -ot ?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n512r15",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There seems to be very vague or Insufficient documentation for overriding tensors. For example, I would like to  load the Attention and KV cache onto GPU, and shared expert (active for every token) in ram ( CPU), while everything else stays just  on ssd,  just for start. and experimenting.  How to do that with -ot ?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8oz07",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n512r15/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753417116,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n51000x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753415940,
            "send_replies": true,
            "parent_id": "t3_1m8oz07",
            "score": 2,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why are you even loading the selected expert into VRAM?\n\nIf the model is already loaded into RAM, the memory bandwidth on the CPU is larger than the interconnect between CPU and GPU. As a result, it's actually faster to run the activated expert on the CPU. This is because the part of the network in an MoE that's conditional is the FFN, which needs to load each parameter once to complete the calculation (meaning that it's memory bound; the number of memory accesses determines the processing speed).\n\nA better solution is something like...You just load conditional experts to CPU (The ones where you don't know which will be selected).\n\nYou load the Attention and KV cache onto GPU (this is the part that determines prompt processing times. It's generally compute bound, and the GPU has waaaaaay more compute).\n\nIf the model has a shared expert (active for every token) you throw it on GPU, which means that you're actually putting a very small amount of computation on the CPU.\n\nWith all of these together, you can run a model like Llama 4 Maverick on a consumer setup at around 10 T/s, and even Deepseek V3 at 3 T/s (and of course, if you'd like to build a workstation or buy a used server, for around $2,500 or $3,500 you can get a system that runs it at around 7-15T/s depending on exactly what you buy).\n\nAnd guess what? This is already possible. LlamaCPP lets you do this manually with tensor overrides, and KTransformers is built specifically for this type of optimization.\n\nThis approach is easier to program, faster, is already implemented in commodity inference platforms, and offers the best balance of price to performance.\n\nIn the case of LlamaCPP specifically, because it uses mmap(), in a Linux based environment, even if you don't have enough memory to load all the experts, it only swaps out experts when it absolutely has to, so as long as you can load one full \"vertical chunk\" (all the selected experts) of the model, you end up only needing to stream the experts that change (per layer) between tokens. Not that many experts swap between tokens (only like, 30-50% based on the performance I get), which works out to only needing to stream a few GB from your SSD per token.\n\nWe already have a great solution.\n\nI don't know why you want to load the selected expert onto GPU. It's slower, harder, and unsupported.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n51000x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why are you even loading the selected expert into VRAM?&lt;/p&gt;\n\n&lt;p&gt;If the model is already loaded into RAM, the memory bandwidth on the CPU is larger than the interconnect between CPU and GPU. As a result, it&amp;#39;s actually faster to run the activated expert on the CPU. This is because the part of the network in an MoE that&amp;#39;s conditional is the FFN, which needs to load each parameter once to complete the calculation (meaning that it&amp;#39;s memory bound; the number of memory accesses determines the processing speed).&lt;/p&gt;\n\n&lt;p&gt;A better solution is something like...You just load conditional experts to CPU (The ones where you don&amp;#39;t know which will be selected).&lt;/p&gt;\n\n&lt;p&gt;You load the Attention and KV cache onto GPU (this is the part that determines prompt processing times. It&amp;#39;s generally compute bound, and the GPU has waaaaaay more compute).&lt;/p&gt;\n\n&lt;p&gt;If the model has a shared expert (active for every token) you throw it on GPU, which means that you&amp;#39;re actually putting a very small amount of computation on the CPU.&lt;/p&gt;\n\n&lt;p&gt;With all of these together, you can run a model like Llama 4 Maverick on a consumer setup at around 10 T/s, and even Deepseek V3 at 3 T/s (and of course, if you&amp;#39;d like to build a workstation or buy a used server, for around $2,500 or $3,500 you can get a system that runs it at around 7-15T/s depending on exactly what you buy).&lt;/p&gt;\n\n&lt;p&gt;And guess what? This is already possible. LlamaCPP lets you do this manually with tensor overrides, and KTransformers is built specifically for this type of optimization.&lt;/p&gt;\n\n&lt;p&gt;This approach is easier to program, faster, is already implemented in commodity inference platforms, and offers the best balance of price to performance.&lt;/p&gt;\n\n&lt;p&gt;In the case of LlamaCPP specifically, because it uses mmap(), in a Linux based environment, even if you don&amp;#39;t have enough memory to load all the experts, it only swaps out experts when it absolutely has to, so as long as you can load one full &amp;quot;vertical chunk&amp;quot; (all the selected experts) of the model, you end up only needing to stream the experts that change (per layer) between tokens. Not that many experts swap between tokens (only like, 30-50% based on the performance I get), which works out to only needing to stream a few GB from your SSD per token.&lt;/p&gt;\n\n&lt;p&gt;We already have a great solution.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know why you want to load the selected expert onto GPU. It&amp;#39;s slower, harder, and unsupported.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8oz07/why_there_is_still_no_a_proper_or_helpful/n51000x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753415940,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8oz07",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]