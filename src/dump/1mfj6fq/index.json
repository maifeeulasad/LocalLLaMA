[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "What's the current recommended local LLM inference HW (**local, always-on inference box)** for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.  \nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.\n\n**What are the Hardware Options?:**\n\n* **Apple M3/M4 Ultra**\n* **AMD AI Max+ 395**\n* NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?\n\nWhat’s the most **practical prosumer option**?  \nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  \n\n\nI'm favoring the Strix Halo, but I think I'll be limited to Gemma 27B with maybe another model loaded at best.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "24/7 local HW buying guide 2025-H2?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfj6fq",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_l0ba7",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754114575,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the current recommended local LLM inference HW (&lt;strong&gt;local, always-on inference box)&lt;/strong&gt; for multimodal LLMs (text, image, audio). Target workloads include home automation agents, real-time coding/writing, and vision models.&lt;br/&gt;\nGoal is obviously largest models and the highest t/s, so highest VRAM and bandwidth, but with a toolchain that works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are the Hardware Options?:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Apple M3/M4 Ultra&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AMD AI Max+ 395&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;NVIDIA (DGX-Spark, etc.) or is Spark vaporware waiting for scalpers?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What’s the most &lt;strong&gt;practical prosumer option&lt;/strong&gt;?&lt;br/&gt;\nIt would need to be lower cost than an RTX PRO 6000 Blackwell. I guess one could build an efficient mITX case around it, but I refuse to be price gouged by Nvidia.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m favoring the Strix Halo, but I think I&amp;#39;ll be limited to Gemma 27B with maybe another model loaded at best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mfj6fq",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "xraybies",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/",
            "subreddit_subscribers": 509054,
            "created_utc": 1754114575,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6i5ioy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "prusswan",
            "can_mod_post": false,
            "created_utc": 1754129693,
            "send_replies": true,
            "parent_id": "t3_1mfj6fq",
            "score": 2,
            "author_fullname": "t2_kegwk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "one of those modded GPUs from China, or used",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6i5ioy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;one of those modded GPUs from China, or used&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6i5ioy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754129693,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfj6fq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6hvbe2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "xraybies",
                      "can_mod_post": false,
                      "created_utc": 1754123494,
                      "send_replies": true,
                      "parent_id": "t1_n6hk51x",
                      "score": 0,
                      "author_fullname": "t2_l0ba7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the reply.   \nI don’t see a contradiction in what I asked. I want the best possible tradeoff between performance, efficiency, and cost. That means maximizing VRAM and tokens per second per watt, within a reasonable power envelope and budget.\n\nI’m agnostic to quantization. If Q4 models meet the performance and accuracy thresholds, that’s perfectly acceptable. My question is about which hardware currently delivers the most practical and sustainable inference performance.\n\nIf you have insights into **hardware** configurations that meet those criteria, I’d be very interested.  \n  \nTo clarify:\n\n* I’m agnostic to quantization; Q4, Q8, BF16, whatever gives the best **tokens/sec per watt** and accurate results.\n* This is strictly for **inference**, not training.\n* Cost &lt;US$5k\n* I need hardware that can run **24/7**, ideally with **low power draw** (&lt;.7 kW total system 100% load), and last **at least a few years**.\n* **≥10 t/s**.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6hvbe2",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the reply.&lt;br/&gt;\nI don’t see a contradiction in what I asked. I want the best possible tradeoff between performance, efficiency, and cost. That means maximizing VRAM and tokens per second per watt, within a reasonable power envelope and budget.&lt;/p&gt;\n\n&lt;p&gt;I’m agnostic to quantization. If Q4 models meet the performance and accuracy thresholds, that’s perfectly acceptable. My question is about which hardware currently delivers the most practical and sustainable inference performance.&lt;/p&gt;\n\n&lt;p&gt;If you have insights into &lt;strong&gt;hardware&lt;/strong&gt; configurations that meet those criteria, I’d be very interested.  &lt;/p&gt;\n\n&lt;p&gt;To clarify:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I’m agnostic to quantization; Q4, Q8, BF16, whatever gives the best &lt;strong&gt;tokens/sec per watt&lt;/strong&gt; and accurate results.&lt;/li&gt;\n&lt;li&gt;This is strictly for &lt;strong&gt;inference&lt;/strong&gt;, not training.&lt;/li&gt;\n&lt;li&gt;Cost &amp;lt;US$5k&lt;/li&gt;\n&lt;li&gt;I need hardware that can run &lt;strong&gt;24/7&lt;/strong&gt;, ideally with &lt;strong&gt;low power draw&lt;/strong&gt; (&amp;lt;.7 kW total system 100% load), and last &lt;strong&gt;at least a few years&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;≥10 t/s&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfj6fq",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6hvbe2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754123494,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6hk51x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754117026,
            "send_replies": true,
            "parent_id": "t3_1mfj6fq",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Your criteria are self-contradictory, and you imply that you would not use quantized models, but why?\n\nPerhaps we can help you if you state clearly:\n\n* The largest model you want to be able to run,\n\n* The lowest performance in tokens/second you would tolerate,\n\n* Your budgetary limit,\n\n* What quantization you are willing to use (weights: BF16? Q8? Q4? Q3? Q2? kvcache: F16? Q8? Q4?)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6hk51x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your criteria are self-contradictory, and you imply that you would not use quantized models, but why?&lt;/p&gt;\n\n&lt;p&gt;Perhaps we can help you if you state clearly:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The largest model you want to be able to run,&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The lowest performance in tokens/second you would tolerate,&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your budgetary limit,&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What quantization you are willing to use (weights: BF16? Q8? Q4? Q3? Q2? kvcache: F16? Q8? Q4?)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6hk51x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754117026,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mfj6fq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6ilabd",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "MelodicRecognition7",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6ijwn9",
                                                    "score": 1,
                                                    "author_fullname": "t2_1eex9ug5",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I don't personally own a Mac and never will so DYOR. Based on https://old.reddit.com/r/LocalLLaMA/search?q=m3+ultra+gemma&amp;restrict_sr=on&amp;sort=relevance&amp;t=all it runs Gemma3 at 11 tps BF16 and 30 tps Q4, so Q8 should be around 15 tps, though these are only text prompt/generation benchmarks and do not count image recognition.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6ilabd",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t personally own a Mac and never will so DYOR. Based on &lt;a href=\"https://old.reddit.com/r/LocalLLaMA/search?q=m3+ultra+gemma&amp;amp;restrict_sr=on&amp;amp;sort=relevance&amp;amp;t=all\"&gt;https://old.reddit.com/r/LocalLLaMA/search?q=m3+ultra+gemma&amp;amp;restrict_sr=on&amp;amp;sort=relevance&amp;amp;t=all&lt;/a&gt; it runs Gemma3 at 11 tps BF16 and 30 tps Q4, so Q8 should be around 15 tps, though these are only text prompt/generation benchmarks and do not count image recognition.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfj6fq",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6ilabd/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754137388,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754137388,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6ijwn9",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "xraybies",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6ianq5",
                                          "score": 1,
                                          "author_fullname": "t2_l0ba7",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "What about an **M3 Ultra** 96GB (819GB/s) it's 30% less than just the RTX Pro 5000 and available.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6ijwn9",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What about an &lt;strong&gt;M3 Ultra&lt;/strong&gt; 96GB (819GB/s) it&amp;#39;s 30% less than just the RTX Pro 5000 and available.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfj6fq",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6ijwn9/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754136819,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754136819,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6jcvjb",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "colin_colout",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6ianq5",
                                          "score": 1,
                                          "author_fullname": "t2_14l4ya",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "If you go igpu at decent speeds, you'll want to focus on MoE models (but you'll still run out of memory if you're at 96-128gb.\n\nYou're looking at qwen3-30b type models if you want yo avoid the perplexity issues with bit-crushing your models to 1-3bit quants. \n\nThe qwen3-235 MoE will barely fit into unified memory at 1bit gguf. \n\nIt's hard to get it all. You'll probably do better with a used server mobo with lots of memory channels and a decent GPU, but you'll need to tweak llama.cpp / vllm parameters manually for each model you run (Ollama will be a bad experience). \n\n...or you can do what I did and get a minipc with an 8845hs (780m igpu) or similar. I loaded a barebones ser8 with 128gb of 5600mhz ram and can  usually tune llama.cpp to get more than half the speed of what people are reporting with strix halo on the models I like (strix halo has shit rocm support, so expect this gap to widen)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6jcvjb",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you go igpu at decent speeds, you&amp;#39;ll want to focus on MoE models (but you&amp;#39;ll still run out of memory if you&amp;#39;re at 96-128gb.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re looking at qwen3-30b type models if you want yo avoid the perplexity issues with bit-crushing your models to 1-3bit quants. &lt;/p&gt;\n\n&lt;p&gt;The qwen3-235 MoE will barely fit into unified memory at 1bit gguf. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s hard to get it all. You&amp;#39;ll probably do better with a used server mobo with lots of memory channels and a decent GPU, but you&amp;#39;ll need to tweak llama.cpp / vllm parameters manually for each model you run (Ollama will be a bad experience). &lt;/p&gt;\n\n&lt;p&gt;...or you can do what I did and get a minipc with an 8845hs (780m igpu) or similar. I loaded a barebones ser8 with 128gb of 5600mhz ram and can  usually tune llama.cpp to get more than half the speed of what people are reporting with strix halo on the models I like (strix halo has shit rocm support, so expect this gap to widen)&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfj6fq",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6jcvjb/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754147254,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754147254,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ianq5",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "MelodicRecognition7",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6i6y68",
                                "score": 1,
                                "author_fullname": "t2_1eex9ug5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Halo Strix has 256 GB/s memory bandwidth so it will serve Gemma 27B Q8 at less than 10 tokens per second, add multimodality and context and it will become an unusable 5 tokens per second, RTX Pro 5000 has 1344 GB/s memory bandwidth so it will serve Gemma 27B Q8 at 50 tokens per second, add multimodality and context and it still will be very good 20 tokens per second, faster than you could read.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ianq5",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Halo Strix has 256 GB/s memory bandwidth so it will serve Gemma 27B Q8 at less than 10 tokens per second, add multimodality and context and it will become an unusable 5 tokens per second, RTX Pro 5000 has 1344 GB/s memory bandwidth so it will serve Gemma 27B Q8 at 50 tokens per second, add multimodality and context and it still will be very good 20 tokens per second, faster than you could read.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfj6fq",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6ianq5/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754132513,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754132513,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6i6y68",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "xraybies",
                      "can_mod_post": false,
                      "created_utc": 1754130509,
                      "send_replies": true,
                      "parent_id": "t1_n6hwbgb",
                      "score": 1,
                      "author_fullname": "t2_l0ba7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "But would this be better for the use case than the Halo Strix 128GB for 70% less? Then you have to add the 24/7 consumption.  \nIn my neck of the woods it's also &gt;US$5k with a PC.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6i6y68",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;But would this be better for the use case than the Halo Strix 128GB for 70% less? Then you have to add the 24/7 consumption.&lt;br/&gt;\nIn my neck of the woods it&amp;#39;s also &amp;gt;US$5k with a PC.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfj6fq",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6i6y68/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754130509,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6hwbgb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754124112,
            "send_replies": true,
            "parent_id": "t3_1mfj6fq",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "RTX Pro 5000 48 GB + cheapest possible motherboard+cpu+ram and expensive high quality psu lol",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6hwbgb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;RTX Pro 5000 48 GB + cheapest possible motherboard+cpu+ram and expensive high quality psu lol&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfj6fq/247_local_hw_buying_guide_2025h2/n6hwbgb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754124112,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfj6fq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]