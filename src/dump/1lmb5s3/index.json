[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;format=png&amp;auto=webp&amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a\n\nHey everyone!  \nWeâ€™ve made it to Day 5 of the *50 Days of Building a Small Language Model from Scratch* journey.\n\nSo far, weâ€™ve covered the basics of what a small language model is, built our own tokenizer from scratch, and identified a major pain point: handling unknown or rare words. Thatâ€™s where today's Byte Pair Encoding (BPE) comes in\n\nInstead of creating everything from the ground up, weâ€™ve now switched gears to use OpenAIâ€™s `tiktoken` library, which powers the GPT-2 tokenizer. It's fast, memory-efficient, and trained on a broad range of English text, making it perfect for small to mid-size model experiments.\n\nBut weâ€™re not just plugging in a tokenizer. Weâ€™re also designing it for storytelling use cases. That means adding special tokens like `&lt;|startofstory|&gt;` and `&lt;|title|&gt;` to guide our model and give it a narrative structure. These little markers help the model \"think\" like a storyteller.\n\nBefore tokenization occurs, we run a cleaning step that normalizes text, trims unnecessary whitespace, and converts it to lowercase, ensuring our inputs are clean and consistent. Itâ€™s a small step that makes a big difference.\n\nThis is how we process the data:\n\n* Each sample gets wrapped with special tokens.\n* We tokenize with error handling.\n* We cap token sequences at 1024 to fit the GPT-2 context window.\n\nFrom there, we move on to dataset loading. Weâ€™re using a curated collection of childrenâ€™s stories and filtering them by token length to ensure quality inputs. We split everything into train, validation, and fine-tune subsets.\n\nThen comes the heavy lifting:  \nWe tokenize the dataset using 8 parallel processes and store the results in binary format using memory-mapped NumPy arrays. This setup enables us to efficiently read large datasets during training without encountering memory issues.\n\nâœ… **Wrapping Up Week 1**  \nWith BPE and `tiktoken`Weâ€™ve built a solid, scalable preprocessing pipeline tailored for training small LLMs. Next week, we start tackling the model itself.\n\nðŸ”— Complete blog: [https://www.ideaweaver.ai/blog/day5.html](https://www.ideaweaver.ai/blog/day5.html)\n\nThanks for following along. If you're building your own LLM or are just curious about the process, feel free to drop a comment on LinkedIn. I'm always happy to chat!\n\nStay tuned, and have a great weekend! ðŸš€  \nâ€” Prashant Lakhera",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Day 5/50] Building a Small Language Model from Scratch - Byte Pair Encoding with tiktoken",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 140,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "0579fta7pk9f1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 122,
                    "x": 108,
                    "u": "https://preview.redd.it/0579fta7pk9f1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7323d1070db04616da69d1e0f8e9788748237a8c"
                  },
                  {
                    "y": 245,
                    "x": 216,
                    "u": "https://preview.redd.it/0579fta7pk9f1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1d3cd5fe7762791390519aee3884107613f506e7"
                  },
                  {
                    "y": 363,
                    "x": 320,
                    "u": "https://preview.redd.it/0579fta7pk9f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=69a68a9e80f7f178460b210bead4def3bf89fb6b"
                  },
                  {
                    "y": 726,
                    "x": 640,
                    "u": "https://preview.redd.it/0579fta7pk9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9060aff70fd582e10c46c66ff3fb2ab6a0efb8e7"
                  }
                ],
                "s": {
                  "y": 1078,
                  "x": 950,
                  "u": "https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;format=png&amp;auto=webp&amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a"
                },
                "id": "0579fta7pk9f1"
              }
            },
            "name": "t3_1lmb5s3",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.87,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 37,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_8ht7a116",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 37,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://a.thumbs.redditmedia.com/bZuU8fFxF1Xzopfioxwh3cANnUztVf0ibuxGQD5IFI4.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1751075270,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a\"&gt;https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey everyone!&lt;br/&gt;\nWeâ€™ve made it to Day 5 of the &lt;em&gt;50 Days of Building a Small Language Model from Scratch&lt;/em&gt; journey.&lt;/p&gt;\n\n&lt;p&gt;So far, weâ€™ve covered the basics of what a small language model is, built our own tokenizer from scratch, and identified a major pain point: handling unknown or rare words. Thatâ€™s where today&amp;#39;s Byte Pair Encoding (BPE) comes in&lt;/p&gt;\n\n&lt;p&gt;Instead of creating everything from the ground up, weâ€™ve now switched gears to use OpenAIâ€™s &lt;code&gt;tiktoken&lt;/code&gt; library, which powers the GPT-2 tokenizer. It&amp;#39;s fast, memory-efficient, and trained on a broad range of English text, making it perfect for small to mid-size model experiments.&lt;/p&gt;\n\n&lt;p&gt;But weâ€™re not just plugging in a tokenizer. Weâ€™re also designing it for storytelling use cases. That means adding special tokens like &lt;code&gt;&amp;lt;|startofstory|&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;|title|&amp;gt;&lt;/code&gt; to guide our model and give it a narrative structure. These little markers help the model &amp;quot;think&amp;quot; like a storyteller.&lt;/p&gt;\n\n&lt;p&gt;Before tokenization occurs, we run a cleaning step that normalizes text, trims unnecessary whitespace, and converts it to lowercase, ensuring our inputs are clean and consistent. Itâ€™s a small step that makes a big difference.&lt;/p&gt;\n\n&lt;p&gt;This is how we process the data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Each sample gets wrapped with special tokens.&lt;/li&gt;\n&lt;li&gt;We tokenize with error handling.&lt;/li&gt;\n&lt;li&gt;We cap token sequences at 1024 to fit the GPT-2 context window.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;From there, we move on to dataset loading. Weâ€™re using a curated collection of childrenâ€™s stories and filtering them by token length to ensure quality inputs. We split everything into train, validation, and fine-tune subsets.&lt;/p&gt;\n\n&lt;p&gt;Then comes the heavy lifting:&lt;br/&gt;\nWe tokenize the dataset using 8 parallel processes and store the results in binary format using memory-mapped NumPy arrays. This setup enables us to efficiently read large datasets during training without encountering memory issues.&lt;/p&gt;\n\n&lt;p&gt;âœ… &lt;strong&gt;Wrapping Up Week 1&lt;/strong&gt;&lt;br/&gt;\nWith BPE and &lt;code&gt;tiktoken&lt;/code&gt;Weâ€™ve built a solid, scalable preprocessing pipeline tailored for training small LLMs. Next week, we start tackling the model itself.&lt;/p&gt;\n\n&lt;p&gt;ðŸ”— Complete blog: &lt;a href=\"https://www.ideaweaver.ai/blog/day5.html\"&gt;https://www.ideaweaver.ai/blog/day5.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for following along. If you&amp;#39;re building your own LLM or are just curious about the process, feel free to drop a comment on LinkedIn. I&amp;#39;m always happy to chat!&lt;/p&gt;\n\n&lt;p&gt;Stay tuned, and have a great weekend! ðŸš€&lt;br/&gt;\nâ€” Prashant Lakhera&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lmb5s3",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Prashant-Lakhera",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/",
            "subreddit_subscribers": 492572,
            "created_utc": 1751075270,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n09sh6m",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Prashant-Lakhera",
                      "can_mod_post": false,
                      "created_utc": 1751131383,
                      "send_replies": true,
                      "parent_id": "t1_n07g48v",
                      "score": 2,
                      "author_fullname": "t2_8ht7a116",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Both models utilize BPE, as mentioned above, to avoid unknown tokens.\n\nGPT-4o: 200 k-entry BPE tokenizer (o200k\\_base)  \nDeepSeek: 102 k-entry BPE tokenizer tuned for English + Chinese.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n09sh6m",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Both models utilize BPE, as mentioned above, to avoid unknown tokens.&lt;/p&gt;\n\n&lt;p&gt;GPT-4o: 200 k-entry BPE tokenizer (o200k_base)&lt;br/&gt;\nDeepSeek: 102 k-entry BPE tokenizer tuned for English + Chinese.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lmb5s3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/n09sh6m/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751131383,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n07g48v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "JadedFig5848",
            "can_mod_post": false,
            "created_utc": 1751096510,
            "send_replies": true,
            "parent_id": "t3_1lmb5s3",
            "score": 1,
            "author_fullname": "t2_1oo56jrgsb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Btw what tokenizer does gpt4o and DeepSeek use?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n07g48v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Btw what tokenizer does gpt4o and DeepSeek use?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/n07g48v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751096510,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lmb5s3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "total_awards_received": 0,
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "ups": 1,
            "removal_reason": null,
            "link_id": "t3_1lmb5s3",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n09rmp6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Prashant-Lakhera",
                      "can_mod_post": false,
                      "created_utc": 1751131126,
                      "send_replies": true,
                      "parent_id": "t1_n09ccie",
                      "score": 1,
                      "author_fullname": "t2_8ht7a116",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I just tried, and it looks good to me\n\n[https://www.ideaweaver.ai/blog/day5.html](https://www.ideaweaver.ai/blog/day5.html)\n\nDay 1: [https://www.ideaweaver.ai/blog/day1.html](https://www.ideaweaver.ai/blog/day1.html)\n\nDay 2: [https://www.ideaweaver.ai/blog/day2.html](https://www.ideaweaver.ai/blog/day2.html)\n\nDay 3: [https://www.ideaweaver.ai/blog/day3.html](https://www.ideaweaver.ai/blog/day3.html)\n\nDay 4: [https://www.ideaweaver.ai/blog/day4.html](https://www.ideaweaver.ai/blog/day4.html)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n09rmp6",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just tried, and it looks good to me&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.ideaweaver.ai/blog/day5.html\"&gt;https://www.ideaweaver.ai/blog/day5.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Day 1: &lt;a href=\"https://www.ideaweaver.ai/blog/day1.html\"&gt;https://www.ideaweaver.ai/blog/day1.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Day 2: &lt;a href=\"https://www.ideaweaver.ai/blog/day2.html\"&gt;https://www.ideaweaver.ai/blog/day2.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Day 3: &lt;a href=\"https://www.ideaweaver.ai/blog/day3.html\"&gt;https://www.ideaweaver.ai/blog/day3.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Day 4: &lt;a href=\"https://www.ideaweaver.ai/blog/day4.html\"&gt;https://www.ideaweaver.ai/blog/day4.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lmb5s3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/n09rmp6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751131126,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n09ccie",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "DELETED",
            "no_follow": true,
            "author": "[deleted]",
            "can_mod_post": false,
            "send_replies": true,
            "parent_id": "t3_1lmb5s3",
            "score": 1,
            "approved_by": null,
            "report_reasons": null,
            "all_awardings": [],
            "subreddit_id": "t5_81eyvm",
            "body": "[deleted]",
            "edited": false,
            "downs": 0,
            "author_flair_css_class": null,
            "collapsed": true,
            "is_submitter": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;",
            "gildings": {},
            "collapsed_reason": null,
            "associated_award": null,
            "stickied": false,
            "subreddit_type": "public",
            "can_gild": false,
            "top_awarded_type": null,
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/n09ccie/",
            "num_reports": null,
            "locked": false,
            "name": "t1_n09ccie",
            "created": 1751126312,
            "subreddit": "LocalLLaMA",
            "author_flair_text": null,
            "treatment_tags": [],
            "created_utc": 1751126312,
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "mod_note": null,
            "distinguished": null
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n0cy37s",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BansheeCraft",
            "can_mod_post": false,
            "created_utc": 1751172626,
            "send_replies": true,
            "parent_id": "t3_1lmb5s3",
            "score": 1,
            "author_fullname": "t2_410sx06v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Wow, this is brilliant. I'll be watching out for the next installment. Many thanks.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0cy37s",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow, this is brilliant. I&amp;#39;ll be watching out for the next installment. Many thanks.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/n0cy37s/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751172626,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lmb5s3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]