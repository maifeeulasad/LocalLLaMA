[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’m trying to repurpose my desktop as a local LLM server. Specs are:\n\n* Ryzen 7 (1st gen)\n* 48GB RAM\n* RTX 3060 12GB\n\nI want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.\n\nHere’s what I’ve tried so far:\n\n* Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. `nvidia-smi` was hit or miss.\n* Tried setting up [vLLM](https://github.com/vllm-project/vllm), but `vllm serve` throws various errors I couldn’t resolve.\n* Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.\n\nAfter 5+ hours of debugging and failure, I’m feeling stuck.\n\nCan anyone share their setup that *actually works*? I’m looking for:\n\n1. A reliable way to get the RTX 3060 working with GPU acceleration.\n2. A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.\n3. Bonus: A way to expose the model server over LAN for remote clients.\n\nThanks in advance",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Need help setting up a local LLM server with RTX 3060",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1lvm3kv",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_54sxk5i5",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752075816,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m trying to repurpose my desktop as a local LLM server. Specs are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Ryzen 7 (1st gen)&lt;/li&gt;\n&lt;li&gt;48GB RAM&lt;/li&gt;\n&lt;li&gt;RTX 3060 12GB&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.&lt;/p&gt;\n\n&lt;p&gt;Here’s what I’ve tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. &lt;code&gt;nvidia-smi&lt;/code&gt; was hit or miss.&lt;/li&gt;\n&lt;li&gt;Tried setting up &lt;a href=\"https://github.com/vllm-project/vllm\"&gt;vLLM&lt;/a&gt;, but &lt;code&gt;vllm serve&lt;/code&gt; throws various errors I couldn’t resolve.&lt;/li&gt;\n&lt;li&gt;Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;After 5+ hours of debugging and failure, I’m feeling stuck.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share their setup that &lt;em&gt;actually works&lt;/em&gt;? I’m looking for:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A reliable way to get the RTX 3060 working with GPU acceleration.&lt;/li&gt;\n&lt;li&gt;A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.&lt;/li&gt;\n&lt;li&gt;Bonus: A way to expose the model server over LAN for remote clients.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?auto=webp&amp;s=cab466f000a9569548ebd3ae8abfd85c32ee31f1",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38d7d905f6a5896e20f689af4bc05612592cc000",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=568fa566629c29f2e0bb183fde6d812148f6062a",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=adeec7f214bf0e350cda96ec601ac78d5bc9f67a",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aabc62c14eb9789f323eecff0f6dff014c9b9b8",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b945a80304a01af644621d72f808cfcac8d23284",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71dcd25b6ef4ccfb096088be00e7b463415d2f2c",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lvm3kv",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Watch-D0g",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/",
            "subreddit_subscribers": 496591,
            "created_utc": 1752075816,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]