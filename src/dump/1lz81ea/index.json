[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’m looking for LLMs to generate questions and answers from physics textbook chapters. The chapters I’ll provide can be up to 10 pages long and may include images. I’ve tried GPT, but the question quality is poor and often too similar to the examples I give. Claude didn’t work either as it rejects the input file, saying it’s too large. Which LLM model would you recommend me to try next? It doesn’t have to be free. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Which LLM should I use to generate high quality Q&amp;A from physics textbook chapters?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lz81ea",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 27,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_3g2onktn",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 27,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752451840,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking for LLMs to generate questions and answers from physics textbook chapters. The chapters I’ll provide can be up to 10 pages long and may include images. I’ve tried GPT, but the question quality is poor and often too similar to the examples I give. Claude didn’t work either as it rejects the input file, saying it’s too large. Which LLM model would you recommend me to try next? It doesn’t have to be free. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lz81ea",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "WhiteTentacle",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/",
            "subreddit_subscribers": 499297,
            "created_utc": 1752451840,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2zvdyy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "nguyenm",
            "can_mod_post": false,
            "created_utc": 1752454428,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 19,
            "author_fullname": "t2_o1y4c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think if you break up the textbook pdf into smaller chunks with just the chapter you want to cover, you'd get better results. This is because I am guessing you happen to upload the entire textbook which takes up too much context token.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2zvdyy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think if you break up the textbook pdf into smaller chunks with just the chapter you want to cover, you&amp;#39;d get better results. This is because I am guessing you happen to upload the entire textbook which takes up too much context token.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zvdyy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752454428,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 19
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2zvlns",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "SandboChang",
            "can_mod_post": false,
            "created_utc": 1752454505,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 5,
            "author_fullname": "t2_10icmj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For Physics questions and brainstorming, I found Gemini to be performing well (comparing to at least GPT, I saved Claude for coding so never used it that way).\n\nChatGPT kind of makes things up in many cases; Gemini does sometimes but much less often. Not sure if it translates to better Q&amp;A generation but it's worth trying. Otherwise, if your work include images and they are important, for local LLM you can try Qwen2.5 VLM 72B.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2zvlns",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For Physics questions and brainstorming, I found Gemini to be performing well (comparing to at least GPT, I saved Claude for coding so never used it that way).&lt;/p&gt;\n\n&lt;p&gt;ChatGPT kind of makes things up in many cases; Gemini does sometimes but much less often. Not sure if it translates to better Q&amp;amp;A generation but it&amp;#39;s worth trying. Otherwise, if your work include images and they are important, for local LLM you can try Qwen2.5 VLM 72B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zvlns/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752454505,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n300h75",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "My_Unbiased_Opinion",
            "can_mod_post": false,
            "created_utc": 1752456236,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 5,
            "author_fullname": "t2_esiyl0yb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Mistral 3.2. It has a solid vision model and the model does not hallucinate at the same level Gemma. It's also pretty good at math. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n300h75",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mistral 3.2. It has a solid vision model and the model does not hallucinate at the same level Gemma. It&amp;#39;s also pretty good at math. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n300h75/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752456236,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2zx22l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "GOGONUT6543",
                      "can_mod_post": false,
                      "created_utc": 1752455024,
                      "send_replies": true,
                      "parent_id": "t1_n2zurqz",
                      "score": 2,
                      "author_fullname": "t2_1a81regnc1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "here is an example:\n\n[https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221U4OYtkZtlZ1LdhPTx4Z6\\_CV447mdclFm%22%5D,%22action%22:%22open%22,%22userId%22:%22102966448311609286198%22,%22resourceKeys%22:%7B%7D%7D&amp;usp=sharing](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221U4OYtkZtlZ1LdhPTx4Z6_CV447mdclFm%22%5D,%22action%22:%22open%22,%22userId%22:%22102966448311609286198%22,%22resourceKeys%22:%7B%7D%7D&amp;usp=sharing)\n\nbased off of this PDF:\n\n[http://pearsonschoolsandfecolleges.co.uk/asset-library/pdf/Secondary/mathematics/edexcel-as-a-level-maths-and-further-maths/free-resources/9781292183350-AS-and-A-level-Further-Mathematics-Further-Pure-Mathematics-1-Textbook.pdf](http://pearsonschoolsandfecolleges.co.uk/asset-library/pdf/Secondary/mathematics/edexcel-as-a-level-maths-and-further-maths/free-resources/9781292183350-AS-and-A-level-Further-Mathematics-Further-Pure-Mathematics-1-Textbook.pdf)",
                      "edited": 1752456205,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2zx22l",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;here is an example:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221U4OYtkZtlZ1LdhPTx4Z6_CV447mdclFm%22%5D,%22action%22:%22open%22,%22userId%22:%22102966448311609286198%22,%22resourceKeys%22:%7B%7D%7D&amp;amp;usp=sharing\"&gt;https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221U4OYtkZtlZ1LdhPTx4Z6_CV447mdclFm%22%5D,%22action%22:%22open%22,%22userId%22:%22102966448311609286198%22,%22resourceKeys%22:%7B%7D%7D&amp;amp;usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;based off of this PDF:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://pearsonschoolsandfecolleges.co.uk/asset-library/pdf/Secondary/mathematics/edexcel-as-a-level-maths-and-further-maths/free-resources/9781292183350-AS-and-A-level-Further-Mathematics-Further-Pure-Mathematics-1-Textbook.pdf\"&gt;http://pearsonschoolsandfecolleges.co.uk/asset-library/pdf/Secondary/mathematics/edexcel-as-a-level-maths-and-further-maths/free-resources/9781292183350-AS-and-A-level-Further-Mathematics-Further-Pure-Mathematics-1-Textbook.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lz81ea",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zx22l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752455024,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2zurqz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GOGONUT6543",
            "can_mod_post": false,
            "created_utc": 1752454211,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 2,
            "author_fullname": "t2_1a81regnc1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "try aistudio gemini 2.5 pro. it has a 1 million token context window",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2zurqz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;try aistudio gemini 2.5 pro. it has a 1 million token context window&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zurqz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752454211,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2zslgt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "InevitableArea1",
            "can_mod_post": false,
            "created_utc": 1752453439,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 2,
            "author_fullname": "t2_giasjuouk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen3 32B 128k context, it's been my go to for similar tasks.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2zslgt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 32B 128k context, it&amp;#39;s been my go to for similar tasks.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zslgt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752453439,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n308rei",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TheRealMasonMac",
            "can_mod_post": false,
            "created_utc": 1752459231,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 1,
            "author_fullname": "t2_101haj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Gemini is very good for this. No other model is even half as good from my testing.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n308rei",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gemini is very good for this. No other model is even half as good from my testing.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n308rei/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752459231,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n30kjog",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DeepWisdomGuy",
            "can_mod_post": false,
            "created_utc": 1752463796,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 1,
            "author_fullname": "t2_lznk2wv8h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Prompt first for a list of specific topics that the chapter covers. Then prompt for a single question at a time focusing on one or two specific topics.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n30kjog",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Prompt first for a list of specific topics that the chapter covers. Then prompt for a single question at a time focusing on one or two specific topics.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n30kjog/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752463796,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n3115jh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "xadiant",
            "can_mod_post": false,
            "created_utc": 1752471439,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 1,
            "author_fullname": "t2_omgp6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A pipeline like this would be smarter:\n\nOCR &gt; formatted text &gt; chunked text &gt; fine-tuned model  &gt; dataset\n\nYou can produce high quality text to QA examples using ChatGPT and fine-tune a local Qwen3-8B",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3115jh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A pipeline like this would be smarter:&lt;/p&gt;\n\n&lt;p&gt;OCR &amp;gt; formatted text &amp;gt; chunked text &amp;gt; fine-tuned model  &amp;gt; dataset&lt;/p&gt;\n\n&lt;p&gt;You can produce high quality text to QA examples using ChatGPT and fine-tune a local Qwen3-8B&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n3115jh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752471439,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n31r42y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Melinda_McCartney",
            "can_mod_post": false,
            "created_utc": 1752486175,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 1,
            "author_fullname": "t2_4rdjjgjv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I  think you should try a model with a long context window like Gemini 2.5.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n31r42y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I  think you should try a model with a long context window like Gemini 2.5.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n31r42y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752486175,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n33s6cl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "KonradFreeman",
            "can_mod_post": false,
            "created_utc": 1752511873,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 1,
            "author_fullname": "t2_hlftmupu5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It totally could be free though.\n\nAll you would have to do is modify something like [https://github.com/adamwlarson/ai-book-writer/tree/OpenTale](https://github.com/adamwlarson/ai-book-writer/tree/OpenTale)\n\nso that you would use the story generation engine and then modify it to take in your PDFs or however you have the data saved and then use RAG to help with the creation of the material.\n\nYou could probably make it very detailed as OpenTale is more than just concatenating things broken down by a table of contents with multiple calls.\n\nIt has more entry points so you have to construct the context around it in order to really generate what you want.\n\nSo you could supply the context from a larger dataset whether it be dataframes or just a folder full of PDFs.\n\nAnd you could use LLaVA plus whichever language model you choose, if you run it all locally you could use MistralSmall3.2 which both run on the new Macbooks.\n\nAnd here is what chatGPT has to add to what I said:\n\nTo set up a free system for generating high-quality questions and answers from physics textbook chapters, you start by extracting the content from your PDFs using tools like PyMuPDF or Unstructured, breaking the text into manageable chunks. These chunks are then indexed using a local vector database such as ChromaDB or FAISS, enabling fast semantic retrieval based on relevance. You pair this with a capable open-source language model like Qwen2-7B, Mistral 7B, or LLaMA 2, which you can run locally using frameworks like Ollama or LM Studio. A Retrieval-Augmented Generation (RAG) pipeline—typically built using LangChain or LlamaIndex—retrieves the most relevant content from your indexed textbook data and feeds it into the model along with a carefully crafted prompt that instructs the model to generate conceptual questions, multiple-choice items, or worked problems with answers. This infrastructure gives you a powerful and cost-free workflow for turning dense educational material into structured, pedagogically useful Q&amp;A content.\n\nhttps://preview.redd.it/s20rrznjdvcf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=12187710862bb2f48fd13c59f0069fb5ef3c7465",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n33s6cl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It totally could be free though.&lt;/p&gt;\n\n&lt;p&gt;All you would have to do is modify something like &lt;a href=\"https://github.com/adamwlarson/ai-book-writer/tree/OpenTale\"&gt;https://github.com/adamwlarson/ai-book-writer/tree/OpenTale&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;so that you would use the story generation engine and then modify it to take in your PDFs or however you have the data saved and then use RAG to help with the creation of the material.&lt;/p&gt;\n\n&lt;p&gt;You could probably make it very detailed as OpenTale is more than just concatenating things broken down by a table of contents with multiple calls.&lt;/p&gt;\n\n&lt;p&gt;It has more entry points so you have to construct the context around it in order to really generate what you want.&lt;/p&gt;\n\n&lt;p&gt;So you could supply the context from a larger dataset whether it be dataframes or just a folder full of PDFs.&lt;/p&gt;\n\n&lt;p&gt;And you could use LLaVA plus whichever language model you choose, if you run it all locally you could use MistralSmall3.2 which both run on the new Macbooks.&lt;/p&gt;\n\n&lt;p&gt;And here is what chatGPT has to add to what I said:&lt;/p&gt;\n\n&lt;p&gt;To set up a free system for generating high-quality questions and answers from physics textbook chapters, you start by extracting the content from your PDFs using tools like PyMuPDF or Unstructured, breaking the text into manageable chunks. These chunks are then indexed using a local vector database such as ChromaDB or FAISS, enabling fast semantic retrieval based on relevance. You pair this with a capable open-source language model like Qwen2-7B, Mistral 7B, or LLaMA 2, which you can run locally using frameworks like Ollama or LM Studio. A Retrieval-Augmented Generation (RAG) pipeline—typically built using LangChain or LlamaIndex—retrieves the most relevant content from your indexed textbook data and feeds it into the model along with a carefully crafted prompt that instructs the model to generate conceptual questions, multiple-choice items, or worked problems with answers. This infrastructure gives you a powerful and cost-free workflow for turning dense educational material into structured, pedagogically useful Q&amp;amp;A content.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s20rrznjdvcf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12187710862bb2f48fd13c59f0069fb5ef3c7465\"&gt;https://preview.redd.it/s20rrznjdvcf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12187710862bb2f48fd13c59f0069fb5ef3c7465&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n33s6cl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752511873,
            "media_metadata": {
              "s20rrznjdvcf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 108,
                    "x": 108,
                    "u": "https://preview.redd.it/s20rrznjdvcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=42cc6623eaca31eed4fd31a37c49019438cd34e1"
                  },
                  {
                    "y": 216,
                    "x": 216,
                    "u": "https://preview.redd.it/s20rrznjdvcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=882f982c3e1a81aa867e475d5119711fb8418b35"
                  },
                  {
                    "y": 320,
                    "x": 320,
                    "u": "https://preview.redd.it/s20rrznjdvcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d2b3db232ea4ad0d6808d4d19cf1996b376c099"
                  },
                  {
                    "y": 640,
                    "x": 640,
                    "u": "https://preview.redd.it/s20rrznjdvcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6aa7ecded272f8d96b1d2410fac0426ced459e4c"
                  },
                  {
                    "y": 960,
                    "x": 960,
                    "u": "https://preview.redd.it/s20rrznjdvcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04007a84c86df1b2cc46921bb8b5cb8b650c443e"
                  }
                ],
                "s": {
                  "y": 1024,
                  "x": 1024,
                  "u": "https://preview.redd.it/s20rrznjdvcf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=12187710862bb2f48fd13c59f0069fb5ef3c7465"
                },
                "id": "s20rrznjdvcf1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n30248v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GlassGhost",
            "can_mod_post": false,
            "created_utc": 1752456831,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 1,
            "author_fullname": "t2_cm2c8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "[https://huggingface.co/bartowski/HelpingAI\\_Dhanishtha-2.0-preview-GGUF](https://huggingface.co/bartowski/HelpingAI_Dhanishtha-2.0-preview-GGUF)\n\nit will use 5x less tokens than anything else.\n\nI would make summaries of the images with a different model or feed it the code used to create the images.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n30248v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/bartowski/HelpingAI_Dhanishtha-2.0-preview-GGUF\"&gt;https://huggingface.co/bartowski/HelpingAI_Dhanishtha-2.0-preview-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;it will use 5x less tokens than anything else.&lt;/p&gt;\n\n&lt;p&gt;I would make summaries of the images with a different model or feed it the code used to create the images.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n30248v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752456831,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n30gz9k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "dash_bro",
            "can_mod_post": false,
            "created_utc": 1752462332,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": 0,
            "author_fullname": "t2_4bzd6saj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\n- make it a review/decision process (ie create criteria for what is a good/bad FAQ and generate objective scores for it) \n- use a thinking style model\n- generate FAQs from this model combination using the decision criteria and details on prompt for creating good FAQs\n- generate FAQs for a few pages at a time (e.g. 3-5 pages), generate up to 5 FAQs each time. Re run 2-5 times and store results, hopefully you generate a total of 7-9 unique FAQs for 3-5 pages\n- sort them in desc. order by the objective scoring strategy you developed\n\nI would recommend trying gemini-2.5 flash/pro with some thinking budget allocated. You might even try deepseek-r1-0528 or o3\n\nTry Gemini first because you can get a free API key for those models via ai studio",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n30gz9k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;make it a review/decision process (ie create criteria for what is a good/bad FAQ and generate objective scores for it) &lt;/li&gt;\n&lt;li&gt;use a thinking style model&lt;/li&gt;\n&lt;li&gt;generate FAQs from this model combination using the decision criteria and details on prompt for creating good FAQs&lt;/li&gt;\n&lt;li&gt;generate FAQs for a few pages at a time (e.g. 3-5 pages), generate up to 5 FAQs each time. Re run 2-5 times and store results, hopefully you generate a total of 7-9 unique FAQs for 3-5 pages&lt;/li&gt;\n&lt;li&gt;sort them in desc. order by the objective scoring strategy you developed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would recommend trying gemini-2.5 flash/pro with some thinking budget allocated. You might even try deepseek-r1-0528 or o3&lt;/p&gt;\n\n&lt;p&gt;Try Gemini first because you can get a free API key for those models via ai studio&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n30gz9k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752462332,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n32tz6z",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "No_Afternoon_4260",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n2zx68h",
                                          "score": 1,
                                          "author_fullname": "t2_cj9kap4bx",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Idk not op but i can run bigger local model yet I wouldn't know which one to use for physics you see?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n32tz6z",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Idk not op but i can run bigger local model yet I wouldn&amp;#39;t know which one to use for physics you see?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1lz81ea",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n32tz6z/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1752502062,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1752502062,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n2zx68h",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "zipzag",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2zv9kb",
                                "score": 1,
                                "author_fullname": "t2_59qc7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I run locally too. But that doesn't mean I don't choose the best tool for the job. \n\nI doubt the OP is capable of running even a 16B locally or they would be asking more sophisticated questions.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2zx68h",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I run locally too. But that doesn&amp;#39;t mean I don&amp;#39;t choose the best tool for the job. &lt;/p&gt;\n\n&lt;p&gt;I doubt the OP is capable of running even a 16B locally or they would be asking more sophisticated questions.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lz81ea",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zx68h/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752455066,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752455066,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 1,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2zv9kb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "texasdude11",
                      "can_mod_post": false,
                      "created_utc": 1752454385,
                      "send_replies": true,
                      "parent_id": "t1_n2zto7r",
                      "score": 5,
                      "author_fullname": "t2_ya9qn",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "r/localllama ?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2zv9kb",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"/r/localllama\"&gt;r/localllama&lt;/a&gt; ?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lz81ea",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zv9kb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752454385,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2zto7r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "LOW_SCORE",
            "no_follow": true,
            "author": "zipzag",
            "can_mod_post": false,
            "created_utc": 1752453822,
            "send_replies": true,
            "parent_id": "t3_1lz81ea",
            "score": -5,
            "author_fullname": "t2_59qc7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": true,
            "body": "Pay openAI $20 and experiment with the different models. Start with o3.  Anything local will be inferior.\n\nIf the generated output is poor, tell the LLM why. \n\nDid you ask an LLM the question you posed above?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2zto7r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pay openAI $20 and experiment with the different models. Start with o3.  Anything local will be inferior.&lt;/p&gt;\n\n&lt;p&gt;If the generated output is poor, tell the LLM why. &lt;/p&gt;\n\n&lt;p&gt;Did you ask an LLM the question you posed above?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": "comment score below threshold",
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lz81ea/which_llm_should_i_use_to_generate_high_quality/n2zto7r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752453822,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lz81ea",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -5
          }
        }
      ],
      "before": null
    }
  }
]