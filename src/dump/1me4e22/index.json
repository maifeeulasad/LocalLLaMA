[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone,  \nI’ve got an AMD Strix Halo (Ryzen AI Max+ 395) running Ubuntu 24.04, and I’ve installed ROCm based on the official documentation. To keep things streamlined, I also went ahead and installed PyTorch via Docker, as recommended by the official docs.\n\nHowever, when I run `import torch` and check for VRAM, I’m only seeing 16GB available instead of the full 96GB that the system claims to have. I’m trying to fully utilize the available VRAM to train large models, but I’m not sure how to access or enable the full 96GB capacity.\n\nHas anyone else run into this issue or know how to configure PyTorch to use the entire VRAM on AMD GPUs with ROCm?\n\nWould really appreciate any guidance on this!\n\nThanks in advance!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Help! How to access the full 96GB VRAM on AMD Strix Halo (Ryzen AI Max+ 395) with PyTorch in Ubuntu 24.04?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1me4e22",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_reelww4w",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753975162,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;br/&gt;\nI’ve got an AMD Strix Halo (Ryzen AI Max+ 395) running Ubuntu 24.04, and I’ve installed ROCm based on the official documentation. To keep things streamlined, I also went ahead and installed PyTorch via Docker, as recommended by the official docs.&lt;/p&gt;\n\n&lt;p&gt;However, when I run &lt;code&gt;import torch&lt;/code&gt; and check for VRAM, I’m only seeing 16GB available instead of the full 96GB that the system claims to have. I’m trying to fully utilize the available VRAM to train large models, but I’m not sure how to access or enable the full 96GB capacity.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else run into this issue or know how to configure PyTorch to use the entire VRAM on AMD GPUs with ROCm?&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any guidance on this!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1me4e22",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ashwin3005",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1me4e22/help_how_to_access_the_full_96gb_vram_on_amd/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1me4e22/help_how_to_access_the_full_96gb_vram_on_amd/",
            "subreddit_subscribers": 507935,
            "created_utc": 1753975162,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n66puaj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "drplan",
            "can_mod_post": false,
            "created_utc": 1753977639,
            "send_replies": true,
            "parent_id": "t3_1me4e22",
            "score": 3,
            "author_fullname": "t2_369s5oed",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've been running the system on Fedora, no major problems there. Can you post some code to reproduce?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n66puaj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running the system on Fedora, no major problems there. Can you post some code to reproduce?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me4e22/help_how_to_access_the_full_96gb_vram_on_amd/n66puaj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753977639,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me4e22",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n66pbih",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RealLightDot",
            "can_mod_post": false,
            "created_utc": 1753977494,
            "send_replies": true,
            "parent_id": "t3_1me4e22",
            "score": 2,
            "author_fullname": "t2_7xwniee1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Which system is this, Bosgame, ASUS or some other hardware? How much RAM did you assign to graphics in UEFI?\n\nIf you run 'free', what do you see?\n\nDid you patch ROCm or perhaps use some adjusted version with better Ryzen AI Max+ 395 support? Where did you install it from, following which instructions?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n66pbih",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Which system is this, Bosgame, ASUS or some other hardware? How much RAM did you assign to graphics in UEFI?&lt;/p&gt;\n\n&lt;p&gt;If you run &amp;#39;free&amp;#39;, what do you see?&lt;/p&gt;\n\n&lt;p&gt;Did you patch ROCm or perhaps use some adjusted version with better Ryzen AI Max+ 395 support? Where did you install it from, following which instructions?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me4e22/help_how_to_access_the_full_96gb_vram_on_amd/n66pbih/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753977494,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me4e22",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n66q1ub",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "UltralKent",
            "can_mod_post": false,
            "created_utc": 1753977699,
            "send_replies": true,
            "parent_id": "t3_1me4e22",
            "score": 2,
            "author_fullname": "t2_nm7qj8vc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "did you configurate the BIOS?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n66q1ub",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;did you configurate the BIOS?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me4e22/help_how_to_access_the_full_96gb_vram_on_amd/n66q1ub/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753977699,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me4e22",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n68z6s1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Fit-Produce420",
            "can_mod_post": false,
            "created_utc": 1754000914,
            "send_replies": true,
            "parent_id": "t3_1me4e22",
            "score": 1,
            "author_fullname": "t2_tewf9bdwg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You should be able to get 112GB allocated on Linux, FYI.\n\n\nCheck your BIOS, you have to set it there, it's not a PyTorch setting it's driver/kernel level.\n\n\nAlso can you try without docker? Or are you sure the container you're using is set for that much ram?\n\n\n ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n68z6s1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You should be able to get 112GB allocated on Linux, FYI.&lt;/p&gt;\n\n&lt;p&gt;Check your BIOS, you have to set it there, it&amp;#39;s not a PyTorch setting it&amp;#39;s driver/kernel level.&lt;/p&gt;\n\n&lt;p&gt;Also can you try without docker? Or are you sure the container you&amp;#39;re using is set for that much ram?&lt;/p&gt;\n\n&lt;p&gt; &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1me4e22/help_how_to_access_the_full_96gb_vram_on_amd/n68z6s1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754000914,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1me4e22",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]