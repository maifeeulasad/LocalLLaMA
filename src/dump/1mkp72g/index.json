[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "\nContinue from my last post, and thanks for valuable comments!\n\n(Moderator blocked my post now, but I don't know what I violated)\n\n\n\nIn the beginning, I set up 4070ti(12GB VRAM) + MI50(32GB VRAM) on my gaming gear,\n\nHowever, I only could access 12 +12 GB of vram in two GPUs - it was restricted by size of first gpu's VRAM(12G)\n\nor, MI 32GB only by turn off using 4070ti on Win11 / Vulkan / LM studio environment.\n\nSince last weekeens, I have been trying to access the rest portion of total 44G Vram(gpu0+gpu1) in Local LLM running.\n\n(It wasn't fault of MI50, it is clearly related with incomplete vulkan/llama.cpp implementation of LM Studio)\n\nMost easy solution may be put MI50 on \"first\" PCI 5.0 slot,  but the MI50 doesn' supports screen output unless bios rom writing.\n\n\nFinally, I found a simple way to exchange gpu0 and 1 postion in Windows. -\n\n\nJust go right Control Panel =&gt; System =&gt; Display =&gt; Graphics\n\nand Let RADEON VII(MI50) as a primary graphic card of LM Studio Apps\n\n\n\nBy this way, I got \"almost\" 32GB VRAMs (sorry it's not 32+12GB yet) in LM Studio \n\nIt not only gluing 32GB of HBM on your gpu, but also can steal prompt processing ability from old Nvidia GPU\n\n\n\nPlease show three results from favorite scenarios. Whole test have conducted Win11/Vulkan Envrionment.\n\n\n\n**1. Legal Document Analysis(21,928 Input tokens)**\n\n\n\nModel : ERNIE-4.5-21B-A3B (Q6\\_K, size: 18.08GB)  to check effects of GPU position between GPU 0 and 1\n\n\n\n\n\nGPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token\n\n MI50(gpu0)+4070TI(gpu1)   23.27(token/s)          1303(tokens)             195.74sec\n\n 4070TI(gpu0)+MI50(gpu1)   24.00(token/s)          1425(tokens)             174.62sec\n\n\n\n**2. Hard SF Novel Writing (929 Input tokens)**\n\n\n\nModel : Qwen3-30B-A3B-Thinking-2507 (Q8\\_0, 32.48GB) - Max accessible memory test\n\n\n\nGPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token\n\n MI50(main)+4070TI(sub)*       13.86(token/s)             6437(tokens)           13.08sec\n\n MI50(32GB only)                 17.93(token/s)             5656(tokens)          17.75sec\n\n* Whole model has landed on MI50(about 21GB) &amp; 4070(11GB) successfully.\n\n\n\n**3. Multilingual Novel Summerization(27,393 Input Tokens)**\n\n Gemma-3-27b-QAT (Q4\\_0, 16.43GB, 4bit KV Cache)\n\n\n\n GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token\n\nMI50(main)+4070TI(sub)          4.19(tokens)               907(tokens)            10min 2sec\n\nMI50(only)                            2.92(tokens)               1058(token)           33min** 41s \n\n\n\nMany GPU poor including me always said that \"I'm patient man\", however, 33 minutes vs. 10 minutes is a good reason to think twice before ordering MI50 and adding Nvidia used Card instead. - P/P is really crawling on AMD but this disadvantage can be overcome by attaching Nvidia Card.\n\n\n\nI still think the MI50 is a very cheap and appropriate investment for hobbiest even considering these drawbacks.\n\nIf anyone is familiar with the Linux environment and llama.cpp, I'd appreciate it if you could share some insights and benchmark result on distributed inference using RPC. Setting it up that way might allow access to all VRAM, excluding any frameworks penalties from using multiple GPUs.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How you could boost P/P rates of AMD MI50",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mkp72g",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1dhesoqqtu",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754639667,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Continue from my last post, and thanks for valuable comments!&lt;/p&gt;\n\n&lt;p&gt;(Moderator blocked my post now, but I don&amp;#39;t know what I violated)&lt;/p&gt;\n\n&lt;p&gt;In the beginning, I set up 4070ti(12GB VRAM) + MI50(32GB VRAM) on my gaming gear,&lt;/p&gt;\n\n&lt;p&gt;However, I only could access 12 +12 GB of vram in two GPUs - it was restricted by size of first gpu&amp;#39;s VRAM(12G)&lt;/p&gt;\n\n&lt;p&gt;or, MI 32GB only by turn off using 4070ti on Win11 / Vulkan / LM studio environment.&lt;/p&gt;\n\n&lt;p&gt;Since last weekeens, I have been trying to access the rest portion of total 44G Vram(gpu0+gpu1) in Local LLM running.&lt;/p&gt;\n\n&lt;p&gt;(It wasn&amp;#39;t fault of MI50, it is clearly related with incomplete vulkan/llama.cpp implementation of LM Studio)&lt;/p&gt;\n\n&lt;p&gt;Most easy solution may be put MI50 on &amp;quot;first&amp;quot; PCI 5.0 slot,  but the MI50 doesn&amp;#39; supports screen output unless bios rom writing.&lt;/p&gt;\n\n&lt;p&gt;Finally, I found a simple way to exchange gpu0 and 1 postion in Windows. -&lt;/p&gt;\n\n&lt;p&gt;Just go right Control Panel =&amp;gt; System =&amp;gt; Display =&amp;gt; Graphics&lt;/p&gt;\n\n&lt;p&gt;and Let RADEON VII(MI50) as a primary graphic card of LM Studio Apps&lt;/p&gt;\n\n&lt;p&gt;By this way, I got &amp;quot;almost&amp;quot; 32GB VRAMs (sorry it&amp;#39;s not 32+12GB yet) in LM Studio &lt;/p&gt;\n\n&lt;p&gt;It not only gluing 32GB of HBM on your gpu, but also can steal prompt processing ability from old Nvidia GPU&lt;/p&gt;\n\n&lt;p&gt;Please show three results from favorite scenarios. Whole test have conducted Win11/Vulkan Envrionment.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Legal Document Analysis(21,928 Input tokens)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Model : ERNIE-4.5-21B-A3B (Q6_K, size: 18.08GB)  to check effects of GPU position between GPU 0 and 1&lt;/p&gt;\n\n&lt;p&gt;GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token&lt;/p&gt;\n\n&lt;p&gt;MI50(gpu0)+4070TI(gpu1)   23.27(token/s)          1303(tokens)             195.74sec&lt;/p&gt;\n\n&lt;p&gt;4070TI(gpu0)+MI50(gpu1)   24.00(token/s)          1425(tokens)             174.62sec&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Hard SF Novel Writing (929 Input tokens)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Model : Qwen3-30B-A3B-Thinking-2507 (Q8_0, 32.48GB) - Max accessible memory test&lt;/p&gt;\n\n&lt;p&gt;GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token&lt;/p&gt;\n\n&lt;p&gt;MI50(main)+4070TI(sub)*       13.86(token/s)             6437(tokens)           13.08sec&lt;/p&gt;\n\n&lt;p&gt;MI50(32GB only)                 17.93(token/s)             5656(tokens)          17.75sec&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Whole model has landed on MI50(about 21GB) &amp;amp; 4070(11GB) successfully.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Multilingual Novel Summerization(27,393 Input Tokens)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Gemma-3-27b-QAT (Q4_0, 16.43GB, 4bit KV Cache)&lt;/p&gt;\n\n&lt;p&gt;GPU Setting                     Token Generation  Total Output(Tokens)   Time to 1st Token&lt;/p&gt;\n\n&lt;p&gt;MI50(main)+4070TI(sub)          4.19(tokens)               907(tokens)            10min 2sec&lt;/p&gt;\n\n&lt;p&gt;MI50(only)                            2.92(tokens)               1058(token)           33min** 41s &lt;/p&gt;\n\n&lt;p&gt;Many GPU poor including me always said that &amp;quot;I&amp;#39;m patient man&amp;quot;, however, 33 minutes vs. 10 minutes is a good reason to think twice before ordering MI50 and adding Nvidia used Card instead. - P/P is really crawling on AMD but this disadvantage can be overcome by attaching Nvidia Card.&lt;/p&gt;\n\n&lt;p&gt;I still think the MI50 is a very cheap and appropriate investment for hobbiest even considering these drawbacks.&lt;/p&gt;\n\n&lt;p&gt;If anyone is familiar with the Linux environment and llama.cpp, I&amp;#39;d appreciate it if you could share some insights and benchmark result on distributed inference using RPC. Setting it up that way might allow access to all VRAM, excluding any frameworks penalties from using multiple GPUs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1mkp72g",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Desperate-Sir-5088",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkp72g/how_you_could_boost_pp_rates_of_amd_mi50/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkp72g/how_you_could_boost_pp_rates_of_amd_mi50/",
            "subreddit_subscribers": 513815,
            "created_utc": 1754639667,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7l2dwi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Wrong-Historian",
            "can_mod_post": false,
            "created_utc": 1754653730,
            "send_replies": true,
            "parent_id": "t3_1mkp72g",
            "score": 1,
            "author_fullname": "t2_69r67vj3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What are you doing on Windows and 'lm studio' with a setup like that?!?\n\nSwitch to Linux, ROCm, mlc-llm for that MI50 immediately.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7l2dwi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What are you doing on Windows and &amp;#39;lm studio&amp;#39; with a setup like that?!?&lt;/p&gt;\n\n&lt;p&gt;Switch to Linux, ROCm, mlc-llm for that MI50 immediately.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkp72g/how_you_could_boost_pp_rates_of_amd_mi50/n7l2dwi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754653730,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkp72g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]