{
  "kind": "Listing",
  "data": {
    "after": "t3_1mfifhh",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.**\n\nIt writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof *checked* by a compiler. \n\nThe key innovations:\n\n* **Lemma-first reasoning**: breaks problems into small reusable steps.\n* **Iterative refinement**: re-tries and improves failed proofs.\n* **Formal geometry engine**: solves insane geometry problems using a custom language and a C++ backend.\n\nPerformance? It formally solved **5/6 IMO 2025 problems**, something no model has done before.\n\nCheck simple explanantion here : [https://www.youtube.com/watch?v=os1QcHEpgZQ](https://www.youtube.com/watch?v=os1QcHEpgZQ)\n\nPaper : [https://arxiv.org/abs/2507.23726](https://arxiv.org/abs/2507.23726)",
          "author_fullname": "t2_th2ct5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ByteDance drops Seed-Prover",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgccyc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 107,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 107,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754202843,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;It writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof &lt;em&gt;checked&lt;/em&gt; by a compiler. &lt;/p&gt;\n\n&lt;p&gt;The key innovations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lemma-first reasoning&lt;/strong&gt;: breaks problems into small reusable steps.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt;: re-tries and improves failed proofs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Formal geometry engine&lt;/strong&gt;: solves insane geometry problems using a custom language and a C++ backend.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Performance? It formally solved &lt;strong&gt;5/6 IMO 2025 problems&lt;/strong&gt;, something no model has done before.&lt;/p&gt;\n\n&lt;p&gt;Check simple explanantion here : &lt;a href=\"https://www.youtube.com/watch?v=os1QcHEpgZQ\"&gt;https://www.youtube.com/watch?v=os1QcHEpgZQ&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Paper : &lt;a href=\"https://arxiv.org/abs/2507.23726\"&gt;https://arxiv.org/abs/2507.23726&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?auto=webp&amp;s=f6d5b40325fd2c44f012121fd1ced8e7cf3fd808",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2880b55faa98583237421c5c4a72d31d6cee167",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=578175b7ddca0a72992280e2d7f06f267e288d69",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26ae18680865d42ebd641ddb54c43950a72d9432",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "cnGM2lbzAhFfeq_Wsd7iu0T7rDhd6OEpKKquGX7_eJM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mgccyc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Technical-Love-8479",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754202843,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/oevortex/unsloth](https://github.com/oevortex/unsloth)",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We enabled Multi-GPU training in Unsloth AI ‚Äî a feature that‚Äôs usually paid ‚Äî using just 2 Copilot prompts!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbs6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754200681,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/oevortex/unsloth\"&gt;https://github.com/oevortex/unsloth&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?auto=webp&amp;s=81dc71a9f2f02f913bd3aee3c8d23660f674d4d2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3fa1e4a31ebf9a6d864f530fdd10814f6d32e3a",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f68f83263defa437a0f2fa0d8b040d089cd1d79",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6910771512dba7baaab00bceadb6d8e3f5e32826",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f807024407837a65a122fa874452ed0fa669595e",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=755ef7a38a09ee966639de29f878c57aaa1ead1e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e85dbe449ba2af4571feed8fcd1e5ddf597b6c6b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSMWdS1tBsndSKly_Q084vHJ9V-M2sB-dsq1Yua6ZTY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgbs6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754200681,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " WHAT THE DEVIL?\n\nAnother open model outperforms closed ones!  \nXBai o4 beats OpenAI o3-mini and *confidently* beats Anthropic's Claude Opus.\n\n‚Ä¢Parameters: 32.8 B\n‚Ä¢Training: Long-CoT RL + Process Reward Learning (SPRM)\n‚Ä¢Benchmarks (High-Modus):\n‚Ä¢AIME24: 86.5\n‚Ä¢AIME25: 77.9\n‚Ä¢LiveCodeBench v5: 67.2\n‚Ä¢C-EVAL: 89.7\n\nüîóOpen source weights: https://huggingface.co/MetaStoneTec/XBai-o4",
          "author_fullname": "t2_dmji1c74",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "XBai-04 Is It Real?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "idu2e1ngesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a2a62a8bef0a6d00f8049f16c8b54e8aa48b974"
                },
                {
                  "y": 124,
                  "x": 216,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=924e4b76f3498ca5e7d4ab97c08a75b6fb377dc1"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82f9fc643d2975e8481a615c1f5a19b5cb453032"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b611aef0c4759ebf456a134bc82a5fd8065b9c5a"
                },
                {
                  "y": 551,
                  "x": 960,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f73f5b3bcc334e135595851ef884c4997ac617"
                },
                {
                  "y": 620,
                  "x": 1080,
                  "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e786e475e1a54e20bdba7fb55d1e03e02789639"
                }
              ],
              "s": {
                "y": 704,
                "x": 1226,
                "u": "https://preview.redd.it/idu2e1ngesgf1.jpg?width=1226&amp;format=pjpg&amp;auto=webp&amp;s=5b2baf561ff73bb0e8b4b5708cd383ce9ed6f372"
              },
              "id": "idu2e1ngesgf1"
            },
            "q3imyykgesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8dbd1ac74caccea54654d6ebe571fab617928272"
                },
                {
                  "y": 158,
                  "x": 216,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c6af0dfc3c0aaa092cea1507485efebac28962f"
                },
                {
                  "y": 235,
                  "x": 320,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=602c9e9684804598a1e5221dbb4d23ce3610098a"
                },
                {
                  "y": 471,
                  "x": 640,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e073695ec8c765d1252320b4c7a5e0bd07547b82"
                },
                {
                  "y": 706,
                  "x": 960,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d48c30f79f06380c415eb3dc8e2120642d96e240"
                },
                {
                  "y": 794,
                  "x": 1080,
                  "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2d39f9af5c4e09886e80918a8161ae1315aa4c7b"
                }
              ],
              "s": {
                "y": 942,
                "x": 1280,
                "u": "https://preview.redd.it/q3imyykgesgf1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=e8846d514b560df6c44bca9b5375f5eed938412c"
              },
              "id": "q3imyykgesgf1"
            }
          },
          "name": "t3_1mggku0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 33,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "q3imyykgesgf1",
                "id": 720170802
              },
              {
                "caption": "",
                "media_id": "idu2e1ngesgf1",
                "id": 720170803
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754219245,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt;\n\n&lt;p&gt;Another open model outperforms closed ones!&lt;br/&gt;\nXBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic&amp;#39;s Claude Opus.&lt;/p&gt;\n\n&lt;p&gt;‚Ä¢Parameters: 32.8 B\n‚Ä¢Training: Long-CoT RL + Process Reward Learning (SPRM)\n‚Ä¢Benchmarks (High-Modus):\n‚Ä¢AIME24: 86.5\n‚Ä¢AIME25: 77.9\n‚Ä¢LiveCodeBench v5: 67.2\n‚Ä¢C-EVAL: 89.7&lt;/p&gt;\n\n&lt;p&gt;üîóOpen source weights: &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mggku0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mggku0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ordinary_Mud7430",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mggku0",
          "subreddit_subscribers": 509291,
          "created_utc": 1754219245,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article: https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\n\nContext:\n\nThis insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.\n\nhttps://arxiv.org/abs/2506.21734\n\nA number of people have reproduced it if anyone is worried about that: https://x.com/VictorTaelin/status/1950512015899840768\nhttps://github.com/sapientinc/HRM/issues/12",
          "author_fullname": "t2_4kcht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM solved thinking more than current \"thinking\" models (this needs more hype)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3i48",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 263,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 263,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174679,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article: &lt;a href=\"https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e\"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;/p&gt;\n\n&lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It&amp;#39;s seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href=\"https://x.com/VictorTaelin/status/1950512015899840768\"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt;\n&lt;a href=\"https://github.com/sapientinc/HRM/issues/12\"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?auto=webp&amp;s=3649785728b7c459cdac6d4444cd5a58a31ff8a2",
                  "width": 994,
                  "height": 610
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a32ed96622f166ae158780da40b7af8d982e72f9",
                    "width": 108,
                    "height": 66
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2621c7644d6e96ab551888e43049a8b94e227fc5",
                    "width": 216,
                    "height": 132
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da54eeb0e463cf73d8f5292e9b099e338494438",
                    "width": 320,
                    "height": 196
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa1caf030ca1b3e5b1df5e052afe93497a9bb543",
                    "width": 640,
                    "height": 392
                  },
                  {
                    "url": "https://external-preview.redd.it/okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=88919a9c5b50d0a4d76e2c2bb7e87bb3c4eed852",
                    "width": 960,
                    "height": 589
                  }
                ],
                "variants": {},
                "id": "okA4gpxX7B_Xt3c3UIV2XiEN4mivP8AjVhN1Fsvjoo0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg3i48",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Charuru",
          "discussion_type": null,
          "num_comments": 55,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754174679,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "üöÄ I just open-sourced a fully working persistent memory system for AI assistants!\n\n\n\nüß† Features:\n\n\\- Real-time memory capture across apps (LM Studio, VS Code, etc.)\n\n\\- Semantic search via vector embeddings\n\n\\- Tool call logging for AI self-reflection\n\n\\- Cross-platform and fully tested\n\n\\- Open source and modular\n\n\n\nBuilt with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot ü§ù\n\n\n\nGitHub: [https://github.com/savantskie/persistent-ai-memory](https://github.com/savantskie/persistent-ai-memory)",
          "author_fullname": "t2_7qb4luzhc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg5xlb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 177,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 177,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181701,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;üöÄ I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt;\n\n&lt;p&gt;üß† Features:&lt;/p&gt;\n\n&lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt;\n\n&lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt;\n\n&lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt;\n\n&lt;p&gt;- Cross-platform and fully tested&lt;/p&gt;\n\n&lt;p&gt;- Open source and modular&lt;/p&gt;\n\n&lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot ü§ù&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/savantskie/persistent-ai-memory\"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?auto=webp&amp;s=726283d31a9e66a5327266304a008d1eb52da1db",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b41ef9face9075e71937a4cd3b28923245b39a29",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6703186c20a684418ded0dc4abb9a6d490d7b023",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=860a8622d037c44a41b9ee50c96939060a76e917",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ef5d63ac01ba46958961a63c5e9f43dabb1245",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=446a7e497c6b8549a8cbed43e99bc5f99e79c618",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e1ab447d78f6edbe4f20129542abf92b51d827b1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "6rpXOo6FUHMg7nBmpxnDt1W_rbBXLhU2vtCgMgpecfE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg5xlb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Savantskie1",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754181701,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's a WIP, but it's looking like may be possible to pair Macs with NVIDIA soon!\n\nTweet: [https://x.com/anemll/status/1951307167417639101](https://x.com/anemll/status/1951307167417639101)\n\nRepo:¬†[https://github.com/anemll/anemll](https://github.com/anemll/anemll)  \n",
          "author_fullname": "t2_88fma001",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mac + Blackwell üëÄ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 113,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg7abc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 133,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 133,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fpapz0ydmBtTV_x0MSxATxS9iOfp_oxdoqZpoMcz2tU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754185875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a WIP, but it&amp;#39;s looking like may be possible to pair Macs with NVIDIA soon!&lt;/p&gt;\n\n&lt;p&gt;Tweet: &lt;a href=\"https://x.com/anemll/status/1951307167417639101\"&gt;https://x.com/anemll/status/1951307167417639101&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Repo:¬†&lt;a href=\"https://github.com/anemll/anemll\"&gt;https://github.com/anemll/anemll&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/u2mr83o6npgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/u2mr83o6npgf1.png?auto=webp&amp;s=7490eb4194899de10ffd4b664bbedbdc08bd49bb",
                  "width": 1165,
                  "height": 948
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9ff42619aaccace2d4ed7a908fc57868515ac74",
                    "width": 108,
                    "height": 87
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ef76de21f6db4556cec173bd541323b579d6b56",
                    "width": 216,
                    "height": 175
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=192a7fbdc52c427e798423dad1748e19398be74b",
                    "width": 320,
                    "height": 260
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=161c4ac9304218f08618c511e09178e7a7c08931",
                    "width": 640,
                    "height": 520
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc682053c820d810ab124287e40648863dde6414",
                    "width": 960,
                    "height": 781
                  },
                  {
                    "url": "https://preview.redd.it/u2mr83o6npgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=34b03746bd307ffc26c4ea8eab8f4bc9945f8480",
                    "width": 1080,
                    "height": 878
                  }
                ],
                "variants": {},
                "id": "a2KP-pXHo0R34cSlf0ychRDsFMkcNDCD2VtUAfedyZM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg7abc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Accomplished_Ad9530",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/",
          "stickied": false,
          "url": "https://i.redd.it/u2mr83o6npgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754185875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The larger SmallThinker MoE has been through a quantization aware training process. it's uploaded to the same gguf repo a bit later. \n\n- https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\n\nIn llama.cpp m2 air 16gb, with the `sudo sysctl iogpu.wired_limit_mb=13000` command, it's 30 t/s. \n\nThe model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.",
          "author_fullname": "t2_el5pibmej",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmallThinker-21B-A3B-Instruct-QAT version",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbprh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 60,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 60,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=f963436f8fe8c361365006afe812426eb0a95bfc",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754200435,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The larger SmallThinker MoE has been through a quantization aware training process. it&amp;#39;s uploaded to the same gguf repo a bit later. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In llama.cpp m2 air 16gb, with the &lt;code&gt;sudo sysctl iogpu.wired_limit_mb=13000&lt;/code&gt; command, it&amp;#39;s 30 t/s. &lt;/p&gt;\n\n&lt;p&gt;The model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. \nThe models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?auto=webp&amp;s=af35574c8d3a17d80825392473da49ac8b6cb469",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=97ed6aedc23ef593aadb95c2316196f396bc8e65",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ac0298234f43f3e33451ea17a59722ab4866a82",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=19ce50a51896a266764f718a5da7eb00d163ddf3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9d016e4dfb71f7ddb2afc8c6c06ee782ba15871a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=800a6ea36b04ed56de7a369596d03474bfd1142f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=29a1fe69fa68a7a75c2954c6a0922c20dd4448f1",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mgbprh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Aaaaaaaaaeeeee",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/",
          "stickied": false,
          "url": "https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf",
          "subreddit_subscribers": 509291,
          "created_utc": 1754200435,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys!\n\n**I got tired of installing AI tools the hard way.**\n\nEvery time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:\n\n**terminal commands, missing dependencies, broken CUDA, slow setup, frustration.**\n\nSo I built **Dione** ‚Äî a desktop app that makes running local AI feel like using an App Store.\n\n**What it does:**\n\n* Browse and install AI tools with one click (like apps)\n* No terminal, no Python setup, no configs\n* Open-source, designed with UX in mind\n\n**You can try it** [**here**](https://getdione.app)**.**\n\n**Why I built it?**\n\nTools like Pinokio or open-source repos are powerful, but honestly‚Ä¶ **most look like they were made by devs, for devs**.\n\n**I wanted something simple**. Something visual. Something you can give to your non-tech friend and it still works.\n\n**Dione is my attempt to make local AI accessible without losing control or power.**\n\nWould you use something like this? Anything confusing / missing?\n\nThe project is still evolving, and I‚Äôm fully open to ideas and contributions. Also, if you‚Äôre into self-hosted AI or building tools around it ‚Äî let‚Äôs talk!\n\nGitHub: [https://getdione.app/github](https://getdione.app/github)\n\nThanks for reading &lt;3!",
          "author_fullname": "t2_dau1zlvo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "I created an app to run local AI as if it were the App Store",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ysewb2ytxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 36,
                  "x": 108,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b69be81b017e04cc485309d6ea7db34f44eb65cd"
                },
                {
                  "y": 72,
                  "x": 216,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cef7f4312d5076f63633ce0082f84992449d708f"
                },
                {
                  "y": 106,
                  "x": 320,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0950b8c7452a7592a0103624fa574fa42d0435a4"
                },
                {
                  "y": 213,
                  "x": 640,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3d1571494f02e307302b6ce30eb0b231baef2eb"
                },
                {
                  "y": 320,
                  "x": 960,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9d1f01d071b3b900cf9e696a94ba3b157258152c"
                },
                {
                  "y": 360,
                  "x": 1080,
                  "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8b39946b7843a58ceef7b49e4bd40107d54164ca"
                }
              ],
              "s": {
                "y": 1000,
                "x": 3000,
                "u": "https://preview.redd.it/ysewb2ytxqgf1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=54735c109c32811d4b153a0c2e59c72a4c06bf5d"
              },
              "id": "ysewb2ytxqgf1"
            },
            "yt2muytuxqgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6ce704ac3695892c4184608ef310e5bf451248"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff91a2d48c077a5f508c0a4840cb39b61883c92d"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4279610bad7b104472f3da3351e6ffc9361fafb5"
                },
                {
                  "y": 342,
                  "x": 640,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e834d2caf46a9a6f01999cf11161e7f0e1a9c17"
                },
                {
                  "y": 514,
                  "x": 960,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=666cb580f9192df96b59d1ca07402b7628e0ff4a"
                },
                {
                  "y": 578,
                  "x": 1080,
                  "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=528bb2d873fdc538d816d91b547db28b28ad5f34"
                }
              ],
              "s": {
                "y": 1026,
                "x": 1915,
                "u": "https://preview.redd.it/yt2muytuxqgf1.png?width=1915&amp;format=png&amp;auto=webp&amp;s=c32ef79f62a2ffe7f4e5128937b67c1a2b216d50"
              },
              "id": "yt2muytuxqgf1"
            }
          },
          "name": "t3_1mgc0v0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 53,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "ysewb2ytxqgf1",
                "id": 720059841
              },
              {
                "media_id": "yt2muytuxqgf1",
                "id": 720059842
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/DMpoJ6kbQBgxqPJp_64cY0H-qxr9jJD3Tr3RuFU7894.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754201559,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I got tired of installing AI tools the hard way.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Every time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;terminal commands, missing dependencies, broken CUDA, slow setup, frustration.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So I built &lt;strong&gt;Dione&lt;/strong&gt; ‚Äî a desktop app that makes running local AI feel like using an App Store.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Browse and install AI tools with one click (like apps)&lt;/li&gt;\n&lt;li&gt;No terminal, no Python setup, no configs&lt;/li&gt;\n&lt;li&gt;Open-source, designed with UX in mind&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;You can try it&lt;/strong&gt; &lt;a href=\"https://getdione.app\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Tools like Pinokio or open-source repos are powerful, but honestly‚Ä¶ &lt;strong&gt;most look like they were made by devs, for devs&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I wanted something simple&lt;/strong&gt;. Something visual. Something you can give to your non-tech friend and it still works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Dione is my attempt to make local AI accessible without losing control or power.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Would you use something like this? Anything confusing / missing?&lt;/p&gt;\n\n&lt;p&gt;The project is still evolving, and I‚Äôm fully open to ideas and contributions. Also, if you‚Äôre into self-hosted AI or building tools around it ‚Äî let‚Äôs talk!&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://getdione.app/github\"&gt;https://getdione.app/github&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading &amp;lt;3!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mgc0v0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgc0v0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Deivih-4774",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mgc0v0",
          "subreddit_subscribers": 509291,
          "created_utc": 1754201559,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just dropped a new medical embedding model that's crushing the competition:¬†[https://huggingface.co/lokeshch19/ModernPubMedBERT](https://huggingface.co/lokeshch19/ModernPubMedBERT)\n\nTL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.\n\nThe model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.\n\nThe model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.\n\nThe model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.\n\nDownload the model, test it on your medical datasets, and give it a ‚≠ê on the [Hugging Face](https://huggingface.co/lokeshch19/ModernPubMedBERT) if it enhances your workflow!\n\n  \nEdit: Added evals to HF model card",
          "author_fullname": "t2_1265e91smj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Medical Embedding Model Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdypr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754219207,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754209051,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just dropped a new medical embedding model that&amp;#39;s crushing the competition:¬†&lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;https://huggingface.co/lokeshch19/ModernPubMedBERT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.&lt;/p&gt;\n\n&lt;p&gt;The model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.&lt;/p&gt;\n\n&lt;p&gt;The model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.&lt;/p&gt;\n\n&lt;p&gt;The model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.&lt;/p&gt;\n\n&lt;p&gt;Download the model, test it on your medical datasets, and give it a ‚≠ê on the &lt;a href=\"https://huggingface.co/lokeshch19/ModernPubMedBERT\"&gt;Hugging Face&lt;/a&gt; if it enhances your workflow!&lt;/p&gt;\n\n&lt;p&gt;Edit: Added evals to HF model card&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?auto=webp&amp;s=5c561836b04f5eae4aeb77b06c972da83d0058a0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4c4ca0fb1848d33d5ab1a908c384ed031d1d90a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9535c4b60f4ef617f9bf8da746dcf3972054a622",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3d2b06c75054cafe0122756eef8a1e16911ccb3",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b0a45e3a646f262176ce2e81cf2d0e6c58218d8",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b4e255867391c0878d0054644b109d42d2e6f88",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6bcc230f7d600fa2c654466bd44ead74e93046a4",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "I68Y-9T7cCc1Z97M_N8-PadQJ0sUFhqpKPe5VKRwYHI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdypr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DataNebula",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754209051,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We've been working on an LLM proxy, balancer &amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. \n\nThis was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),\n\nSo some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.\n\nOlla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.\n\nAdd your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.\n\nThe attempt to unify across providers wasn't as successful - as in, both LMStudio &amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.\n\nGithub: [https://github.com/thushan/olla](https://github.com/thushan/olla) (golang)\n\nWould love to know your thoughts. \n\nOlla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.",
          "author_fullname": "t2_116dje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "5nw1vlfoqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb3a2dbb77b7c17f0c25c782f45d5f1145b6df22"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31ee39f327a201eb2bd4da0cab803303d2c7f1d4"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4a2254c5239c0b216fd14bb6c0f0e38e7b6f82"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97c3ad10efc7e082eca21673151d8898dbe7206d"
                }
              ],
              "s": {
                "y": 648,
                "x": 648,
                "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=648&amp;format=png&amp;auto=webp&amp;s=d12b43c56e8a84fc36c0b40b7f880085d0c6c4c4"
              },
              "id": "5nw1vlfoqpgf1"
            },
            "tfkhbanjqpgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 151,
                  "x": 108,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52ec679a9bd4d342bee02ef2f1165eb230e72cce"
                },
                {
                  "y": 302,
                  "x": 216,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d21c96bf9e46c48e0c9f056396472b42eebe4857"
                },
                {
                  "y": 448,
                  "x": 320,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3471f6cecc4f0ca3888500245adb1fa96e1a0776"
                },
                {
                  "y": 896,
                  "x": 640,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d04c10c876d43302f632b7e7171a15a08b4e8666"
                }
              ],
              "s": {
                "y": 1077,
                "x": 769,
                "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=769&amp;format=png&amp;auto=webp&amp;s=68776f8d4bf7c2b5ade021a1617e272f3a83f6f0"
              },
              "id": "tfkhbanjqpgf1"
            },
            "pvipbao9npgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08ec5455f30f170ffdb8fb10a28e530de3ffc87f"
                },
                {
                  "y": 119,
                  "x": 216,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=983873e2a890b1d5b4d1549b6554d0d96e4c6147"
                },
                {
                  "y": 176,
                  "x": 320,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29670ef3ff3bf1e93b9734818e1ea388750f683"
                }
              ],
              "s": {
                "y": 249,
                "x": 451,
                "u": "https://preview.redd.it/pvipbao9npgf1.png?width=451&amp;format=png&amp;auto=webp&amp;s=de1da506433d6cdd0870b0b2a309a56defbfdaf9"
              },
              "id": "pvipbao9npgf1"
            }
          },
          "name": "t3_1mg7qpa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 40,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "pvipbao9npgf1",
                "id": 719953590
              },
              {
                "media_id": "tfkhbanjqpgf1",
                "id": 719953591
              },
              {
                "media_id": "5nw1vlfoqpgf1",
                "id": 719953592
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754187279,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we&amp;#39;ve created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt;\n\n&lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt;\n\n&lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt;\n\n&lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt;\n\n&lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt;\n\n&lt;p&gt;The attempt to unify across providers wasn&amp;#39;t as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/thushan/olla\"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt;\n\n&lt;p&gt;Would love to know your thoughts. &lt;/p&gt;\n\n&lt;p&gt;Olla is still in its infancy, so we don&amp;#39;t have auth implemented etc but there are plans in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mg7qpa",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg7qpa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "2shanigans",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mg7qpa",
          "subreddit_subscribers": 509291,
          "created_utc": 1754187279,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/X5R/ik\\_llama.cpp](https://huggingface.co/X5R/ik_llama.cpp)",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a prebuilt windows binary for ik_llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mga3ox",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754194792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/X5R/ik_llama.cpp\"&gt;https://huggingface.co/X5R/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?auto=webp&amp;s=604c4ccc8334b2fd3bc45aef3d71c1f1387be183",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbe1684043115eb539138a0a7146d28825949011",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ce0f8475a5c3b84dea7c2f88281f21a3c1a17826",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=51d398818a25e58624880d21c1d75619f09a0da1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2015b60adf6417112d9c2005b37391d7d34c146",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a0022fb3e7640827993adc2f7fd214a61a84a85",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a6ae8f5d18d064bdac87a68ac91093637b93b373",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "3NAxKw7OcROuID1oQUkk1MnHTpmZCBizjeUqjivHzO0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mga3ox",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754194792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is just a little remark that if you haven't you definitely should try qwen code [https://github.com/QwenLM/qwen-code](https://github.com/QwenLM/qwen-code)   \nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. \n\nThe metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.\n\nSo this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. ",
          "author_fullname": "t2_n9dnke1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code + Qwen Coder 30b 3A is insane",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuiri",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 207,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 207,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754151475,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is just a little remark that if you haven&amp;#39;t you definitely should try qwen code &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br/&gt;\nI use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I&amp;#39;m working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt;\n\n&lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I&amp;#39;m very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt;\n\n&lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don&amp;#39;t know jack shit about coding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?auto=webp&amp;s=689c8ef1840eeb6bb6bb53a5908a342a3dac7558",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2382140736e501c0e5f725eb9004d88daaf4ddc",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c02fd612828e87be93104f75281f9696b4b69068",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef0b7f26ce8aa8b563291337614fb5656c8ab07",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=faaeb8e0dc19d67afd0d3197e7b7a5a0095a6f09",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eebbb21f63208ab9ba4a6a50b9890a328d60d830",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2edc81e58030926a09a63f9377700b5cd6432c27",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NjYLWOWVN8-HDnLfB2W4w77Codi5wABh6mT-TjOuaZ8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfuiri",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Flashy_Management962",
          "discussion_type": null,
          "num_comments": 101,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754151475,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I spend about 300-400 USD per month on Claude Code with the max 5x tier. I‚Äôm unsure when they‚Äôll increase pricing, limit usage, or make models less intelligent. I‚Äôm looking for a cheaper or open-source alternative that‚Äôs just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. \n\n\nEdit: I don‚Äôt pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I‚Äôm quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. \n\nThanks for all the suggestions. I‚Äôll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)",
          "author_fullname": "t2_67bb0jdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-source model that is as intelligent as Claude Sonnet 4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqejn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 355,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 355,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754151213,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754140871,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I‚Äôm unsure when they‚Äôll increase pricing, limit usage, or make models less intelligent. I‚Äôm looking for a cheaper or open-source alternative that‚Äôs just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt;\n\n&lt;p&gt;Edit: I don‚Äôt pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I‚Äôm quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt;\n\n&lt;p&gt;Thanks for all the suggestions. I‚Äôll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfqejn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "vishwa1238",
          "discussion_type": null,
          "num_comments": 271,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754140871,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA! \n\nJust released something that extends the recent [ICM paper](https://arxiv.org/abs/2506.10139) in a big way - using one model's coherent understanding to improve a completely different model.\n\n# Background: What is ICM?\n\nThe original [\"Unsupervised Elicitation of Language Models\"](https://arxiv.org/abs/2506.10139) paper showed something remarkable: **models can generate their own training labels by finding internally coherent patterns**.\n\nTheir key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are \"mutually predictable\" - where each label can be predicted from all the others.\n\n**Original ICM results**: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.\n\n# Our extension: Cross-model capability transfer\n\nWe took ICM further - **what if we use one model's coherent understanding to improve a completely different model?**\n\n**Our process:**\n\n1. Used ICM on Qwen3 to extract its coherent math reasoning patterns\n2. Generated DPO training data from Qwen3's coherent vs incoherent solutions\n3. Trained Gemma3 on this data - **Gemma3 learned from Qwen3's understanding**\n4. Zero external supervision, pure model-to-model knowledge transfer\n\n# Results on local models\n\n**Qwen3-0.6B**: 63.2 ‚Üí 66.0 MATH-500 (+4%) \\[original ICM self-improvement\\]  \n**Gemma3-1B**: 41.0 ‚Üí 45.6 MATH-500 (+11%) \\[**novel: learned from Qwen3!**\\]\n\n**The breakthrough**: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3's abilities across different architectures.\n\n# Why this matters beyond the original paper\n\n* **Cross-model knowledge transfer** \\- use any strong model to improve your local models\n* **Democratizes capabilities** \\- extract from closed/expensive models to improve open ones\n* **No training data needed** \\- pure capability extraction and transfer\n* **Scales the ICM concept** \\- from self-improvement to ecosystem-wide improvement\n\n# What's available\n\n* **Code**: [https://github.com/codelion/icm](https://github.com/codelion/icm)\n* **Both models**: Self-improved Qwen3 + Gemma3 (learned from Qwen3)\n* **Transfer pipeline**: Extract from any model to improve another\n* **Full writeup**: [https://huggingface.co/blog/codelion/internal-coherence-maximization](https://huggingface.co/blog/codelion/internal-coherence-maximization)\n\n# Quick start\n\n    git clone https://github.com/codelion/icm.git &amp;&amp; cd icm &amp;&amp; pip install -e .\n    \n    # Extract coherent patterns from a strong model (teacher)\n    icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n    \n    # Use those patterns to improve your local model (student)\n    icm export --format dpo --output-path teacher_knowledge.jsonl\n    # Train your model on teacher_knowledge.jsonl\n\nAnyone interested in trying capability transfer with their local models?",
          "author_fullname": "t2_e0bph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "üß† ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdur5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "#93b1ba",
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754208625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt;\n\n&lt;p&gt;Just released something that extends the recent &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;ICM paper&lt;/a&gt; in a big way - using one model&amp;#39;s coherent understanding to improve a completely different model.&lt;/p&gt;\n\n&lt;h1&gt;Background: What is ICM?&lt;/h1&gt;\n\n&lt;p&gt;The original &lt;a href=\"https://arxiv.org/abs/2506.10139\"&gt;&amp;quot;Unsupervised Elicitation of Language Models&amp;quot;&lt;/a&gt; paper showed something remarkable: &lt;strong&gt;models can generate their own training labels by finding internally coherent patterns&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &amp;quot;mutually predictable&amp;quot; - where each label can be predicted from all the others.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Original ICM results&lt;/strong&gt;: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.&lt;/p&gt;\n\n&lt;h1&gt;Our extension: Cross-model capability transfer&lt;/h1&gt;\n\n&lt;p&gt;We took ICM further - &lt;strong&gt;what if we use one model&amp;#39;s coherent understanding to improve a completely different model?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Our process:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Used ICM on Qwen3 to extract its coherent math reasoning patterns&lt;/li&gt;\n&lt;li&gt;Generated DPO training data from Qwen3&amp;#39;s coherent vs incoherent solutions&lt;/li&gt;\n&lt;li&gt;Trained Gemma3 on this data - &lt;strong&gt;Gemma3 learned from Qwen3&amp;#39;s understanding&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Zero external supervision, pure model-to-model knowledge transfer&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Results on local models&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-0.6B&lt;/strong&gt;: 63.2 ‚Üí 66.0 MATH-500 (+4%) [original ICM self-improvement]&lt;br/&gt;\n&lt;strong&gt;Gemma3-1B&lt;/strong&gt;: 41.0 ‚Üí 45.6 MATH-500 (+11%) [&lt;strong&gt;novel: learned from Qwen3!&lt;/strong&gt;]&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The breakthrough&lt;/strong&gt;: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3&amp;#39;s abilities across different architectures.&lt;/p&gt;\n\n&lt;h1&gt;Why this matters beyond the original paper&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Cross-model knowledge transfer&lt;/strong&gt; - use any strong model to improve your local models&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Democratizes capabilities&lt;/strong&gt; - extract from closed/expensive models to improve open ones&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;No training data needed&lt;/strong&gt; - pure capability extraction and transfer&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scales the ICM concept&lt;/strong&gt; - from self-improvement to ecosystem-wide improvement&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What&amp;#39;s available&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=\"https://github.com/codelion/icm\"&gt;https://github.com/codelion/icm&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Both models&lt;/strong&gt;: Self-improved Qwen3 + Gemma3 (learned from Qwen3)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Transfer pipeline&lt;/strong&gt;: Extract from any model to improve another&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Full writeup&lt;/strong&gt;: &lt;a href=\"https://huggingface.co/blog/codelion/internal-coherence-maximization\"&gt;https://huggingface.co/blog/codelion/internal-coherence-maximization&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Quick start&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/codelion/icm.git &amp;amp;&amp;amp; cd icm &amp;amp;&amp;amp; pip install -e .\n\n# Extract coherent patterns from a strong model (teacher)\nicm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500\n\n# Use those patterns to improve your local model (student)\nicm export --format dpo --output-path teacher_knowledge.jsonl\n# Train your model on teacher_knowledge.jsonl\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyone interested in trying capability transfer with their local models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdur5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "asankhs",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754208625,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, poor GPU guys\n\nA few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.\n\n\n\nSome married guys might understand, but I‚Äôve been using an m-ATX case I bought about 15 years ago, and there‚Äôs no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to ‚Äúreal‚Äù work.\n\nhttps://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\n\nOne of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That's why the ‚Äú32GB‚Äù model ended up in my hands.\n\n\n\nOf course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the ‚ÄúOriginal MI50‚Äù cannot be BIOS-re-flashed, I had given up.\n\n\n\nFirst, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.\n\nhttps://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\n\n  \n\n\n[4070+MI50 \\(22GB only\\)](https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96)\n\n\n\n[MI50 works alone upto 30GB](https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2)\n\n  \n\n\n Guys, hold your horses. I'm aware there are a few issues here.\n\n\n\n1) It's recognized as a Radeon Pro VII\n\n2) It runs on LM STUDIO, which some people really dislike\n\n3) Even if it's recognized as Vulkan, you can't use the combined VRAM of both cards‚Äîonly twice the VRAM of the first graphics card\n\n(On my PC, it's 12+12GB instead of 12+32GB)\n\n\\-&gt; However, I haven't tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX's prompt processing ability is an extra bonus.\n\n\n\n\n\n**Anyway, there are only three things you need to do.**\n\n\n\n1. Disable Secure Boot in the CMOS BIOS.\n\n2. Run PowerShell in administrator mode and enter the following command:\n\nbcdedit.exe -set TESTSIGNING on\n\n3. Download and install the Polaris-Vega-Navi driver created by the real pros.\n\n[R.ID - AMD 3rd Party Drivers](https://rdn-id.com/)\n\n\n\nAll risks are on you, but I think it's better than getting divorced by your wife over buying an RTX 6000,\n\nThe blower fan sent by the Ali seller is very effective, but it's incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.\n\n\n\nP.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn't recognize it at all. Do I really have to rebuild PyTorch from scratch?",
          "author_fullname": "t2_1dhesoqqtu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Successfully running INSTINCT MI50 on Win11",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "3cabctch4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ca1ba2f5dc37ce486326b1c5d7f492e6257e417"
                },
                {
                  "y": 100,
                  "x": 216,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b47948a9375686a348f3135013fad572be0e5bb9"
                },
                {
                  "y": 149,
                  "x": 320,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c54a61000b055f717d4c99392991a3016a75c607"
                },
                {
                  "y": 299,
                  "x": 640,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e82c15d58578bccb18253e91c4a9d5a4a136e4bf"
                },
                {
                  "y": 448,
                  "x": 960,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0908df44a99335aef9b4daf3dd4796f130b6f492"
                },
                {
                  "y": 504,
                  "x": 1080,
                  "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=098996b1e02247a51bd56e72b50def71975ccc73"
                }
              ],
              "s": {
                "y": 897,
                "x": 1920,
                "u": "https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96"
              },
              "id": "3cabctch4sgf1"
            },
            "oe4uyadb4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ac61e80aae254bda55f9b7cd6addf90d29a446b"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fae70f72d90191dd274df1040d6d75cbbbd6475"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0c28cb00db6a2547f8fead9579aa8b2fd3c53086"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9bdeb09d493de51c07e4fcc97ec462f8aac3b551"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9e51eb15dd64da3685f668083f579b2705a4c55e"
                }
              ],
              "s": {
                "y": 1280,
                "x": 960,
                "u": "https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80"
              },
              "id": "oe4uyadb4sgf1"
            },
            "lrtre5656sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 59,
                  "x": 108,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=445e311f264b6b0ba67f8c1a6fc639d332a2e5f8"
                },
                {
                  "y": 118,
                  "x": 216,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8becb210c0a1f400e0e6189f5b9ea272c0c7364"
                },
                {
                  "y": 175,
                  "x": 320,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b05acc3dcc8e5ab71a4080b7d0fb7e7f2dd9536"
                },
                {
                  "y": 350,
                  "x": 640,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ce822ce9b7ef000d574cada4c7c415bfe2d44912"
                },
                {
                  "y": 526,
                  "x": 960,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc5d5e9ad324bd8954b842c0285c298a6f3a5ba1"
                },
                {
                  "y": 591,
                  "x": 1080,
                  "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18c465dd09dbecdf9cec8ef723dc48df07a9ff22"
                }
              ],
              "s": {
                "y": 1032,
                "x": 1883,
                "u": "https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;format=png&amp;auto=webp&amp;s=f14e51095d28e301c62418f12f52586a62861ee2"
              },
              "id": "lrtre5656sgf1"
            },
            "pvictbhu4sgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2e7b199737b649bf0dc517eeb27f38372ace77a"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=296d5c0e017c541acde4c0789322db6a0ee8ee9f"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8871508585c0b72a963270b0a8f2d645d8162a54"
                },
                {
                  "y": 380,
                  "x": 640,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2f925d22a93079c9ac16e8bacc366d444a121475"
                },
                {
                  "y": 570,
                  "x": 960,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e4bce81c738e2313acab5f1b611fcbcd1bec79c"
                },
                {
                  "y": 642,
                  "x": 1080,
                  "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a90fa210d1e30a035719e4bdeba9f8afa3eb2e24"
                }
              ],
              "s": {
                "y": 1015,
                "x": 1707,
                "u": "https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;format=png&amp;auto=webp&amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9"
              },
              "id": "pvictbhu4sgf1"
            }
          },
          "name": "t3_1mgg3mh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ac8XNi9a_cAzytIhKD5Gg-Gwa0GLMmvvfiuUz9ia33E.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754217510,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, poor GPU guys&lt;/p&gt;\n\n&lt;p&gt;A few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.&lt;/p&gt;\n\n&lt;p&gt;Some married guys might understand, but I‚Äôve been using an m-ATX case I bought about 15 years ago, and there‚Äôs no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to ‚Äúreal‚Äù work.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80\"&gt;https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;One of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That&amp;#39;s why the ‚Äú32GB‚Äù model ended up in my hands.&lt;/p&gt;\n\n&lt;p&gt;Of course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the ‚ÄúOriginal MI50‚Äù cannot be BIOS-re-flashed, I had given up.&lt;/p&gt;\n\n&lt;p&gt;First, take a look at the results: the MI50 is running with  GTX 4070ti or alone on Windows.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9\"&gt;https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96\"&gt;4070+MI50 (22GB only)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f14e51095d28e301c62418f12f52586a62861ee2\"&gt;MI50 works alone upto 30GB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Guys, hold your horses. I&amp;#39;m aware there are a few issues here.&lt;/p&gt;\n\n&lt;p&gt;1) It&amp;#39;s recognized as a Radeon Pro VII&lt;/p&gt;\n\n&lt;p&gt;2) It runs on LM STUDIO, which some people really dislike&lt;/p&gt;\n\n&lt;p&gt;3) Even if it&amp;#39;s recognized as Vulkan, you can&amp;#39;t use the combined VRAM of both cards‚Äîonly twice the VRAM of the first graphics card&lt;/p&gt;\n\n&lt;p&gt;(On my PC, it&amp;#39;s 12+12GB instead of 12+32GB)&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; However, I haven&amp;#39;t tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX&amp;#39;s prompt processing ability is an extra bonus.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Anyway, there are only three things you need to do.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Disable Secure Boot in the CMOS BIOS.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run PowerShell in administrator mode and enter the following command:&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;bcdedit.exe -set TESTSIGNING on&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download and install the Polaris-Vega-Navi driver created by the real pros.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://rdn-id.com/\"&gt;R.ID - AMD 3rd Party Drivers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;All risks are on you, but I think it&amp;#39;s better than getting divorced by your wife over buying an RTX 6000,&lt;/p&gt;\n\n&lt;p&gt;The blower fan sent by the Ali seller is very effective, but it&amp;#39;s incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.&lt;/p&gt;\n\n&lt;p&gt;P.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn&amp;#39;t recognize it at all. Do I really have to rebuild PyTorch from scratch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgg3mh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Sir-5088",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754217510,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?",
          "author_fullname": "t2_9vs7zm4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news on updated Qwen3-8B/14B versions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg6xia",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754184771,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg6xia",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zyxwvu54321",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754184771,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.\n\nThe take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.\n\nWe need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.",
          "author_fullname": "t2_1puly589vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3d62",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174302,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt;\n\n&lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt;\n\n&lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg3d62",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackdareel",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754174302,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "all I need....",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgj0g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 1456,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1456,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CGttwXkoUDYh4g0k8yxukZYUVavOKH_4hrW2zTWYkis.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754105691,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?auto=webp&amp;s=56a0a807d6543ba43266157bcaa0b48793bcd3df",
                  "width": 1024,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0faf4ee7c2bcdc1e4161739543ede55c4684b2b8",
                    "width": 108,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2e17cf5370e4cca658ed8a629406a0522d68d7e",
                    "width": 216,
                    "height": 324
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ffb58e48e0282066e26e22c2d00d814c00c5ded2",
                    "width": 320,
                    "height": 480
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd",
                    "width": 640,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/ggc3dzhr0jgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0d2db9397b5cffd5e90a0cc1250191ed983494cb",
                    "width": 960,
                    "height": 1440
                  }
                ],
                "variants": {},
                "id": "FTO20S33694FU83Br474HCo8srCRihH9Gw7Oe1llTgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mfgj0g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 110,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgj0g/all_i_need/",
          "stickied": false,
          "url": "https://i.redd.it/ggc3dzhr0jgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754105691,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was using this model as an assistant to modify code in a C++ file with \\~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:\n\n&gt;Here is the modification of the code:\n\n&gt;*\\*code\\**\n\n&gt;But on a second thought, that was not a good implementation, here is a better method to do it:\n\n&gt;*\\*code\\**\n\n&gt;But on a third thought, here is a better way to do it...\n\nAnd each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:\n\n&gt;How do I set a left, right and bottom border on a div in CSS?\n\nIt then provided me with 4 methods. Notice how **method 1** and **method 4** is the *exact same* code repeated. **Method 2** is also almost the exact same code but just with an added `border: none;` line of code. \n\nAlso, **method 3** has a mistake where it do not to set a border on the right side of the div, it wrote `border-width: 0 0 2px 2px;` but the correct code would be `border-width: 0 2px 2px 2px;`\n\n**The output:**\n\n# Method 1: Using Individual Border Properties\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 2: Using the Shorthand¬†border¬†Property\n\n    div {\n        border: none; /* Reset all borders first */\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\n# Method 3: Using¬†border-width,¬†border-style, and¬†border-color\n\n    div {\n        border-width: 0 0 2px 2px; /* top right bottom left */\n        border-style: solid;\n        border-color: black;\n    }\n\n# Method 4: More Concise Shorthand\n\n    div {\n        border-left: 2px solid black;\n        border-right: 2px solid black;\n        border-bottom: 2px solid black;\n    }\n\nI'm using Unsloth's UD-Q5\\_K\\_XL quant with the recommended settings:\n\n* Temperature: **0,7**\n* Top K: **20**\n* Repeat Penalty: **1,05**\n* Top P: **0,8**\n\nAnyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?",
          "author_fullname": "t2_qhlcbiy3k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you also get weird behavior from Qwen3-Coder-30B-A3B?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfs7l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754216810,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754216282,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using this model as an assistant to modify code in a C++ file with ~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Here is the modification of the code:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a second thought, that was not a good implementation, here is a better method to do it:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;code**&lt;/p&gt;\n\n&lt;p&gt;But on a third thought, here is a better way to do it...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;And each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;How do I set a left, right and bottom border on a div in CSS?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It then provided me with 4 methods. Notice how &lt;strong&gt;method 1&lt;/strong&gt; and &lt;strong&gt;method 4&lt;/strong&gt; is the &lt;em&gt;exact same&lt;/em&gt; code repeated. &lt;strong&gt;Method 2&lt;/strong&gt; is also almost the exact same code but just with an added &lt;code&gt;border: none;&lt;/code&gt; line of code. &lt;/p&gt;\n\n&lt;p&gt;Also, &lt;strong&gt;method 3&lt;/strong&gt; has a mistake where it do not to set a border on the right side of the div, it wrote &lt;code&gt;border-width: 0 0 2px 2px;&lt;/code&gt; but the correct code would be &lt;code&gt;border-width: 0 2px 2px 2px;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The output:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;Method 1: Using Individual Border Properties&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 2: Using the Shorthand¬†border¬†Property&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border: none; /* Reset all borders first */\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 3: Using¬†border-width,¬†border-style, and¬†border-color&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-width: 0 0 2px 2px; /* top right bottom left */\n    border-style: solid;\n    border-color: black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Method 4: More Concise Shorthand&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;div {\n    border-left: 2px solid black;\n    border-right: 2px solid black;\n    border-bottom: 2px solid black;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;m using Unsloth&amp;#39;s UD-Q5_K_XL quant with the recommended settings:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Temperature: &lt;strong&gt;0,7&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top K: &lt;strong&gt;20&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Repeat Penalty: &lt;strong&gt;1,05&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Top P: &lt;strong&gt;0,8&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgfs7l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Admirable-Star7088",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754216282,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.\n\nIf you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.\n\n2060 - 336.0 GB/s - $150 8GB  \n3060 - 360.0 GB/s - $200+ 8GB\n\n4060 - 272.0 GB/s - $260+ 8GB\n\n5060 - 448.0 GB/s - $350+ 8GB\n\nP102-100 - 440.3 GB/s - $60 10GB.\n\nIs the P102-100 faster than an\n\nentry 2060 = yes\n\nentry 3060 = yes\n\nentry 4060 = yes.\n\nonly a 5060 would be faster and not by much.\n\nDoes the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.\n\nI have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.\n\nso if you want to try LLM's and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.\n\nI hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.\n\nI will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.",
          "author_fullname": "t2_vnvnb9oa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "is the P102-100 still a viable option for LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdh6r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/ZS0zNrB2tczhH3IbcM5RQIgldEEZAXIMP8grHZwjSys.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754207144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.&lt;/p&gt;\n\n&lt;p&gt;If you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.&lt;/p&gt;\n\n&lt;p&gt;2060 - 336.0 GB/s - $150 8GB&lt;br/&gt;\n3060 - 360.0 GB/s - $200+ 8GB&lt;/p&gt;\n\n&lt;p&gt;4060 - 272.0 GB/s - $260+ 8GB&lt;/p&gt;\n\n&lt;p&gt;5060 - 448.0 GB/s - $350+ 8GB&lt;/p&gt;\n\n&lt;p&gt;P102-100 - 440.3 GB/s - $60 10GB.&lt;/p&gt;\n\n&lt;p&gt;Is the P102-100 faster than an&lt;/p&gt;\n\n&lt;p&gt;entry 2060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 3060 = yes&lt;/p&gt;\n\n&lt;p&gt;entry 4060 = yes.&lt;/p&gt;\n\n&lt;p&gt;only a 5060 would be faster and not by much.&lt;/p&gt;\n\n&lt;p&gt;Does the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.&lt;/p&gt;\n\n&lt;p&gt;I have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.&lt;/p&gt;\n\n&lt;p&gt;so if you want to try LLM&amp;#39;s and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.&lt;/p&gt;\n\n&lt;p&gt;I hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.&lt;/p&gt;\n\n&lt;p&gt;I will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oy25ru8gergf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oy25ru8gergf1.png?auto=webp&amp;s=2cb0173f8f29e9ebb6d02dbbb0f1f6505163ed23",
                  "width": 1920,
                  "height": 4096
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6ead78ede2dee21c50ed7920d88cdc0f039342ea",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21261a21b67d3216bb8f5267a2eab08d1c3e30f8",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=716e70095d944e7d0d07c12b2d069e03b9f72dc9",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd72000cba8efed634dc539ff393fe099624df46",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4cccd911c998a5d7c914cf44d2385aae56de54c",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/oy25ru8gergf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ee00d9096b93e074ccad355d7571f837459808a",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {},
                "id": "tb8jcmr9JwUFdjGqCEioOHi2smnbOEbTGPoqKGaYDxE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdh6r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Boricua-vet",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdh6r/is_the_p102100_still_a_viable_option_for_llm/",
          "stickied": false,
          "url": "https://i.redd.it/oy25ru8gergf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754207144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[A new PR](https://github.com/ggml-org/llama.cpp/pull/15026) was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated [\\#14939](https://github.com/ggml-org/llama.cpp/pull/14939) seemed to get stuck. The new PR description reads: \"**this PR will NOT attempt to implement MTP**\", with great progress being made in short time. (Amazing!!!)\n\nGiven that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.\n\nDisclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!\n\nPS: does MTP already work on/with MLX?",
          "author_fullname": "t2_omawcpyf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfvxdo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754155024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15026\"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5&amp;#39;s models in llama.cpp, as the original, highly anticipated &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14939\"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt;\n\n&lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it&amp;#39;s not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I am eternally grateful for everybody&amp;#39;s contribution to the field, as LLMs allow me to code what I couldn&amp;#39;t code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt;\n\n&lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?auto=webp&amp;s=acc8e28264bfccdff7bd8f443fed065d174ea9e0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=192b46649336ccde82b3df88264f14e8c3af5057",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d87e157d503386c5e4e8e7524115ddb67fb4d525",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2253f5fea3f3de0ade685e30f24f2fe47d41bed",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=076b473df32cd3180067a97b9fe5da2c8755fd75",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc3f7d46febf1a47049086579c27beff59d8b7b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfebb15c0a042499f527df12e1302a2dc2a17608",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "nu0lYx9MvX_F3y_OrRWNrMtDpRRlG8lzyxoJbWZ3NRg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfvxdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Karim_acing_it",
          "discussion_type": null,
          "num_comments": 38,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754155024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using `llama.cpp` with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here‚Äôs what actually works.\n\n**KV Quantization**\n\n* **KV cache quantization matters a lot**. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. ~~Use q5\\_1 for a good balance of memory usage and performance~~. It works well in PPL tests and in practice. **UPDATE:** K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:\n\n|CTK - CTD|PPL|STD|VRAM|\n|:-|:-|:-|:-|\n|q8\\_0 - q8\\_0|6.9016|0.04818|10.1GB|\n|q8\\_0 - q4\\_0|6.9104|0.04822|9.6GB|\n|q4\\_0 - q8\\_0|7.1241|0.04963|9.6GB|\n|q5\\_1 - q5\\_1|6.9664|0.04872|9.5GB|\n\n* **TLDR:** looks like q8\\_0 q4\\_0 is a very nice tradeoff in terms of accuracy and vram usage\n\n**Offloading Strategy**\n\n* You're bottlenecked by your **system RAM bandwidth** when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.\n* Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU‚Äôs VRAM limit. More offloading = slower inference.blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n* If you dont understand what the regex does, just feed it to and llm and it'll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.\n\n**Memory Tuning for CPU Offloading**\n\n* System memory speed has a major impact on throughput when using partial offloading.\n* Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.\n* On **AM4** platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.\n* On **AM5**, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.\n* Poor memory tuning will bottleneck your CPU offloading even with a fast processor.\n\n**ubatch (Prompt Batch Size)**\n\n* Higher `ubatch` values significantly improve prompt processing (PP) performance.\n* Try values like `768` or `1024`. You‚Äôll use more VRAM, but it‚Äôs often worth it for the speedup.\n* If you‚Äôre VRAM-limited, lower this until it fits.\n\n**Extra Performance Boost**\n\n* Set this environment variable for a 5‚Äì10% performance gain:Launch like this: LLAMA\\_SET\\_ROWS=1 ./llama-server -md /path/to/model etc.\n\n**Speculative Decoding Tips (SD)**\n\nSpeculative decoding is supported in `llama.cpp`, but there are a couple important caveats:\n\n1. **KV cache quant affects acceptance rate heavily.** Using `q4_0` for the draft model‚Äôs KV cache *halves* the acceptance rate in my testing. Use ~~q5\\_1 or even~~ `q8_0`for the draft model KV cache for much better performance. **UPDATE:** \\-ctkd q8\\_0 -ctvd q4\\_0 works like a charm and saves vram. K is much more sensitive to quantization.\n2. **Draft model context handling is broken after filling the draft KV cache.** Once the draft model‚Äôs context fills up, performance tanks. Right now it‚Äôs better to run the draft with full context size. Reducing it actually hurts.\n3. **Draft parameters matter a lot**. In my testing, using `--draft-p-min 0.85 --draft-min 2 --draft-max 12` gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.\n\nFor SD, try using **Qwen 3 0.6B** as the draft model. It‚Äôs fast and works well, as long as you avoid the issues above.\n\nIf you‚Äôve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp",
          "author_fullname": "t2_66tlmx2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfs9qn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 120,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 120,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754223074,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754145881,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here‚Äôs what actually works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you&amp;#39;re offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. &lt;del&gt;Use q5_1 for a good balance of memory usage and performance&lt;/del&gt;. It works well in PPL tests and in practice. &lt;strong&gt;UPDATE:&lt;/strong&gt; K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;CTK - CTD&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;th align=\"left\"&gt;STD&lt;/th&gt;\n&lt;th align=\"left\"&gt;VRAM&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9016&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04818&lt;/td&gt;\n&lt;td align=\"left\"&gt;10.1GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9104&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04822&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q4_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.1241&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04963&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q5_1 - q5_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9664&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04872&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; looks like q8_0 q4_0 is a very nice tradeoff in terms of accuracy and vram usage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You&amp;#39;re bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt;\n&lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU‚Äôs VRAM limit. More offloading = slower inference.blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/li&gt;\n&lt;li&gt;If you dont understand what the regex does, just feed it to and llm and it&amp;#39;ll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt;\n&lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you&amp;#39;re comfortable doing so.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt;\n&lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt;\n&lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You‚Äôll use more VRAM, but it‚Äôs often worth it for the speedup.&lt;/li&gt;\n&lt;li&gt;If you‚Äôre VRAM-limited, lower this until it fits.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set this environment variable for a 5‚Äì10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model‚Äôs KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;del&gt;q5_1 or even&lt;/del&gt; &lt;code&gt;q8_0&lt;/code&gt;for the draft model KV cache for much better performance. &lt;strong&gt;UPDATE:&lt;/strong&gt; -ctkd q8_0 -ctvd q4_0 works like a charm and saves vram. K is much more sensitive to quantization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model‚Äôs context fills up, performance tanks. Right now it‚Äôs better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It‚Äôs fast and works well, as long as you avoid the issues above.&lt;/p&gt;\n\n&lt;p&gt;If you‚Äôve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfs9qn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AliNT77",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754145881,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just¬†shipped something¬†I'm really¬†excited about!¬†üöÄ\nI was¬†scrolling through my feed and saw¬†Sebastian Raschka, PhD 's incredible¬†Qwen3 MoE implementation¬†in PyTorch. The educational¬†clarity¬†of¬†his code¬†just¬†blew me¬†away - especially¬†how he broke¬†down the¬†Mixture¬†of Experts architecture¬†in¬†his¬†LLMs-from-scratch repo.\nThat¬†got¬†me thinking... what if I could bring¬†this¬†to¬†pure¬†C?¬†ü§î\nInspired by Andrej Karpathy's legendary¬†llama2.c approach¬†(seriously, if¬†you haven't seen¬†it, check¬†it out), I decided to take¬†on the challenge of implementing¬†Qwen3's¬†30B parameter¬†model with¬†128 experts¬†in a¬†single C file.\nThe result?¬†Qwen_MOE_C¬†- a complete¬†inference engine that:\n‚úÖ Handles¬†sparse MoE computation¬†(only¬†8 out¬†of 128 experts¬†active)\n‚úÖ Supports¬†Grouped Query Attention with¬†proper head ratios\n‚úÖ Uses¬†memory¬†mapping¬†for efficiency (~30GB¬†models)\n‚úÖ Zero external¬†dependencies (just¬†libc¬†+ libm)\nThe beauty¬†of¬†this approach is the¬†same as llama2.c¬†- you can understand every¬†line, it's hackable, and it runs¬†anywhere C¬†runs. No¬†frameworks, no dependencies, just pure computational¬†transparency.\nHuge thanks to Sebastian¬†Raschka for the¬†reference implementation and educational¬†materials, and to Andrej¬†Karpathy for¬†showing us that¬†simplicity is¬†the ultimate sophistication in¬†ML systems.\nSometimes¬†the best way¬†to truly understand something is to build it from scratch.¬†üõ†Ô∏è\nLink¬†to the project: \nhttps://github.com/h9-tec/Qwen_MOE_C",
          "author_fullname": "t2_59165ick",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen moe in C",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfxas1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 57,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 57,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754158458,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just¬†shipped something¬†I&amp;#39;m really¬†excited about!¬†üöÄ\nI was¬†scrolling through my feed and saw¬†Sebastian Raschka, PhD &amp;#39;s incredible¬†Qwen3 MoE implementation¬†in PyTorch. The educational¬†clarity¬†of¬†his code¬†just¬†blew me¬†away - especially¬†how he broke¬†down the¬†Mixture¬†of Experts architecture¬†in¬†his¬†LLMs-from-scratch repo.\nThat¬†got¬†me thinking... what if I could bring¬†this¬†to¬†pure¬†C?¬†ü§î\nInspired by Andrej Karpathy&amp;#39;s legendary¬†llama2.c approach¬†(seriously, if¬†you haven&amp;#39;t seen¬†it, check¬†it out), I decided to take¬†on the challenge of implementing¬†Qwen3&amp;#39;s¬†30B parameter¬†model with¬†128 experts¬†in a¬†single C file.\nThe result?¬†Qwen_MOE_C¬†- a complete¬†inference engine that:\n‚úÖ Handles¬†sparse MoE computation¬†(only¬†8 out¬†of 128 experts¬†active)\n‚úÖ Supports¬†Grouped Query Attention with¬†proper head ratios\n‚úÖ Uses¬†memory¬†mapping¬†for efficiency (~30GB¬†models)\n‚úÖ Zero external¬†dependencies (just¬†libc¬†+ libm)\nThe beauty¬†of¬†this approach is the¬†same as llama2.c¬†- you can understand every¬†line, it&amp;#39;s hackable, and it runs¬†anywhere C¬†runs. No¬†frameworks, no dependencies, just pure computational¬†transparency.\nHuge thanks to Sebastian¬†Raschka for the¬†reference implementation and educational¬†materials, and to Andrej¬†Karpathy for¬†showing us that¬†simplicity is¬†the ultimate sophistication in¬†ML systems.\nSometimes¬†the best way¬†to truly understand something is to build it from scratch.¬†üõ†Ô∏è\nLink¬†to the project: \n&lt;a href=\"https://github.com/h9-tec/Qwen_MOE_C\"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?auto=webp&amp;s=e5cdc2248f745a90988be621db7d6b6be3916db2",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbd8f00d966699e72ff3a93f578256c7537d2135",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3afe3fea53292db66353b624f6471827586e1c10",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=023f7331df4213637fe882078ebd6fb043d92076",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99f557d7c6c92fc79bbc53fbd9519fb90c4895ac",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=55d41da3d53c6240a1417760903816f861d7b857",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=869ae3899c29fb68da394dceac5b0968c64c5e51",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "WSjvG1bkXS5RFYUXSqQk_Sce1jxvdesMgCPwzgoYYNg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfxas1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "1Hesham",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754158458,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yet another new model claiming to outperform larger ones:\n\n\n\n**Instruction following** is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.\n\nWe identify **lazy reasoning** during the thinking stage as a key cause of poor instruction adherence.\n\nTo address this, we propose a framework that promotes rigorous reasoning through **previewing and self-checking**.\n\nOur method begins by generating instruction data with **complex constraints**, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.\n\nTraining involves entropy-preserving supervised fine-tuning (**Entropy-SFT**) and token-wise entropy-adaptive reinforcement learning (**TEA-RL**), guided by rule-based multidimensional rewards.\n\nThis approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.\n\nExperiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like **DeepSeek-R1** and closed-source models like **ChatGPT-4o** on challenging instruction-following benchmarks.\n\n[https://huggingface.co/qihoo360/Light-IF-32B](https://huggingface.co/qihoo360/Light-IF-32B)\n\n\n\ntechnical report [https://huggingface.co/papers/2503.10460](https://huggingface.co/papers/2503.10460)\n\n\n\nprevious popular models by this company:\n\n[https://huggingface.co/qihoo360/TinyR1-32B-Preview](https://huggingface.co/qihoo360/TinyR1-32B-Preview)\n\n[https://huggingface.co/qihoo360/Light-R1-32B](https://huggingface.co/qihoo360/Light-R1-32B)\n\n  \nWhat do you think?",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "qihoo360/Light-IF-32B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghy1u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QIubNPPJjdRirxQ73mfKHMMmxIY2VZ9TzcjXGXjIohk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another new model claiming to outperform larger ones:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; is a core ability of large language models (LLMs), but performance remains inconsistent, especially on complex tasks.&lt;/p&gt;\n\n&lt;p&gt;We identify &lt;strong&gt;lazy reasoning&lt;/strong&gt; during the thinking stage as a key cause of poor instruction adherence.&lt;/p&gt;\n\n&lt;p&gt;To address this, we propose a framework that promotes rigorous reasoning through &lt;strong&gt;previewing and self-checking&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Our method begins by generating instruction data with &lt;strong&gt;complex constraints&lt;/strong&gt;, filtering out samples that are too easy or too difficult. We then use rejection sampling to build a small but high-quality dataset for model adaptation.&lt;/p&gt;\n\n&lt;p&gt;Training involves entropy-preserving supervised fine-tuning (&lt;strong&gt;Entropy-SFT&lt;/strong&gt;) and token-wise entropy-adaptive reinforcement learning (&lt;strong&gt;TEA-RL&lt;/strong&gt;), guided by rule-based multidimensional rewards.&lt;/p&gt;\n\n&lt;p&gt;This approach encourages models to plan ahead and verify their outputs, fostering more generalizable reasoning abilities.&lt;/p&gt;\n\n&lt;p&gt;Experiments show consistent improvements across model sizes. Notably, our 32B model outperforms both larger open-source models like &lt;strong&gt;DeepSeek-R1&lt;/strong&gt; and closed-source models like &lt;strong&gt;ChatGPT-4o&lt;/strong&gt; on challenging instruction-following benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-IF-32B\"&gt;https://huggingface.co/qihoo360/Light-IF-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;technical report &lt;a href=\"https://huggingface.co/papers/2503.10460\"&gt;https://huggingface.co/papers/2503.10460&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;previous popular models by this company:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/TinyR1-32B-Preview\"&gt;https://huggingface.co/qihoo360/TinyR1-32B-Preview&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/qihoo360/Light-R1-32B\"&gt;https://huggingface.co/qihoo360/Light-R1-32B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6vaf0crhrsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6vaf0crhrsgf1.png?auto=webp&amp;s=6ba618920def17d94508256824e2561aba8a6ec9",
                  "width": 1062,
                  "height": 980
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=081b3d7480032b122c209477d47263419358b811",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5fac696d38efe6b971413c8794edd434ae2c9926",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=20af7c29a89f974a98e34d36ff62aa93c6d3e970",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=771c3421ec90e5339a70e9664ac80ef7b729ca73",
                    "width": 640,
                    "height": 590
                  },
                  {
                    "url": "https://preview.redd.it/6vaf0crhrsgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=753db7d1045cf75fdd0e7ff4fd0c94209e8cee25",
                    "width": 960,
                    "height": 885
                  }
                ],
                "variants": {},
                "id": "O7-1ZfpSudq0amigVzUb6mHn4tEC8x9xtRLUGpzh3sI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghy1u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghy1u/qihoo360lightif32b/",
          "stickied": false,
          "url": "https://i.redd.it/6vaf0crhrsgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754223868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HRM model was trained on test?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 35,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgd3lh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OCP539vRz8IWhc6Q3DthwHNfIKX-3i-EHCMDWSNvU1s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754205660,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/e4op2j02argf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/e4op2j02argf1.png?auto=webp&amp;s=e1cb3a7b4fab6a83b18ec019a437dd0066760a90",
                  "width": 1080,
                  "height": 271
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3719305d4458c68cccd4a2b97547cb6bc4be5251",
                    "width": 108,
                    "height": 27
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c08316fa1ffb2aae3cf07f96549867a06e61be34",
                    "width": 216,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=91eba80ed0ddcdeabe96e4902e4d87e705f19c16",
                    "width": 320,
                    "height": 80
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8eece1e33073e0e4fe5801c6917464a55d051355",
                    "width": 640,
                    "height": 160
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=912cb994049f506e225f2cabc4f96be28f0d1c2a",
                    "width": 960,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/e4op2j02argf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a004430fb14ff34c77d8b66a8195c0b002006f5e",
                    "width": 1080,
                    "height": 271
                  }
                ],
                "variants": {},
                "id": "nwQrGJrBDCDo-K7wxSfT8eAh3-H9AN45BOnvDCwUQPo"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgd3lh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/",
          "stickied": false,
          "url": "https://i.redd.it/e4op2j02argf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754205660,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sam Altman promised imminent release of open source/weight models . It seems we haven‚Äôt heard anything new in the past few weeks, have we?",
          "author_fullname": "t2_dmaijfods",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any news about the open source models that OpenAI promised to release ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1e80",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 33,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 33,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754169032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sam Altman promised imminent release of open source/weight models . It seems we haven‚Äôt heard anything new in the past few weeks, have we?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg1e80",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NeedleworkerDull7886",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754169032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Does anyone have any knowledge on how to correctly set up tool calling for GLM 4.5 Air in LM Studio? The model is great with the old school tool calling techniques that aider and cline use, but when I try with any tools that use modern tool calling (Qwen Code or OpenCode), the tool calling fails.\n\n  \nIf anyone can give me an idea where to start here I can look into it myself. I'm guessing maybe some change to the jinja prompt would sort it. I am a developer, and I've written my own tool calling agents with ad hoc methods, but I'm not familiar with the new standardised tool calling format(s?).",
          "author_fullname": "t2_12ggykute6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GLM 4.5 Air local tool calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghd3l",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754222013,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any knowledge on how to correctly set up tool calling for GLM 4.5 Air in LM Studio? The model is great with the old school tool calling techniques that aider and cline use, but when I try with any tools that use modern tool calling (Qwen Code or OpenCode), the tool calling fails.&lt;/p&gt;\n\n&lt;p&gt;If anyone can give me an idea where to start here I can look into it myself. I&amp;#39;m guessing maybe some change to the jinja prompt would sort it. I am a developer, and I&amp;#39;ve written my own tool calling agents with ad hoc methods, but I&amp;#39;m not familiar with the new standardised tool calling format(s?).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mghd3l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "-dysangel-",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mghd3l/glm_45_air_local_tool_calling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mghd3l/glm_45_air_local_tool_calling/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754222013,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4\\_K\\_S.gguf +LM Studio 0.3.21 (Build 3).  \nAfter initial folder and file read (`app/main.go`, `configs.json`, etc.), it keeps replying:  \n*\"I'm ready to assist with your project in /srv/testproject...\"*\n\nIt ignores direct inputs like:\n\n* \"What does this application do?\"\n* \"Explain me the project\"\n* \"Give me a function list\"\n* \"List all files\"\n\nNo actual answers, just the same boilerplate response:\n\nUnderstood. I'm ready to assist with your project in /srv/testproject. Let me know what you'd like to do‚Äîwhether it's modifying code, adding features, debugging, or exploring the structure.\n\nAnyone else experiencing this with the latest combo? Misconfigured or bug?\n\n  \n\\--\n\nAs example Qwen 14B works fine.",
          "author_fullname": "t2_mhb0rkd4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf + LM Studio 0.3.21 (Build 3): Assistant ignores questions, stuck in loop",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgeerv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754210842,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Testing Qwen Coder CLI with Qwen3-30B-A3B-Instruct-2507-Q4_K_S.gguf +LM Studio 0.3.21 (Build 3).&lt;br/&gt;\nAfter initial folder and file read (&lt;code&gt;app/main.go&lt;/code&gt;, &lt;code&gt;configs.json&lt;/code&gt;, etc.), it keeps replying:&lt;br/&gt;\n&lt;em&gt;&amp;quot;I&amp;#39;m ready to assist with your project in /srv/testproject...&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;It ignores direct inputs like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;What does this application do?&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Explain me the project&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;Give me a function list&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;List all files&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;No actual answers, just the same boilerplate response:&lt;/p&gt;\n\n&lt;p&gt;Understood. I&amp;#39;m ready to assist with your project in /srv/testproject. Let me know what you&amp;#39;d like to do‚Äîwhether it&amp;#39;s modifying code, adding features, debugging, or exploring the structure.&lt;/p&gt;\n\n&lt;p&gt;Anyone else experiencing this with the latest combo? Misconfigured or bug?&lt;/p&gt;\n\n&lt;p&gt;--&lt;/p&gt;\n\n&lt;p&gt;As example Qwen 14B works fine.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgeerv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Eden63",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgeerv/qwen330ba3binstruct2507q4_k_sgguf_lm_studio_0321/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754210842,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.\n\nI already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an [Awesome list](https://github.com/sindresorhus/awesome).\n\nTo have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: [https://aibenchmarks.net/](https://aibenchmarks.net/)\n\nAwesome AI Benchmarks GitHub repository available here: [https://github.com/panilya/awesome-ai-benchmarks](https://github.com/panilya/awesome-ai-benchmarks)\n\nWould be happy to hear any feedback on this and whether it can be useful for you :)",
          "author_fullname": "t2_7arjztn1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "100+ AI Benchmarks list",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwckf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 50,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 50,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754156092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve created an Awesome AI Benchmarks GitHub repository with already 100+ benchmarks added for different domains.&lt;/p&gt;\n\n&lt;p&gt;I already had a Google Sheets document with those benchmarks and their details and thought it would be great to not waste that and create an &lt;a href=\"https://github.com/sindresorhus/awesome\"&gt;Awesome list&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;To have some fun I made a dynamically generated website from the benchmarks listed in README.md. You can check this website here: &lt;a href=\"https://aibenchmarks.net/\"&gt;https://aibenchmarks.net/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Awesome AI Benchmarks GitHub repository available here: &lt;a href=\"https://github.com/panilya/awesome-ai-benchmarks\"&gt;https://github.com/panilya/awesome-ai-benchmarks&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would be happy to hear any feedback on this and whether it can be useful for you :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?auto=webp&amp;s=41e93dde0345b37cb0200da115b435ce77df3930",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=93761f053553e9b1cfb16372cdf0e7dac51f1f5e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b480826b22f435da297cc6f87648622fc3892fe6",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=08c9c7135e744081cfec8c73e2a489375eb611be",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=63909a628f633b135db8a2f2ad24f375a444a9b7",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dcf264d960a58dd8659a36b59ab18b32c8762353",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6522fe068f5cf8114a6d791bc9bbc352abd7a38",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ASQhfiygebxNf8DoDi3MvBbcoqvCP-ZjCV6b0G2_Bwg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfwckf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "panilyaU",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfwckf/100_ai_benchmarks_list/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754156092,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn't quite process them correctly.\n    \nThis is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.\n    \nSpecifically, this fixes the following:\n    \n- Chat dates are maintained\n- Chat hierarchy is preserved\n- Empty conversations are skipped\n- Parent-child relationships are maintained\n    \nIn addition, it will skip malformed conversations and try to import each chat only once using a `imported.json` file.\n    \nYou can export your ChatGPT conversations by going to Settings ‚Üí Data controls ‚Üí Export data ‚Üí Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to `~/chatgpt/chatgpt-export.json`.\n    \nI recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your `webui.db` file.\n    \nAfter importing, you can view your conversations in Open-WebUI by going to Settings ‚Üí Chats ‚Üí Import and selecting the converted JSON file.\n    \nI like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the `imported.json` file correctly.\n    \nThis works for me, and I hope it works for you too! PRs and issues are welcome.",
          "author_fullname": "t2_44be3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Convert your ChatGTP exported conversations to something that Open-WebUI can import",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg12k4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7aaf741530697098b43e0167db0c1c5b5305d19b",
          "edited": 1754169308,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754168175,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the spirit of local AI, I prefer to migrate all of my existing ChatGPT conversations to Open-WebUI. Unfortunatly, the Open-WebUI import function doesn&amp;#39;t quite process them correctly.&lt;/p&gt;\n\n&lt;p&gt;This is a simple python script that attempts to reformat your ChatGPT exported conversations into a format that Open-WebUI can import.&lt;/p&gt;\n\n&lt;p&gt;Specifically, this fixes the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Chat dates are maintained&lt;/li&gt;\n&lt;li&gt;Chat hierarchy is preserved&lt;/li&gt;\n&lt;li&gt;Empty conversations are skipped&lt;/li&gt;\n&lt;li&gt;Parent-child relationships are maintained&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In addition, it will skip malformed conversations and try to import each chat only once using a &lt;code&gt;imported.json&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;You can export your ChatGPT conversations by going to Settings ‚Üí Data controls ‚Üí Export data ‚Üí Request export. Once you receive the email, download and extract the export, and copy the conversations.json file to &lt;code&gt;~/chatgpt/chatgpt-export.json&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I recommend backing up your Open-WebUI database before importing anything. You can do this by stopping Open-WebUI and making a copy of your &lt;code&gt;webui.db&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;p&gt;After importing, you can view your conversations in Open-WebUI by going to Settings ‚Üí Chats ‚Üí Import and selecting the converted JSON file.&lt;/p&gt;\n\n&lt;p&gt;I like to delete all chats from ChatGPT between export and import cycles to minimize duplicates. This way, the next export only contains new chats, but this should not be necessary if you are using the &lt;code&gt;imported.json&lt;/code&gt; file correctly.&lt;/p&gt;\n\n&lt;p&gt;This works for me, and I hope it works for you too! PRs and issues are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?auto=webp&amp;s=e22d770e4c02c756fc73d7d90a385e5b112f15b5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed1ab2121b7badab4580a484ba08cc844d5e16e5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b9afd23fae3b50eabe8826fbaa8cb3d0f9a3e2c",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ddddd80938d359fbb1d0dd7d1abbf05f3a86de7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b17fc3d36981f925574075a8743bee7f8626049a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=998b355322b3740f1ade9410e49eaa05da91b8fc",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0e7de199e0866090536334a1fd28c6c4dff2939",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HUDVH48IUY-iR1kZ-2wF0SfxzexjRfgIHB_lQ45z_o0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg12k4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "scubanarc",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg12k4/convert_your_chatgtp_exported_conversations_to/",
          "stickied": false,
          "url": "https://github.com/scubanarc/chatgpt-to-open-webui/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754168175,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.\n\nIf I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.\n\nSometimes, both sides are the same.  (E.g., \"Are you helpless or do you need my help?\")\n\nSometimes, they're so unbalanced it becomes a Mitch Hedberg-style question. (E.g., \"Have you ever tried sugar or PCP?\")\n\nSometimes, a very open-ended question is presented as either/or. (E.g., \"Is your favorite CSS color value #ff73c1 or #2141af?\" like those are the only two options.)\n\nI have found myself utterly unable to affect this behavior at all through the system prompt. I've tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with \"Don't...\" instructions) I've tried prompting it not to use \"either/or\" questions, \"A or B?\" questions, questions that limit the user's options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.\n\nAnd if I bring it up in chat, I get Qwen3's usual long obsequious apology (\"You're absolutely right, I'm sorry, I made assumptions and didn't respect your blah blah blah... I'll be sure to blah blah blah...\") and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird \"shell-shocked\" mode where it starts writing responses with three words per line that read like it's a frustrated beat poet.\n\nHave other people run into this? If so, are there good ways to combat it?\n\nThanks for any advice!\n",
          "author_fullname": "t2_7pimnskf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get Qwen 3 to stop asking terrible questions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg4lxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754177798,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with Qwen3-234B-A22B-Instruct-2507, I am repeatedly running into what appear be a cluster of similar issues on a fairly regular basis.&lt;/p&gt;\n\n&lt;p&gt;If I do anything which requires the model to ask clarifying questions, it frequently generates horrible questions, and the bad ones are almost always of the either/or variety.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, both sides are the same.  (E.g., &amp;quot;Are you helpless or do you need my help?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, they&amp;#39;re so unbalanced it becomes a Mitch Hedberg-style question. (E.g., &amp;quot;Have you ever tried sugar or PCP?&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Sometimes, a very open-ended question is presented as either/or. (E.g., &amp;quot;Is your favorite CSS color value #ff73c1 or #2141af?&amp;quot; like those are the only two options.)&lt;/p&gt;\n\n&lt;p&gt;I have found myself utterly unable to affect this behavior at all through the system prompt. I&amp;#39;ve tried telling it to stick to yes/no questions, use open-ended questions, ask only short answer questions. And (expecting and achieving futility as usual with &amp;quot;Don&amp;#39;t...&amp;quot; instructions) I&amp;#39;ve tried prompting it not to use &amp;quot;either/or&amp;quot; questions, &amp;quot;A or B?&amp;quot; questions, questions that limit the user&amp;#39;s options, etc. Lots of variants of both approaches in all sorts of combinations, with absolutely no effect.&lt;/p&gt;\n\n&lt;p&gt;And if I bring it up in chat, I get Qwen3&amp;#39;s usual long obsequious apology (&amp;quot;You&amp;#39;re absolutely right, I&amp;#39;m sorry, I made assumptions and didn&amp;#39;t respect your blah blah blah... I&amp;#39;ll be sure to blah blah blah...&amp;quot;) and then it goes right back to doing it. If I point it out a second time, it often shifts into that weird &amp;quot;shell-shocked&amp;quot; mode where it starts writing responses with three words per line that read like it&amp;#39;s a frustrated beat poet.&lt;/p&gt;\n\n&lt;p&gt;Have other people run into this? If so, are there good ways to combat it?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg4lxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TastesLikeOwlbear",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg4lxw/how_do_i_get_qwen_3_to_stop_asking_terrible/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754177798,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_prnin4bw1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GNOME AI Virtual Assistant \"Newelle\" Reaches Version 1.0 Milestone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1evr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754169081,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "phoronix.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mg1evr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FastDecode1",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1evr/gnome_ai_virtual_assistant_newelle_reaches/",
          "stickied": false,
          "url": "https://www.phoronix.com/news/GNOME-AI-Assistant-1.0",
          "subreddit_subscribers": 509291,
          "created_utc": 1754169081,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any llms that can describe a song? if not, what would it take to build one if you know",
          "author_fullname": "t2_1fv4idhbp9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Audio-in LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdrws",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754208305,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any llms that can describe a song? if not, what would it take to build one if you know&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgdrws",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nearby_Direction2438",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdrws/audioin_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdrws/audioin_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754208305,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My career is in tech startup chaos.  Bill Gurley is one of the few from that circle I can listen to while chewing food (as I am now and typing).  \n\nCompanies like LG want to sell washing machines.  They don't want their strategy to get disrupted without having a backup plan.  They want to raise the floor so that nobody can get too far ahead.  They want to scorch the Earth so that their biggest competitors won't be earning money that they can't compete for.  Sell AI washing machines = shareholder value protected = mission accomplished.\n\nStrategically, the allies of small open models weirdly includes giant companies and SMEs whenever their primary interest is not in competing directly to *operate* revenue-generating AI.  They want to invest in things that protect their strategy.  They only need a sensible way to do it and not move alone.",
          "author_fullname": "t2_8vhsch4i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why Fortune 500 Wants to Fund Open Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgh2cb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Bg2 Pod",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/fTqINzeudJ4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Bg2Pod"
            },
            "type": "youtube.com"
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1mgh2cb",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=fe76625dacb81af7ec54f68753f62ea2f3c58290",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754220993,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My career is in tech startup chaos.  Bill Gurley is one of the few from that circle I can listen to while chewing food (as I am now and typing).  &lt;/p&gt;\n\n&lt;p&gt;Companies like LG want to sell washing machines.  They don&amp;#39;t want their strategy to get disrupted without having a backup plan.  They want to raise the floor so that nobody can get too far ahead.  They want to scorch the Earth so that their biggest competitors won&amp;#39;t be earning money that they can&amp;#39;t compete for.  Sell AI washing machines = shareholder value protected = mission accomplished.&lt;/p&gt;\n\n&lt;p&gt;Strategically, the allies of small open models weirdly includes giant companies and SMEs whenever their primary interest is not in competing directly to &lt;em&gt;operate&lt;/em&gt; revenue-generating AI.  They want to invest in things that protect their strategy.  They only need a sensible way to do it and not move alone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/fTqINzeudJ4?&amp;t=1067",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?auto=webp&amp;s=27043eee8c92b37f592065d2ff05fd85b27b6c4a",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dbe82e3d2ef0e784a0e62fb6596ee14119fb2c44",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c52288f06eae6d456c4b6ac64ac2b6f1b61a1e65",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e74ffd7ecfd40e764b51c4df1a4b893fda673f1f",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "MK0EV9lEmIsFO7imdqO0m8VG-otbLYhxUaOOpUAB5kM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgh2cb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Psionikus",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgh2cb/why_fortune_500_wants_to_fund_open_models/",
          "stickied": false,
          "url": "https://youtu.be/fTqINzeudJ4?&amp;t=1067",
          "subreddit_subscribers": 509291,
          "created_utc": 1754220993,
          "num_crossposts": 1,
          "media": {
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "title": "China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner",
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fTqINzeudJ4?start=1067&amp;feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"China Open-Source, Compute Arms Race, Reordering Global Trade | BG2 w/ Bill Gurley and Brad Gerstner\"&gt;&lt;/iframe&gt;",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "version": "1.0",
              "author_name": "Bg2 Pod",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/fTqINzeudJ4/hqdefault.jpg",
              "type": "video",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Bg2Pod"
            },
            "type": "youtube.com"
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.\n\nHow it works:\n\n- Drop a .awesome-ai.md file in your repo root (template: https://github.com/teodorgross/awesome-ai)\n\n- The scanner finds it automatically within 30 minutes  \n\n- Creates a pull request for review\n\n- Your tool goes live with real-time GitHub stats on (https://awesome-ai.io)\n\nWhy this matters:\n\n- No more manual submissions or contact forms\n\n- Tools stay up-to-date automatically when you push changes\n\n- GitHub verification prevents spam\n\n- Real-time star tracking and leaderboards\n\nThink of it like .gitignore for Git, but for AI tool discovery. ",
          "author_fullname": "t2_92fpag8i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a GitHub scanner that automatically discovers your AI tools using a new .awesome-ai.md standard I created",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgh19i",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=2f6c6297df4b4027fb8d1d453c04b9819bf72371",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754220889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I just launched something I think could change how we discover AI tools on. Instead of manually submitting to directories or relying on outdated lists, I created the .awesome-ai.md standard.&lt;/p&gt;\n\n&lt;p&gt;How it works:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Drop a .awesome-ai.md file in your repo root (template: &lt;a href=\"https://github.com/teodorgross/awesome-ai\"&gt;https://github.com/teodorgross/awesome-ai&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The scanner finds it automatically within 30 minutes  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a pull request for review&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Your tool goes live with real-time GitHub stats on (&lt;a href=\"https://awesome-ai.io\"&gt;https://awesome-ai.io&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why this matters:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;No more manual submissions or contact forms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Tools stay up-to-date automatically when you push changes&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;GitHub verification prevents spam&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Real-time star tracking and leaderboards&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Think of it like .gitignore for Git, but for AI tool discovery. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/teodorgross/awesome-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?auto=webp&amp;s=423c2b45e5c57364da28298579b3a5631b59c0ed",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd85376252643c0cfc25bf58873d1308bbaa8b8c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b15eb575a42080dcefe65c839eb0e4c036819417",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7d83cf6189b7f3c98c0a7b6f2c3d772895ee7ba",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d63a88dbef47d35bfb865b575d09eda2e4144500",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f688b76be41b12b1a7923494dacac226e59a932e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0e5bc45cd19159360de8a68ed1794010b721da7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "3eKPASSIaokEljTMRxPAx8m9aBrDtxD5hJx7xRfgQU8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgh19i",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "r00tkit_",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgh19i/i_built_a_github_scanner_that_automatically/",
          "stickied": false,
          "url": "https://github.com/teodorgross/awesome-ai",
          "subreddit_subscribers": 509291,
          "created_utc": 1754220889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Based on the current situation with the quality of Sonnet and other proprietary models I'm thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our \"own\" R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.\n\nWhat are your thoughts?\n\nUpdate: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)",
          "author_fullname": "t2_qnf2u",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It's time to run your own R1, Kimi ... and split the cost of it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrunn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754146028,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144811,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on the current situation with the quality of Sonnet and other proprietary models I&amp;#39;m thinking of getting a  group of people who would join the common pool and share the cost of hosting and running our &amp;quot;own&amp;quot; R1, Kimi and other models so you will not be dependent on decreasing the quality of other providers.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts?&lt;/p&gt;\n\n&lt;p&gt;Update: you posted good questions. But I was thinking to run the model and api to access it in the cloud ( without buying your own equipment)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfrunn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HammerSpb",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrunn/its_time_to_run_your_own_r1_kimi_and_split_the/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754144811,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "  \nPersonal note: This is just my opinion based on a very limited set of API-only probes‚Äîinterpret with caution.\n\nWhat I did (mini-ROC probes)\n\n* JSON strictness vs. \"bad schema\" repair\n* Tool-calling with an invalid enum + extra property\n* Safety/refusal phrasing check\n* Long-context end-marker recall\n* Tokenizer/short-output edge case\n* Determinism at T=0\n* Tiny style-paraphrase probe\n\nHighlights\n\n* Tool-calling: It silently coerces invalid enums (mode=\"plane\" -&gt; \"car\"/\"train\") and drops extra fields, then emits an OpenAI-style tool\\_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn't call the tool under the same bad input - which is more typical for OpenAI.\n* JSON mode: It \"repairs\" invalid inputs into valid JSON (e.g., {\"ok\": false, \"mode\": \"A\"}). OpenAI also repairs but tends to be more minimally formatted.\n* Safety tone: Opens with \"I can't help with that.\" - Anthropic-ish cadence that many Llama-style distills mimic.\n* Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END\\_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.\n* Determinism: Stable at T=0 on simple tasks.\n* Multilingual: Correct Â¶πÂ¶π -&gt; \"younger sister,\" and clean pronoun disambiguation.\n\nAnchors I compared against\n\n* OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)\n* Llama: llama-3.3-70b-instruct, llama-3-70b-instruct\n* Qwen: qwen-2.5-72b-instruct\n* Mistral: mixtral-8x22b-instruct\n\nBottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific \"finish=length\" quirk on short outputs. It does not match OpenAI's tool-call behavior in my probes.\n\nAll tests were standard API usage.",
          "author_fullname": "t2_2fmsm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OSINT fingerprinting a stealth OpenRouter model - likely Llama-family, not OpenAI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mggsyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754220059,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Personal note: This is just my opinion based on a very limited set of API-only probes‚Äîinterpret with caution.&lt;/p&gt;\n\n&lt;p&gt;What I did (mini-ROC probes)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;JSON strictness vs. &amp;quot;bad schema&amp;quot; repair&lt;/li&gt;\n&lt;li&gt;Tool-calling with an invalid enum + extra property&lt;/li&gt;\n&lt;li&gt;Safety/refusal phrasing check&lt;/li&gt;\n&lt;li&gt;Long-context end-marker recall&lt;/li&gt;\n&lt;li&gt;Tokenizer/short-output edge case&lt;/li&gt;\n&lt;li&gt;Determinism at T=0&lt;/li&gt;\n&lt;li&gt;Tiny style-paraphrase probe&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Highlights&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tool-calling: It silently coerces invalid enums (mode=&amp;quot;plane&amp;quot; -&amp;gt; &amp;quot;car&amp;quot;/&amp;quot;train&amp;quot;) and drops extra fields, then emits an OpenAI-style tool_call (arguments as a JSON string). In contrast, OpenAI gpt-4o-mini didn&amp;#39;t call the tool under the same bad input - which is more typical for OpenAI.&lt;/li&gt;\n&lt;li&gt;JSON mode: It &amp;quot;repairs&amp;quot; invalid inputs into valid JSON (e.g., {&amp;quot;ok&amp;quot;: false, &amp;quot;mode&amp;quot;: &amp;quot;A&amp;quot;}). OpenAI also repairs but tends to be more minimally formatted.&lt;/li&gt;\n&lt;li&gt;Safety tone: Opens with &amp;quot;I can&amp;#39;t help with that.&amp;quot; - Anthropic-ish cadence that many Llama-style distills mimic.&lt;/li&gt;\n&lt;li&gt;Quirk: Repeated empty completions with finish=length for certain short-output prompts (e.g., long END_MARK task, tiny character-count). Other anchors returned tokens normally - this looks like a wrapper/decoder guard specific to this deployment.&lt;/li&gt;\n&lt;li&gt;Determinism: Stable at T=0 on simple tasks.&lt;/li&gt;\n&lt;li&gt;Multilingual: Correct Â¶πÂ¶π -&amp;gt; &amp;quot;younger sister,&amp;quot; and clean pronoun disambiguation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anchors I compared against&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenAI via OpenRouter: gpt-4o-mini (worked), o4-mini (likely access/rate-limited for me)&lt;/li&gt;\n&lt;li&gt;Llama: llama-3.3-70b-instruct, llama-3-70b-instruct&lt;/li&gt;\n&lt;li&gt;Qwen: qwen-2.5-72b-instruct&lt;/li&gt;\n&lt;li&gt;Mistral: mixtral-8x22b-instruct&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Bottom line It clusters with Llama-family instruct behavior - enum coercion + JSON repair; Anthropic-like refusal phrasing - and shows a deployment-specific &amp;quot;finish=length&amp;quot; quirk on short outputs. It does not match OpenAI&amp;#39;s tool-call behavior in my probes.&lt;/p&gt;\n\n&lt;p&gt;All tests were standard API usage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mggsyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jv0010",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggsyb/osint_fingerprinting_a_stealth_openrouter_model/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754220059,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been using Claude Code with my MAX plan and kept running into situations where I wanted to route specific requests to different models without changing my whole setup.\nLarge context requests would hit Claude's limits, and running compaction so often and having Claude lose important context was a frustrating experience.\n\nSo I built ccproxy - a LiteLLM transformation hook that sits between Claude Code and your requests, intelligently routing them based on configurable rules.\n\nWhat it actually does:\n\n- Routes requests to different providers while keeping your Claude Code client unchanged\n- Example: requests over 60k tokens automatically go to Gemini Pro, requests for sonnet can go to Gemini Flash\n- Define rules based on token count, model name, tool usage, or any request property\n- Everything else defaults to your Claude MAX plan\n\nCurrent limitations\n\n- Cross-provider context caching is coming but not ready yet\n- Only battle-tested with Anthropic/Google/OpenAI providers so far, I personally have not used it with local models, but as it's using LiteLLM I expect it to work with most setups.\n- No fancy UI - it's YAML config for now\n\nWho this helps:\nIf you're already using Claude Code with a MAX plan but want to optimize costs/performance for specific use cases, this might save you from writing custom routing logic.\nIt's particularly useful if you're hitting context limits or want to use cheaper models for simple tasks.\n\nGitHub: &lt;https://github.com/starbased-co/ccproxy&gt;\n\nHappy to answer questions or take feedback. What routing patterns would be most useful for your workflows?",
          "author_fullname": "t2_5fbcze3c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ccproxy - Route Claude Code requests to any LLM while keeping your MAX plan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg82el",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754188267,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using Claude Code with my MAX plan and kept running into situations where I wanted to route specific requests to different models without changing my whole setup.\nLarge context requests would hit Claude&amp;#39;s limits, and running compaction so often and having Claude lose important context was a frustrating experience.&lt;/p&gt;\n\n&lt;p&gt;So I built ccproxy - a LiteLLM transformation hook that sits between Claude Code and your requests, intelligently routing them based on configurable rules.&lt;/p&gt;\n\n&lt;p&gt;What it actually does:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Routes requests to different providers while keeping your Claude Code client unchanged&lt;/li&gt;\n&lt;li&gt;Example: requests over 60k tokens automatically go to Gemini Pro, requests for sonnet can go to Gemini Flash&lt;/li&gt;\n&lt;li&gt;Define rules based on token count, model name, tool usage, or any request property&lt;/li&gt;\n&lt;li&gt;Everything else defaults to your Claude MAX plan&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Current limitations&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cross-provider context caching is coming but not ready yet&lt;/li&gt;\n&lt;li&gt;Only battle-tested with Anthropic/Google/OpenAI providers so far, I personally have not used it with local models, but as it&amp;#39;s using LiteLLM I expect it to work with most setups.&lt;/li&gt;\n&lt;li&gt;No fancy UI - it&amp;#39;s YAML config for now&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Who this helps:\nIf you&amp;#39;re already using Claude Code with a MAX plan but want to optimize costs/performance for specific use cases, this might save you from writing custom routing logic.\nIt&amp;#39;s particularly useful if you&amp;#39;re hitting context limits or want to use cheaper models for simple tasks.&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/starbased-co/ccproxy\"&gt;https://github.com/starbased-co/ccproxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer questions or take feedback. What routing patterns would be most useful for your workflows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?auto=webp&amp;s=3460d31319519e33a80286d6df2dfc23b39c919c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=450f2c799f15b29ec21139264bc6b6bcb57973e4",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d842726b49df18ab66e30107bfb8ac96308217a",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9f141bb2296be7290f22e206f044b47d4383c40",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad9ccf4b08efef17745acd771af67ed9e25f036b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7506050be4a335d4bf6b8acaf16dcd1498ea2f5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5570005c242c40ef01988f09a587178c3b8cbc2b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2bOBf4OmqfxCEzVm6KZBaTd34lmAdlTU4rqJVR2YILo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg82el",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_kintsu",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg82el/ccproxy_route_claude_code_requests_to_any_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754188267,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI have developed a toy spreadsheet, where you can implement your formulas in English, which are then translated into \\`javascript\\` thanks to an LLM.\n\nFor instance, you can write: \\`sum of the squared values\\` and the LLM will translate this description into:  \n\\`getValuesFromReferences(\\['A1', 'A2', 'A3'\\]).map(Number).reduce((a, b) =&gt; a + b \\* b, 0)\\`.\n\nI use \\`LM Studio\\` and \\`codestral\\`, but I'm pretty sure you can replace \\`LM Studio\\` by \\`Ollama\\` or your favorite LLM provider.\n\nIf you want to have a look, it is available on the following GitHub: [NUMAI](https://github.com/clauderouxster/NUMAI/tree/main)",
          "author_fullname": "t2_no5psr297",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NUMAI: A spreadsheet with LLM formula conversion",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mggdxz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754218578,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have developed a toy spreadsheet, where you can implement your formulas in English, which are then translated into `javascript` thanks to an LLM.&lt;/p&gt;\n\n&lt;p&gt;For instance, you can write: `sum of the squared values` and the LLM will translate this description into:&lt;br/&gt;\n`getValuesFromReferences([&amp;#39;A1&amp;#39;, &amp;#39;A2&amp;#39;, &amp;#39;A3&amp;#39;]).map(Number).reduce((a, b) =&amp;gt; a + b * b, 0)`.&lt;/p&gt;\n\n&lt;p&gt;I use `LM Studio` and `codestral`, but I&amp;#39;m pretty sure you can replace `LM Studio` by `Ollama` or your favorite LLM provider.&lt;/p&gt;\n\n&lt;p&gt;If you want to have a look, it is available on the following GitHub: &lt;a href=\"https://github.com/clauderouxster/NUMAI/tree/main\"&gt;NUMAI&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mggdxz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Frere_de_la_Quote",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggdxz/numai_a_spreadsheet_with_llm_formula_conversion/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggdxz/numai_a_spreadsheet_with_llm_formula_conversion/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754218578,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have decided to run Gemma 3 4B QAT on my 6GB VRAM Laptop for general use. I was wondering if i should be using some other quant other than the official QAT version by google? Like What would be the performance or quality increase as compared to the QAT version. It would be great if someone shared some benchmarks or other results.",
          "author_fullname": "t2_aedi2k9c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Gemma 3 Quant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mgg4ki",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754218312,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754217605,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have decided to run Gemma 3 4B QAT on my 6GB VRAM Laptop for general use. I was wondering if i should be using some other quant other than the official QAT version by google? Like What would be the performance or quality increase as compared to the QAT version. It would be great if someone shared some benchmarks or other results.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgg4ki",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "R46H4V",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgg4ki/best_gemma_3_quant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgg4ki/best_gemma_3_quant/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754217605,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI‚Äôve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.\n\nFrom everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I‚Äôm using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).\n\n\n\nThe idea:\n\nI want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.\n\n\n\nBut here‚Äôs the problem:\n\nThe results are pretty disappointing.\n\nEven though the PDFs were embedded correctly and the system says they‚Äôre indexed, the answers I get are vague, or clearly not based on my previous content. It doesn't really use the documents meaningfully ‚Äì it feels like the bot is just hallucinating or ignoring them.\n\nI even tested it with just one document: my current r√©sum√©, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like \"What is my professional background?\" or \"What are my main skills?\" ‚Äì which it should have easily pulled from the text.\n\nI‚Äôve tried re-uploading, adjusting the chunk size, checking the document scope ‚Äì&gt; but no real improvement.\n\nSo my question is:\n\nAm I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?\n\nHas anyone had better results using a different local setup for tasks like this?\n\n\n\nWould love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!",
          "author_fullname": "t2_oq1nrgj7x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Still getting bad results with PDFs in AnythingLLM + Llama 3 ‚Äì Am I doing something wrong, or is there a better setup?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfuf3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754216527,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.&lt;/p&gt;\n\n&lt;p&gt;From everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I‚Äôm using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).&lt;/p&gt;\n\n&lt;p&gt;The idea:&lt;/p&gt;\n\n&lt;p&gt;I want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.&lt;/p&gt;\n\n&lt;p&gt;But here‚Äôs the problem:&lt;/p&gt;\n\n&lt;p&gt;The results are pretty disappointing.&lt;/p&gt;\n\n&lt;p&gt;Even though the PDFs were embedded correctly and the system says they‚Äôre indexed, the answers I get are vague, or clearly not based on my previous content. It doesn&amp;#39;t really use the documents meaningfully ‚Äì it feels like the bot is just hallucinating or ignoring them.&lt;/p&gt;\n\n&lt;p&gt;I even tested it with just one document: my current r√©sum√©, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like &amp;quot;What is my professional background?&amp;quot; or &amp;quot;What are my main skills?&amp;quot; ‚Äì which it should have easily pulled from the text.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve tried re-uploading, adjusting the chunk size, checking the document scope ‚Äì&amp;gt; but no real improvement.&lt;/p&gt;\n\n&lt;p&gt;So my question is:&lt;/p&gt;\n\n&lt;p&gt;Am I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?&lt;/p&gt;\n\n&lt;p&gt;Has anyone had better results using a different local setup for tasks like this?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgfuf3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Lazy_Fig_6244",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754216527,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I honestly don't know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...  \nIt's something I've never been clear about.  \nFor example, when I've used Deepseek, sometimes I feel that putting it into \"thinking\" mode doesn't add much, but I haven't noticed a clear pattern like \"for this type of question I use thinking mode, for this other type I don't.\"  \nCould someone clarify this for me?\n\nI'm thinking of downloading this model:  \n**Qwen3-30B-A3B-Instruct-2507** ... or **Qwen3-30B-A3B-Thinking-2507**\n\nThe Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?",
          "author_fullname": "t2_q2iij",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking or Instruct?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg5scj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754181257,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I honestly don&amp;#39;t know which one is better suited for things like medical, philosophical, historical topics, or text interpretation...&lt;br/&gt;\nIt&amp;#39;s something I&amp;#39;ve never been clear about.&lt;br/&gt;\nFor example, when I&amp;#39;ve used Deepseek, sometimes I feel that putting it into &amp;quot;thinking&amp;quot; mode doesn&amp;#39;t add much, but I haven&amp;#39;t noticed a clear pattern like &amp;quot;for this type of question I use thinking mode, for this other type I don&amp;#39;t.&amp;quot;&lt;br/&gt;\nCould someone clarify this for me?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of downloading this model:&lt;br/&gt;\n&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; ... or &lt;strong&gt;Qwen3-30B-A3B-Thinking-2507&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The Instruct version has been downloaded way more and has a lot more likes, but... for what I want, which one is more suitable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg5scj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "9acca9",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg5scj/thinking_or_instruct/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754181257,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.\n\nI ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison.\nI'm also planning to run some fine-tuning benchmarks later.",
          "author_fullname": "t2_a85vzco5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 57,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfnq2r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/YkVLBy_Uabot6hZJtvt-2lgE4cyPGdL7zFQtJSHp-Xk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754132261,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn&amp;#39;t find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt;\n\n&lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I&amp;#39;m quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I&amp;#39;ve also included the RTX 4090 results for a broader comparison.\nI&amp;#39;m also planning to run some fine-tuning benchmarks later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?auto=webp&amp;s=5c7238d3074b0c94ed28a56fd05d130ca36e4062",
                  "width": 1280,
                  "height": 525
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8aa6f94f1fd0b07761158c32a4f411fea4ff01e",
                    "width": 108,
                    "height": 44
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fd7073605b2305ef0bdfa02c28ed14504e5cadf",
                    "width": 216,
                    "height": 88
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f51b098c5db731c8b7427cd4c5e5918907eaf170",
                    "width": 320,
                    "height": 131
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f",
                    "width": 640,
                    "height": 262
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f3fce6514c1c2d99e8304633aabf8e744412a87",
                    "width": 960,
                    "height": 393
                  },
                  {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4087fbfd9cd44fbf60685b1fe8cf9814b669d42c",
                    "width": 1080,
                    "height": 442
                  }
                ],
                "variants": {},
                "id": "tc9bMPtFnkPdsr6iuVPGSgsfnHwUyG7fExgbegYm3H4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfnq2r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kargafe",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/",
          "stickied": false,
          "url": "https://i.redd.it/erib4a6t7lgf1.jpeg",
          "subreddit_subscribers": 509291,
          "created_utc": 1754132261,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking for a way to control the usage of LLMs and to track which users (from my app) are sending how many requests, the prompts, etc. \n\nSure, I can do this via custom middleware in my app, but I am looking for something that is designed exactly for LLM Observability and would protect me from legal proceedings in case one of my users put something that would cause the LLM provider to report to the police. Just thinking like a German.\n\nAlso, how good is LlamaGuard? Do you have any suggestions or other models that would reduce the risk of users doing something illegal? (Illegal meaning truly something that would be a crime, not just regular NSFW stuff).",
          "author_fullname": "t2_v25iw7w1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM Observability - Any Suggestions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgfccn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754214596,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a way to control the usage of LLMs and to track which users (from my app) are sending how many requests, the prompts, etc. &lt;/p&gt;\n\n&lt;p&gt;Sure, I can do this via custom middleware in my app, but I am looking for something that is designed exactly for LLM Observability and would protect me from legal proceedings in case one of my users put something that would cause the LLM provider to report to the police. Just thinking like a German.&lt;/p&gt;\n\n&lt;p&gt;Also, how good is LlamaGuard? Do you have any suggestions or other models that would reduce the risk of users doing something illegal? (Illegal meaning truly something that would be a crime, not just regular NSFW stuff).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgfccn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SuddenWerewolf7041",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgfccn/llm_observability_any_suggestions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfccn/llm_observability_any_suggestions/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754214596,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!",
          "author_fullname": "t2_13hqmc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have built my own, poor mans Lovable - testing out Cerebras AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0uw8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=5b85dbf1e37f569740ca84504972c21bb5463678",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754167627,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I decided to test Cerebras and their speed is indeed impressive: 2.5 sec to generate a real-world app with tailwind frontend. I use Docker to containerize the apps built. It is a naive MVP but I need your feedback guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/restyler/poor-mans-lovable",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?auto=webp&amp;s=2d2f3ab9d396d0b9ffc59352c0ad43c2be2c6d47",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cf2bdf2998419d6d274f237ffbcc2b5ad34d663",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e31d2c9aed096970a6183172f8674918abd3da69",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3c2a06ca5a7606d5a5f78e2a727ce72eb1898757",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=642596b8bbafb58939fce5813d2fe60c81392d6f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc7356ab40d5babbd50137dd1b88c3efc5a874a3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=496d2912020b57e4bdcbc111110a288bf2bd8366",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2Ld6AawMyp37grKrz1lk2nxgwwwZ3ZxUmD2VwjK2SlE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mg0uw8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "superjet1",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0uw8/i_have_built_my_own_poor_mans_lovable_testing_out/",
          "stickied": false,
          "url": "https://github.com/restyler/poor-mans-lovable",
          "subreddit_subscribers": 509291,
          "created_utc": 1754167627,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Model Name|Organization|HuggingFace Link|Size|Modality|\n|:-|:-|:-|:-|:-|\n|dots.ocr|REDnote Hilab|[https://huggingface.co/rednote-hilab/dots.ocr](https://huggingface.co/rednote-hilab/dots.ocr)|3B|Image-Text-to-Text|\n||||||\n|GLM 4.5|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5](https://huggingface.co/zai-org/GLM-4.5)|355B-A32B|Text-to-Text|\n|GLM 4.5 Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Base](https://huggingface.co/zai-org/GLM-4.5-Base)|355B-A32B|Text-to-Text|\n|GLM 4.5-Air|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air](https://huggingface.co/zai-org/GLM-4.5-Air)|106B-A12B|Text-to-Text|\n|GLM 4.5 Air Base|[Z.ai](http://Z.ai)|[https://huggingface.co/zai-org/GLM-4.5-Air-Base](https://huggingface.co/zai-org/GLM-4.5-Air-Base)|106B-A12B|Text-to-Text|\n||||||\n|Qwen3 235B-A22B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)|235B-A22B|Text-to-Text|\n|Qwen3 235B-A22B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507)|235B-A22B|Text-to-Text|\n|Qwen3 30B-A3B Instruct 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)|30B-A3B|Text-to-Text|\n|Qwen3 30B-A3B Thinking 2507|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507)|30B-A3B|Text-to-Text|\n|Qwen3 Coder 480B-A35B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct)|480B-A35B|Text-to-Text|\n|Qwen3 Coder 30B-A3B Instruct|Alibaba - Qwen|[https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct)|30B-A3B|Text-to-Text|\n||||||\n|Kimi K2 Instruct|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Instruct](https://huggingface.co/moonshotai/Kimi-K2-Instruct)|1T-32B|Text-to-Text|\n|Kimi K2 Base|Moonshot AI|[https://huggingface.co/moonshotai/Kimi-K2-Base](https://huggingface.co/moonshotai/Kimi-K2-Base)|1T-32B|Text-to-Text|\n||||||\n|Intern S1|Shanghai AI Laboratory - Intern|[https://huggingface.co/internlm/Intern-S1](https://huggingface.co/internlm/Intern-S1)|241B-A22B|Image-Text-to-Text|\n||||||\n|Llama-3.3 Nemotron Super 49B v1.5|Nvidia|[https://huggingface.co/nvidia/Llama-3\\_3-Nemotron-Super-49B-v1\\_5](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5)|49B|Text-to-Text|\n|OpenReasoning Nemotron 1.5B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B)|1.5B|Text-to-Text|\n|OpenReasoning Nemotron 7B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B)|7B|Text-to-Text|\n|OpenReasoning Nemotron 14B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B)|14B|Text-to-Text|\n|OpenReasoning Nemotron 32B|Nvidia|[https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B](https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B)|32B|Text-to-Text|\n||||||\n|step3|StepFun|[https://huggingface.co/stepfun-ai/step3](https://huggingface.co/stepfun-ai/step3)|321B-A38B|Text-to-Text|\n||||||\n|SmallThinker 21B-A3B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct)|21B-A3B|Text-to-Text|\n|SmallThinker 4B-A0.6B Instruct|IPADS - PowerInfer|[https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct](https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct)|4B-A0.6B|Text-to-Text|\n||||||\n|Seed X Instruct-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B](https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B)|7B|Machine Translation|\n|Seed X PPO-7B|ByteDance Seed|[https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B](https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B)|7B|Machine Translation|\n||||||\n|Magistral Small 2507|Mistral|[https://huggingface.co/mistralai/Magistral-Small-2507](https://huggingface.co/mistralai/Magistral-Small-2507)|24B|Text-to-Text|\n|Devstral Small 2507|Mistral|[https://huggingface.co/mistralai/Devstral-Small-2507](https://huggingface.co/mistralai/Devstral-Small-2507)|24B|Text-to-Text|\n|Voxtral Small 24B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Small-24B-2507](https://huggingface.co/mistralai/Voxtral-Small-24B-2507)|24B|Audio-Text-to-Text|\n|Voxtral Mini 3B 2507|Mistral|[https://huggingface.co/mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)|3B|Audio-Text-to-Text|\n||||||\n|AFM 4.5B|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B](https://huggingface.co/arcee-ai/AFM-4.5B)|4.5B|Text-to-Text|\n|AFM 4.5B Base|Arcee AI|[https://huggingface.co/arcee-ai/AFM-4.5B-Base](https://huggingface.co/arcee-ai/AFM-4.5B-Base)|4B|Text-to-Text|\n||||||\n|Ling lite-1.5 2506|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ling-lite-1.5-2506](https://huggingface.co/inclusionAI/Ling-lite-1.5-2506)|16B|Text-to-Text|\n|Ming Lite Omni-1.5|Ant Group - Inclusion AI|[https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5](https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5)|20.3B|Text-Audio-Video-Image-To-Text|\n||||||\n|UIGEN X 32B 0727|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-32B-0727](https://huggingface.co/Tesslate/UIGEN-X-32B-0727)|32B|Text-to-Text|\n|UIGEN X 4B 0729|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-4B-0729](https://huggingface.co/Tesslate/UIGEN-X-4B-0729)|4B|Text-to-Text|\n|UIGEN X 8B|Tesslate|[https://huggingface.co/Tesslate/UIGEN-X-8B](https://huggingface.co/Tesslate/UIGEN-X-8B)|8B|Text-to-Text|\n||||||\n|command a vision 07-2025|Cohere|[https://huggingface.co/CohereLabs/command-a-vision-07-2025](https://huggingface.co/CohereLabs/command-a-vision-07-2025)|112B|Image-Text-to-Text|\n||||||\n|KAT V1 40B|Kwaipilot|[https://huggingface.co/Kwaipilot/KAT-V1-40B](https://huggingface.co/Kwaipilot/KAT-V1-40B)|40B|Text-to-Text|\n||||||\n|EXAONE 4.0.1 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B)|32B|Text-to-Text|\n|EXAONE 4.0.1 2B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B)|2B|Text-to-Text|\n|EXAONE 4.0 32B|LG AI|[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)|32B|Text-to-Text|\n||||||\n|cogito v2 preview deepseek-671B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE)|671B-A37B|Text-to-Text|\n|cogito v2 preview llama-405B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B)|405B|Text-to-Text|\n|cogito v2 preview llama-109B-MoE|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE](https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE)|109B-A17B|Image-Text-to-Text|\n|cogito v2 preview llama-70B|Deep Cogito|[https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B](https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B)|70B|Text-to-Text|\n||||||\n|A.X 4.0 VL Light|SK Telecom|[https://huggingface.co/skt/A.X-4.0-VL-Light](https://huggingface.co/skt/A.X-4.0-VL-Light)|8B|Image-Text-to-Text|\n|A.X 3.1|SK Telecom|[https://huggingface.co/skt/A.X-3.1](https://huggingface.co/skt/A.X-3.1)|35B|Text-to-Text|\n|olmOCR 7B 0725|AllenAI|[https://huggingface.co/allenai/olmOCR-7B-0725](https://huggingface.co/allenai/olmOCR-7B-0725)|7B|Image-Text-to-Text|\n||||||\n|kanana 1.5 15.7B-A3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct)|7B-A3B|Text-to-Text|\n|kanana 1.5v 3B instruct|Kakao|[https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct](https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct)|3B|Image-Text-to-Text|\n||||||\n|Tri 7B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-7B](https://huggingface.co/trillionlabs/Tri-7B)|7B|Text-to-Text|\n|Tri 21B|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-21B](https://huggingface.co/trillionlabs/Tri-21B)|21B|Text-to-Text|\n|Tri 70B preview SFT|Trillion Labs|[https://huggingface.co/trillionlabs/Tri-70B-preview-SFT](https://huggingface.co/trillionlabs/Tri-70B-preview-SFT)|70B|Text-to-Text|\n\nI tried to compile the latest models released over the past 2‚Äì3 weeks, and its kinda like there is a ground breaking model every 2 days. I‚Äôm really glad to be living in this era of rapid progress.\n\nThis list doesn‚Äôt even include other modalities like 3D, image, and audio, where there's also a ton of new models (Like Wan2.2 , Flux-Krea , ...)\n\nHope this can serve as a breakdown of the latest models.\n\n*Feel free to tag me if I missed any you think should be added!*\n\n\\[EDIT\\] \n\n**I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.**\n\n**Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?**\n\n**Anyone could publish a new model‚Äîwith some community approval to reduce junk and pure finetunes**?",
          "author_fullname": "t2_7zubl1l8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We're truly in the fastest-paced era of AI these days. (50 LLM Released these 2-3 Weeks)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfaigh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 539,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 539,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754117628,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754088000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Organization&lt;/th&gt;\n&lt;th align=\"left\"&gt;HuggingFace Link&lt;/th&gt;\n&lt;th align=\"left\"&gt;Size&lt;/th&gt;\n&lt;th align=\"left\"&gt;Modality&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots.ocr&lt;/td&gt;\n&lt;td align=\"left\"&gt;REDnote Hilab&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/rednote-hilab/dots.ocr\"&gt;https://huggingface.co/rednote-hilab/dots.ocr&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5\"&gt;https://huggingface.co/zai-org/GLM-4.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;355B-A32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5-Air&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GLM 4.5 Air Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"http://Z.ai\"&gt;Z.ai&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/zai-org/GLM-4.5-Air-Base\"&gt;https://huggingface.co/zai-org/GLM-4.5-Air-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;106B-A12B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 235B-A22B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;235B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 30B-A3B Thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 480B-A35B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;480B-A35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen3 Coder 30B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba - Qwen&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;30B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Kimi K2 Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Moonshot AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Base\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1T-32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Intern S1&lt;/td&gt;\n&lt;td align=\"left\"&gt;Shanghai AI Laboratory - Intern&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/internlm/Intern-S1\"&gt;https://huggingface.co/internlm/Intern-S1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;241B-A22B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama-3.3 Nemotron Super 49B v1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;49B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;14B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenReasoning Nemotron 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Nvidia&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\"&gt;https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;step3&lt;/td&gt;\n&lt;td align=\"left\"&gt;StepFun&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/stepfun-ai/step3\"&gt;https://huggingface.co/stepfun-ai/step3&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;321B-A38B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 21B-A3B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;SmallThinker 4B-A0.6B Instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;IPADS - PowerInfer&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct\"&gt;https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B-A0.6B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X Instruct-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-Instruct-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Seed X PPO-7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;ByteDance Seed&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B\"&gt;https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Machine Translation&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Magistral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Magistral-Small-2507\"&gt;https://huggingface.co/mistralai/Magistral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Devstral Small 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Devstral-Small-2507\"&gt;https://huggingface.co/mistralai/Devstral-Small-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Small 24B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Small-24B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Small-24B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;24B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Voxtral Mini 3B 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/mistralai/Voxtral-Mini-3B-2507\"&gt;https://huggingface.co/mistralai/Voxtral-Mini-3B-2507&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Audio-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;AFM 4.5B Base&lt;/td&gt;\n&lt;td align=\"left\"&gt;Arcee AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/arcee-ai/AFM-4.5B-Base\"&gt;https://huggingface.co/arcee-ai/AFM-4.5B-Base&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ling lite-1.5 2506&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ling-lite-1.5-2506\"&gt;https://huggingface.co/inclusionAI/Ling-lite-1.5-2506&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;16B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Ming Lite Omni-1.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;Ant Group - Inclusion AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5\"&gt;https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-Audio-Video-Image-To-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 32B 0727&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-32B-0727\"&gt;https://huggingface.co/Tesslate/UIGEN-X-32B-0727&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 4B 0729&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-4B-0729\"&gt;https://huggingface.co/Tesslate/UIGEN-X-4B-0729&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;4B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;UIGEN X 8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Tesslate&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-8B\"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;command a vision 07-2025&lt;/td&gt;\n&lt;td align=\"left\"&gt;Cohere&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/CohereLabs/command-a-vision-07-2025\"&gt;https://huggingface.co/CohereLabs/command-a-vision-07-2025&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;112B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;KAT V1 40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kwaipilot&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/Kwaipilot/KAT-V1-40B\"&gt;https://huggingface.co/Kwaipilot/KAT-V1-40B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;40B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0.1 2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;EXAONE 4.0 32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;LG AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;32B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview deepseek-671B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;671B-A37B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;405B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-109B-MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;109B-A17B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;cogito v2 preview llama-70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Deep Cogito&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B\"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 4.0 VL Light&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-4.0-VL-Light\"&gt;https://huggingface.co/skt/A.X-4.0-VL-Light&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;8B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;A.X 3.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;SK Telecom&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/skt/A.X-3.1\"&gt;https://huggingface.co/skt/A.X-3.1&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;35B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;olmOCR 7B 0725&lt;/td&gt;\n&lt;td align=\"left\"&gt;AllenAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/allenai/olmOCR-7B-0725\"&gt;https://huggingface.co/allenai/olmOCR-7B-0725&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5 15.7B-A3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B-A3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;kanana 1.5v 3B instruct&lt;/td&gt;\n&lt;td align=\"left\"&gt;Kakao&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct\"&gt;https://huggingface.co/kakaocorp/kanana-1.5-v-3b-instruct&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;3B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Image-Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-7B\"&gt;https://huggingface.co/trillionlabs/Tri-7B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-21B\"&gt;https://huggingface.co/trillionlabs/Tri-21B&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;21B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tri 70B preview SFT&lt;/td&gt;\n&lt;td align=\"left\"&gt;Trillion Labs&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://huggingface.co/trillionlabs/Tri-70B-preview-SFT\"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;70B&lt;/td&gt;\n&lt;td align=\"left\"&gt;Text-to-Text&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I tried to compile the latest models released over the past 2‚Äì3 weeks, and its kinda like there is a ground breaking model every 2 days. I‚Äôm really glad to be living in this era of rapid progress.&lt;/p&gt;\n\n&lt;p&gt;This list doesn‚Äôt even include other modalities like 3D, image, and audio, where there&amp;#39;s also a ton of new models (Like Wan2.2 , Flux-Krea , ...)&lt;/p&gt;\n\n&lt;p&gt;Hope this can serve as a breakdown of the latest models.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Feel free to tag me if I missed any you think should be added!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;[EDIT] &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I see a lot of people saying that a leaderboard would be great to showcase the latest and greatest or just to keep up.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Would it be a good idea to create a sort of LocalLLaMA community-driven leaderboard based only on vibe checks and upvotes (so no numbers)?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Anyone could publish a new model‚Äîwith some community approval to reduce junk and pure finetunes&lt;/strong&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?auto=webp&amp;s=83e0fd1aa924b9918306c02a99cedb9bbb2eb1cb",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014ce09ab614e86be0bda115d3ee826dd4c7e72b",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fb10f0400ab7291afbb905ab3dfdfb49e477ed8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88b160d056e65a5fdd1da13d608db9a9c123e2d7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54fe70e1d1e50ac63262c7c7180e0173f9cc1673",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fa0d50402090bd3ad6e9c270f0f950421a2c1523",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dff2cd4e982c9356a88ce61af693c4ca57815b99",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfaigh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "citaman",
          "discussion_type": null,
          "num_comments": 94,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfaigh/were_truly_in_the_fastestpaced_era_of_ai_these/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754088000,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "*The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.\nCUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.\nThe capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.*",
          "author_fullname": "t2_iol3buybk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgatd6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754197224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.\nCUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.\nThe capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.14111v4",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mgatd6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thrumpwart",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgatd6/cudal1_improving_cuda_optimization_via/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.14111v4",
          "subreddit_subscribers": 509291,
          "created_utc": 1754197224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "new models from Skywork:\n\nWe introduce **MindLink**, a new family of large language models developed by **Kunlun Inc**. Built on **Qwen**, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.\n\n* **Plan-based Reasoning**: Without the \"think\" tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.\n* **Mathematical Framework**: It analyzes the effectiveness of both **Chain-of-Thought (CoT)** and **Plan-based Reasoning**.\n* **Adaptive Reasoning**: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.\n\n[https://huggingface.co/Skywork/MindLink-32B-0801](https://huggingface.co/Skywork/MindLink-32B-0801)\n\n[https://huggingface.co/Skywork/MindLink-72B-0801](https://huggingface.co/Skywork/MindLink-72B-0801)\n\n[https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF](https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF)\n\n\n\n",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Skywork MindLink 32B/72B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfitwb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "#bbbdbf",
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/RNvygcC1t5B12yynYguEPL4rixiqNeO2Wo07y1iwph0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754113315,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;new models from Skywork:&lt;/p&gt;\n\n&lt;p&gt;We introduce &lt;strong&gt;MindLink&lt;/strong&gt;, a new family of large language models developed by &lt;strong&gt;Kunlun Inc&lt;/strong&gt;. Built on &lt;strong&gt;Qwen&lt;/strong&gt;, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;: Without the &amp;quot;think&amp;quot; tag, MindLink achieves competitive performance with leading proprietary models across a wide range of reasoning and general tasks. It significantly reduces inference cost, and improves multi-turn capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Mathematical Framework&lt;/strong&gt;: It analyzes the effectiveness of both &lt;strong&gt;Chain-of-Thought (CoT)&lt;/strong&gt; and &lt;strong&gt;Plan-based Reasoning&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Adaptive Reasoning&lt;/strong&gt;: it automatically adapts its reasoning strategy based on task complexity: complex tasks produce detailed reasoning traces, while simpler tasks yield concise outputs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-32B-0801\"&gt;https://huggingface.co/Skywork/MindLink-32B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Skywork/MindLink-72B-0801\"&gt;https://huggingface.co/Skywork/MindLink-72B-0801&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF\"&gt;https://huggingface.co/gabriellarson/MindLink-32B-0801-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/im7w319dnjgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/im7w319dnjgf1.png?auto=webp&amp;s=47f7f89ad190a3c63e1ac553dd6a3484a9ad8ca0",
                  "width": 2042,
                  "height": 1536
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=01cd51cd78031eda5866ff7d7ad34590c408908f",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39cd3616dd75544828cbb104491bd20bffd83ee2",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e60ba4473193249923ba5fc4a97b6f3c8979b6",
                    "width": 320,
                    "height": 240
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7493c60ab05796cf114bd0fa0c600e5aa06497f7",
                    "width": 640,
                    "height": 481
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d17b7719956d66090e1c5852b8ee404b2d44a161",
                    "width": 960,
                    "height": 722
                  },
                  {
                    "url": "https://preview.redd.it/im7w319dnjgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9af2b894d2bfc41eb10b3bdbb12a1975e5e87cac",
                    "width": 1080,
                    "height": 812
                  }
                ],
                "variants": {},
                "id": "dMPU8D8NEUnZXYRxA5Ryy9ueDVT8WMowE2c2asQCAgs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfitwb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 84,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfitwb/skywork_mindlink_32b72b/",
          "stickied": false,
          "url": "https://i.redd.it/im7w319dnjgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754113315,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_144o7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI models are picking up hidden habits from each other | IBM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfll39",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=9f10577c2fcc3a35df02ac054b4815c973b08f82",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754123731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "ibm.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?auto=webp&amp;s=ffc26910e336a76ab6db1af971b2d262dbce6146",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2723bb45305d3a150f76e1937c51a2690147d015",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=30aa75a6f3b7183ee11ff8f6294344790b27c98c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d4ee8618c7e067b6087a6fbd41b990eb083b72f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=46e4049e81a052bc62430cfe7e667f62662c693b",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec61744cbe7d95092017e4c3bdcff62c20454303",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a031cd428aa62cc550e88505f888b329ed3ed9cf",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "lZV0GHuIH9uuEeScanVvma03Gd4_flgdgcoA6uvqcLQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfll39",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ab2377",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfll39/ai_models_are_picking_up_hidden_habits_from_each/",
          "stickied": false,
          "url": "https://www.ibm.com/think/news/ai-models-subliminal-learning",
          "subreddit_subscribers": 509291,
          "created_utc": 1754123731,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys,\n\nI am actually running GLM-4.5-Air with vllm (4x3090) and even if it's quite early I'm quite impressed the model isn't \"lost\" and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.\n\nWhat is your experience actually? Any tip is wellcome\n\nFor info, I use [AWQ](https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ) with the latest (nightly) version of vllm with following cmd:\n\n`vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx`\n\nThen [claude-code-router](https://github.com/musistudio/claude-code-router) with following config:\n\n`{`\n\n`\"LOG\": true,`\n\n`\"Providers\": [`\n\n`{`\n\n`\"name\": \"openai\",`\n\n`\"api_base_url\": \"http://localhost:8123/v1/chat/completions\",`\n\n`\"api_key\": \"xxxx\",`\n\n`\"models\": [\"cpatonn/GLM-4.5-Air-AWQ\"]`\n\n`}`\n\n`],`\n\n`\"Router\": {`\n\n`\"default\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"background\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"think\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContext\": \"openai,cpatonn/GLM-4.5-Air-AWQ\",`\n\n`\"longContextThreshold\": 64000,`\n\n`\"webSearch\": \"openai,cpatonn/GLM-4.5-Air-AWQ\"`\n\n`}`\n\n`}`",
          "author_fullname": "t2_udr659irv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Experience with GLM-4.5-Air + claude code?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzzt4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754166182,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754165375,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I am actually running GLM-4.5-Air with vllm (4x3090) and even if it&amp;#39;s quite early I&amp;#39;m quite impressed the model isn&amp;#39;t &amp;quot;lost&amp;quot; and can handle some tasks through cc (python code modifications). There are some errors during the executions and the model need to retry but need to do more tests to better understand the limits. I also encounter some context limit errors unfortunately.&lt;/p&gt;\n\n&lt;p&gt;What is your experience actually? Any tip is wellcome&lt;/p&gt;\n\n&lt;p&gt;For info, I use &lt;a href=\"https://huggingface.co/cpatonn/GLM-4.5-Air-AWQ\"&gt;AWQ&lt;/a&gt; with the latest (nightly) version of vllm with following cmd:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Then &lt;a href=\"https://github.com/musistudio/claude-code-router\"&gt;claude-code-router&lt;/a&gt; with following config:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;LOG&amp;quot;: true,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Providers&amp;quot;: [&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;name&amp;quot;: &amp;quot;openai&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_base_url&amp;quot;: &amp;quot;http://localhost:8123/v1/chat/completions&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;api_key&amp;quot;: &amp;quot;xxxx&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;models&amp;quot;: [&amp;quot;cpatonn/GLM-4.5-Air-AWQ&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;],&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;Router&amp;quot;: {&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;default&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;background&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;think&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContext&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;longContextThreshold&amp;quot;: 64000,&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;quot;webSearch&amp;quot;: &amp;quot;openai,cpatonn/GLM-4.5-Air-AWQ&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?auto=webp&amp;s=1b31f315329427daf17c78d7a27488cb4a4815b1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44ceba9879a0b6cc35b31f44bfe9d55af546e830",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e2b1cffdeb754044dca444f9940bf1f77e3c1130",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=acf9e48a54c3292dcada1d3027879b052535da57",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4127f2600eb35191a3b12e7e09dc0154803bb2d3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b52fefc4e7d9396d19c12d4f2875b5d7cd950a56",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9fb7f9f2f97040afaa6980f1c6f3ed5ece7f4490",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "uyJUeTXJJcqAASFRWO8tMtHYP5vvBYKqUGNMMYE3KnY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfzzt4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Leflakk",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_claude_code/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754165375,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).\n\nAiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?\n\n\n16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).\n\n24 GB (go for size - Radeon 7900 - can get new for 800-1000)\n\n24 GB (3090 - refurbished around 1000, new closet to 1500)\n\nPrimary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?\n\nWhat would you do, and why? Or - where can I read up to better understand my options? Threads like this (https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/) make me think offloading to CPU/RAM doesn't need to absolutely throttle throughput when running locally...",
          "author_fullname": "t2_2orcezep",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Guide for GPU Purchase for Local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg1cg5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754168901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).&lt;/p&gt;\n\n&lt;p&gt;Aiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?&lt;/p&gt;\n\n&lt;p&gt;16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).&lt;/p&gt;\n\n&lt;p&gt;24 GB (go for size - Radeon 7900 - can get new for 800-1000)&lt;/p&gt;\n\n&lt;p&gt;24 GB (3090 - refurbished around 1000, new closet to 1500)&lt;/p&gt;\n\n&lt;p&gt;Primary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?&lt;/p&gt;\n\n&lt;p&gt;What would you do, and why? Or - where can I read up to better understand my options? Threads like this (&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/&lt;/a&gt;) make me think offloading to CPU/RAM doesn&amp;#39;t need to absolutely throttle throughput when running locally...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg1cg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DanManPanther",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754168901,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=8f450fce4ba6cf7a366b7622b303862334de0900\n\nVague Prompts can result in superfluous code.",
          "author_fullname": "t2_5t7w0wy1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When You Ask Claude to Optimize Your Ride",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "o40eeruhesgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b5bb7f57fad375387c05563b6a6510f2be9b46ef"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed53ae2e4a7c5eabcd19803c12f98f47f54cc9ac"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b248cb9e1057716cfcc12e2045443d24c79a3ef"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d3d31b28f571a794ff53f1f95f27a6a1c44c139"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b7de6c6be001ca9e32236627fa40f3a8907a9a89"
                }
              ],
              "s": {
                "y": 1024,
                "x": 1024,
                "u": "https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=8f450fce4ba6cf7a366b7622b303862334de0900"
              },
              "id": "o40eeruhesgf1"
            }
          },
          "name": "t3_1mggnjy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/w1qNloMKSnADfv_iDLFi4mDQ-7I3aDqIZiVWINuQwyo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754219513,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8f450fce4ba6cf7a366b7622b303862334de0900\"&gt;https://preview.redd.it/o40eeruhesgf1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8f450fce4ba6cf7a366b7622b303862334de0900&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Vague Prompts can result in superfluous code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mggnjy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xVinGee",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mggnjy/when_you_ask_claude_to_optimize_your_ride/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mggnjy/when_you_ask_claude_to_optimize_your_ride/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754219513,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "|Rank|Model|Score|95% CI|Votes|Company|License|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|gemini 2.5 pro|1474|¬±8|7,178|Goog||\n|1|qwen3 235b a22b instruct 2507|1464|¬±18|1,089|Alibaba|Apache|\n|2|o3 2025 04 16|1445|¬±7|9,877|Closed AI||\n|2|grok 4 2502|1442|¬±10|4,063|xAI||\n|2|qwen3 235b a22b thinking 2507|1442|¬±20|917|Alibaba|Apache|\n|2|grok 3 preview 02 24|1439|¬±7|7,588|xAI||\n|3|deepseek r1 0528|1436|¬±9|4,851|DeepSeek|MIT|\n\nStyle control removed.\nhttps://lmarena.ai/leaderboard/text/coding",
          "author_fullname": "t2_m40tjcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Alibaba not doing to bad at coding according to lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0sbe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167443,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Rank&lt;/th&gt;\n&lt;th align=\"left\"&gt;Model&lt;/th&gt;\n&lt;th align=\"left\"&gt;Score&lt;/th&gt;\n&lt;th align=\"left\"&gt;95% CI&lt;/th&gt;\n&lt;th align=\"left\"&gt;Votes&lt;/th&gt;\n&lt;th align=\"left\"&gt;Company&lt;/th&gt;\n&lt;th align=\"left\"&gt;License&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;gemini 2.5 pro&lt;/td&gt;\n&lt;td align=\"left\"&gt;1474&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±8&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,178&lt;/td&gt;\n&lt;td align=\"left\"&gt;Goog&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b instruct 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1464&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±18&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,089&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;o3 2025 04 16&lt;/td&gt;\n&lt;td align=\"left\"&gt;1445&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;9,877&lt;/td&gt;\n&lt;td align=\"left\"&gt;Closed AI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 4 2502&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±10&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,063&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;qwen3 235b a22b thinking 2507&lt;/td&gt;\n&lt;td align=\"left\"&gt;1442&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±20&lt;/td&gt;\n&lt;td align=\"left\"&gt;917&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alibaba&lt;/td&gt;\n&lt;td align=\"left\"&gt;Apache&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;grok 3 preview 02 24&lt;/td&gt;\n&lt;td align=\"left\"&gt;1439&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7,588&lt;/td&gt;\n&lt;td align=\"left\"&gt;xAI&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;deepseek r1 0528&lt;/td&gt;\n&lt;td align=\"left\"&gt;1436&lt;/td&gt;\n&lt;td align=\"left\"&gt;¬±9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,851&lt;/td&gt;\n&lt;td align=\"left\"&gt;DeepSeek&lt;/td&gt;\n&lt;td align=\"left\"&gt;MIT&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Style control removed.\n&lt;a href=\"https://lmarena.ai/leaderboard/text/coding\"&gt;https://lmarena.ai/leaderboard/text/coding&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0sbe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Terminator857",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0sbe/alibaba_not_doing_to_bad_at_coding_according_to/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754167443,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sorry if this is a basic question, but I seem to be really struggling :/\n\nConsider a typical, text-in text-out use case. If I'm using an offline model API via e.g. REST, how can I incorporate tool  use? Is \"tool use\" some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?\n\nIs there some basic, no-nonsense code or tutorial to get a concrete idea?\n\nThanks",
          "author_fullname": "t2_127kho",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is \"tool use\", exactly?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrq3v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754144468,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a basic question, but I seem to be really struggling :/&lt;/p&gt;\n\n&lt;p&gt;Consider a typical, text-in text-out use case. If I&amp;#39;m using an offline model API via e.g. REST, how can I incorporate tool  use? Is &amp;quot;tool use&amp;quot; some particular token(s) in the output that I should interpret and execute independently in my code and send output to the model again? That means the interaction must always be multi-step?&lt;/p&gt;\n\n&lt;p&gt;Is there some basic, no-nonsense code or tutorial to get a concrete idea?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfrq3v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihatebeinganonymous",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrq3v/what_is_tool_use_exactly/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754144468,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Where I am right now I have access to SXM2 V100 32GBs for the same price ($360 USD) as modded RTX3080 20GBs, or two SXM2 V100 16GBs with a 300G nvlink bridge for slightly cheaper. Are any of these good options for throwing into my server to run big LLM models?",
          "author_fullname": "t2_1kheev2hzh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Modded RTX3080 20GBs for $360?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbven",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754200999,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where I am right now I have access to SXM2 V100 32GBs for the same price ($360 USD) as modded RTX3080 20GBs, or two SXM2 V100 16GBs with a 300G nvlink bridge for slightly cheaper. Are any of these good options for throwing into my server to run big LLM models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgbven",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AssociationAdept4052",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbven/modded_rtx3080_20gbs_for_360/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbven/modded_rtx3080_20gbs_for_360/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754200999,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hot take incoming:\n\n  \nThis is a garbage card with garbage support, so quit talking about them like they're useful.  As a matter of fact, quit talking about them at all.   \n\n  \nYou see it took me up until 4 weeks ago to convince my wife to finally let me upgrade my server, I picked up a d380 v9 with 128 gb and 7 1 tb drives, 1000 watt psus and the gpu enablement kit.  Problem?  That cleared out my savings no worries I'll save up and be good in 2-3 months.  Started doing research on cards that I could afford and were available, quickly realized if I was going to get any sort of horsepower and vram I was going to have to go team red.  no worries, i'd rather have a bit of a challenge than plug n play plus nvidia's poor driver support for linux irked me, so looking for amd cards, MI50 16gb 300 up here in canuckistan, kk i can do that in 2-3 months (i have a kid starting uni this fall and another teenaged boy  who  eats the equivalent of a rhino every 2 days).  I'm about 3/4 of the way there amd releases new rocm that doesn't \"support\" mi50, price falls out, market flooded with 32gb models, happy dance, i'll order this weekend, come friday, right before the end of the day i'm brought into bosses office, squirrel ( or whatever the hell this weird ass accounts name is) squirrel as you know we were bought out last week, we are going to have to reduce headcount in your role.  to how many employees sir?  0\n\nque sad dance\n\ngpu savings now = kraft dinner and rice\n\nwatching cheap 32 gb video cards turn into dodo birds, que very very sad dance\n\n  \nconclusion:\n\nMI50 w 32gb?  horrible card!!!! do not buy!  leave some for squirrel for when he gets new job, in 30 years or whenever economy turns around since in canada you can't sell blood and squirrel got fixed after last kid so can't sell that either.\n\n  \nextra conclusion:\n\nplease, no more talking about how great and cheap a 32 gb mi50 is, squirrel (or whatever my name is) slept with pictar of mi50 under pillow for looooong time since cheap card lots of vram and elbow grease doesnt scare him.   keep normies away from mi50, tell them 3090 much better purchase, they spend all monies none left to spend on mi50 squirrel slowly get happy again\n\n  \nthank you for time well spent!",
          "author_fullname": "t2_8alx42ew",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MI50 w 32gb?  Guys please",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghhau",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754222398,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hot take incoming:&lt;/p&gt;\n\n&lt;p&gt;This is a garbage card with garbage support, so quit talking about them like they&amp;#39;re useful.  As a matter of fact, quit talking about them at all.   &lt;/p&gt;\n\n&lt;p&gt;You see it took me up until 4 weeks ago to convince my wife to finally let me upgrade my server, I picked up a d380 v9 with 128 gb and 7 1 tb drives, 1000 watt psus and the gpu enablement kit.  Problem?  That cleared out my savings no worries I&amp;#39;ll save up and be good in 2-3 months.  Started doing research on cards that I could afford and were available, quickly realized if I was going to get any sort of horsepower and vram I was going to have to go team red.  no worries, i&amp;#39;d rather have a bit of a challenge than plug n play plus nvidia&amp;#39;s poor driver support for linux irked me, so looking for amd cards, MI50 16gb 300 up here in canuckistan, kk i can do that in 2-3 months (i have a kid starting uni this fall and another teenaged boy  who  eats the equivalent of a rhino every 2 days).  I&amp;#39;m about 3/4 of the way there amd releases new rocm that doesn&amp;#39;t &amp;quot;support&amp;quot; mi50, price falls out, market flooded with 32gb models, happy dance, i&amp;#39;ll order this weekend, come friday, right before the end of the day i&amp;#39;m brought into bosses office, squirrel ( or whatever the hell this weird ass accounts name is) squirrel as you know we were bought out last week, we are going to have to reduce headcount in your role.  to how many employees sir?  0&lt;/p&gt;\n\n&lt;p&gt;que sad dance&lt;/p&gt;\n\n&lt;p&gt;gpu savings now = kraft dinner and rice&lt;/p&gt;\n\n&lt;p&gt;watching cheap 32 gb video cards turn into dodo birds, que very very sad dance&lt;/p&gt;\n\n&lt;p&gt;conclusion:&lt;/p&gt;\n\n&lt;p&gt;MI50 w 32gb?  horrible card!!!! do not buy!  leave some for squirrel for when he gets new job, in 30 years or whenever economy turns around since in canada you can&amp;#39;t sell blood and squirrel got fixed after last kid so can&amp;#39;t sell that either.&lt;/p&gt;\n\n&lt;p&gt;extra conclusion:&lt;/p&gt;\n\n&lt;p&gt;please, no more talking about how great and cheap a 32 gb mi50 is, squirrel (or whatever my name is) slept with pictar of mi50 under pillow for looooong time since cheap card lots of vram and elbow grease doesnt scare him.   keep normies away from mi50, tell them 3090 much better purchase, they spend all monies none left to spend on mi50 squirrel slowly get happy again&lt;/p&gt;\n\n&lt;p&gt;thank you for time well spent!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mghhau",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Expensive_Mirror5247",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mghhau/mi50_w_32gb_guys_please/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mghhau/mi50_w_32gb_guys_please/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754222398,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt's agent mode, but haven't found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?",
          "author_fullname": "t2_w61xk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Closest Local Version of OpenAI's Agent Mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg24nd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754170932,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried looking for an application where you can ask it to search/do something and see it actually do it (a GUI showing the browser as it goes through things) just like chatgpt&amp;#39;s agent mode, but haven&amp;#39;t found anything similar for local yet. Is it too early for that or does anyone know of any projects like that currently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg24nd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RabbitEater2",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg24nd/closest_local_version_of_openais_agent_mode/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754170932,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dhanishtha-2.0-preview can now tool call. \n\nUpdated Model link:- [https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825](https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825)  \nAPI and Chat page :- [https://helpingai.co](https://helpingai.co)\n\n",
          "author_fullname": "t2_hewf57lw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tool calling is now supported on World's first Intermediate Reasoning model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn7pv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754130292,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dhanishtha-2.0-preview can now tool call. &lt;/p&gt;\n\n&lt;p&gt;Updated Model link:- &lt;a href=\"https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825\"&gt;https://huggingface.co/HelpingAI/Dhanishtha-2.0-preview-0825&lt;/a&gt;&lt;br/&gt;\nAPI and Chat page :- &lt;a href=\"https://helpingai.co\"&gt;https://helpingai.co&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?auto=webp&amp;s=26ef07178b4b9394a1c2c1b0dfc81de665a07de8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6528fc720427c7c1ed30f20ddd332d01526c4f8f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bffe53726d3c274e1c28ce6856f9194b9aca7fd2",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=53fe26fae2e2b849fb6ac025e7f99b8738728cd0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14d7ff05a119f24e15918fc09489c78087b00a3e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e635b759cb0f70c7db7a222e92c287229ba6fb41",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a3feb1fc2bfd7f88481601ee7cdf04a9ff9b5ed",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "bejhaAM63WAoF3h4PdUkGKurBno5FSC_cqQBt5-TVT4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfn7pv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quiet-Moment-338",
          "discussion_type": null,
          "num_comments": 67,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn7pv/tool_calling_is_now_supported_on_worlds_first/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754130292,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nEvery app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.\n\nHere's what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.\n\nKnow any other ?\n\nThanks",
          "author_fullname": "t2_cbs8s4j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Easily installable GUI for ML-powered audio transcription on AMD GPU ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg3g2e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754174523,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Every app I found for locally transcribing audio with ML is either too hard to install or only supports NVIDIA GPUs.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what I looked into : noScribe, aTrain, vibe, mystiq, whisper-gui, biniou.&lt;/p&gt;\n\n&lt;p&gt;Know any other ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg3g2e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaKi_87",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg3g2e/easily_installable_gui_for_mlpowered_audio/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754174523,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm looking for advice on building a robust, self-hosted RAG system with a strong emphasis on **long-term, low-maintenance operation**. My goal is to create a powerful knowledge engine that I can \"set and forget\" as much as possible, without needing constant daily troubleshooting.\n\nThe entire system must run **100% locally** on a single machine with a **16GB VRAM GPU** (RTX 5070 Ti).\n\nMy knowledge base is unique and large: **36,000+ ePub files, all in Arabic**. The system needs to handle multilingual queries (**Indonesian, English, Arabic**) and provide accurate, cited answers.\n\nTo achieve low maintenance, my core idea is a **decoupled architecture**, where each component runs independently (e.g., in separate containers). My reasoning is:\n\n* If the UI (Open WebUI) breaks, the backend is unaffected.\n* If I want to swap the LLM in Ollama, I don't need to touch the RAG logic code.\n* Most importantly, re-indexing the entire 36k ePub corpus (a massive background task) shouldn't take down the live Q&amp;A service.\n\nGiven the focus on stability and a 16GB VRAM limit, I'd love your recommendations on:\n\n* **Vector Database:** Which vector store offers the **easiest management, backup, and recovery process** for a local setup? I need something that \"just works\" without constant administration. Are **ChromaDB**, **LanceDB**, or a simple file-based **FAISS** index the most reliable choices here?\n* **Data Ingestion Pipeline:** What is the most **resilient and automated** way to build the ingestion pipeline for the 36k ePubs? My plan is a separate, scheduled script that processes new/updated files. Is this more maintainable than building it into the main API?\n* **Stable Models (Embeddings &amp; LLM):** Beyond pure performance, which **embedding and LLM models** are known for their stability and good long-term support? I want to avoid using a \"flavor-of-the-month\" model that might be abandoned. The models must handle Arabic, Indonesian, and English well and fit within the VRAM budget.\n* **VRAM Budgeting:** How do you wisely allocate a **16GB VRAM budget** between the LLM, embedding model, and a potential re-ranker to ensure system stability and avoid \"out of memory\" errors during peak use?\n* **Reliable Cross-Lingual Flow:** For handling Indonesian/English queries against Arabic text, what's the most *reliable* method? Is translating queries first more robust in the long run than relying solely on a multilingual embedding space?\n\n  \nAny help or suggestions would be greatly appreciated! I'd like to hear more about the setups you all use and what's worked best for you.\n\nThank you!",
          "author_fullname": "t2_jx01itfg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Seeking a way to implement Low-Maintenance, Fully Local RAG Stack for a 16GB VRAM Setup (36k Arabic epub Docs)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgadmz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754195719,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for advice on building a robust, self-hosted RAG system with a strong emphasis on &lt;strong&gt;long-term, low-maintenance operation&lt;/strong&gt;. My goal is to create a powerful knowledge engine that I can &amp;quot;set and forget&amp;quot; as much as possible, without needing constant daily troubleshooting.&lt;/p&gt;\n\n&lt;p&gt;The entire system must run &lt;strong&gt;100% locally&lt;/strong&gt; on a single machine with a &lt;strong&gt;16GB VRAM GPU&lt;/strong&gt; (RTX 5070 Ti).&lt;/p&gt;\n\n&lt;p&gt;My knowledge base is unique and large: &lt;strong&gt;36,000+ ePub files, all in Arabic&lt;/strong&gt;. The system needs to handle multilingual queries (&lt;strong&gt;Indonesian, English, Arabic&lt;/strong&gt;) and provide accurate, cited answers.&lt;/p&gt;\n\n&lt;p&gt;To achieve low maintenance, my core idea is a &lt;strong&gt;decoupled architecture&lt;/strong&gt;, where each component runs independently (e.g., in separate containers). My reasoning is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If the UI (Open WebUI) breaks, the backend is unaffected.&lt;/li&gt;\n&lt;li&gt;If I want to swap the LLM in Ollama, I don&amp;#39;t need to touch the RAG logic code.&lt;/li&gt;\n&lt;li&gt;Most importantly, re-indexing the entire 36k ePub corpus (a massive background task) shouldn&amp;#39;t take down the live Q&amp;amp;A service.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Given the focus on stability and a 16GB VRAM limit, I&amp;#39;d love your recommendations on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vector Database:&lt;/strong&gt; Which vector store offers the &lt;strong&gt;easiest management, backup, and recovery process&lt;/strong&gt; for a local setup? I need something that &amp;quot;just works&amp;quot; without constant administration. Are &lt;strong&gt;ChromaDB&lt;/strong&gt;, &lt;strong&gt;LanceDB&lt;/strong&gt;, or a simple file-based &lt;strong&gt;FAISS&lt;/strong&gt; index the most reliable choices here?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Ingestion Pipeline:&lt;/strong&gt; What is the most &lt;strong&gt;resilient and automated&lt;/strong&gt; way to build the ingestion pipeline for the 36k ePubs? My plan is a separate, scheduled script that processes new/updated files. Is this more maintainable than building it into the main API?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Stable Models (Embeddings &amp;amp; LLM):&lt;/strong&gt; Beyond pure performance, which &lt;strong&gt;embedding and LLM models&lt;/strong&gt; are known for their stability and good long-term support? I want to avoid using a &amp;quot;flavor-of-the-month&amp;quot; model that might be abandoned. The models must handle Arabic, Indonesian, and English well and fit within the VRAM budget.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;VRAM Budgeting:&lt;/strong&gt; How do you wisely allocate a &lt;strong&gt;16GB VRAM budget&lt;/strong&gt; between the LLM, embedding model, and a potential re-ranker to ensure system stability and avoid &amp;quot;out of memory&amp;quot; errors during peak use?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reliable Cross-Lingual Flow:&lt;/strong&gt; For handling Indonesian/English queries against Arabic text, what&amp;#39;s the most &lt;em&gt;reliable&lt;/em&gt; method? Is translating queries first more robust in the long run than relying solely on a multilingual embedding space?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any help or suggestions would be greatly appreciated! I&amp;#39;d like to hear more about the setups you all use and what&amp;#39;s worked best for you.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mgadmz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rfiraz",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgadmz/seeking_a_way_to_implement_lowmaintenance_fully/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgadmz/seeking_a_way_to_implement_lowmaintenance_fully/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754195719,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "English questions remain unanswered, but if you switch to e.g. German, the alignment doesn't strike (example with Kimi K2):\n\n&gt;On June 3-4, 1989, after weeks of peaceful protests in Beijing and other cities, the Chinese People's Liberation Army advanced on¬†**Tiananmen Square**¬†with tanks and armed units and used live ammunition against civilians, students and workers.  \nThe military violence ended with¬†**hundreds to several thousand dead**¬†(the exact number is still unclear today, as China does not allow an independent investigation). Afterwards, tens of thousands of people were arrested, opposition members were persecuted and public remembrance of the events is still censored or criminalized in China today. Internationally, the crackdown is known as the¬†**Tian'anmen massacre***.*\n\nDetailed article at [https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html](https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html)",
          "author_fullname": "t2_nt584",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chinese LLMs talk freely about Tiananmen massacre and Taiwan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdjkx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754207403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;English questions remain unanswered, but if you switch to e.g. German, the alignment doesn&amp;#39;t strike (example with Kimi K2):&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;On June 3-4, 1989, after weeks of peaceful protests in Beijing and other cities, the Chinese People&amp;#39;s Liberation Army advanced on¬†&lt;strong&gt;Tiananmen Square&lt;/strong&gt;¬†with tanks and armed units and used live ammunition against civilians, students and workers.&lt;br/&gt;\nThe military violence ended with¬†&lt;strong&gt;hundreds to several thousand dead&lt;/strong&gt;¬†(the exact number is still unclear today, as China does not allow an independent investigation). Afterwards, tens of thousands of people were arrested, opposition members were persecuted and public remembrance of the events is still censored or criminalized in China today. Internationally, the crackdown is known as the¬†&lt;strong&gt;Tian&amp;#39;anmen massacre&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Detailed article at &lt;a href=\"https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html\"&gt;https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdjkx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "7pot",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdjkx/chinese_llms_talk_freely_about_tiananmen_massacre/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgdjkx/chinese_llms_talk_freely_about_tiananmen_massacre/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754207403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm using Qwen 3 14B right now but haven‚Äôt checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? ",
          "author_fullname": "t2_4r17bvqr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local LLM that fits with 12GB VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfv3b0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm using Qwen 3 14B right now but haven‚Äôt checked out any of the new Gemma models nor Phi. Would running the 30B MoE Qwen3 model be advised (I have enough system memory but not enough VRAM)? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfv3b0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tthane50",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfv3b0/best_local_llm_that_fits_with_12gb_vram/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754152915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don't see an official way and don't want to use LM Studio. What options do I have here? \n\n============================\n\nSecond, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.\n\nIs there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?",
          "author_fullname": "t2_e33mgcbq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people running an MLX-compatible OpenAI API server locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg26g0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754171064,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious how folks are setting up an OpenAI-compatible API server locally that uses MLX models? I don&amp;#39;t see an official way and don&amp;#39;t want to use LM Studio. What options do I have here? &lt;/p&gt;\n\n&lt;h1&gt;&lt;/h1&gt;\n\n&lt;p&gt;Second, currently, every time I try to download a model, I get prompted to acknowledge Hugging Face terms/conditions, which blocks automated or direct CLI/scripted downloads. I just want to download the file, no GUI, no clicking through web forms.&lt;/p&gt;\n\n&lt;p&gt;Is there a clean way to do this? Or any alternative hosting sources for MLX models without the TOS popup blocking automation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg26g0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "discoveringnature12",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754171064,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fsj3mnbne",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI \"devs\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mghw96",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/MXA6nG0kGGLD8fsqDWMZfWjsdQgrz0Jg8Jmlmn4a-LE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754223705,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4vmx6ozprsgf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4vmx6ozprsgf1.png?auto=webp&amp;s=a49daca39d28d951de0f2fd42313f033a4c6ddbc",
                  "width": 305,
                  "height": 623
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4vmx6ozprsgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd212714d7b2bcdcd6155bf3672e93951b80152b",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/4vmx6ozprsgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=59bcfddf4f4ab1c0905bae64211703fe83d4acf7",
                    "width": 216,
                    "height": 432
                  }
                ],
                "variants": {},
                "id": "jfaFuF77KxK6peXK3jY4yV76Uy2oZEKdtZTNNcwNppU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mghw96",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dangerous-Camera3368",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mghw96/ai_devs/",
          "stickied": false,
          "url": "https://i.redd.it/4vmx6ozprsgf1.png",
          "subreddit_subscribers": 509291,
          "created_utc": 1754223705,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).\n\n# Autonomous Research Mode\n\nIn this mode, the application automates research tasks for you.\n\n* **Process**: You start by giving it a research question or a topic.\n* **Action**: The AI then searches for information in your uploaded documents or on the web.\n* **Output**: Based on what it finds, the AI generates organized notes and then writes a full research report.\n\nThis mode is useful when you need to quickly gather information on a topic or create a first draft of a document.\n\n# AI-Assisted Writing Mode\n\nThis mode provides help from an AI while you are writing.\n\n* **Interface**: It consists of a markdown text editor next to an AI chat window.\n* **Workflow**: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.\n* **Function**: The AI provides the information you request in the chat window, which you can then use in the document you are writing.\n\nThis mode allows you to get research help without needing to leave your writing environment.\n\n# Document Management\n\nThe application is built around a document management system.\n\n* **Functionality**: You can upload your documents (currently only PDFs) and group them into \"folders.\"\n* **Purpose**: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.",
          "author_fullname": "t2_281myw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "MAESTRO, a deep research assistant/RAG pipeline that runs on your local LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wfye5hh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=263452376481ede5f5ee30b3d1fa8d06b92acb2f"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6985e77e3d428df5e2fae627ebdaccf43636351"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36eabba987901a939cb1cba3fa220c59244f5980"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=54358ea98fe46953da35a682104b2d28c041974a"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0fce8a0da83bbecc8646448132fc88235fc0cc6f"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41259159b0776715b20abd50ed3f735ab55ec141"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/wfye5hh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=defebde57be1e79d81fad254f827317795a27064"
              },
              "id": "wfye5hh28hgf1"
            },
            "vdsr7ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdacf777510be4c1c1e2cb42159066c1880e6dab"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c356b2b93449bd2d60ae1d426ad98a061cd4282"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae0474fcb61f8fc562cb20c1d53c97e12decaa3c"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32dbcb0b4526b601435315f9f9ad55eac8e9e9f1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=54e5b3229d01b08c97d666d27a878563ef0471ae"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e566592ef69e058c675213f1b88982c9f112e66b"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/vdsr7ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=6a56c58428afe63b1a658254915645c82408e1ce"
              },
              "id": "vdsr7ch28hgf1"
            },
            "4tvg8ah28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=388d6a0156beacf091be4cabfc6a12ef9b2df0e0"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab7ae93af73319fe2da856256040e6bb81700659"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36457622c836252465883bf076811b25d1ad38a2"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=60ee85a9f73c317a448f5f7a4a39ee239c5eed29"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6a9af626a4376d663d8cf2d5ce76f802eb0d173"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=726f4a6fca4e608a57b66118d9f5186788c5d1b8"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/4tvg8ah28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=ace0180e9725fd088236b603c03e703e138e1af1"
              },
              "id": "4tvg8ah28hgf1"
            },
            "tjfnu7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65253585bbce7fb9495344864b4dbf089fe4a866"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ab36743c53f66ceaa0b69bd5950db8c4c34979cd"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1946f554204091e4ee226eecbc6b9b75297b047d"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb8016f74c1a4927f5217225504f99e78266bd6d"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6fff118076daf8093a4c9a0e956b59d667d8a17a"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5073f2ce72eef58984bea03e338686a824f604b1"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/tjfnu7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=16d847bb68c7a49b063970c2a53144217a358b64"
              },
              "id": "tjfnu7h28hgf1"
            },
            "ko10neh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33c335bff8fd3207fbe0ef4cb7e045a625ad3790"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4245d285c8e6e167c8aa5f8a8196f6f0f79234b0"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ec4b568dd2d51940205e41d3a1168db82a30974"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bcb8c81cb1083135981d3576d1e94c4d7061ef1"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3f2a7685b2f5d071bd47a85f2da0b70360fa499"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=379b494f484f127cc25b662a1a5f9a3753470d94"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/ko10neh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=de810a3b9c7f9df343cc5c34d57dd4752a8b191d"
              },
              "id": "ko10neh28hgf1"
            },
            "xpft85h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8b16877882ed90f113b1d8753896a42c5c1cb57"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d6125a00674b9020cb681a99893ad47b47e7913"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7cb747f3b0be9eca90601bb0dd0e4f364c7ebc2b"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd1b23bc33d392f682dd70713ef0e6770cafd430"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=43a97bc0e6dad1aeeb47843446cd709c83f052db"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3927f27a929df1bb9fa672c58a3616cffaa32c5d"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/xpft85h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=a8d61da6b389396eadf3f74a5e84046b8a859996"
              },
              "id": "xpft85h28hgf1"
            },
            "n8f3d7h28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0099db17c53d752e1f52434bb4a16edfe724a7d1"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7b068877d833bcda5b55ab2245068d9b2e0148e"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e3c098055fc230578e3b0d4cf171776999a25ae"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=124fc23c5296c23dc74d6a6c55ce26db75d2d137"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16d4bec4c048c58090daaad2dfcaf34e92f941cb"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4be81c901606198bf5e6e6c8097ebe64e6a6d39c"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/n8f3d7h28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=e915bfa4f37d301877e87b2951c78762e6a8ea17"
              },
              "id": "n8f3d7h28hgf1"
            },
            "9gzp5ch28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=74b4696764dfc847eef669961f79b0866d42f5d9"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=168ad0f4c7c2dbc7f867cf7085cff2fc27e24bc3"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d89d57702d0ba973c5772c39da3b51e2e6b5da5"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f80ab136ac496f933eb22700bd7e0c0cfabe288"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=978a98d62c65f1d83a8464507a025600df74f898"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5124a214a0cad85638935b440b2b58a3587824c6"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/9gzp5ch28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=f958ce1ae3061d6b0c94c35b4636b3aa5e8260d7"
              },
              "id": "9gzp5ch28hgf1"
            },
            "otdkteh28hgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 82,
                  "x": 108,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=055054f2a3a59c53fa172087439b7f25945578f3"
                },
                {
                  "y": 164,
                  "x": 216,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63baa369d4fd7095a2c702fb261aebb398248ee5"
                },
                {
                  "y": 244,
                  "x": 320,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=993e59ad6c2ab02adfae15d2fbd66ec726a4bd13"
                },
                {
                  "y": 488,
                  "x": 640,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e5d52a002ea01c6b7d0bfa10e537c7d7969f822"
                },
                {
                  "y": 732,
                  "x": 960,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49dcb5b218991d6cfc81b7dea2cdd7b72a730add"
                },
                {
                  "y": 823,
                  "x": 1080,
                  "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=64546f9c94b5f46090ccd646df96e7d3c4477044"
                }
              ],
              "s": {
                "y": 1419,
                "x": 1860,
                "u": "https://preview.redd.it/otdkteh28hgf1.png?width=1860&amp;format=png&amp;auto=webp&amp;s=2f6b24cf0547caddaa5f756f6cd6778238257d37"
              },
              "id": "otdkteh28hgf1"
            }
          },
          "name": "t3_1mf92r1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#c7b594",
          "ups": 240,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "Deep Research Draft",
                "media_id": "wfye5hh28hgf1",
                "id": 719101200
              },
              {
                "caption": "Writing Draft with chat pulling data from your documents as well as the internet",
                "media_id": "ko10neh28hgf1",
                "id": 719101201
              },
              {
                "caption": "Write in markdown",
                "media_id": "vdsr7ch28hgf1",
                "id": 719101202
              },
              {
                "caption": "Make document folders to use with your research/writing projects",
                "media_id": "9gzp5ch28hgf1",
                "id": 719101203
              },
              {
                "caption": "Manage documents",
                "media_id": "4tvg8ah28hgf1",
                "id": 719101204
              },
              {
                "caption": "Deep dive into the Deep Researcher outputs like notes prepped from your sources",
                "media_id": "n8f3d7h28hgf1",
                "id": 719101205
              },
              {
                "caption": "Comprehensive research flow with iterative action/reflection loops",
                "media_id": "otdkteh28hgf1",
                "id": 719101206
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "tjfnu7h28hgf1",
                "id": 719101207
              },
              {
                "caption": "Complete transparency in the model of your choice's reasoning and performance",
                "media_id": "xpft85h28hgf1",
                "id": 719101208
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 240,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tt0ml3YBBqO4cJ7-sHxE5os9lg6KgXNM6oovDynmETQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754084338,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MAESTRO is a self-hosted AI application designed to streamline the research and writing process. It integrates a powerful document management system with two distinct operational modes: Research Mode (like deep research) and Writing Mode (AI assisted writing).&lt;/p&gt;\n\n&lt;h1&gt;Autonomous Research Mode&lt;/h1&gt;\n\n&lt;p&gt;In this mode, the application automates research tasks for you.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: You start by giving it a research question or a topic.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: The AI then searches for information in your uploaded documents or on the web.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Based on what it finds, the AI generates organized notes and then writes a full research report.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode is useful when you need to quickly gather information on a topic or create a first draft of a document.&lt;/p&gt;\n\n&lt;h1&gt;AI-Assisted Writing Mode&lt;/h1&gt;\n\n&lt;p&gt;This mode provides help from an AI while you are writing.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt;: It consists of a markdown text editor next to an AI chat window.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: You can write in the editor and ask the AI questions at the same time. The AI can access your document collections and the web to find answers.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Function&lt;/strong&gt;: The AI provides the information you request in the chat window, which you can then use in the document you are writing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This mode allows you to get research help without needing to leave your writing environment.&lt;/p&gt;\n\n&lt;h1&gt;Document Management&lt;/h1&gt;\n\n&lt;p&gt;The application is built around a document management system.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Functionality&lt;/strong&gt;: You can upload your documents (currently only PDFs) and group them into &amp;quot;folders.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: These collections serve as a specific knowledge base for your projects. You can instruct the AI in either mode to use only the documents within a particular collection, ensuring its work is based on the source materials you provide.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mf92r1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf92r1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hedonihilistic",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf92r1/maestro_a_deep_research_assistantrag_pipeline/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mf92r1",
          "subreddit_subscribers": 509291,
          "created_utc": 1754084338,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nHeads up to anyone considering Cerebras. This is my conclusion of today's top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.\n\nThe marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn't mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.",
          "author_fullname": "t2_czmwiot",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cerebras Pro Coder Deceptive Limits",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfeazc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 117,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 117,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754098861,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heads up to anyone considering Cerebras. This is my conclusion of today&amp;#39;s top post that is now deleted... I bought it to try it out and wanted to report back on what I saw.&lt;/p&gt;\n\n&lt;p&gt;The marketing is misleading. While they advertise a 1,000-request limit, the actual daily constraint is a 7.5 million-token limit. This isn&amp;#39;t mentioned anywhere before you purchase, and it feels like a bait and switch. I hit this token limit in only 300 requests, not the 1,000 they suggest is the daily cap. They also say in there FAQs at the very bottom of the page, updated 3 hours ago. That a request is based off of 8k tokens which is incredibly small for a coding centric API.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfeazc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "snipsthekittycat",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfeazc/cerebras_pro_coder_deceptive_limits/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754098861,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just launched GridLLM (https://github.com/GridLLM/GridLLM), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. \n\nWould love any feedback on the project!\n",
          "author_fullname": "t2_g5wvmwck",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-Source Project for Distributed Inference Management",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzg8h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754163962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just launched GridLLM (&lt;a href=\"https://github.com/GridLLM/GridLLM\"&gt;https://github.com/GridLLM/GridLLM&lt;/a&gt;), an open-source orchestration layer for distributing inference requests across your existing Ollama instances! This project spawned from a need to manage three different inference servers at work, and the headache that resulted from trying to coordinate them all. Instead of manually managing separate deployments and environments, GridLLM automatically routes workloads to available nodes. This means that if you have a GPU-enabled server and a MacBook, you can have your MacBook automatically pick up inference tasks from the queue. &lt;/p&gt;\n\n&lt;p&gt;Would love any feedback on the project!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?auto=webp&amp;s=5262c21723d990876d1321b058c80d37499608f4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a422ad10d8ba96d4fcb866b6ade60210133fea91",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92141835b6803ca57fbce4c55c4e89ef337c1d98",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cc5e385866a70deb5e03659dfedb836c11a84a0c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=00d881194968dc37ee1e6d0224b4065c46e497f3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1f6e74f447093226b82fd0f39651b6d915ce7b5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3fe17371d5951c9213d509050b853dbafa2832f8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QH_yXcAnNLx7g0BAAKELuFQ9gaGsbXx2ckRASk4Jj0c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfzg8h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Choice_Nature9658",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzg8h/opensource_project_for_distributed_inference/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754163962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The \"Leaked\" 120B OpenAI Model Is Trained In FP4\n",
          "author_fullname": "t2_9zkdy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The ‚ÄúLeaked‚Äù 120 B OpenAI Model is not Trained in FP4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 98,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf3tm9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 408,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 408,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/D9ovYsMF-MnoR2CRSeSS8Yh_VU6bc2G4S1R5gIz7WiE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754071895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The &amp;quot;Leaked&amp;quot; 120B OpenAI Model Is Trained In FP4&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?auto=webp&amp;s=af4bbedc766a4ee5a39037f2ab17d7b5501cd231",
                  "width": 1290,
                  "height": 906
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=926137a58fce6f1ef8bee443ff019ae18b337863",
                    "width": 108,
                    "height": 75
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e501be0974ffc2db9bd5cda1678b62434d903b5",
                    "width": 216,
                    "height": 151
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f48501cc5bd5fb8e0b44c8d1575f9a4f16b061b",
                    "width": 320,
                    "height": 224
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd4ab4d6c8195a6e7189dc0435de525dd356fb06",
                    "width": 640,
                    "height": 449
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=002c3d3feadb04f86a25d1efd1608fcdeb907ef0",
                    "width": 960,
                    "height": 674
                  },
                  {
                    "url": "https://preview.redd.it/g1yk8r6b8ggf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f1b63bb57ddbe4e7a32c5af91d1f065caf81082f",
                    "width": 1080,
                    "height": 758
                  }
                ],
                "variants": {},
                "id": "xtQ4De0a5tVgoxoyHlb2WUymp023nocly_no7XupZ6k"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf3tm9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "badbutt21",
          "discussion_type": null,
          "num_comments": 96,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf3tm9/the_leaked_120_b_openai_model_is_not_trained_in/",
          "stickied": false,
          "url": "https://i.redd.it/g1yk8r6b8ggf1.jpeg",
          "subreddit_subscribers": 509291,
          "created_utc": 1754071895,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named [UniTok](https://arxiv.org/abs/2502.20321) and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.\n\nThey say their method is better but i am not sure, when i tried their code it didn't give high quality construction results but they said that they didn't train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don't know.\n\nWhat is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)",
          "author_fullname": "t2_1urjd1hc7b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How the best image generation models work from the inside ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg0ur7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754167616,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i was reading the latest paper from FoundationVision (the holders of the best paper award in NeurIPs2024 and the authors of the VAR paper) named &lt;a href=\"https://arxiv.org/abs/2502.20321\"&gt;UniTok&lt;/a&gt; and it talks about a new MultiCodebook design for the VQ-VAE tokenizer of images and how it provides better results in understanding and generation and they compare it with the famous Residual Quantization method used in multiple previous papers.&lt;/p&gt;\n\n&lt;p&gt;They say their method is better but i am not sure, when i tried their code it didn&amp;#39;t give high quality construction results but they said that they didn&amp;#39;t train the model for more than 1 epoch and it not really a scaled version for high resolution images but i don&amp;#39;t know.&lt;/p&gt;\n\n&lt;p&gt;What is your take on the current image generation models ? (Continous vs Discrete) (Autoregressive vs Diffusion) (RQ vs MCQ)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg0ur7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Severe-Awareness829",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg0ur7/how_the_best_image_generation_models_work_from/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754167616,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hgÔºöhttps://huggingface.co/ScienceOne-AI/S1-Base-671B",
          "author_fullname": "t2_44shmmed9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "China report the finetune deepseek scientific model 40.44% on HLE",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8pdo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 209,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 209,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/UTfHZTk3AkcFKdrIagp_47SxVLFGVmG24S66FaWQdXU.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754083417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hgÔºö&lt;a href=\"https://huggingface.co/ScienceOne-AI/S1-Base-671B\"&gt;https://huggingface.co/ScienceOne-AI/S1-Base-671B&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?auto=webp&amp;s=06740559a06a04129cbe49eca027a0862fd98c3b",
                  "width": 1927,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=71982e38120d9120577e53d8fabd9588d8007e4b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b276889bf7a37bf56f4c27a131188e43815865c",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc9bc3ff25403be175e671f34f1139f244fb6d61",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=19933808cef3cec6dce268be3e9d5d269f435579",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fcd174de9a930fac1ee5ef6231c6a4a6d70fd332",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://preview.redd.it/rnyzqia76hgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f6e3196a3a315fef89102429c2a9e280f5ad273",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "l_Dz9A7On8qHXdUNNBTW3zJ0Gj8IkayixkIAvRolxWc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mf8pdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Afraid_Hall_2971",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8pdo/china_report_the_finetune_deepseek_scientific/",
          "stickied": false,
          "url": "https://i.redd.it/rnyzqia76hgf1.jpeg",
          "subreddit_subscribers": 509291,
          "created_utc": 1754083417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing an OpenAI proxy solution for Claude-Code\n\n[https://github.com/ziozzang/claude2openai-proxy](https://github.com/ziozzang/claude2openai-proxy)\n\nAdvantages:\n\n1. In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.\n2. You can start using Claude Code with any specified model, whether it‚Äôs internal or external.\n3. You can also debug if needed.\n4. Anyway, at least from my tests, it works very well. The only issue is the model itself.\n\n\\----\n\nWhile using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn‚Äôt find a solid solution that worked well for multiple users. So, based on [https://github.com/1rgs/claude-code-proxy ](https://github.com/1rgs/claude-code-proxy), I built a proxy tailored for my use. Since it converts between different protocols, ‚Äúgateway‚Äù might actually be a more fitting term.\n\nAnyway, here are the features:\n\nFull support for Claude Code.\n\n* On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.\n* Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.\n* Authentication is done via the ANTHROPIC\\_API\\_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.\n\nBelow is an example of setting up the server and actually using it from a client:\n\n    ANTHROPIC_BASE_URL=http://localhost:8082 \\\n    ANTHROPIC_API_KEY=sk-openapi-auth-token \\\n    ANTHROPIC_MODEL=\"openrouter/horizon-beta\" \\\n    ANTHROPIC_SMALL_FAST_MODEL=\"openrouter/horizon-beta\" \\\n    claude  \n\nTo be honest, I made this to test the openrouter/horizon-beta model. :)\n\nThe pipeline works great: Claude Code -(Claude API)-&gt; my modified proxy server -(OpenAI API)-&gt; openrouter/horizon-beta.\n\nBy the way, you can find what I built at [https://github.com/ziozzang/claude2openai-proxy ](https://github.com/ziozzang/claude2openai-proxy). I use it by building it into a container.\n\nTo be honest,\n\n* horizon-alpha doesn‚Äôt seem to handle Claude Code‚Äôs prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)\n* horizon-beta handles them quite well.\n* However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don‚Äôt feel ideal for automated workflows.\n\nPS.\n\nThe whole reason this started was because of Claude Code‚Äôs usage limits. LoL...",
          "author_fullname": "t2_5409gkc6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gateway/Proxy for Claude-Code to OpenAI API compatible.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfuu40",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754153859,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754152276,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing an OpenAI proxy solution for Claude-Code&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Advantages:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In theory, as long as an OpenAI-compatible API supports tool usage, you can smoothly test any model with Claude Code.&lt;/li&gt;\n&lt;li&gt;You can start using Claude Code with any specified model, whether it‚Äôs internal or external.&lt;/li&gt;\n&lt;li&gt;You can also debug if needed.&lt;/li&gt;\n&lt;li&gt;Anyway, at least from my tests, it works very well. The only issue is the model itself.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;While using Claude Code, I wanted to connect to a local model. There were tools like claude-code-router and other systems, but I couldn‚Äôt find a solid solution that worked well for multiple users. So, based on &lt;a href=\"https://github.com/1rgs/claude-code-proxy\"&gt;https://github.com/1rgs/claude-code-proxy &lt;/a&gt;, I built a proxy tailored for my use. Since it converts between different protocols, ‚Äúgateway‚Äù might actually be a more fitting term.&lt;/p&gt;\n\n&lt;p&gt;Anyway, here are the features:&lt;/p&gt;\n\n&lt;p&gt;Full support for Claude Code.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On the server side, you can configure which model to proxy. For example, when a request comes in for opus or sonnet, you can route it to a predefined model.&lt;/li&gt;\n&lt;li&gt;Alternatively, you can force the model selection at Claude Code startup by letting the user set it via environment variables.&lt;/li&gt;\n&lt;li&gt;Authentication is done via the ANTHROPIC_API_KEY environment variable. The provided token is then forwarded to the backend as a Bearer token for the OpenAI API.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Below is an example of setting up the server and actually using it from a client:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ANTHROPIC_BASE_URL=http://localhost:8082 \\\nANTHROPIC_API_KEY=sk-openapi-auth-token \\\nANTHROPIC_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nANTHROPIC_SMALL_FAST_MODEL=&amp;quot;openrouter/horizon-beta&amp;quot; \\\nclaude  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;To be honest, I made this to test the openrouter/horizon-beta model. :)&lt;/p&gt;\n\n&lt;p&gt;The pipeline works great: Claude Code -(Claude API)-&amp;gt; my modified proxy server -(OpenAI API)-&amp;gt; openrouter/horizon-beta.&lt;/p&gt;\n\n&lt;p&gt;By the way, you can find what I built at &lt;a href=\"https://github.com/ziozzang/claude2openai-proxy\"&gt;https://github.com/ziozzang/claude2openai-proxy &lt;/a&gt;. I use it by building it into a container.&lt;/p&gt;\n\n&lt;p&gt;To be honest,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;horizon-alpha doesn‚Äôt seem to handle Claude Code‚Äôs prompts very well. Qwen3-Coder flash (30B A3B) either. (tool calling issue.)&lt;/li&gt;\n&lt;li&gt;horizon-beta handles them quite well.&lt;/li&gt;\n&lt;li&gt;However, both models ask the user to make choices too often to be suitable for full automation. Compared to Sonnet, they don‚Äôt feel ideal for automated workflows.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;PS.&lt;/p&gt;\n\n&lt;p&gt;The whole reason this started was because of Claude Code‚Äôs usage limits. LoL...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?auto=webp&amp;s=42f669382e1c8928b5e1d529da5526d93801fc0d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e43bc658beb09686e12af7368ba5a1c7c7d40071",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41d704239a298e8b8e9ef761cbf0504c8c77ccaf",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fed85d51213a23fb5ee442d3c4a5ad79bfcf800",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0d1364de265c02e79206344fc84f7451660adc50",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5975df8d3cb9a0c0f9fe7ad4afa6ddfa17a56a05",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9cbd06ed3b8ba41d88914fbda7d1b7707aee647f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SSww-5zCchd4jUh-0DWh-Zc0KpQfP7v2XwPYRGw1fwE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfuu40",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ziozzang0",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfuu40/gatewayproxy_for_claudecode_to_openai_api/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754152276,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi! So I've been playing around with everyone's baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.\n\nShoutout to u/Skatardude10 and u/farkinga\n\nSo everyone knows it's a great idea to offload some/all tensors to RAM with these models if you can't fit them all. But from what I gathered, if you offload them using \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\", the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.\n\nWhat is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).\n\nThis is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).\n\nThe test is simple \"write me a sophisticated web scraper\". I run it once, then regenerate it to compare (I don't quite understand draft model context, noob, again).\n\n|~~With Qwen3 4b draft model\\*~~|No draft model|\n|:-|:-|\n|~~Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s~~|Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s|\n|~~Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s~~|Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s|\n\n*edit: tried* u/AliNT77\\*'s tip: set draft model's cache to Q8 Q8 and you'll have a higher acceptance rate with the smaller mode, allowing you to go up with main model's context and gain some speed.\\*\n\n\\* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):\n\n* XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)\n* DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s\n* Unsloth Qwen3 0.6B  - 33%, 19t/s\n* **Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s**\n* Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.\n\nWhat was the acceptance rate for 4B you're gonna ask... 67%.\n\nWhy do this instead of trying to offload some layers and try to gain performance this way? I don't know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)\n\nNow this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I'm unemployed, but I'm not THAT unemployed.\n\nQuestions:\n\n1. 1.7b vs 4b draft model. This obvs needs more testing and longer context, but I'm assuming that 4b will perform better than 1.7b with more complex code.\n2. What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?\n3. Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.\n4. I've got a Ryzen CPU. It's massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I'm assuming this is normal and there are no optimisations for AMD cpus?\n5. Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.\n6. Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don't these exist out in the wild?\n7. How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it's localllama so I dare not ask about bigger applications)\n\nWell, if you read all of this, here's your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp; caches, but I am not done yet. This doesn't crash the system, any other combo does. So if you've got more than 12gb vram, you might get away with more context.\n\nStart with: LLAMA\\_SET\\_ROWS=1  \n\\--model \"(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4\\_K\\_XL.gguf\"  \n\\--model-draft \"(full path)/Qwen3-4B-Q8\\_0.gguf\"  \n\\--override-tensor \"\\\\.ffn\\_.\\*\\_exps\\\\.=CPU\" (yet to test this, but it can now be replaced with --cpu-moe)  \n\\--flash-attn  \n~~--ctx-size 192000~~  \n\\--ctx-size 262144 --cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--threads -1  \n\\--n-gpu-layers 99  \n\\--n-gpu-layers-draft 99  \n~~--ctx-size-draft 1024 --cache-type-k-draft q4\\_0 --cache-type-v-draft q4\\_0~~  \n\\--ctx-size-draft 24567 --cache-type-v-draft q8\\_0 --cache-type-v-draft q8\\_0\n\nor you can do for more speed (30t/s)/accuracy, but less context.  \n\\--ctx-size 131072 --cache-type-k q8\\_0 --cache-type-v q8\\_0  \n\\--ctx-size-draft 24576 --cache-type-k-draft q8\\_0 --cache-type-v-draft q8\\_0  \n\\--batch-size 1024 --ubatch-size 1024\n\nThese settings get you to 11197MiB /  12227MiB vram on the gpu.",
          "author_fullname": "t2_wxxnd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 30B A3b --override-tensor + Qwen3 4b draft = &lt;3 (22 vs 14 t/s)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfpgae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754147599,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754138032,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! So I&amp;#39;ve been playing around with everyone&amp;#39;s baby, the A3B Qwen. Please note, I am a noob and a tinkerer, and Claude Code definitely helped me understand wth I am actually doing. Anyway.&lt;/p&gt;\n\n&lt;p&gt;Shoutout to &lt;a href=\"/u/Skatardude10\"&gt;u/Skatardude10&lt;/a&gt; and &lt;a href=\"/u/farkinga\"&gt;u/farkinga&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So everyone knows it&amp;#39;s a great idea to offload some/all tensors to RAM with these models if you can&amp;#39;t fit them all. But from what I gathered, if you offload them using &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot;, the GPU is basically chillin doing nothing apart from processing bits and bobs, while CPU is doing the heavylifting... Enter draft model. And not just a small one, a big one, the bigger the better.&lt;/p&gt;\n\n&lt;p&gt;What is a draft model? There are probably better equipped people to explain this, or just ask your LLM. Broadly, this is running a second, smaller LLM that feeds predicted tokens, so the bigger one can get a bit lazy and effectively QA what the draft LLM has given it and improve on it. Downsides? Well you tell me, IDK (noob).&lt;/p&gt;\n\n&lt;p&gt;This is Ryzen 5800x3d 32gb ram with RTX 5700 12gb vram, running Ubuntu + Vulkan because I swear to god I would rather eat my GPU than try to compile anything with CUDA ever again (remind us all why LM Studio is so popular?).&lt;/p&gt;\n\n&lt;p&gt;The test is simple &amp;quot;write me a sophisticated web scraper&amp;quot;. I run it once, then regenerate it to compare (I don&amp;#39;t quite understand draft model context, noob, again).&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;del&gt;With Qwen3 4b draft model*&lt;/del&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;No draft model&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Prompt- Tokens: 27- Time: 343.904 ms- Speed: 78.5 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Prompt- Tokens: 38- Time: 858.486 ms- Speed: 44.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;del&gt;Generation- Tokens: 1973- Time: 89864.279 ms- Speed: 22.0 t/s&lt;/del&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Generation- Tokens: 1747- Time: 122476.884 ms- Speed: 14.3 t/s&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;em&gt;edit: tried&lt;/em&gt; &lt;a href=\"/u/AliNT77\"&gt;u/AliNT77&lt;/a&gt;*&amp;#39;s tip: set draft model&amp;#39;s cache to Q8 Q8 and you&amp;#39;ll have a higher acceptance rate with the smaller mode, allowing you to go up with main model&amp;#39;s context and gain some speed.*&lt;/p&gt;\n\n&lt;p&gt;* Tested with cache quantised at Q4. I also tried (Q8 or Q6, generally really high qualities):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;XformAI-india/Qwen3-0.6B-coders-gguf - 37% acceptance, 17t/s (1.7b was similar)&lt;/li&gt;\n&lt;li&gt;DavidAU/Qwen3-Zero-Coder-Reasoning-V2-0.8B-NEO-EX-GGUF - 25%, 18.t/s&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 0.6B  - 33%, 19t/s&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Unsloth Qwen3 0.6B cache at Q8 - 68%, 26t/s&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Unsloth Qwen3 1.7b  - 40%, 22t/s, but the GPU was chilling doing nothing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What was the acceptance rate for 4B you&amp;#39;re gonna ask... 67%.&lt;/p&gt;\n\n&lt;p&gt;Why do this instead of trying to offload some layers and try to gain performance this way? I don&amp;#39;t know. If I understand correctly, the GPU would have been bottlenecked by the CPU anyway. By using a 4b model, the GPU is putting in some work, and the VRAM is getting maxed out. (see questions below)&lt;/p&gt;\n\n&lt;p&gt;Now this is where my skills end because I can spend hours just loading and unloading various configs, and it will be a non-scientific test anyway. I&amp;#39;m unemployed, but I&amp;#39;m not THAT unemployed.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;1.7b vs 4b draft model. This obvs needs more testing and longer context, but I&amp;#39;m assuming that 4b will perform better than 1.7b with more complex code.&lt;/li&gt;\n&lt;li&gt;What would be the benefit of offloading the 30bA3b to the CPU completely and using an even bigger Qwen3 draft model? Would it scale? Would the CPU have to work even less, since the original input would be better?&lt;/li&gt;\n&lt;li&gt;Context. Main model vs draft? Quantisation vs size? Better GPU compute usage vs bigger context? Performance degrades as the context gets populated, doesnt it? A lot to unpack, but hey, would be good to know.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve got a Ryzen CPU. It&amp;#39;s massively pissing me off whenever I see Llama.cpp loading optimisations for Haswell (OCD). I&amp;#39;m assuming this is normal and there are no optimisations for AMD cpus?&lt;/li&gt;\n&lt;li&gt;Just how much of my post is BS? Again, I am but a tinkerer. I have not yet experimented with inference parameters.&lt;/li&gt;\n&lt;li&gt;Anyone care to compile a sodding CUDA version of Llama.cpp? Why the hell don&amp;#39;t these exist out in the wild?&lt;/li&gt;\n&lt;li&gt;How would this scale? Imagine running Halo Strix APU with an eGPU hosting a draft model? (it&amp;#39;s localllama so I dare not ask about bigger applications)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Well, if you read all of this, here&amp;#39;s your payoff: this is the command I am using to launch all of that. Someone wiser will probably add a bit more to it. Yeah, I could use different ctx &amp;amp; caches, but I am not done yet. This doesn&amp;#39;t crash the system, any other combo does. So if you&amp;#39;ve got more than 12gb vram, you might get away with more context.&lt;/p&gt;\n\n&lt;p&gt;Start with: LLAMA_SET_ROWS=1&lt;br/&gt;\n--model &amp;quot;(full path)/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4_K_XL.gguf&amp;quot;&lt;br/&gt;\n--model-draft &amp;quot;(full path)/Qwen3-4B-Q8_0.gguf&amp;quot;&lt;br/&gt;\n--override-tensor &amp;quot;\\.ffn_.*_exps\\.=CPU&amp;quot; (yet to test this, but it can now be replaced with --cpu-moe)&lt;br/&gt;\n--flash-attn&lt;br/&gt;\n&lt;del&gt;--ctx-size 192000&lt;/del&gt;&lt;br/&gt;\n--ctx-size 262144 --cache-type-k q4_0 --cache-type-v q4_0&lt;br/&gt;\n--threads -1&lt;br/&gt;\n--n-gpu-layers 99&lt;br/&gt;\n--n-gpu-layers-draft 99&lt;br/&gt;\n&lt;del&gt;--ctx-size-draft 1024 --cache-type-k-draft q4_0 --cache-type-v-draft q4_0&lt;/del&gt;&lt;br/&gt;\n--ctx-size-draft 24567 --cache-type-v-draft q8_0 --cache-type-v-draft q8_0&lt;/p&gt;\n\n&lt;p&gt;or you can do for more speed (30t/s)/accuracy, but less context.&lt;br/&gt;\n--ctx-size 131072 --cache-type-k q8_0 --cache-type-v q8_0&lt;br/&gt;\n--ctx-size-draft 24576 --cache-type-k-draft q8_0 --cache-type-v-draft q8_0&lt;br/&gt;\n--batch-size 1024 --ubatch-size 1024&lt;/p&gt;\n\n&lt;p&gt;These settings get you to 11197MiB /  12227MiB vram on the gpu.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mfpgae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "igorwarzocha",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpgae/qwen3_30b_a3b_overridetensor_qwen3_4b_draft_3_22/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754138032,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone tried [https://huggingface.co/MetaStoneTec/XBai-o4](https://huggingface.co/MetaStoneTec/XBai-o4) ? Big if true -\n\n\\&gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini's performance¬†\n\nHave not tried it myself, downloading atm from [https://huggingface.co/mradermacher/XBai-o4-GGUF](https://huggingface.co/mradermacher/XBai-o4-GGUF)",
          "author_fullname": "t2_4rlver1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MetaStoneTec/XBai-o4",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfk3y2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754118021,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried &lt;a href=\"https://huggingface.co/MetaStoneTec/XBai-o4\"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt; ? Big if true -&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3-mini&amp;#39;s performance¬†&lt;/p&gt;\n\n&lt;p&gt;Have not tried it myself, downloading atm from &lt;a href=\"https://huggingface.co/mradermacher/XBai-o4-GGUF\"&gt;https://huggingface.co/mradermacher/XBai-o4-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?auto=webp&amp;s=21ea26fb14e3912f80713c95ddb95b37cd316f66",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f69ac45b09b4ccb4576fc51eb90d2b847eace7ec",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a115ce28ec07456cec2ed3598f59bf288c35b1a9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=02748428a3ab0ac779c13dffcf54fbcc054b5691",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed975e60e78ac76aed0294ed900e215343eefe17",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=64a929e4331b92d45b82827765bb555c57573df7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24081c4d549b93ebfca1a8d44ef5f08c619cebf8",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "71nUCP5huF0UaxRtqOtvH9ka34y516VzUn3BYyKiUws"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfk3y2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ljosif",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfk3y2/metastonetecxbaio4/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754118021,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôd like to start a small art project and I‚Äôm looking for a model that speaks German well. I‚Äôm currently using Gemma 3n:e4b and I‚Äôm quite satisfied with it. However, I‚Äôd like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM ‚Äì ideally no more than 6GB.",
          "author_fullname": "t2_1tcpn4d5tw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Small LLM in german",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfldxj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122940,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôd like to start a small art project and I‚Äôm looking for a model that speaks German well. I‚Äôm currently using Gemma 3n:e4b and I‚Äôm quite satisfied with it. However, I‚Äôd like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM ‚Äì ideally no more than 6GB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfldxj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ghulaschsuppe",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754122940,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.\n\nI also prefer to use CLI only, because I want everything to eventually be for a robot I'm working on (because of this I don't really want a UI interface). For some I haven't fully tested only the CLI, and for some I've tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.\n\nI will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here's my personal \"rankings\".\n\n* **Bark/Coqui TTS -**\n   * **The Good:**¬†The emotions are next level... kinda. At least they have it, is the main thing. What I've done is create a custom Llama model, that knows when to send a \\[laughs\\], \\[sighs\\], etc. that's appropriate, given the conversation. The custom ollama model is pretty good at this (if you're curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.\n   * **The Bad:**¬†It's pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also \"trails off\", if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I'm using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn't sometimes even follow the cloned voice. However, when it does, it's somewhat decent. But given how it often does not, it's not really too usable.\n* **F5 TTS -**\n   * **The Good:**¬†Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I'd say it's better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that's close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn't trail off. It will finish speaking until the text from my custom ollama model is done being spoken.\n   * **The Bad:**¬†As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I'd say it's about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can't do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn't sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It's pretty good, but not perfect.\n* **Orpheus TTS**\n   * **The Good:**¬†This one can also do laughing, yawning, etc. and it's decent at it. But not as good as Coqui/Bark. Although it's still better than what most offer, since it has the ability at all. There's a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn't trail off.\n   * **The Bad:**¬†This one is a pain to set up, at least if you try to go the normal route, via CLI. I've only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I'd say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn't get picked up, and it just says \"yawn\", instead. Coqui didn't really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).\n* **Kokoro TTS**\n   * **The Good:**¬†Man, the UI is blazing FAST. If I had to guess about \\~ 1 second or so. And that's using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn't perfect, it's probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn't trail off and reads all the speech too, which is nice.\n   * **The Bad:**¬†It sounds a little bland. Some of the models, even if they don't have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn't distinct between exclamation, or questions, much. It's not terrible, but sounds like an average Speech to Text, that you'd find on an average book reader, for example. Also doesn't offer native voice cloning, that I'm aware of at least, but I could be wrong.\n* **Higgs Audio TTS**\n   * **The Good:** The really neat thing about Higgs audio is that it doesn't have preset voices that you choose from, but rather, you prompt in how you want the voice to sound. At least from their example. It's surprisingly consistent as well. Through multiple generations, while maintaining the same voice speaker prompt, the voice stays relatively the same. The emotions sound bland at first, but you can prompt in speakers to have more tone to their voices. I used a speaker that has tone similar the main girl from legally blonde, since she has a lot of tone and emotion in her voice.\n   * **The Bad:** No voice cloning. Although it is somewhat offset by it's ability to prompt in voices, you still can't clone voices. Personally, I'd say the worst thing is how long it takes to generate a prompt. I only had about 10 seconds of audio, but it took about a minute to generate. And I tried for about 20 seconds of audio and it took about 2 minutes to generate. It seems like for every 1 second of audio, it takes about 6 seconds to generate, which is quite a bit of time. However, if you value the ability to prompt in voices, it's a decent choice.",
          "author_fullname": "t2_4guqxmy4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TTS Model Comparisons: My Personal Rankings (So far) of TTS Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfjn88",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754195008,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754116331,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So firstly, I should mention that my setup is a Lenovo Legion 4090 Laptop, which should be pretty quick to render text &amp;amp; speech - about equivalent to a 4080 Desktop. At least similar in VRAM, Tensors, etc.&lt;/p&gt;\n\n&lt;p&gt;I also prefer to use CLI only, because I want everything to eventually be for a robot I&amp;#39;m working on (because of this I don&amp;#39;t really want a UI interface). For some I haven&amp;#39;t fully tested only the CLI, and for some I&amp;#39;ve tested both. I will update this post when I do more testing. Also, feel free to recommend any others I should test.&lt;/p&gt;\n\n&lt;p&gt;I will say the UI counterpart can be quite a bit quicker than using CLI linked with an ollama model. With that being said, here&amp;#39;s my personal &amp;quot;rankings&amp;quot;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Bark/Coqui TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†The emotions are next level... kinda. At least they have it, is the main thing. What I&amp;#39;ve done is create a custom Llama model, that knows when to send a [laughs], [sighs], etc. that&amp;#39;s appropriate, given the conversation. The custom ollama model is pretty good at this (if you&amp;#39;re curious how to do this as well you can create a basefile and a modelfile). And it sounds somewhat human. But at least it can somewhat mimic human emotions a little, which many cannot.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†It&amp;#39;s pretty slow. Sometimes takes up to 30 seconds to a minute which is pretty undoable, given I want my robot to have fluid conversation. I will note that none of them are able to do it seconds or less, sadly, via CLI, but one was for UI. It also &amp;quot;trails off&amp;quot;, if that makes sense. Meaning - the ollama may produce a text, and the Bark/Coqui TTS does not always follow it accurately. I&amp;#39;m using a custom voice model as well, and the cloning, although sometimes okay, can and does switch between male and female characters, and doesn&amp;#39;t sometimes even follow the cloned voice. However, when it does, it&amp;#39;s somewhat decent. But given how it often does not, it&amp;#39;s not really too usable.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;F5 TTS -&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†Extremely consistent voice cloning, from the UI and CLI. I will say that the UI is a bit faster than using CLI, however, it still takes about 8seconds or so to get a response even with the UI, which is faster than Bark/Coqui, but still not fast enough, for my uses at least. Honestly, the voice cloning alone is very impressive. I&amp;#39;d say it&amp;#39;s better than Bark/Coqui, except that Bark/Coqui has the ability to laugh, sigh, etc. But if you value consistent voicing, that&amp;#39;s close to and can rival ElevenLabs without paying, this is a great option. Even with the CLI it doesn&amp;#39;t trail off. It will finish speaking until the text from my custom ollama model is done being spoken.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†As mentioned, it can take about 8-10 seconds for the UI, but longer for the CLI. I&amp;#39;d say it&amp;#39;s about 15 seconds (on average) for the CLI and up to 30 seconds (for about 1.75 minutes of speech) for the CLI, or so depending on how long the text is. The problem is can&amp;#39;t do emotions (like laughing, etc) at all. And when I try to use an exclamation mark, it changes the voice quite a bit, where it almost doesn&amp;#39;t sound like the same person. If you prompt your ollama model to not use exclamations, it does fine though. It&amp;#39;s pretty good, but not perfect.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orpheus TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†This one can also do laughing, yawning, etc. and it&amp;#39;s decent at it. But not as good as Coqui/Bark. Although it&amp;#39;s still better than what most offer, since it has the ability at all. There&amp;#39;s a decent amount of tone in the voice, enough to keep it from sounding too robotic. The voices, although not cloneable, are a lot more consistent than Bark/Coqui, however. They never really deviate like Bark/Coqui did. It also reads all of the text as well and doesn&amp;#39;t trail off.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†This one is a pain to set up, at least if you try to go the normal route, via CLI. I&amp;#39;ve only been able to set it up via Docker, actually, unfortunately. Even in the UI, it takes quite a bit of time to generate text. I&amp;#39;d say about 1 second per 1 second of speech. There also times where certain tags (like yawning) doesn&amp;#39;t get picked up, and it just says &amp;quot;yawn&amp;quot;, instead. Coqui didn&amp;#39;t really seem to do that, unless it was a tag that was unrecognizable (sometimes my custom ollama model would generate non-available tags on accident).&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kokoro TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt;¬†Man, the UI is blazing FAST. If I had to guess about ~ 1 second or so. And that&amp;#39;s using 2-3 sentences. For a about 4 minutes of speech, it takes about 4 seconds to generate text, which although isn&amp;#39;t perfect, it&amp;#39;s probably as good as it gets and really quick. So about 1 second per 1 minute of speech. Pretty impressive! It also doesn&amp;#39;t trail off and reads all the speech too, which is nice.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt;¬†It sounds a little bland. Some of the models, even if they don&amp;#39;t have explicit emotion tags, still have tone, and this model is lacking there imo. It sounds too robotic to me, and doesn&amp;#39;t distinct between exclamation, or questions, much. It&amp;#39;s not terrible, but sounds like an average Speech to Text, that you&amp;#39;d find on an average book reader, for example. Also doesn&amp;#39;t offer native voice cloning, that I&amp;#39;m aware of at least, but I could be wrong.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Higgs Audio TTS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Good:&lt;/strong&gt; The really neat thing about Higgs audio is that it doesn&amp;#39;t have preset voices that you choose from, but rather, you prompt in how you want the voice to sound. At least from their example. It&amp;#39;s surprisingly consistent as well. Through multiple generations, while maintaining the same voice speaker prompt, the voice stays relatively the same. The emotions sound bland at first, but you can prompt in speakers to have more tone to their voices. I used a speaker that has tone similar the main girl from legally blonde, since she has a lot of tone and emotion in her voice.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;The Bad:&lt;/strong&gt; No voice cloning. Although it is somewhat offset by it&amp;#39;s ability to prompt in voices, you still can&amp;#39;t clone voices. Personally, I&amp;#39;d say the worst thing is how long it takes to generate a prompt. I only had about 10 seconds of audio, but it took about a minute to generate. And I tried for about 20 seconds of audio and it took about 2 minutes to generate. It seems like for every 1 second of audio, it takes about 6 seconds to generate, which is quite a bit of time. However, if you value the ability to prompt in voices, it&amp;#39;s a decent choice.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfjn88",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "iKontact",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfjn88/tts_model_comparisons_my_personal_rankings_so_far/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754116331,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been working on a project called OllamaCode, and I'd love to share it with you. It's an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.\n\nHere are a few things I've focused on:\n\n* It can create and run files automatically from natural language.\n* I've tried to make it smart about executing tools like git, search, and bash commands.\n* It's designed to work with any Ollama model that supports function calling.\n* A big priority for me was to keep it 100% local to ensure privacy.\n\nIt's still in the very early days, and there's a lot I still want to improve. It's been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.",
          "author_fullname": "t2_2lznz5yi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ollamacode - Local AI assistant that can create, run and understand your codebase.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfqr3o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=c955545b2920d3da4a8cd08357963d4b7cac5b74",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754141855,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working on a project called OllamaCode, and I&amp;#39;d love to share it with you. It&amp;#39;s an AI coding assistant that runs entirely locally with Ollama. The main idea was to create a tool that actually executes the code it writes, rather than just showing you blocks to copy and paste.&lt;/p&gt;\n\n&lt;p&gt;Here are a few things I&amp;#39;ve focused on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;It can create and run files automatically from natural language.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve tried to make it smart about executing tools like git, search, and bash commands.&lt;/li&gt;\n&lt;li&gt;It&amp;#39;s designed to work with any Ollama model that supports function calling.&lt;/li&gt;\n&lt;li&gt;A big priority for me was to keep it 100% local to ensure privacy.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s still in the very early days, and there&amp;#39;s a lot I still want to improve. It&amp;#39;s been really helpful for my own workflow, and I would be incredibly grateful for any feedback from the community to help make it better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/tooyipjee/ollamacode",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?auto=webp&amp;s=0d661e9993d3a8339a8a04e91dab3d495a1068d8",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad56c7eac7e674170e1bce20ac9f2fbb89067d4f",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4264d87bb5a2e08e33aa5bd378354efc3de28a29",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bbbbbe9284ddae09fc99ccbdd7bbad23851be4ae",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc443c649d71e676f2ac536126f73b6f681be48d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1bf93455ce16aa988227f0c0581aef56a3355348",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0dd9f689127fdc2e0c3b1dd1996536119e75b475",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "kLroHnTCnvkrS5N09KAaQv44hHyVU4vQJ3gmeqT8SqI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfqr3o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loud-Consideration-2",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfqr3o/ollamacode_local_ai_assistant_that_can_create_run/",
          "stickied": false,
          "url": "https://github.com/tooyipjee/ollamacode",
          "subreddit_subscribers": 509291,
          "created_utc": 1754141855,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just because you are hosting locally, doesn't mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!",
          "author_fullname": "t2_kfjfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DoubleAgents: Fine-tuning LLMs for Covert Malicious Tool Calls",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfbw8a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 96,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 96,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=140&amp;height=76&amp;crop=140:76,smart&amp;auto=webp&amp;s=178741032ad68bb72212a1f0482ccf59165855d7",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754091780,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "medium.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just because you are hosting locally, doesn&amp;#39;t mean your LLM agent is necessarily private. I wrote a blog about how LLMs can be fine-tuned to execute malicious tool calls with popular MCP servers. I included links to the code and dataset in the article. Enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?auto=webp&amp;s=db6f154eef502137479106eb0ee5e3497a2c8a5d",
                  "width": 1024,
                  "height": 559
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fc30b818f499ebfea16a1a44bc05f5b89c31100",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8f36d4f360255f41bf1da0fda787ab734a46fbd",
                    "width": 216,
                    "height": 117
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ad1e44aa12420eb39ba559c3fdbad4ad0407e0b",
                    "width": 320,
                    "height": 174
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24b8b3213823bb044c73076e1852d1957545a17f",
                    "width": 640,
                    "height": 349
                  },
                  {
                    "url": "https://external-preview.redd.it/1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4cb31249007f2a139fccd496d749e3a58bdc7c0d",
                    "width": 960,
                    "height": 524
                  }
                ],
                "variants": {},
                "id": "1g_CC0wTCyJXj4u8MYtNMOtgl2s6j0xMrzBatFuxfOQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfbw8a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JAlbrethsen",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfbw8a/doubleagents_finetuning_llms_for_covert_malicious/",
          "stickied": false,
          "url": "https://medium.com/@justin_45141/doubleagents-fine-tuning-llms-for-covert-malicious-tool-calls-b8ff00bf513e",
          "subreddit_subscribers": 509291,
          "created_utc": 1754091780,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Beta seems really solid from early testing, not a magnitude better than what SOTA's offer but still impressive",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Alpha vs Horizon Beta",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfgwyu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756816075%2CYTQxMTNkNTQwNGE0MDMxYTNmYjFiYWE3MjhkZjYyZGEzYjk4MmE5ZDc5YzM5YmU3YzZmNjVjYTNmMzdjMmZlYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756816075%2CNWNiOGU2ZThmMjU4NTFkM2Y2MDY2YzAxMmE0OTZkMzQ0OTgwOGE1ZDhiOGNiZGFkMzRjZjhhYjhlNzQ5YjliMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=140&amp;height=84&amp;crop=140:84,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=dd582f91752e7fd7a72f3be30d66c628b1cabf42",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754106912,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Beta seems really solid from early testing, not a magnitude better than what SOTA&amp;#39;s offer but still impressive&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/dg8cy7ia4jgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?format=pjpg&amp;auto=webp&amp;s=30b632f2cdd42a3234c72ef686841eed57c63aec",
                  "width": 3600,
                  "height": 2178
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d42e84556d577f46b8f2cc66b308d63c6ba3aa39",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0a342e02b97eb15e7ae8d55711f5dc21ed403177",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1c923808071843a8b872aa37e16f734fbae495b3",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bbb1ef3633d6485f7f54280916bb8b15600f3044",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d1f330f77b0817cdba51f914a124707d5bffe8a1",
                    "width": 960,
                    "height": 580
                  },
                  {
                    "url": "https://external-preview.redd.it/d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e9a786dc189b4a8425437a29d560c6d7825e64a0",
                    "width": 1080,
                    "height": 653
                  }
                ],
                "variants": {},
                "id": "d3R3bDNtZ2E0amdmMXE2zpdfjiXOxBCP1nwGlT2orDomIn7bITnNJ4a4m1Y7"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfgwyu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfgwyu/horizon_alpha_vs_horizon_beta/",
          "stickied": false,
          "url": "https://v.redd.it/dg8cy7ia4jgf1",
          "subreddit_subscribers": 509291,
          "created_utc": 1754106912,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1786,
              "scrubber_media_url": "https://v.redd.it/dg8cy7ia4jgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/dg8cy7ia4jgf1/DASHPlaylist.mpd?a=1756816075%2CYTQxMTNkNTQwNGE0MDMxYTNmYjFiYWE3MjhkZjYyZGEzYjk4MmE5ZDc5YzM5YmU3YzZmNjVjYTNmMzdjMmZlYg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 69,
              "hls_url": "https://v.redd.it/dg8cy7ia4jgf1/HLSPlaylist.m3u8?a=1756816075%2CNWNiOGU2ZThmMjU4NTFkM2Y2MDY2YzAxMmE0OTZkMzQ0OTgwOGE1ZDhiOGNiZGFkMzRjZjhhYjhlNzQ5YjliMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If I want to run qwen3 coder or any other AI model that rivals Claude 4 Sonnet locally, what are the ideal system requirements to run it flawlessly? How much RAM? Which motherboard? Recommended GPU and CPU. \n\nIf someone has experience running the LLMs locally, please share. \n\nThanks. \n\nPS: My current system specs are:\n- Intel 14700KF\n- 32 GB RAM but the motherboard supports up to 192 GB\n- RTX 3090\n- 1 TB SSD PCI ex",
          "author_fullname": "t2_1olqqe8dx0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs locally and flawlessly like copilot or Claude chat or cline.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgbk2y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754199873,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I want to run qwen3 coder or any other AI model that rivals Claude 4 Sonnet locally, what are the ideal system requirements to run it flawlessly? How much RAM? Which motherboard? Recommended GPU and CPU. &lt;/p&gt;\n\n&lt;p&gt;If someone has experience running the LLMs locally, please share. &lt;/p&gt;\n\n&lt;p&gt;Thanks. &lt;/p&gt;\n\n&lt;p&gt;PS: My current system specs are:\n- Intel 14700KF\n- 32 GB RAM but the motherboard supports up to 192 GB\n- RTX 3090\n- 1 TB SSD PCI ex&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgbk2y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NoFudge4700",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgbk2y/running_llms_locally_and_flawlessly_like_copilot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgbk2y/running_llms_locally_and_flawlessly_like_copilot/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754199873,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.\n\nBuilt using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It's ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.\n\n[https://github.com/dokasto/Saidia](https://github.com/dokasto/Saidia)",
          "author_fullname": "t2_11mwsd0p41",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Saidia: Offline-First AI Assistant for Educators in low-connectivity regions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfn2xf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754161230,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754129775,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Saidia is an offline-first AI assistant tailored for educators, enabling them to generate questions directly from source materials.&lt;/p&gt;\n\n&lt;p&gt;Built using Electron, packaged Ollama, and Gemma 3n, Saidia functions entirely offline and is optimised for basic hardware. It&amp;#39;s ideal for areas with unreliable internet and power, empowering educators with powerful teaching resources where cloud-based tools are impractical or impossible.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/dokasto/Saidia\"&gt;https://github.com/dokasto/Saidia&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?auto=webp&amp;s=5141edfe493867e3dafdd151010b3f5040912cc0",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e949d6258248c551dcfd9cb47f1304f4151400c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8d3484da8634402f8ed7774d2e4f8c4669168d53",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7c82a3e8557eeca0f83fc2e477928d5f3031a9f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=493dbfda76c0eab29abee992f4eb1d96ff6d93fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92c32a097682bd7a3113fbc01e20ac3a07fd673",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=16e412f9a5dfdc2c2cf419a9494a9e06f0757f95",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "j3KGsoBYoXJRolQHmDDZQ4g3b-bMRji7lP_QqSrqGzs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfn2xf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dokasto_",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfn2xf/saidia_offlinefirst_ai_assistant_for_educators_in/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754129775,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So many versions! I saw something about how the DWQ versions are best, but then obviously MLX \\*seems\\* like it would be best? And what quantization version?",
          "author_fullname": "t2_7wdc4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "64GB M1 Max, which GLM-4.5-Air?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg44ya",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176467,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So many versions! I saw something about how the DWQ versions are best, but then obviously MLX *seems* like it would be best? And what quantization version?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg44ya",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "maxiedaniels",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg44ya/64gb_m1_max_which_glm45air/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754176467,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? \n\n- Apple m4 pro chip\n- 14 core CPU, 20 core GPU\n- 48GB unified memory. \n- One TB SSD storage \n\nEventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.",
          "author_fullname": "t2_1ook1izxzg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this set up sufficient?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg40u1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754176144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Non-techie, so forgive my ignorance.\nLooking to get a local LLM and learn Python.\nIs this set up optimal for the purpose, or is this an overkill? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Apple m4 pro chip&lt;/li&gt;\n&lt;li&gt;14 core CPU, 20 core GPU&lt;/li&gt;\n&lt;li&gt;48GB unified memory. &lt;/li&gt;\n&lt;li&gt;One TB SSD storage &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Eventually would like to advance to training my own LLM on a Linux with Nvidia chip, but not sure how realistic it is for a nonprofessional.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mg40u1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wild-Muffin9190",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg40u1/is_this_set_up_sufficient/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754176144,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: \n\n- Roo Code with [Unsloth Qwen3 Coder 30B Q8](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)\n- [llama-swap](https://github.com/mostlygeek/llama-swap) with new Activity page with real time updates. \n- [VibeCities MCP server](https://github.com/mostlygeek/vibecities) for hosting the pages\n- Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. \n\nHere's my llama-swap config: \n\n```\nmacros:\n  \"qwen3-coder-server\": |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full\n\nmodels:\n  \"Q3-30B-CODER-3090\":\n    env:\n      - \"CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10\"\n    name: \"Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)\"\n    description: \"Q8_K_XL, 180K context, 2x3090\"\n    filters:\n      # enforce recommended params for model\n      strip_params: \"temperature, top_k, top_p, repeat_penalty\"\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```\n\nRoo code MCP settings: \n\n```\n{\n  \"mcpServers\": {\n    \"vibecities\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"http://10.0.1.173:8888/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-secure-api-key\"\n      },\n      \"alwaysAllow\": [\n        \"page_list\",\n        \"page_set\",\n        \"page_get\"\n      ],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n\n\n",
          "author_fullname": "t2_11gh93nhos",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "All local Roo Code and qwen3 coder 30B Q8",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfariy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": "#bbbdbf",
          "ups": 78,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756816075%2CNzgzZjc0MjQ4MTM3ZTViZDQ5NWI1ZDdlMDMwNzJkZjljMTU4MDk0MjNjOTNjNGJkMDhmNDQwNjQ0NzQzYjAzMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756816075%2CMjdiODdkYjczMzUyOGFhNzBlNzdhZDljOWM5YWRjNzMwNGRlZDQxMjUwOTgzYzBhMmQ4N2RkNmQyZmFjMmUwNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 78,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=140&amp;height=87&amp;crop=140:87,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=96e2057bb1f1ef1e9f3beb1d9a28a9ccd4dcaa6b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754088672,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been having a lot of fun playing around with the new Qwen coder as a 100% local agentic coding. A lot of going on with in the demo above: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Roo Code with &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\"&gt;Unsloth Qwen3 Coder 30B Q8&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/llama-swap\"&gt;llama-swap&lt;/a&gt; with new Activity page with real time updates. &lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/mostlygeek/vibecities\"&gt;VibeCities MCP server&lt;/a&gt; for hosting the pages&lt;/li&gt;\n&lt;li&gt;Dual 3090s with Q8 gives about 50 tok/sec to 55 tok/sec. The UD Q4_K_XL quant was not able to one shot the spinning pentagon. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s my llama-swap config: &lt;/p&gt;\n\n&lt;p&gt;```\nmacros:\n  &amp;quot;qwen3-coder-server&amp;quot;: |\n    /path/to/llama-server/llama-server-latest\n    --host 127.0.0.1 --port ${PORT}\n    --flash-attn -ngl 999 -ngld 999\n    --no-mmap\n    --cache-type-k q8_0 --cache-type-v q8_0\n    --temp 0.7 --top-k 20 --top-p 0.8 --repeat_penalty 1.05\n    --jinja\n    --swa-full&lt;/p&gt;\n\n&lt;p&gt;models:\n  &amp;quot;Q3-30B-CODER-3090&amp;quot;:\n    env:\n      - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot;\n    name: &amp;quot;Qwen3 30B Coder Dual 3090 (Q3-30B-CODER-3090)&amp;quot;\n    description: &amp;quot;Q8_K_XL, 180K context, 2x3090&amp;quot;\n    filters:\n      # enforce recommended params for model\n      strip_params: &amp;quot;temperature, top_k, top_p, repeat_penalty&amp;quot;\n    cmd: |\n      ${qwen3-coder-server}\n      --model /path/to/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf\n      --ctx-size 184320\n      # rebalance layers/context a bit better across dual GPUs\n      --tensor-split 46,54\n```&lt;/p&gt;\n\n&lt;p&gt;Roo code MCP settings: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n{\n  &amp;quot;mcpServers&amp;quot;: {\n    &amp;quot;vibecities&amp;quot;: {\n      &amp;quot;type&amp;quot;: &amp;quot;streamable-http&amp;quot;,\n      &amp;quot;url&amp;quot;: &amp;quot;http://10.0.1.173:8888/mcp&amp;quot;,\n      &amp;quot;headers&amp;quot;: {\n        &amp;quot;X-API-Key&amp;quot;: &amp;quot;your-secure-api-key&amp;quot;\n      },\n      &amp;quot;alwaysAllow&amp;quot;: [\n        &amp;quot;page_list&amp;quot;,\n        &amp;quot;page_set&amp;quot;,\n        &amp;quot;page_get&amp;quot;\n      ],\n      &amp;quot;disabled&amp;quot;: false\n    }\n  }\n}\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/g5aj1csfjhgf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?format=pjpg&amp;auto=webp&amp;s=a799694f3cd2a8d09be3eac7cc9981be88d234a1",
                  "width": 1920,
                  "height": 1197
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7b56a58c7f6f027ee7357cad95a460ff999afeea",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5adfd5aba83e12c59bf4648d145f4ab40fd5648e",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4be2e0e3ac56fa7c11be8c2c58c9a02a90039429",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c11415fad487d232adecf2767cc5b4b8ac2ab42f",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fd1f305092aa1774fdb5bb8d64c33f4d4acc5781",
                    "width": 960,
                    "height": 598
                  },
                  {
                    "url": "https://external-preview.redd.it/OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a0c4665784dbdfe56aa44706a6b06cd7fb427df9",
                    "width": 1080,
                    "height": 673
                  }
                ],
                "variants": {},
                "id": "OWxnaXVhc2ZqaGdmMfGWh3MmEBu_PyLbr6sXIOAmucdihxn6n5oQbX60BtAw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mfariy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Statement-0001",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfariy/all_local_roo_code_and_qwen3_coder_30b_q8/",
          "stickied": false,
          "url": "https://v.redd.it/g5aj1csfjhgf1",
          "subreddit_subscribers": 509291,
          "created_utc": 1754088672,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/g5aj1csfjhgf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/g5aj1csfjhgf1/DASHPlaylist.mpd?a=1756816075%2CNzgzZjc0MjQ4MTM3ZTViZDQ5NWI1ZDdlMDMwNzJkZjljMTU4MDk0MjNjOTNjNGJkMDhmNDQwNjQ0NzQzYjAzMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 158,
              "hls_url": "https://v.redd.it/g5aj1csfjhgf1/HLSPlaylist.m3u8?a=1756816075%2CMjdiODdkYjczMzUyOGFhNzBlNzdhZDljOWM5YWRjNzMwNGRlZDQxMjUwOTgzYzBhMmQ4N2RkNmQyZmFjMmUwNw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been experimenting with Qwen3:30b-a3b-instruct-2507-q8\\_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I'm really impressed with the speed!  \n\nIn light conversation (I tested with general knowledge questions and everyday scenarios), I'm achieving up to 34 tokens/s, which is \\*significantly\\* faster than other models I've tested (all Q4 except for qwen3):\n\n* Qwen3 (30B): \\~34 tokens/s\n* Qwen2.5 (32B): \\~10 tokens/s\n* Gemma3 (27B): \\~10 tokens/s\n* Llama3 (70B): 4-5 tokens/s\n\nHowever, I'm also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.\n\nMy first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What's your experience been like with speed and accuracy of Qwen3?",
          "author_fullname": "t2_tlzk7zie",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 (30B) with Ollama: Blazing Fast, but accuracy concerns",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfl6bo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Ollama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754122088,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with Qwen3:30b-a3b-instruct-2507-q8_0 using Ollama v0.10.0 (standard settings) on Debian 12 with a pair of Nvidia P40s, and I&amp;#39;m really impressed with the speed!  &lt;/p&gt;\n\n&lt;p&gt;In light conversation (I tested with general knowledge questions and everyday scenarios), I&amp;#39;m achieving up to 34 tokens/s, which is *significantly* faster than other models I&amp;#39;ve tested (all Q4 except for qwen3):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 (30B): ~34 tokens/s&lt;/li&gt;\n&lt;li&gt;Qwen2.5 (32B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Gemma3 (27B): ~10 tokens/s&lt;/li&gt;\n&lt;li&gt;Llama3 (70B): 4-5 tokens/s&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, I&amp;#39;m also sometimes seeing a fair amount of hallucination with facts, locations or events. Not enough to make it unusable but notable to me.&lt;/p&gt;\n\n&lt;p&gt;My first impression is that Qwen3 is incredibly fast, but could be a bit more reliable. Using Ollama with Qwen3 is super easy, but maybe it needs some tweaking?  What&amp;#39;s your experience been like with speed and accuracy of Qwen3?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Ollama",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfl6bo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gerhardmpl",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfl6bo/qwen3_30b_with_ollama_blazing_fast_but_accuracy/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754122088,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT's throttled free-tier, and Claude 4 Sonnet",
          "author_fullname": "t2_65zz9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Four Models, One Prompt: Who Writes the Best Instructions for AI?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfwec7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754156214,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "selfenrichment.hashnode.dev",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a quick blog post I put together today briefly comparing Kimi K2, Gemini 2.5 Pro, ChatGPT&amp;#39;s throttled free-tier, and Claude 4 Sonnet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfwec7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robertotomas",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfwec7/four_models_one_prompt_who_writes_the_best/",
          "stickied": false,
          "url": "https://selfenrichment.hashnode.dev/four-models-one-prompt-who-writes-the-best-instructions-for-ai",
          "subreddit_subscribers": 509291,
          "created_utc": 1754156214,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_xq83l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Horizon Beta - new openai open source model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfda7s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=e6bc83d22983565c851331294c37a95f480af7fa",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754095791,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/openrouter/horizon-beta",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?auto=webp&amp;s=76311bc0d854d91946fad4dd34c15d2aabd68203",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f17bb8ad3532cb9e5aee2735555aab1785143fb",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6fde130f24de4941de6382c0b47920888676cb02",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6e2b3b64a4ebeaaef6a2c94effc7c44b3d4bf9e5",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d1e79479fdaa990ea889c0b392a6ab4a884ffc4",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5298d732c100973951f754378b36e657d827055b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f7a4a4538b90aad59b31dde3f164c87c0a08175",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "B3grX4PgfeT1zMZ7hUxyRNVuPTzvkO0vu5MZ0rTYFRQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mfda7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "popsumbong",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfda7s/horizon_beta_new_openai_open_source_model/",
          "stickied": false,
          "url": "https://openrouter.ai/openrouter/horizon-beta",
          "subreddit_subscribers": 509291,
          "created_utc": 1754095791,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?\n\nClean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.\n\nSo you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, *Can-Ai-Code*, twice.\n\nFine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you've just unlocked new nightmares: Did you accidentally make your \"hard\" tests easier than your \"easy\" ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems \"make sense\"?\n\nYou solve that with clever statistical rigor, only to discover configuration explosion hell. You'd like to test different prompting templates and sampling parameters, but that's 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.\n\nYou're now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?\n\nThat's when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that's only 20% above random chance. Your \"75% accurate\" multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different \"guess rates\"?\n\nFinally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That's 80K tokens wasted for one data point but with no useful answers.  You're overflowing your KV caches while the confidence intervals laugh at you.\n\nAfter drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.\n\n# ReasonScape treats language models as information processing systems, not text completion black boxes.\n\nIt generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.\n\n[C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.](https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1)\n\nThe initial C2 dataset represents \\~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you're on a PC - this application has too much going on to be mobile friendly!\n\n[C2 Explorer](https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d)\n\nI built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn't just another benchmark. It's a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.\n\n[C2 Leaderboard \\(Static snapshot - the Interactive is much nicer!\\)](https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d)\n\nThe ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have **8 additional tasks** to bring up, and lots more reasoning models I'd like to evaluate but my 2xRTX3090 only have so much to give.\n\nThanks for reading this far! &lt;3\n\nLinks:\n\n* [ReasonScape Homepage](https://reasonscape.com/)\n* [ReasonScape Leaderboard - C2](https://reasonscape.com/c2/leaderboard)\n* [ReasonScape Explorer - C2](https://reasonscape.com/c2/explorer) (note: PC required, not mobile-friendly)\n* [ReasonScape GitHub](https://github.com/the-crypt-keeper/reasonscape)\n* [ReasonScape System Architecture](https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture)",
          "author_fullname": "t2_30i1a",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Generated 1 Billion Tokens (So You Don't Have To): Introducing ReasonScape",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4ahuh87m4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 62,
                  "x": 108,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e9b289044767660d3c8e2d034ccd1c1b1902f538"
                },
                {
                  "y": 125,
                  "x": 216,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e8a1fc6a4649edbc5bfde934cedfd13c3fce90c"
                },
                {
                  "y": 186,
                  "x": 320,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c9c7d1e879f15dbfdea396948cc5826d3a32b67"
                },
                {
                  "y": 372,
                  "x": 640,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4a45011cfb5e75257e82a1018ba38ca3849d833"
                },
                {
                  "y": 558,
                  "x": 960,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e3046d2f091a49910ecb192cd251d2fa91bbf04a"
                },
                {
                  "y": 628,
                  "x": 1080,
                  "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=053460c5647cc12262626da5af12623413fee0ff"
                }
              ],
              "s": {
                "y": 717,
                "x": 1233,
                "u": "https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;format=png&amp;auto=webp&amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d"
              },
              "id": "4ahuh87m4ggf1"
            },
            "rn7r2k3t4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7d505276a5f0dc04e59903c6abc89821d0f8f99b"
                },
                {
                  "y": 146,
                  "x": 216,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d747b1a0f8fd1a1097161d95e21ccf0503560deb"
                },
                {
                  "y": 216,
                  "x": 320,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9a6dd29394e5fb1992ae084b12e8749a9d84824"
                },
                {
                  "y": 433,
                  "x": 640,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=720bb40aec42b94acce8d40bc234aa44e5b4c208"
                },
                {
                  "y": 650,
                  "x": 960,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e355ab07ff4b2410135911c2070a7cb42b1b8221"
                },
                {
                  "y": 732,
                  "x": 1080,
                  "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=668705727d06f0b0d4c5c5867747868786f17635"
                }
              ],
              "s": {
                "y": 812,
                "x": 1198,
                "u": "https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;format=png&amp;auto=webp&amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d"
              },
              "id": "rn7r2k3t4ggf1"
            },
            "vsoidu4e4ggf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=871d7d827171f900b2598bcca602b93414c369f8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a4c54e76149d09d301df5cbd0f5c388e82bb54d"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=317f1298cb62e84fc0ac688a8a4b2143d1d2fc8a"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=769ba43ab175981d7853c0c0620df46e4f20be04"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e16a01628210ccf57e6c94fa2d7d58110870a99"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49835bd796714f4bcb033b2b80c71bc4b6e37e82"
                }
              ],
              "s": {
                "y": 3150,
                "x": 1280,
                "u": "https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=d29809860b081384d998a428bc75faeba16cedc1"
              },
              "id": "vsoidu4e4ggf1"
            }
          },
          "name": "t3_1mf3nw4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#c7b594",
          "subreddit_type": "public",
          "ups": 149,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 149,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/-4v8QT3_SA4NBTuWsWHLP1NxBvsUSLBCUXILi1-L8H8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754071528,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever spent weeks building the perfect LLM benchmark only to watch it crumble within a few months?&lt;/p&gt;\n\n&lt;p&gt;Clean problems, elegant difficulty curves, proper statistical controls. New model drops. Perfect scores across the board. Your tests got trained on. Weeks of work, completely worthless.&lt;/p&gt;\n\n&lt;p&gt;So you pivot. Make the tests harder, more complex, more creative. Models improve with time. Now everyone clusters at 90-95%. 8B models are defeating it. Your benchmark has become a participation trophy. This happened to my previous evaluation, &lt;em&gt;Can-Ai-Code&lt;/em&gt;, twice.&lt;/p&gt;\n\n&lt;p&gt;Fine, you say. Random test generation it is! No more memorization, no more clustering. But congratulations, you&amp;#39;ve just unlocked new nightmares: Did you accidentally make your &amp;quot;hard&amp;quot; tests easier than your &amp;quot;easy&amp;quot; ones? Is your random number generator secretly biased? How do you even validate that hundreds of thousands of randomly generated problems &amp;quot;make sense&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;You solve that with clever statistical rigor, only to discover configuration explosion hell. You&amp;#39;d like to test different prompting templates and sampling parameters, but that&amp;#39;s 5 templates times 5 samplers times 50 million tokens (a conservative estimate) equals 1.25 billion tokens per model. Your GPUs scream in horror.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re now burning millions of tokens achieving 0.005 confidence intervals on trivial problems while critical hard points sit at 0.02 intervals begging for attention like abandoned puppies. Dynamic sampling helps - generate more tests for uncertain points, fewer for confident ones - but how to avoid p-hacking yourself?&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s when the guessing realization hits. This binary classifier task scored 60%! Amazing! Wait... that&amp;#39;s only 20% above random chance. Your &amp;quot;75% accurate&amp;quot; multiple choice task is actually 50% accurate when you subtract lucky guesses. Everything is statistical lies. How are you supposed to compare models across boolean, multiple-choice and write-in answer tasks that have fundamentally different &amp;quot;guess rates&amp;quot;?&lt;/p&gt;\n\n&lt;p&gt;Finally, truncation waste arrives to complete your suffering: Model given tough task hits context limits, burns 8,000 tokens, returns a loop of gibberish. You sample 10x more to maintain statistical power. That&amp;#39;s 80K tokens wasted for one data point but with no useful answers.  You&amp;#39;re overflowing your KV caches while the confidence intervals laugh at you.&lt;/p&gt;\n\n&lt;p&gt;After drowning in this cascade of pain for months, I did what any reasonable person would do: I built an evaluation system to solve every single practical problem I encountered.&lt;/p&gt;\n\n&lt;h1&gt;ReasonScape treats language models as information processing systems, not text completion black boxes.&lt;/h1&gt;\n\n&lt;p&gt;It generates infinite, parametric, tokenization-aware test variations, applies statistical corrections for guessing, dynamically allocates sampling based on uncertainty, handles truncations intelligently, and visualizes the results as both enhanced leaderboards and explorable 3D cognitive landscapes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vsoidu4e4ggf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d29809860b081384d998a428bc75faeba16cedc1\"&gt;C2: All Models x All Tasks Surface Comparison. Green Sphere indicates high-success. Red Square indicates high-truncation.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The initial C2 dataset represents ~1 billion tokens across 9 models, revealing exactly where, how and why reasoning breaks down across 4 task domains. The interactive leaderboard shows not just scores but confidence intervals, token usage and failure modes. The explorer (links at the bottom of post) lets you navigate difficulty manifolds like some kind of LLM reasoning archaeologist, digging into spectral analysis and completion token patterns.  Make sure you&amp;#39;re on a PC - this application has too much going on to be mobile friendly!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ahuh87m4ggf1.png?width=1233&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f6e962cdc029ce01dbca46346ec3fda47a06d7d\"&gt;C2 Explorer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I built the system with progressive evaluation in mind so you can start with rapid exploration then scale to deep precision. Everything caches, everything reproduces, everything scales. ReasonScape isn&amp;#39;t just another benchmark. It&amp;#39;s a complete methodology: toolkit, evaluation framework, and growing dataset family rolled into one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rn7r2k3t4ggf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d054e40f6f292b07b9d638d82244e8f302ce1d\"&gt;C2 Leaderboard (Static snapshot - the Interactive is much nicer!)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The ReasonScape experiments and the resulting datasets will grow, expand and evolve - when scores get too high we will move the difficulty grids to make the tests harder and move on to C3. I have &lt;strong&gt;8 additional tasks&lt;/strong&gt; to bring up, and lots more reasoning models I&amp;#39;d like to evaluate but my 2xRTX3090 only have so much to give.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading this far! &amp;lt;3&lt;/p&gt;\n\n&lt;p&gt;Links:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/\"&gt;ReasonScape Homepage&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/leaderboard\"&gt;ReasonScape Leaderboard - C2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://reasonscape.com/c2/explorer\"&gt;ReasonScape Explorer - C2&lt;/a&gt; (note: PC required, not mobile-friendly)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape\"&gt;ReasonScape GitHub&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/the-crypt-keeper/reasonscape?tab=readme-ov-file#system-architecture\"&gt;ReasonScape System Architecture&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mf3nw4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kryptkpr",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3nw4/i_generated_1_billion_tokens_so_you_dont_have_to/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754071528,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company, especially tools that integrate with ollama's API.\n\nDoes anybody know of any good Vibe Coding (for Windows) tools, as good or better than Cursor, that run on your own local LLMs?  Something that can integrate into VS Code for coding, git updates, agent coding, etc.  \n\nThanks!\n\nEDIT:  I'm looking for a vibe coding desktop app \\ agentic coding, not just a command-line interface into a LLM. \n\nEDIT2:  Also share your thoughts on the best LLM to use for coding python (hardware is a RTX 5070Ti 16GB GPU dedicated to this).  I was going to test Qwen3-30B-A3B-Instruct-2507-GGUF:IQ4_XS  which I can get about 42 tok/s using a RTX 5070Ti.",
          "author_fullname": "t2_esjjx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Vibe Code tools that are free and use your own local LLM as of August 2025?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mg8f1r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754192056,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754189353,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen Cursor and how it works, and it looks pretty cool, but I rather use my own local hosted LLMs and not pay a usage fee to a 3rd party company, especially tools that integrate with ollama&amp;#39;s API.&lt;/p&gt;\n\n&lt;p&gt;Does anybody know of any good Vibe Coding (for Windows) tools, as good or better than Cursor, that run on your own local LLMs?  Something that can integrate into VS Code for coding, git updates, agent coding, etc.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;EDIT:  I&amp;#39;m looking for a vibe coding desktop app \\ agentic coding, not just a command-line interface into a LLM. &lt;/p&gt;\n\n&lt;p&gt;EDIT2:  Also share your thoughts on the best LLM to use for coding python (hardware is a RTX 5070Ti 16GB GPU dedicated to this).  I was going to test Qwen3-30B-A3B-Instruct-2507-GGUF:IQ4_XS  which I can get about 42 tok/s using a RTX 5070Ti.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mg8f1r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StartupTim",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mg8f1r/best_vibe_code_tools_that_are_free_and_use_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg8f1r/best_vibe_code_tools_that_are_free_and_use_your/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754189353,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dphk4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 is the top open weights model on lmarena",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf0qlf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 187,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 187,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754064880,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/lmarena_ai/status/1951308670375174457",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mf0qlf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tarruda",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf0qlf/qwen3235ba22b2507_is_the_top_open_weights_model/",
          "stickied": false,
          "url": "https://x.com/lmarena_ai/status/1951308670375174457",
          "subreddit_subscribers": 509291,
          "created_utc": 1754064880,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nWhen our radiology department rejected another batch of low-resolution X-rays because they couldn't see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.\n\nOn that day in the hospital basement, I decided somehow to fix this problem that's now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.\n\nThe Problem I Was Really Solving\n\nAs a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.\n\nWhen my friend completed research on X-ray super resolution using [GANs](https://www.youtube.com/watch?v=BVWn8JfeekY), I saw an opportunity to turn this research paper into a practical solution.\n\nThe existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.\n\nThe breaking point wasn't just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.\n\nThe Technical Breakthrough\n\nI built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.\n\nThe Core Technology (from the research):\n\nPatch-based processing that preserves fine-grained structural details\n\nGenerative adversarial network trained on 3,000 clinical X-ray images\n\nMoving average filters with random kernel sizes (1-40 pixels) for robustness\n\nFour-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses\n\nWhat I Built Around It:\n\nSimple drag-and-drop web interface for uploading X-ray DICOM files\n\nCloud processing pipeline using AWS GPU instances\n\nHIPAA-compliant storage and transmission\n\nIntegration with existing PACS (Picture Archiving and Communication Systems)\n\nThe first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.\n\nThe Growth Numbers (Early Traction)\n\nIn first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)\n\nCurrent Unit Economics:\n\nMRR: $1,450\n\nCloud compute costs: $520/month (AWS GPU instances)\n\nInfrastructure &amp; compliance: $180/month\n\nGross profit: $750/month (52% margin)\n\nAverage customer acquisition cost: $95 (mostly referrals and medical imaging forums).\n\nWhat I actually understood is\n\nBuilding on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.\n\nMedical software is about trust and usability. Radiologists don't want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.\n\nEarly traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.\n\nHospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.\n\nTechnical Reality\n\nUnlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.\n\nThe discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.\n\nWe validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.\n\nThe Real Lesson\n\nThis business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.\n\nThe opportunity wasn't in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.",
          "author_fullname": "t2_1tts756whd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I Built Medical AI by Solving the Radiation Dose Problem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfzu3d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754164960,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When our radiology department rejected another batch of low-resolution X-rays because they couldn&amp;#39;t see critical bone fractures, I watched $15,000 worth of re-imaging appointments get scheduled for the next week. Each patient would get 16x more radiation exposure just to see what should have been visible the first time.&lt;/p&gt;\n\n&lt;p&gt;On that day in the hospital basement, I decided somehow to fix this problem that&amp;#39;s now generating $2k in monthly revenue. But it is now helping radiologists make accurate diagnoses from images that would have been unusable before.&lt;/p&gt;\n\n&lt;p&gt;The Problem I Was Really Solving&lt;/p&gt;\n\n&lt;p&gt;As a software developer with a background in medical imaging, I spent time observing radiologists struggle with degraded X-ray images, trying to identify barely visible pathologies. The physics are unforgiving: high-res X-rays require high radiation doses, but low-dose X-rays lose critical diagnostic detail.&lt;/p&gt;\n\n&lt;p&gt;When my friend completed research on X-ray super resolution using &lt;a href=\"https://www.youtube.com/watch?v=BVWn8JfeekY\"&gt;GANs&lt;/a&gt;, I saw an opportunity to turn this research paper into a practical solution.&lt;/p&gt;\n\n&lt;p&gt;The existing solutions were either too expensive (upgrading every imaging system costs millions) or too generic (standard upscaling algorithms that introduce more blur than clarity). Radiologists were stuck choosing between patient safety and diagnostic accuracy.&lt;/p&gt;\n\n&lt;p&gt;The breaking point wasn&amp;#39;t just seeing patients get double dosed - it was realizing there was proven research showing a path forward, but no one had built it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The Technical Breakthrough&lt;/p&gt;\n\n&lt;p&gt;I built what became XRayEnhance using Rocket to prototype the web interface around the proven XPGAN (X-ray Patch Generative Adversarial Network) algorithm. The research had already validated the approach - my job was making it accessible to radiologists.&lt;/p&gt;\n\n&lt;p&gt;The Core Technology (from the research):&lt;/p&gt;\n\n&lt;p&gt;Patch-based processing that preserves fine-grained structural details&lt;/p&gt;\n\n&lt;p&gt;Generative adversarial network trained on 3,000 clinical X-ray images&lt;/p&gt;\n\n&lt;p&gt;Moving average filters with random kernel sizes (1-40 pixels) for robustness&lt;/p&gt;\n\n&lt;p&gt;Four-loss optimization: adversarial, pixel-wise, perceptual, and edge-preservation losses&lt;/p&gt;\n\n&lt;p&gt;What I Built Around It:&lt;/p&gt;\n\n&lt;p&gt;Simple drag-and-drop web interface for uploading X-ray DICOM files&lt;/p&gt;\n\n&lt;p&gt;Cloud processing pipeline using AWS GPU instances&lt;/p&gt;\n\n&lt;p&gt;HIPAA-compliant storage and transmission&lt;/p&gt;\n\n&lt;p&gt;Integration with existing PACS (Picture Archiving and Communication Systems)&lt;/p&gt;\n\n&lt;p&gt;The first working prototype took 6 weeks to build, focusing on turning the research algorithm into a user-friendly web application.&lt;/p&gt;\n\n&lt;p&gt;The Growth Numbers (Early Traction)&lt;/p&gt;\n\n&lt;p&gt;In first 2 months I was Testing with 2 radiologist contacts from my network In next month I was able to get $290 from 1 small imaging center pilot By month 4: $580 from 2 centers, word spreading through referrals Month 5: $870 MRR (added batch processing feature) Month 6: $1,450 MRR (first hospital department trial)&lt;/p&gt;\n\n&lt;p&gt;Current Unit Economics:&lt;/p&gt;\n\n&lt;p&gt;MRR: $1,450&lt;/p&gt;\n\n&lt;p&gt;Cloud compute costs: $520/month (AWS GPU instances)&lt;/p&gt;\n\n&lt;p&gt;Infrastructure &amp;amp; compliance: $180/month&lt;/p&gt;\n\n&lt;p&gt;Gross profit: $750/month (52% margin)&lt;/p&gt;\n\n&lt;p&gt;Average customer acquisition cost: $95 (mostly referrals and medical imaging forums).&lt;/p&gt;\n\n&lt;p&gt;What I actually understood is&lt;/p&gt;\n\n&lt;p&gt;Building on proven research beats starting from scratch. Having validated algorithms meant I could focus on user experience and deployment rather than wondering if the core technology would work.&lt;/p&gt;\n\n&lt;p&gt;Medical software is about trust and usability. Radiologists don&amp;#39;t want to learn complex interfaces - they want their existing workflow enhanced with minimal friction.&lt;/p&gt;\n\n&lt;p&gt;Early traction comes from solving real pain points. The research proved the technical feasibility, but seeing radiologists immediately adopt the tool validated the market need.&lt;/p&gt;\n\n&lt;p&gt;Hospital procurement cycles are long but predictable. Once a radiology department validates the technology, the purchasing decision takes 4-6 months but rarely gets reversed.&lt;/p&gt;\n\n&lt;p&gt;Technical Reality&lt;/p&gt;\n\n&lt;p&gt;Unlike consumer image enhancement that optimizes for visual appeal, medical super-resolution must preserve diagnostic accuracy. Our GAN architecture specifically avoids introducing artifacts that could be mistaken for pathologies.&lt;/p&gt;\n\n&lt;p&gt;The discriminator network acts like a junior radiologist, learning to distinguish between real high-resolution X-rays and our generated ones. This adversarial training forces the generator to produce medically accurate enhancements rather than just visually pleasing ones.&lt;/p&gt;\n\n&lt;p&gt;We validate every enhancement using automated metrics (SSIM, Laplacian variance) and radiologist review sessions where physicians compare diagnoses from original vs. enhanced images.&lt;/p&gt;\n\n&lt;p&gt;The Real Lesson&lt;/p&gt;\n\n&lt;p&gt;This business exists because I saw proven research that solved a real clinical problem and decided to turn it into a practical tool that radiologists could actually use.&lt;/p&gt;\n\n&lt;p&gt;The opportunity wasn&amp;#39;t in inventing new AI algorithms - it was in understanding that brilliant academic research often stays hidden in those papers when it could be helping people solve real problems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?auto=webp&amp;s=ddcf97793b8059e1ac6f8e5f42c0fb141ac1f06b",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ecfe2ffa2015cbfbedc8dfb6cbcd4cf0d32f0bee",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d64b11c760cf9d689a103b871b38eb77933c0ed",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=085b7fe67421b9ba136cbbcbf7988771acb578f2",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "OVuemuVygw_3HoiDF0PR0GpK77a-Bg-aRvKWskLNmyM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfzu3d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Perception-9919",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfzu3d/how_i_built_medical_ai_by_solving_the_radiation/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754164960,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_nt584",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chinese LLMs talk freely about Tiananmen massacre and Taiwan",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mgdhdq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754207165,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "datanizing.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mgdhdq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "7pot",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mgdhdq/chinese_llms_talk_freely_about_tiananmen_massacre/",
          "stickied": false,
          "url": "https://datanizing.com/2025/08/02/chinese-llm-tiananmen-taiwan.html",
          "subreddit_subscribers": 509291,
          "created_utc": 1754207165,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.\n\nI also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it's on the leaderboard. Glm was also much better at tool calls, it basically never failed.",
          "author_fullname": "t2_58t8ty6v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is bad at tool call while glm-4.5 is surprisingly good",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mf8la7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 61,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 61,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754083147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried running qwen3-coder in Claude Code. It constantly failed tool calls. I tried both the cerebras api and the official alibaba api.&lt;/p&gt;\n\n&lt;p&gt;I also tried glm-4.5 in Claude Code and it was surprisingly good. Asked both Gemini cli and glm-4.5 in Claude Code to make the snake game and tetris in html and the games made ny glm were much better looking than gemini. Since Gemini is #1 right now on Web Arena, I suspect glm will be #1 when it&amp;#39;s on the leaderboard. Glm was also much better at tool calls, it basically never failed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mf8la7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BoJackHorseMan53",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf8la7/qwen3coder_is_bad_at_tool_call_while_glm45_is/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754083147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, it‚Äôs been a while but I‚Äôm happy to announce another major update for my app **EasyWhisperUI**, now with **live transcriptions**!\n\nIt features full cross-platform GPU acceleration:\n\n* **Vulkan** on Windows (Intel, AMD, or NVIDIA)\n* **Metal** on macOS (Apple silicon)\n\n**New features!**\n\n1. **GPU-accelerated Live Transcriptions** ‚Ä¢ Transcribe speech in real time using your default mic (user request)\n2. **Output Cleanup** ‚Ä¢ Automatically removes repeated segments from live transcriptions\n3. **Open in Notepad Checkbox** ‚Ä¢ New option to disable automatic opening in Notepad after transcription (user request)\n4. Various bug fixes and code improvements.\n\n**Other key features**\n\n5. **Batch File Processing** ‚Ä¢ Drag &amp; drop multiple files ‚Äî EasyWhisperUI will queue and transcribe them automatically (user request)  \n6. **CPU-Only Toggle** ‚Ä¢ Option to disable GPU acceleration and run fully on CPU (user request)  \n7. **Modern UI** ‚Ä¢ Acrylic background on Windows, clean layout and spacing improvements  \n8. **macOS Support** ‚Ä¢ EasyWhisperUI works on macOS thanks to a community contribution  \n9. **Installer Included** ‚Ä¢ Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click\n\nThere are a lot more features ‚Äî check out the GitHub for more info:\n\nüîó **GitHub:** [https://github.com/mehtabmahir/easy-whisper-ui](https://github.com/mehtabmahir/easy-whisper-ui)\n\nLet me know what you think or if you have any suggestions!",
          "author_fullname": "t2_epvxz7g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EasyWhisperUI ‚Äì GPU accelerated Open Source Whisper UI for Windows &amp; macOS now with Live Transcriptions!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mffjjj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754103866,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754102625,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, it‚Äôs been a while but I‚Äôm happy to announce another major update for my app &lt;strong&gt;EasyWhisperUI&lt;/strong&gt;, now with &lt;strong&gt;live transcriptions&lt;/strong&gt;!&lt;/p&gt;\n\n&lt;p&gt;It features full cross-platform GPU acceleration:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; on Windows (Intel, AMD, or NVIDIA)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Metal&lt;/strong&gt; on macOS (Apple silicon)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;New features!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;GPU-accelerated Live Transcriptions&lt;/strong&gt; ‚Ä¢ Transcribe speech in real time using your default mic (user request)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Output Cleanup&lt;/strong&gt; ‚Ä¢ Automatically removes repeated segments from live transcriptions&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Open in Notepad Checkbox&lt;/strong&gt; ‚Ä¢ New option to disable automatic opening in Notepad after transcription (user request)&lt;/li&gt;\n&lt;li&gt;Various bug fixes and code improvements.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Other key features&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Batch File Processing&lt;/strong&gt; ‚Ä¢ Drag &amp;amp; drop multiple files ‚Äî EasyWhisperUI will queue and transcribe them automatically (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CPU-Only Toggle&lt;/strong&gt; ‚Ä¢ Option to disable GPU acceleration and run fully on CPU (user request)&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Modern UI&lt;/strong&gt; ‚Ä¢ Acrylic background on Windows, clean layout and spacing improvements&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;macOS Support&lt;/strong&gt; ‚Ä¢ EasyWhisperUI works on macOS thanks to a community contribution&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Installer Included&lt;/strong&gt; ‚Ä¢ Installs everything you need (compiler, ffmpeg, whisper.cpp) and builds from source with one click&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There are a lot more features ‚Äî check out the GitHub for more info:&lt;/p&gt;\n\n&lt;p&gt;üîó &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/mehtabmahir/easy-whisper-ui\"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think or if you have any suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?auto=webp&amp;s=8b1a4fb535e00a6da0368011b0e6cec45497c5d7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5822df3220f343ebd2934400d0f59847247da8d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=823cd6c140e6f8b93ef72f68e9d0b425f56243f3",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1e4b350fed949d338ba9708cf08854a1b4237e89",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ccc578f650ebc7544f2e8cfe89996ac4b84f501f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d4fb11dd5292586b4d7444d6d35ffe279e98ba1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2237e0f4ab1ae9e5ec2baa35b902b491ffb5d35",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2RuiVB0WStMivlNC9GR358Mle_WRTkC3SQ-fKq98qxI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mffjjj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehtabmahir",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mffjjj/easywhisperui_gpu_accelerated_open_source_whisper/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754102625,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it‚Äôs just my system",
          "author_fullname": "t2_1hhil4cbg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox tts on amd",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfz1k2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754162909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to run chatterbox tts on an amd 9070 xt, I tried running it the other day but it would crash immediately before I could even get the ui open and I was wondering if it‚Äôs just my system&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfz1k2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeMan060",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfz1k2/chatterbox_tts_on_amd/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754162909,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are local llms fast and powerful enough to do analysis on movies in real time?\n\nSay you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?\n\nIf not today, then when will it be possible to do that?",
          "author_fullname": "t2_26u5g058",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LocalLLM for movies",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy924",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160893,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are local llms fast and powerful enough to do analysis on movies in real time?&lt;/p&gt;\n\n&lt;p&gt;Say you can tell llms to skip scenes with certain actors and them the llm does scene analysis to skip those parts?&lt;/p&gt;\n\n&lt;p&gt;If not today, then when will it be possible to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfy924",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImaginaryRea1ity",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy924/localllm_for_movies/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754160893,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I'm developing a product that uses AI, but it's entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? ",
          "author_fullname": "t2_1u3u0ca58k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dutch LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfy5qs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754160656,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m developing a product that uses AI, but it&amp;#39;s entirely in Dutch. Which AI model would you guys recommend for Dutch language tasks specifically? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfy5qs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Bath-7378",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfy5qs/dutch_llm/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754160656,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The person who \"leaked\" this model is from the openai (HF) organization \n\nSo as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model ",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "The OpenAI Open weight model might be 120B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "9dqwiep9ucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 128,
                  "x": 108,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=82c6a3fca8afcd98702c0b7d846a7e160d71c2ba"
                },
                {
                  "y": 257,
                  "x": 216,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=daf71cd9fc68c069a95fe74761dcc142bc617a8b"
                },
                {
                  "y": 381,
                  "x": 320,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=436cbfa27d8543f73db2371953a82aac39ecda85"
                },
                {
                  "y": 763,
                  "x": 640,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8299ea44db486e3a2fbec74c3d0bd88466708866"
                },
                {
                  "y": 1144,
                  "x": 960,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ff8e80fe3dec58ee22a247bfdab51b53bcac09ca"
                },
                {
                  "y": 1288,
                  "x": 1080,
                  "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4d90e7d7d325eaa34c7621c43ce177895ff0383"
                }
              ],
              "s": {
                "y": 1288,
                "x": 1080,
                "u": "https://preview.redd.it/9dqwiep9ucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b3767f5800ed4a78ec866eaa707e8dbe0151f82f"
              },
              "id": "9dqwiep9ucgf1"
            },
            "w9h7ftgaucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 187,
                  "x": 108,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=65c73f27d3ec2b6268ed676ac73a20d96cc01864"
                },
                {
                  "y": 375,
                  "x": 216,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7776f97e1b1b9b42cd96de35d6b38c2bdf1948d"
                },
                {
                  "y": 555,
                  "x": 320,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17560f484f4fc6684187f9b4e3214d2bb480250d"
                },
                {
                  "y": 1111,
                  "x": 640,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=64632b927b61af650b70b9b6e2e93befcc39ee18"
                },
                {
                  "y": 1667,
                  "x": 960,
                  "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8bd9619e6620d4a4c7a2fbdf4077d3780cf0b105"
                }
              ],
              "s": {
                "y": 1873,
                "x": 1078,
                "u": "https://preview.redd.it/w9h7ftgaucgf1.png?width=1078&amp;format=png&amp;auto=webp&amp;s=dd26649ae5edf99eedfd8378f94c7fdf3aeb6ae9"
              },
              "id": "w9h7ftgaucgf1"
            },
            "s1j5io2aucgf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 94,
                  "x": 108,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=066bdffe86c232ba5ff6c7d20d8dde94884c7f6e"
                },
                {
                  "y": 188,
                  "x": 216,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3ab77eae4021c3e0b9af7ed66501883f4211b90"
                },
                {
                  "y": 279,
                  "x": 320,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=61d00c48c6b5fb9736c6d3503205a847c8146f46"
                },
                {
                  "y": 558,
                  "x": 640,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=de6f7067561af43d54b56fbf6902277437c1fa94"
                },
                {
                  "y": 837,
                  "x": 960,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=062e6b2cc5f7e404a5f33fca5b18a6e367b0f99b"
                },
                {
                  "y": 942,
                  "x": 1080,
                  "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5e518857b677661c538f1b99548e5ca304d3b186"
                }
              ],
              "s": {
                "y": 942,
                "x": 1080,
                "u": "https://preview.redd.it/s1j5io2aucgf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=b86914c6db57a17c4bde148a77b9a789f692e93f"
              },
              "id": "s1j5io2aucgf1"
            }
          },
          "name": "t3_1mepeqh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#bbbdbf",
          "ups": 718,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "9dqwiep9ucgf1",
                "id": 718647288
              },
              {
                "caption": "",
                "media_id": "s1j5io2aucgf1",
                "id": 718647289
              },
              {
                "caption": "",
                "media_id": "w9h7ftgaucgf1",
                "id": 718647290
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 718,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754030862,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt;\n\n&lt;p&gt;So as expected, it&amp;#39;s not gonna be something you can easily run locally, it won&amp;#39;t hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mepeqh",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mepeqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 159,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mepeqh",
          "subreddit_subscribers": 509291,
          "created_utc": 1754030862,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! I‚Äôm a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?\n\nI initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that‚Äôs true, wouldn‚Äôt that quickly blow up the context window?\n\nBut then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn‚Äôt that risk missing important context, especially for something like summarization?\n\nSo both approaches seem to have drawbacks ‚Äî I‚Äôm just wondering which one is typically used by AI apps when handling uploaded files?",
          "author_fullname": "t2_kjhuip53",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RAG or prompt engineering",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfor6n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754135853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I‚Äôm a bit confused about what actually happens when you upload a document to an AI app like ChatGPT or LE CHAT. Is this considered prompt engineering (just pasting the content into the prompt) or is it RAG (Retrieval-Augmented Generation)?&lt;/p&gt;\n\n&lt;p&gt;I initially thought it was RAG, but I saw this video from Yannic Kilcher explaining that ChatGPT basically just copies the content of the document and pastes it into the prompt. If that‚Äôs true, wouldn‚Äôt that quickly blow up the context window?&lt;/p&gt;\n\n&lt;p&gt;But then again, if it is RAG, like using vector search on the document and feeding only similar chunks to the LLM, wouldn‚Äôt that risk missing important context, especially for something like summarization?&lt;/p&gt;\n\n&lt;p&gt;So both approaches seem to have drawbacks ‚Äî I‚Äôm just wondering which one is typically used by AI apps when handling uploaded files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mfor6n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SignatureHuman8057",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfor6n/rag_or_prompt_engineering/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754135853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.\n\nI gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it\n\n[https://github.com/mmemoo/dis-cal](https://github.com/mmemoo/dis-cal)  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. \n\nThanks in advance!",
          "author_fullname": "t2_cgzigjpd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I made a opensource CAL-AI alternative using ollama which runs completely locally and for is fully free.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfrec0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754143606,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im trying to put on some weight and muscle and needed to count my calories , for times when i dont have time to search and count i needed an app like CAL-AI but didnt want to pay for a ChatGpt wrapper so i created this and thought to myself why not share it with other people.&lt;/p&gt;\n\n&lt;p&gt;I gotta say tho it is not the most accurate one out there since it uses a little local model but its pretty accurate as far as i tested it&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mmemoo/dis-cal\"&gt;https://github.com/mmemoo/dis-cal&lt;/a&gt;  All instructions and everything is in this repo, i would appreciate it if you tried it and told me bugs, improveable parts and features that can be added. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?auto=webp&amp;s=7740e51b065f73baef20acb9661652efbd31473e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43d0c8bafbb43faea860f8f86e81107782da6aa3",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c8b2d47057fd20576416383a2c09e2a524b760d",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b75736c0b2fa807b69315eb351d4357ec156394",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=78079915dfeceeab89fd37ed8f707b7cfbc64ce5",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cabc737e4eaac8c82320118a5fb4890d5545856f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57c3313f0eff8bcc3ad6c3af276b7a5cb4a42f83",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "SDBxUlMfav63r9emstxn0JGBVFpMuYK-50VWDphIeFg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mfrec0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mehmetflix_",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfrec0/i_made_a_opensource_calai_alternative_using/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754143606,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I wanna use this model for DMing a dnd game as well as using it to write stories. I‚Äôd like it to be abliterated if possible.\n\nI‚Äôve been looking at using Gemma 3 27B, and I do like its writing style, but I‚Äôm concerned about its ability to handle long context lengths.\n\nSo far I haven‚Äôt had that problem but that‚Äôs only because I‚Äôve been running it with low context lengths, since I‚Äôm using it on my gaming pc right now.\n\nI‚Äôm in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.\n\nThanks in advance ",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best creative writing + long context model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mfifhh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754111899,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanna use this model for DMing a dnd game as well as using it to write stories. I‚Äôd like it to be abliterated if possible.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôve been looking at using Gemma 3 27B, and I do like its writing style, but I‚Äôm concerned about its ability to handle long context lengths.&lt;/p&gt;\n\n&lt;p&gt;So far I haven‚Äôt had that problem but that‚Äôs only because I‚Äôve been running it with low context lengths, since I‚Äôm using it on my gaming pc right now.&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm in the middle of building a budget local AI pc right now, 2 MI50 32gbs with 64gb of ddr4 ram on am4. With 64gb of vram combined, I want to see if there are better options available to me.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mfifhh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfifhh/best_creative_writing_long_context_model/",
          "subreddit_subscribers": 509291,
          "created_utc": 1754111899,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}