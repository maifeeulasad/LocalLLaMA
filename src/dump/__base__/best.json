{
  "kind": "Listing",
  "data": {
    "after": "t3_1lxufzz",
    "dist": 100,
    "modhash": "",
    "geo_filter": null,
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://eqbench.com/](https://eqbench.com/)\n\nWriting samples:\n\n[https://eqbench.com/results/creative-writing-v3/moonshotai\\_\\_Kimi-K2-Instruct.html](https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html)\n\nEQ-Bench responses:\n\n[https://eqbench.com/results/eqbench3\\_reports/moonshotai\\_\\_kimi-k2-instruct.html](https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html)",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bltx3wip1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c91069e6404ebfdd239fca53b74ad27ebf361cb9"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=46558502029394a5450bb7a8b1fd7772f775b63e"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f8b74bea5bfacbcf65bd04fad681f91f30e63cb"
                },
                {
                  "y": 657,
                  "x": 640,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea6d2eccaab12c39f50549d82e4485fbfe48cf88"
                },
                {
                  "y": 985,
                  "x": 960,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f04df6fdbd708b96f6a89ab335a6247ac5c6fab"
                },
                {
                  "y": 1109,
                  "x": 1080,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f38d0ec7cc27f92b56854e865fea4d742abb0b6d"
                }
              ],
              "s": {
                "y": 1641,
                "x": 1598,
                "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=1598&amp;format=png&amp;auto=webp&amp;s=17f01c666d495066ceec9a5e3e74c86504144791"
              },
              "id": "bltx3wip1lcf1"
            },
            "slfo69pq1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 104,
                  "x": 108,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80dd9f9ea9f4c233481f726f596a93be59337d18"
                },
                {
                  "y": 208,
                  "x": 216,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7dc16b4166123dad30767234d268c2ba9cfe207b"
                },
                {
                  "y": 309,
                  "x": 320,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2aa653aee8a77a1c60f613c8eee527eaad88bd09"
                },
                {
                  "y": 618,
                  "x": 640,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=920c5b3b7ca71ba826946d79ece48662e7e61663"
                },
                {
                  "y": 928,
                  "x": 960,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7644b88db8a83e82b2c5386f3aafca392976d38"
                },
                {
                  "y": 1044,
                  "x": 1080,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc43edacc6efbed8788f53ba27cc9baf10ea878"
                }
              ],
              "s": {
                "y": 1552,
                "x": 1605,
                "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=1605&amp;format=png&amp;auto=webp&amp;s=bc768dc8812c1370ff4a7130928bfb3b76a0bc2d"
              },
              "id": "slfo69pq1lcf1"
            },
            "7dy0n72s1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f769e577a79198993e84c8eb762589e7a68bfb8"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86550c6ee8e16b8738d8dc52faa646a74726b22f"
                },
                {
                  "y": 206,
                  "x": 320,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4098fae6e966e2e07a7fe5fa68dc8f1e0842b77a"
                },
                {
                  "y": 413,
                  "x": 640,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4a695523e542b1689dbec502a3e44b4523f96dc"
                },
                {
                  "y": 620,
                  "x": 960,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=69f2c06f0306a5e4a0df7529218415f269ff0ecb"
                },
                {
                  "y": 697,
                  "x": 1080,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d1a2d4446112aab3c4639bd2b2bafcd744e7628"
                }
              ],
              "s": {
                "y": 1031,
                "x": 1596,
                "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=141d64df955b06e799e4f4c0b45e24ddb1211acd"
              },
              "id": "7dy0n72s1lcf1"
            },
            "46yhtq9r1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47a8d5e353281c77928ebe675d58f1a425091614"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dfbb994f47e60e6719c2c54e5a812ca2c8cdc38e"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5cb664395530a7771d865e53841a1040c702271"
                },
                {
                  "y": 657,
                  "x": 640,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c1174fd8b0b51963ad2c4fce60bc48d732f7da7"
                },
                {
                  "y": 986,
                  "x": 960,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16afcea1d3023190bbe776ef801cbf5ac2a7cc39"
                },
                {
                  "y": 1109,
                  "x": 1080,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b2ef583e66dfa8ed312dfce180f7388aaf630c8b"
                }
              ],
              "s": {
                "y": 1634,
                "x": 1590,
                "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=9dade8cde09a83f0bab6a44c0f0352e0c91ac03c"
              },
              "id": "46yhtq9r1lcf1"
            }
          },
          "name": "t3_1lylo75",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "transparent",
          "ups": 506,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "bltx3wip1lcf1",
                "id": 704803294
              },
              {
                "media_id": "slfo69pq1lcf1",
                "id": 704803295
              },
              {
                "media_id": "46yhtq9r1lcf1",
                "id": 704803296
              },
              {
                "media_id": "7dy0n72s1lcf1",
                "id": 704803297
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 506,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_mu9EQ2-CS-NLztYt8TCn8nhmS5cqsN6BOfAQW9BupA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":Llama:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/23w2nhjj1e9f1_t5_81eyvm/Llama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752386963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://eqbench.com/\"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Writing samples:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html\"&gt;https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;EQ-Bench responses:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html\"&gt;https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lylo75",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":Llama:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lylo75",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 108,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lylo75",
          "subreddit_subscribers": 498343,
          "created_utc": 1752386963,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "- Screenshot: https://openrouter.ai/moonshotai\n- Announcement: https://moonshotai.github.io/Kimi-K2/\n- Model: https://huggingface.co/moonshotai/Kimi-K2-Instruct",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Moonshot AI just made their moonshot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyaozv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 724,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 724,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/xVPlJS3y_4S_JgFzimM_t38EhPQ9pdHyidqsBpHzW4M.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752353207,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;Screenshot: &lt;a href=\"https://openrouter.ai/moonshotai\"&gt;https://openrouter.ai/moonshotai&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Announcement: &lt;a href=\"https://moonshotai.github.io/Kimi-K2/\"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Model: &lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/95q67pnr9icf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?auto=webp&amp;s=c3212fdbf5d068289e4e365e2b57a3a2158a32cf",
                  "width": 1941,
                  "height": 1782
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5225d3e72ca9cbedd6a6f3b7456742c3f7a2d71",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a90fd4c772f2ce00a3aa69f9a7d6023ff092d4b7",
                    "width": 216,
                    "height": 198
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5a8c70504908df2e6c29a0bef03f9c709496057",
                    "width": 320,
                    "height": 293
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2af006a61647e3c965c2e033c957c97e3e1f42cd",
                    "width": 640,
                    "height": 587
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f360d1f154858d51ea49b78d8794de1120ff330",
                    "width": 960,
                    "height": 881
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72830cc0e94d363d613f18dbe56b2210ac66ee99",
                    "width": 1080,
                    "height": 991
                  }
                ],
                "variants": {},
                "id": "iIm4EKYFT0antT-Zwv7DxEcCvUorkX5MOtF-h16njUw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lyaozv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 125,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/",
          "stickied": false,
          "url": "https://i.redd.it/95q67pnr9icf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752353207,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "we have to delay it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxyvto",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 2555,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 2555,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/2YTkmABEvZ-xDvkpMbhGYL3GX_nvQgreScEcpz6UlEE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752322106,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oma34zdapfcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?auto=webp&amp;s=954d7d66f90b748d8e5d4feb24c6aab764476f51",
                  "width": 1200,
                  "height": 880
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc062ecd361f54588924802e9a8d113aeaaaa827",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4c82b3926fc1a6c6306e5b970dfd4784fdc38d7",
                    "width": 216,
                    "height": 158
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f9c1cd39db814db4c5915419c4d114e086fd641",
                    "width": 320,
                    "height": 234
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5",
                    "width": 640,
                    "height": 469
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7072dec54580b443d3fc3fa563ae25d5f249bd31",
                    "width": 960,
                    "height": 704
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bdb9eaf6243bf6a7a2c8c28bc6460844654f7236",
                    "width": 1080,
                    "height": 792
                  }
                ],
                "variants": {},
                "id": "kv1bPWQK0QfjBY4lkNaJeVxIc-aQd3DEWwNTmfHkEhw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lxyvto",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 176,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/",
          "stickied": false,
          "url": "https://i.redd.it/oma34zdapfcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752322106,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like \"here's a scenario, what do you think is the most realistic thing to happen?\" or \"what do you think would be a good solution to this issue?\". I found it quite bad in this regard.\n\n* It frequently made things up, even when specifically instructed not to do so. **It then clarified it was trying to come up with a helpful looking answer using fragmented data**, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.\n\n* If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion by using sources that do not exist. **At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.** It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.\n\n* Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it's own idea.\n\n* If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like \"why are you not revealing the character's thoughts here?\" or \"why are you not taking X into account?\". Free ChatGPT is actually much better in this regard.\n\n* It has possibly the most restrictive content filters i have seen out of all the AI chat bots i have tried. It's very prudish.",
          "author_fullname": "t2_11hnt7w9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Kimi K2 for writing and reasoning, and was not impressed.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyvah4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like &amp;quot;here&amp;#39;s a scenario, what do you think is the most realistic thing to happen?&amp;quot; or &amp;quot;what do you think would be a good solution to this issue?&amp;quot;. I found it quite bad in this regard.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;It frequently made things up, even when specifically instructed not to do so. &lt;strong&gt;It then clarified it was trying to come up with a helpful looking answer using fragmented data&lt;/strong&gt;, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion by using sources that do not exist. &lt;strong&gt;At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.&lt;/strong&gt; It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it&amp;#39;s own idea.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like &amp;quot;why are you not revealing the character&amp;#39;s thoughts here?&amp;quot; or &amp;quot;why are you not taking X into account?&amp;quot;. Free ChatGPT is actually much better in this regard.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It has possibly the most restrictive content filters i have seen out of all the AI chat bots i have tried. It&amp;#39;s very prudish.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyvah4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlompSpark",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So 6 months ago I discussed some information about the at the time not launched [910C accelerator here](https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/).\n\nThe details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/\n\nAnd semianalysis (regarding the 800 tflop bf16 performance) https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/\n\nSince then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.\n\nOn the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.\n\nRecently attended a conference where senior Huawei executives verbally discussed their future plans:\n\n1. They are aiming for a launch of the 920 in H2 2026 or H1 2027 \n\n2. The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?\n\n3. The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.\n\n4. A quote near verbatim \"the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.\" It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.",
          "author_fullname": "t2_nm52x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Rumor] Huawei 920 accelerator coming H2 2026",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lysqk7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752413617,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752413033,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So 6 months ago I discussed some information about the at the time not launched &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/\"&gt;910C accelerator here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) &lt;a href=\"https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/\"&gt;https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And semianalysis (regarding the 800 tflop bf16 performance) &lt;a href=\"https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/\"&gt;https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Since then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.&lt;/p&gt;\n\n&lt;p&gt;Recently attended a conference where senior Huawei executives verbally discussed their future plans:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;They are aiming for a launch of the 920 in H2 2026 or H1 2027 &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A quote near verbatim &amp;quot;the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.&amp;quot; It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lysqk7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "44seconds",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752413033,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1aluyxp2yo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"We will release o3 wieghts next week\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxyj92",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 1328,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 800,
              "fallback_url": "https://v.redd.it/8iqku5brlfcf1/DASH_360.mp4?source=fallback",
              "has_audio": true,
              "height": 360,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/8iqku5brlfcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8iqku5brlfcf1/DASHPlaylist.mpd?a=1755013029%2CMzU3ZTAyYTc5NDNjYzkxNTA0YjkxMTIzMzc2N2U4Y2E0ZDkxNThmZjNjMDg3Yzc4MTkxYzYxNDcwNGNjMGM2MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/8iqku5brlfcf1/HLSPlaylist.m3u8?a=1755013029%2CZjVmMjdlZTcwNDkxMWZmN2FkZWU3YzE0YTg1YTZkODE2NDczYTU4NDdjOGM3NzM1N2MwNWJlNjUwNTc4OWUwNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1328,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=140&amp;height=105&amp;crop=140:105,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=95673b6951668a67f35ba057d964af1a0c494b31",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752320929,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8iqku5brlfcf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?format=pjpg&amp;auto=webp&amp;s=e5e2c20582d3be0b2fae4e0de79232d5060ef676",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6338a91401d613f89c37a6107a739c43ba939ca6",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c7d30b78ccd98502de6db6c25e26ba6c69ef48da",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5b18a28ecd27599e16af8fe9723eaa668cb9067c",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lxyj92",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Qparadisee",
          "discussion_type": null,
          "num_comments": 79,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/",
          "stickied": false,
          "url": "https://v.redd.it/8iqku5brlfcf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320929,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 800,
              "fallback_url": "https://v.redd.it/8iqku5brlfcf1/DASH_360.mp4?source=fallback",
              "has_audio": true,
              "height": 360,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/8iqku5brlfcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8iqku5brlfcf1/DASHPlaylist.mpd?a=1755013029%2CMzU3ZTAyYTc5NDNjYzkxNTA0YjkxMTIzMzc2N2U4Y2E0ZDkxNThmZjNjMDg3Yzc4MTkxYzYxNDcwNGNjMGM2MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/8iqku5brlfcf1/HLSPlaylist.m3u8?a=1755013029%2CZjVmMjdlZTcwNDkxMWZmN2FkZWU3YzE0YTg1YTZkODE2NDczYTU4NDdjOGM3NzM1N2MwNWJlNjUwNTc4OWUwNQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The International Math Olympiad will take place on 15th and 16th July in Australia.\nGoogle Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024.\nAny open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.\n\nhttps://youtu.be/vJjgtOcXq8A",
          "author_fullname": "t2_2y1uo4e2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you think an AI will achieve gold medal in 2025 International Math Olympad (tomorrow)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyj81f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 68,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 68,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752378438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The International Math Olympiad will take place on 15th and 16th July in Australia.\nGoogle Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024.\nAny open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/vJjgtOcXq8A\"&gt;https://youtu.be/vJjgtOcXq8A&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?auto=webp&amp;s=e75662789bd989c9174581c02e50a281f52c2bbd",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f46c2aab57d92285291510c11c6203406a312438",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb4bd5e2ac6ba3f9443d7d7290197bbc4f7f2f51",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df1b5c91733b08002716383c6d970bec36dfbf46",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyj81f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mathsTeacher82",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752378438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was testing the [SmolLM3-3B-WebGPU](https://huggingface.co/spaces/HuggingFaceTB/SmolLM3-3B-WebGPU) Hugging Face Space to check its token speed on my machine (a solid 46 t/s!) before downloading and running it locally. When I prompted it with: \"Are you peter griffin?\", it just generated a 4000-token list of \"Key Takeaways\" about its existence:\n\nhttps://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0\n\nI was only able to trigger this behavior on that specific HF Space (Although, it doesn't seem to be a one time thing. I was able to get *very* similar responses by asking it the same question again in a new tab, after refreshing). I've since downloaded the model and wasn't able to replicate this locally. The model via the Hugging Face Inference also behaves as expected. Could this be caused by the ONNX conversion for WebGPU, or maybe some specific sampling parameters on the space? Has anyone seen anything like this?",
          "author_fullname": "t2_11ymelm88b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM-3B when asked if it was Peter Griffin",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "qn27gqdtqkcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c31923b903748465c2259725f23e079c2ea73a8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b441435486467eb144ac3a85148118bb880565bd"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=37bcaa29f3570585da99d0d25a9438e8459920fb"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ed1acc873492d4236adb25dd0e709e3df9377d"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3d8ad902650fb83dd6448352c83bae33a5753233"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1565304f9d2b0ba64264e8ed511d4be2b1b9dfe4"
                }
              ],
              "s": {
                "y": 4000,
                "x": 1080,
                "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0"
              },
              "id": "qn27gqdtqkcf1"
            }
          },
          "name": "t3_1lykqbu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=a7d62e9ff53b573cd757755fe5e699f00ac015e3",
          "edited": 1752384451,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752383565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was testing the &lt;a href=\"https://huggingface.co/spaces/HuggingFaceTB/SmolLM3-3B-WebGPU\"&gt;SmolLM3-3B-WebGPU&lt;/a&gt; Hugging Face Space to check its token speed on my machine (a solid 46 t/s!) before downloading and running it locally. When I prompted it with: &amp;quot;Are you peter griffin?&amp;quot;, it just generated a 4000-token list of &amp;quot;Key Takeaways&amp;quot; about its existence:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0\"&gt;https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was only able to trigger this behavior on that specific HF Space (Although, it doesn&amp;#39;t seem to be a one time thing. I was able to get &lt;em&gt;very&lt;/em&gt; similar responses by asking it the same question again in a new tab, after refreshing). I&amp;#39;ve since downloaded the model and wasn&amp;#39;t able to replicate this locally. The model via the Hugging Face Inference also behaves as expected. Could this be caused by the ONNX conversion for WebGPU, or maybe some specific sampling parameters on the space? Has anyone seen anything like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?auto=webp&amp;s=d140bfdc43765e123922c8ba668ec751fc4dc9d7",
                  "width": 1895,
                  "height": 1066
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=879705209dd5c5f6fab885d338990dee069409f5",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=112e2bc99d3df6e13bff48eee3d8bf43a406d375",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36f952335f74de0ad9ad33f45afb0c1187581bb7",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=74c41be70bfcf32292dc40a30f75326535854875",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=99b886056edb2c2a7133fe2596573b623ffe50da",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa87a49c5e5dfcadbe097b266f345195db9eec42",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lykqbu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Humble_Hovercraft199",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752383565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually [better to do RAG](https://www.databricks.com/blog/long-context-rag-performance-llms).\n\nWhy is that? Does the limit come from architecture or training data?\n\nI could see one problem being too much noise/distraction in the attention scores (like in [this paper](https://arxiv.org/pdf/2410.05258)).\n\nHowever, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.\n\nWhat is the consensus, and how long might it be until the problem is solved?",
          "author_fullname": "t2_1bvkixdlpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What Causes Poor Long-Context Performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lykf92",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752382484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually &lt;a href=\"https://www.databricks.com/blog/long-context-rag-performance-llms\"&gt;better to do RAG&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Why is that? Does the limit come from architecture or training data?&lt;/p&gt;\n\n&lt;p&gt;I could see one problem being too much noise/distraction in the attention scores (like in &lt;a href=\"https://arxiv.org/pdf/2410.05258\"&gt;this paper&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;However, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.&lt;/p&gt;\n\n&lt;p&gt;What is the consensus, and how long might it be until the problem is solved?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?auto=webp&amp;s=446351fd930a6a3c2288234f2b8553cdffe12e5b",
                  "width": 3047,
                  "height": 1600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=afbb01428ea71e1677bafb9d67ec3dca25236c74",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=66d49f2ee6b1a122d5ca0ad7a49746262dbd0110",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=78d808914c4c69536a9936bd8a7d16f7632e83e8",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=06d504ba6e45772c5e08586046db3251ad0af53b",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d3882037f7df9fd68eaf79f4448fa60bccbb01b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5854508c74dcb490f587749a0bee1621d5c15b2e",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lykf92",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "simulated-souls",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752382484,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.\n\nSource: @rasbt on X",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Interesting info about Kimi K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly42e5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 430,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 430,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3oX4rR_wq13aNVBkwN9gy-7Ly3awWuKmao4xX-wZHPw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752336334,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.&lt;/p&gt;\n\n&lt;p&gt;Source: @rasbt on X&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/klm2b78lvgcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?auto=webp&amp;s=d65bbcd2f22adb9cfb21adc9eac026b92732d6e6",
                  "width": 4096,
                  "height": 2142
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=de7eb96ece8068540bfea48d2417469c7f222dea",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3e2c8e32db02490d668ba0df54614ac60fe47",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1e27745898fe8ede1bbd082b7fec7ccae87d2bf",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32a0ebb795c06ba955385d6c0102e57e0fd85423",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c7c244ab2396650f24e6eb34e6826ebd524c6e5",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc382959f7974e6a25c0797c6abdfd2b739eef68",
                    "width": 1080,
                    "height": 564
                  }
                ],
                "variants": {},
                "id": "Mc3JulkX7jC-xZrG5vXyMeTKbsu1euUGH0q8C22y1zs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly42e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/",
          "stickied": false,
          "url": "https://i.redd.it/klm2b78lvgcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752336334,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This whole thing is giving me WizardLM2 vibes.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly8fyj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 185,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 185,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/0MvAUHXRbVLvw9fpxJeHJvJiKmKUs3UmwSpvxALQEM4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752347384,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/kn56m7cgshcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?auto=webp&amp;s=59a0a5113d7efda17e67eb417bc9bdfdc972845f",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d47fc551fde0f69d3b2c5b28f04df0cfdf9c410",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8510a3928b99b1f0d4c5fdba76a7b4a4ed90d447",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c547c00d3c5ded80b139847756e6f24778c292a4",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a11d5998e82e27b041a8e6dd74d76c55a2f8a104",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2df93661a1461d0901e12ea5b5b41cb79d2f94e3",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49489e01812662b0bc81aa38fd05c5b31a316b45",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "GepSeYli2R4WjSQ1YHbDEYhIkFkRkNdQpj4AK3DFoS4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ly8fyj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/",
          "stickied": false,
          "url": "https://i.redd.it/kn56m7cgshcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752347384,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Okay kimi-k2 is an INSANE model WTF those one-shot animations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly4zh8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 218,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/74d8efoh2hcf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1796,
              "scrubber_media_url": "https://v.redd.it/74d8efoh2hcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/74d8efoh2hcf1/DASHPlaylist.mpd?a=1755013029%2CNDg2YzQwZTM0NTU0NjJlNmJhYWRmNmM3NzAyZjgzZTcxNTEyMGIzNGM3ZWQ5NDQ2YmI3NTY5NDU4OTk5ODFhNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/74d8efoh2hcf1/HLSPlaylist.m3u8?a=1755013029%2CMDhkZmI5NDEwOGQ0YTgzZmMwNDdmNzMxZDlhMDVmOWY3MDQwNTY5ODk2YzBkMTAxNjMwNDJhOGEzMzZlMzNhYw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 218,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=140&amp;height=84&amp;crop=140:84,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=22934730e256af34e21d0c4f768ea8f376a8ffc8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752338690,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/74d8efoh2hcf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?format=pjpg&amp;auto=webp&amp;s=787b0533766b72327a39325971e8912f4bb1d0dd",
                  "width": 3600,
                  "height": 2166
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1b3a502b5cf8dd662aa70b308038b5980910d273",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=30f85b0338577881956c844b4be93f247975442b",
                    "width": 216,
                    "height": 129
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0bb1a95f277373f56d7d9edc672b02e5ad1d58cc",
                    "width": 320,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=173e65bd6cfe73a4442df7bc6c13aca66dcd0395",
                    "width": 640,
                    "height": 385
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d49c640223ead711b9bdfa961ec2ce1b91602d1f",
                    "width": 960,
                    "height": 577
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2642ff27b27bfc061a053f6d322fc7bb44dbf90f",
                    "width": 1080,
                    "height": 649
                  }
                ],
                "variants": {},
                "id": "amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly4zh8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/",
          "stickied": false,
          "url": "https://v.redd.it/74d8efoh2hcf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752338690,
          "num_crossposts": 2,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/74d8efoh2hcf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1796,
              "scrubber_media_url": "https://v.redd.it/74d8efoh2hcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/74d8efoh2hcf1/DASHPlaylist.mpd?a=1755013029%2CNDg2YzQwZTM0NTU0NjJlNmJhYWRmNmM3NzAyZjgzZTcxNTEyMGIzNGM3ZWQ5NDQ2YmI3NTY5NDU4OTk5ODFhNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/74d8efoh2hcf1/HLSPlaylist.m3u8?a=1755013029%2CMDhkZmI5NDEwOGQ0YTgzZmMwNDdmNzMxZDlhMDVmOWY3MDQwNTY5ODk2YzBkMTAxNjMwNDJhOGEzMzZlMzNhYw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While I am extremely grateful that people do post the leaked system prompt online for inspiration, but also curious how its actually possible?\n\nThere are three things that come to my mind:\n\n1. ***Using some prompt injection (re-iteratively)***: Some kind of jailbreak prompt and see if same things are being repeated, assuming that is what the actual system prompt is\n2. ***Inspecting the client side code if possible***: For applications intercepting the api requests / client side bundle to find system prompts if any? This sounds hard\n3. ***Changing the request server***: Maybe having a custom model running on my server and changing the base url for the request to hit my resource instead of the default one? Somehow getting the information from there?\n\nIf anyone has any idea how it works, would love to understand. If any resources to read would also be super helpful! Thanks!",
          "author_fullname": "t2_9oi573ps0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people actually able to get the system prompt of these AI companies?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyonb4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752398786,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While I am extremely grateful that people do post the leaked system prompt online for inspiration, but also curious how its actually possible?&lt;/p&gt;\n\n&lt;p&gt;There are three things that come to my mind:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Using some prompt injection (re-iteratively)&lt;/em&gt;&lt;/strong&gt;: Some kind of jailbreak prompt and see if same things are being repeated, assuming that is what the actual system prompt is&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Inspecting the client side code if possible&lt;/em&gt;&lt;/strong&gt;: For applications intercepting the api requests / client side bundle to find system prompts if any? This sounds hard&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Changing the request server&lt;/em&gt;&lt;/strong&gt;: Maybe having a custom model running on my server and changing the base url for the request to hit my resource instead of the default one? Somehow getting the information from there?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If anyone has any idea how it works, would love to understand. If any resources to read would also be super helpful! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyonb4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "divyamchandel",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752398786,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_2q7ua5a8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyozcn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=65fdf939c9fee2a51d4ce7ef5c5a36373a796f6a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752400163,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "muthuishere.medium.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?auto=webp&amp;s=ef577e27fadc95bfe3b1744d7ac8872e6e554c70",
                  "width": 610,
                  "height": 936
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc6872452b4829cdf316ac0294b8c4c189b660af",
                    "width": 108,
                    "height": 165
                  },
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=389d4bc4bbd718d16dd11308be232fde6ab93a01",
                    "width": 216,
                    "height": 331
                  },
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6b426f7ec6446f2ee47476b0c04d150cea9b8a5",
                    "width": 320,
                    "height": 491
                  }
                ],
                "variants": {},
                "id": "1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyozcn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "muthuishere2101",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/",
          "stickied": false,
          "url": "https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b",
          "subreddit_subscribers": 498343,
          "created_utc": 1752400163,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all,\n\nSoon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I've done... helps.\n\nSending code to an internet service is usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.\n\nI am looking at the p16s Gen 4 AMD  with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.\n\nThe first one will be more affordable than the second one...\n\nWould current big models run normally on a laptop like that P16s?\n\nThank you all in advance.",
          "author_fullname": "t2_albb0nfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for my next laptop soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lytioc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752415228,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Soon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I&amp;#39;ve done... helps.&lt;/p&gt;\n\n&lt;p&gt;Sending code to an internet service is usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.&lt;/p&gt;\n\n&lt;p&gt;I am looking at the p16s Gen 4 AMD  with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.&lt;/p&gt;\n\n&lt;p&gt;The first one will be more affordable than the second one...&lt;/p&gt;\n\n&lt;p&gt;Would current big models run normally on a laptop like that P16s?&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lytioc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robotecnik",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752415228,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever since there're code completions, I wish I could have something similar when texting people. Now there's finally a decent method for that.\n\nThe app works on any endpoint that's OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.\n\nI tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!\n\nHere's a brief guide to make this work with ollama:\n\n* Download the app from GitHub: [https://github.com/coreply/coreply](https://github.com/coreply/coreply)\n* Download `gemma3:4b-it-qat` in ollama\n* Set environment variable `OLLAMA_HOST` to [`0.0.0.0`](http://0.0.0.0) on the computer running ollama and restart ollama\n* In the Coreply app, set the API URL to `http://192.168.xxx.xxx:11434/v1/`(replace [`192.168.xxx.xxx`](http://192.168.xxx.xxx) with the IP address of the ollama machine), Model name `gemma3:4b-it-qat`\n* Grant permissions and turn on the app. Enjoy your texting suggestions!\n\nMy laptop isn't powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.\n\nLet me know how's your experience with it!",
          "author_fullname": "t2_l6eo8ggy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I use Gemma 3 to help me reply my texts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyv750",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755013029%2COGMyZWE0ZWZjYzU0MGFkZTY0N2QzNzExYmI2NzJhNzBiNTgxY2QzYmJlNmRlY2U5M2MxOWFiNjM4NTA2OTJjNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755013029%2CY2ZkODY4YTJhZWViYjkyMDQ3YmMzNjZmNjQ4OWUzMzdiZThlYzg0ZTljNmYzNDYxZjY3NGMyMTQxZTY0NTk1MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=140&amp;height=34&amp;crop=140:34,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4ea93f7d06620c65936cfa4562d1f7121ab5794f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752419560,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since there&amp;#39;re code completions, I wish I could have something similar when texting people. Now there&amp;#39;s finally a decent method for that.&lt;/p&gt;\n\n&lt;p&gt;The app works on any endpoint that&amp;#39;s OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.&lt;/p&gt;\n\n&lt;p&gt;I tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief guide to make this work with ollama:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Download the app from GitHub: &lt;a href=\"https://github.com/coreply/coreply\"&gt;https://github.com/coreply/coreply&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Download &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt; in ollama&lt;/li&gt;\n&lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt; to &lt;a href=\"http://0.0.0.0\"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; on the computer running ollama and restart ollama&lt;/li&gt;\n&lt;li&gt;In the Coreply app, set the API URL to &lt;code&gt;http://192.168.xxx.xxx:11434/v1/&lt;/code&gt;(replace &lt;a href=\"http://192.168.xxx.xxx\"&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/a&gt; with the IP address of the ollama machine), Model name &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Grant permissions and turn on the app. Enjoy your texting suggestions!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My laptop isn&amp;#39;t powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.&lt;/p&gt;\n\n&lt;p&gt;Let me know how&amp;#39;s your experience with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/48w6qb1mincf1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?format=pjpg&amp;auto=webp&amp;s=4315959609b1452ddac1019bc82dfcd0d2ecd932",
                  "width": 1080,
                  "height": 264
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca55479fe3e543c483ca9fd1a6e1c7663b1e1469",
                    "width": 108,
                    "height": 26
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bb9e1e7f65de33e3f9e6aac96c0c769244d7247e",
                    "width": 216,
                    "height": 52
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cb1d98d3e21b6fb43e522b5e2c12bfd0b0606cc7",
                    "width": 320,
                    "height": 78
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=150f56510dfbd5b6d84de28d6521c41bb3feede9",
                    "width": 640,
                    "height": 156
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0f618077403fd8aaf60fa19cfeb0ddd4cf30fc8d",
                    "width": 960,
                    "height": 234
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b18333794ad268d43549283f55cc34c53f54a5c2",
                    "width": 1080,
                    "height": 264
                  }
                ],
                "variants": {},
                "id": "NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyv750",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sean01-eth",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/",
          "stickied": false,
          "url": "https://v.redd.it/48w6qb1mincf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419560,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755013029%2COGMyZWE0ZWZjYzU0MGFkZTY0N2QzNzExYmI2NzJhNzBiNTgxY2QzYmJlNmRlY2U5M2MxOWFiNjM4NTA2OTJjNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755013029%2CY2ZkODY4YTJhZWViYjkyMDQ3YmMzNjZmNjQ4OWUzMzdiZThlYzg0ZTljNmYzNDYxZjY3NGMyMTQxZTY0NTk1MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I feel like everyday I come here someone mentions a a new tool or a newly released model or software that I never heard off. Where in earth are you going to get your most up to dated trusted news/info? ",
          "author_fullname": "t2_21pp8tew",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you keep up with all these things?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyfngg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752367147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like everyday I come here someone mentions a a new tool or a newly released model or software that I never heard off. Where in earth are you going to get your most up to dated trusted news/info? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyfngg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ontologicalmemes",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752367147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI'm building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.\n\nI've shortlisted Meta's LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic's model requirements . I'm planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.\n\nI did look at [https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix](https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix) but it is somewhat out of date now.\n\nI have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)\n\nLooking for help with:\n\n* Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?\n* Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?\n* Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?\n\nI have some constraints:\n\n* Must be air-gapped\n* I can't use Chinese, Israeli or similar products. CISO doesn't allow it. I know some of the Chinese models would be a good fit, but its a no-go.\n* Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure\n\nWould love to hear from anyone who’s done this in production or lab.\n\nThanks in advance!\n\n",
          "author_fullname": "t2_s8xklsb6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM to back Elastic AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq22j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve shortlisted Meta&amp;#39;s LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic&amp;#39;s model requirements . I&amp;#39;m planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.&lt;/p&gt;\n\n&lt;p&gt;I did look at &lt;a href=\"https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix\"&gt;https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix&lt;/a&gt; but it is somewhat out of date now.&lt;/p&gt;\n\n&lt;p&gt;I have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)&lt;/p&gt;\n\n&lt;p&gt;Looking for help with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?&lt;/li&gt;\n&lt;li&gt;Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?&lt;/li&gt;\n&lt;li&gt;Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have some constraints:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be air-gapped&lt;/li&gt;\n&lt;li&gt;I can&amp;#39;t use Chinese, Israeli or similar products. CISO doesn&amp;#39;t allow it. I know some of the Chinese models would be a good fit, but its a no-go.&lt;/li&gt;\n&lt;li&gt;Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear from anyone who’s done this in production or lab.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?auto=webp&amp;s=9a1b4684102bb8c94296cbfa71ad3a31d0c0f257",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7dcc983e13bd8aed1654a54d718d49f54cdaae",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6db7b95b72b6ba957c849b5433b50158cc281a2e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=54ac1a61a829459657d4fbc629848f4a7b86377b",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19ae5d46a63ca2b66411c4548a6cff1870e42360",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5dbba47c48a53ff43f7f01db90cc69cd595f0e8",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=994bf868b62aa1188f41e6e7f154a8365d35c7fe",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyq22j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OldManCyberNinja",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752404403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone!\n\nWanted to ask a question that's been on my mind recently.\n\nI've done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.\n\nBut how is LLM evaluation done in real life (i.e. in industry)? If I'm a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it's doing a good job?\n\nIs it only product related metrics like customer satisfaction and existing benchmarks like in the industry? ",
          "author_fullname": "t2_owqzu42m8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM evaluation in real life?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq1yh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;Wanted to ask a question that&amp;#39;s been on my mind recently.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.&lt;/p&gt;\n\n&lt;p&gt;But how is LLM evaluation done in real life (i.e. in industry)? If I&amp;#39;m a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it&amp;#39;s doing a good job?&lt;/p&gt;\n\n&lt;p&gt;Is it only product related metrics like customer satisfaction and existing benchmarks like in the industry? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyq1yh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Plastic-Bus-7003",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752404393,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. \n\nI had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. \n\nI prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Let’s talk about models you believed are more Hyped than Hot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyvkhr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752420473,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. &lt;/p&gt;\n\n&lt;p&gt;I had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. &lt;/p&gt;\n\n&lt;p&gt;I prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyvkhr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752420473,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.\\]\n\nLet's say you have a local TTS model that takes a speaker embedding `spk_emb`, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.\n\nHere is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.\n\n    import torch\n    # 1. Initialize the embedding, either randomly or nearest neighbor\n    spk_emb = torch.randn(1, 512) # if batch size 1, dim 512\n    spk_emb.requires_grad = True\n    # 2. Initialize the model and freeze its parameters\n    model = YourModelClass.from_pretrained('TODO')\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device).eval()\n    for p in model.parameters():\n        p.requires_grad = False\n    # 3. Optimizer and dataset, LR is up to you\n    optimizer = torch.optim.Adam([spk_emb], lr=0.001)\n    TODO_your_dataset_of_text_audio_pairs = [\n    ('This is some text.', 'corresponding_audio.wav'),\n    # ...\n    ]\n    # 4. Barebones training loop. You can add a learning rate scheduler, etc.\n    for epoch in range(10): # how many epochs is up to you\n        for text, audio in TODO_your_dataset_of_text_audio_pairs:\n            loss = model.forward_with_loss(text, audio, spk_emb)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\nThe big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.\n\nAnd that's it. If you have any questions you can post them below.",
          "author_fullname": "t2_1e2jjp1mqg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dark Arts: Speaker embedding gradient descent for local TTS models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lymlgp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752392740,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752390519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.]&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you have a local TTS model that takes a speaker embedding &lt;code&gt;spk_emb&lt;/code&gt;, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.&lt;/p&gt;\n\n&lt;p&gt;Here is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import torch\n# 1. Initialize the embedding, either randomly or nearest neighbor\nspk_emb = torch.randn(1, 512) # if batch size 1, dim 512\nspk_emb.requires_grad = True\n# 2. Initialize the model and freeze its parameters\nmodel = YourModelClass.from_pretrained(&amp;#39;TODO&amp;#39;)\ndevice = &amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;\nmodel.to(device).eval()\nfor p in model.parameters():\n    p.requires_grad = False\n# 3. Optimizer and dataset, LR is up to you\noptimizer = torch.optim.Adam([spk_emb], lr=0.001)\nTODO_your_dataset_of_text_audio_pairs = [\n(&amp;#39;This is some text.&amp;#39;, &amp;#39;corresponding_audio.wav&amp;#39;),\n# ...\n]\n# 4. Barebones training loop. You can add a learning rate scheduler, etc.\nfor epoch in range(10): # how many epochs is up to you\n    for text, audio in TODO_your_dataset_of_text_audio_pairs:\n        loss = model.forward_with_loss(text, audio, spk_emb)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.&lt;/p&gt;\n\n&lt;p&gt;And that&amp;#39;s it. If you have any questions you can post them below.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lymlgp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rzvzn",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752390519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kyutai is one of the best text to speech models, with very low latency, real-time \"text streaming to audio\" generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it's able to generate very long audio files.\n\nIt's [one of the chart leaders in benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/).\n\nBut it's completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.\n\n---\n\nNow they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:\n\n# [https://github.com/kyutai-labs/delayed-streams-modeling/issues/64](https://github.com/kyutai-labs/delayed-streams-modeling/issues/64)",
          "author_fullname": "t2_4a13s1mr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kyutai Text-to-Speech is considering opening up custom voice model training, but they are asking for community support!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly6cg6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 83,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 83,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752342641,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752342109,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kyutai is one of the best text to speech models, with very low latency, real-time &amp;quot;text streaming to audio&amp;quot; generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it&amp;#39;s able to generate very long audio files.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/\"&gt;one of the chart leaders in benchmarks&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Now they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://github.com/kyutai-labs/delayed-streams-modeling/issues/64\"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/issues/64&lt;/a&gt;&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?auto=webp&amp;s=bb0d78e71acc290feb3be820e0ac3817a8b57580",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=367ade6726e17c3965008974bc2f09c6e6bc8ba5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=96ca6419efae923e4873042319bfe37633d90d02",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc4a96547220f829be62cf5452397d01f6b35fa2",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=52dd6c876880a544fb8f6932f59210dca6e626a0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=381b0108d07c03c8894ff27591994d720b9a28a0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5977dd10040b49d6bd075f3f3ecfe7d8d4a969d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1ly6cg6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pilkyton",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752342109,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m working on a browser automation system that follows a planned sequence of UI actions, but needs an LLM to resolve which DOM element to click when there are multiple similar options. I’ve been using **Browser-Use**, which is solid for tracking state/actions, but execution is too slow — especially when an LLM is in the loop at each step.\n\n**Example flow (on Google settings):**\n\n1. Go to [myaccount.google.com](https://myaccount.google.com)\n2. Click “Data &amp; privacy”\n3. Scroll down\n4. Click “Delete a service or your account”\n5. Click “Delete your Google Account”\n\n**Looking for suggestions:**\n\n* Fastest models for small structured decision tasks \n* Ways to be under 1s per step (ideally &lt;500ms)\n\nI don’t need full chat reasoning — just high-confidence decisions from small JSON lists.\n\nWould love to hear what setups/models have worked for you in similar low-latency UI agent tasks 🙏",
          "author_fullname": "t2_8fuwhsmu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Help] Fastest model for real-time UI automation? (Browser-Use too slow)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyjgwv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752379254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m working on a browser automation system that follows a planned sequence of UI actions, but needs an LLM to resolve which DOM element to click when there are multiple similar options. I’ve been using &lt;strong&gt;Browser-Use&lt;/strong&gt;, which is solid for tracking state/actions, but execution is too slow — especially when an LLM is in the loop at each step.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Example flow (on Google settings):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to &lt;a href=\"https://myaccount.google.com\"&gt;myaccount.google.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Click “Data &amp;amp; privacy”&lt;/li&gt;\n&lt;li&gt;Scroll down&lt;/li&gt;\n&lt;li&gt;Click “Delete a service or your account”&lt;/li&gt;\n&lt;li&gt;Click “Delete your Google Account”&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Looking for suggestions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fastest models for small structured decision tasks &lt;/li&gt;\n&lt;li&gt;Ways to be under 1s per step (ideally &amp;lt;500ms)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I don’t need full chat reasoning — just high-confidence decisions from small JSON lists.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear what setups/models have worked for you in similar low-latency UI agent tasks 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyjgwv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BulkyAd7044",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752379254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am wondering if anyone did this before, at least I couldn't find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? ",
          "author_fullname": "t2_iaby02kl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you make Loras for Qwen coder /  devstral?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyl697",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752385140,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering if anyone did this before, at least I couldn&amp;#39;t find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lyl697",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ComprehensiveBird317",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752385140,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_2kndo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mlx-community/Kimi-Dev-72B-4bit-DWQ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly894z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=72d2b357e724e0f945e73129c5d84c2c90c3cea5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752346900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?auto=webp&amp;s=c64aab1aed9e2d49380cc716994670f2d827840d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e7e74ad5490178118f9e4b34721c8061ae3ecfe",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df80ea7eb2a7c2b5c2ce50ed1283cab52fe4dd03",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64920f558efe0645329b540325846e2773ccb0ea",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d6276c1fe0578c79ffa2210b8dfa820b87e4242",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ae2c71d5c9d82b6adcd1de3f6d1f56ecae619a7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3ec1f50878546ff7d199e6e418dcaae525ef63a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ly894z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Recoil42",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/",
          "stickied": false,
          "url": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ",
          "subreddit_subscribers": 498343,
          "created_utc": 1752346900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone has tried something like that? I just put: create a google chrome extension that blocks websites. it's just something that takes a list of websites and blocks them. The extension does not work in both codes provided by the LLMs.",
          "author_fullname": "t2_cihv7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Testing ChatGPT and Claude capabilities to \"simple projects\": Block Site extension for Google Chrome",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyv7s7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419608,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone has tried something like that? I just put: create a google chrome extension that blocks websites. it&amp;#39;s just something that takes a list of websites and blocks them. The extension does not work in both codes provided by the LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyv7s7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "helioscarbex",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv7s7/testing_chatgpt_and_claude_capabilities_to_simple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyv7s7/testing_chatgpt_and_claude_capabilities_to_simple/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419608,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, noobie here.\n\nI am using OBS and there is a plugin called 'localvocal'.  \nI can choose there several LLMs etc.  \nWhich one should be the best for my use case? How can I add other LLMs from huggingface?\n\nAny help is appreciated, thank you!",
          "author_fullname": "t2_7mkjq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM model for live translation into subtitles [RU-EN]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyv5uc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, noobie here.&lt;/p&gt;\n\n&lt;p&gt;I am using OBS and there is a plugin called &amp;#39;localvocal&amp;#39;.&lt;br/&gt;\nI can choose there several LLMs etc.&lt;br/&gt;\nWhich one should be the best for my use case? How can I add other LLMs from huggingface?&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated, thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyv5uc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TuGuX",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv5uc/llm_model_for_live_translation_into_subtitles_ruen/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyv5uc/llm_model_for_live_translation_into_subtitles_ruen/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419469,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Safety first, or whatever🙄",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxycdh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 159,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 159,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Zzym4XXD6JbiVex-1mXTA-GS_8h0f2iI0X4SmG7Ksy8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752320256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/idk5uvesjfcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?auto=webp&amp;s=6d69525514eee5345b46dc8b914fc4c1b26be574",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cc3224e9e3248f9795321cf57fc214ae879cd51",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12b0b288f01c6a23a110d4522f182ac091e2ed48",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7b0701a1ed34c1b6bf8d8a74218fff22b70f96a",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=326cdedce8274c918a8336924d8741c3576c2f5a",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=17aff077f82095d1ab7a525405702f4326c4bb85",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b65e41b0a6339e460b04ff0532103f867ef5a74",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "Id2wk8n7gqUJk-T46y2nf1kFJ57mzqGvG8Wuh0Ws0XU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lxycdh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/",
          "stickied": false,
          "url": "https://i.redd.it/idk5uvesjfcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been building an AI assistant (Caelum) that can control a system using natural language, but I didn’t want it running raw shell commands or hallucinating `subprocess` calls. That’s unreliable and messy, so I built a structured `do()` system with plugin routing, safety flags, and argument parsing. Each command is a plugin, and you can write one in like 10–15 lines of code. Plugins auto-register and are isolated, so there’s no hardcoded logic or brittle wrappers.\n\nRight now it supports 39 commands, all modular, and you can interact with it using structured phrases or natural language if you add a mapping layer. It’s async-friendly, works with local agents, and is designed to grow without becoming a spaghetti monster.\n\nI originally posted this in another thread and realized quickly that it was the wrong crowd. This isn’t a CLI enhancement. It’s a system automation backbone that gives LLMs a safe, predictable way to control the OS through plugins, not shell access.\n\nIf you’re working on local agents or LLM-powered tools and want something that bridges into actual system control without chaos, I’d be happy to talk more about how it works. \n\n[https://github.com/BlackBeardJW/caelum-sys](https://github.com/BlackBeardJW/caelum-sys)  \n[https://pypi.org/project/caelum-sys/](https://pypi.org/project/caelum-sys/)",
          "author_fullname": "t2_dhf7dmo0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a plugin-based system automation layer for LLMs, safe, modular, and dead simple to extend",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyuxj5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752418915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been building an AI assistant (Caelum) that can control a system using natural language, but I didn’t want it running raw shell commands or hallucinating &lt;code&gt;subprocess&lt;/code&gt; calls. That’s unreliable and messy, so I built a structured &lt;code&gt;do()&lt;/code&gt; system with plugin routing, safety flags, and argument parsing. Each command is a plugin, and you can write one in like 10–15 lines of code. Plugins auto-register and are isolated, so there’s no hardcoded logic or brittle wrappers.&lt;/p&gt;\n\n&lt;p&gt;Right now it supports 39 commands, all modular, and you can interact with it using structured phrases or natural language if you add a mapping layer. It’s async-friendly, works with local agents, and is designed to grow without becoming a spaghetti monster.&lt;/p&gt;\n\n&lt;p&gt;I originally posted this in another thread and realized quickly that it was the wrong crowd. This isn’t a CLI enhancement. It’s a system automation backbone that gives LLMs a safe, predictable way to control the OS through plugins, not shell access.&lt;/p&gt;\n\n&lt;p&gt;If you’re working on local agents or LLM-powered tools and want something that bridges into actual system control without chaos, I’d be happy to talk more about how it works. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/BlackBeardJW/caelum-sys\"&gt;https://github.com/BlackBeardJW/caelum-sys&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://pypi.org/project/caelum-sys/\"&gt;https://pypi.org/project/caelum-sys/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?auto=webp&amp;s=2a2a988a09da5bc5ad4f13e5a79bd3559c0d9808",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=603529e7fcabe20144806792dd5e7c2476b13cfe",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64e3526af339fa9467fd5d5e7d75032005389d24",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb3942c412a45a44ec1098ffb7f97bab33ea85e4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=417f48a9f26f6fdf936c7f469c3797e861d17912",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=de6ca03a4ece691622a59b07094afb40147cfb4a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4bd124f47d49ba62d35cdffc6ded4e2d57584d7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyuxj5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BlackBeardJW",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyuxj5/built_a_pluginbased_system_automation_layer_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyuxj5/built_a_pluginbased_system_automation_layer_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752418915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tried vertex.. 35 tps\n\nHuggingFace\nwith q6 from unsloth 48 tps\noriginal from Google 35 tps\n\nI need 100tps.. please help\n\nI know not much about inference infrastructure.\n",
          "author_fullname": "t2_86t4cp3p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed for MedGemma 27B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq7mc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404980,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried vertex.. 35 tps&lt;/p&gt;\n\n&lt;p&gt;HuggingFace\nwith q6 from unsloth 48 tps\noriginal from Google 35 tps&lt;/p&gt;\n\n&lt;p&gt;I need 100tps.. please help&lt;/p&gt;\n\n&lt;p&gt;I know not much about inference infrastructure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyq7mc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FewOwl9332",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq7mc/help_needed_for_medgemma_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq7mc/help_needed_for_medgemma_27b/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752404980,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In time-honored tradition we present the relative physical dimensions of the Workstation Pro 6000.",
          "author_fullname": "t2_1t7r9dkpud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Banana for scale",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 91,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyb8tz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jCng4TuAvwrX1AV7UemR4VzBuvP3-H0aGZ6F9oKJ7rk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752354670,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In time-honored tradition we present the relative physical dimensions of the Workstation Pro 6000.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3gsbxg74eicf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?auto=webp&amp;s=5f8a046e9b08931daa9153d62b9cae47c75a77a9",
                  "width": 3962,
                  "height": 2580
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4cff1cf527148cb67e28ceb515a52bec33ece4d",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3802e417956d3284405dd8858361c8090a9d021e",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=659728b96cbf1ffe05578dd8f8f8dd3dedd76ecf",
                    "width": 320,
                    "height": 208
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4f149ec7379ca441b40ea5b4c75ffb6609f7405",
                    "width": 640,
                    "height": 416
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=31f73ee59fb986766a53e5cdc594d55c3f686c35",
                    "width": 960,
                    "height": 625
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8679591edcc69b07083d11b0c446de23eccc8a09",
                    "width": 1080,
                    "height": 703
                  }
                ],
                "variants": {},
                "id": "mDdP7oD7tzwjvVGCKQ4KVJzRxTtVIJCTqyz3vcP2noQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyb8tz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackwell_tart",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/",
          "stickied": false,
          "url": "https://i.redd.it/3gsbxg74eicf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752354670,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_dq1a6l1h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI delays its open weight model again for \"safety tests\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxnsh1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 909,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 909,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/bDm8WdOvFeT3ztbG-s9FAJBH8gJSyrGjpMIMZaT6MiY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752282578,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/z5xvjxzefccf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/z5xvjxzefccf1.png?auto=webp&amp;s=f6c9ebda1f303365e6770239572b7b70d3971ce0",
                  "width": 664,
                  "height": 369
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/z5xvjxzefccf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=de0e9d503cf6aeab73ce34032122c88cd39b8b53",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/z5xvjxzefccf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70447df947bb5e0e03f8958244a1cdfdfa4b61d5",
                    "width": 216,
                    "height": 120
                  },
                  {
                    "url": "https://preview.redd.it/z5xvjxzefccf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=68f15fa77701a2e1f5a30182a8351692362c2952",
                    "width": 320,
                    "height": 177
                  },
                  {
                    "url": "https://preview.redd.it/z5xvjxzefccf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe88bccce70567bd39edea238607127c143134db",
                    "width": 640,
                    "height": 355
                  }
                ],
                "variants": {},
                "id": "zhTzssGOq0yIAw5b9wZmMs9jd1RS2cE7t1uG0nxExgQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lxnsh1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lyceras",
          "discussion_type": null,
          "num_comments": 240,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/",
          "stickied": false,
          "url": "https://i.redd.it/z5xvjxzefccf1.png",
          "subreddit_subscribers": 498343,
          "created_utc": 1752282578,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys,\n\nI'm working on a project for multiple speakers, and was wondering what is the most natural sounding TTS model right now?\n\nI saw XTTS and ChatTTS, but those have been around for a while. Is there anything new that's local that sounds pretty good?\n\nThanks!",
          "author_fullname": "t2_86i4l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the most natural sounding TTS model for local right now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly5g2t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752339858,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a project for multiple speakers, and was wondering what is the most natural sounding TTS model right now?&lt;/p&gt;\n\n&lt;p&gt;I saw XTTS and ChatTTS, but those have been around for a while. Is there anything new that&amp;#39;s local that sounds pretty good?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly5g2t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Siigari",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752339858,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD's GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?\n",
          "author_fullname": "t2_9wlqkcl5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can I figure out the speed in tokens per second that my model will run on the CPU?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyt372",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752414028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD&amp;#39;s GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyt372",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Holiday-Picture6796",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752414028,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where that Unsloth Q0.01_K_M GGUF at?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxpidc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 608,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 608,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EDvGpwrJbGoDqgFUlnrOG6u2rjob-WEc_cUroT4VetA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752287825,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/e2em6rucvccf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/e2em6rucvccf1.jpeg?auto=webp&amp;s=b31ab99ee8084bc03a9a32ab253760eedf04ab2a",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/e2em6rucvccf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d086bd0581bd67e0ab1809820331054699c24205",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/e2em6rucvccf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53552ee8da77fc5558dccbff543e2ce95db3a1ec",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/e2em6rucvccf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=119de6485072ce8c932511ac9212b2c54226d3c3",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/e2em6rucvccf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4380544f532ff369f435679247aa08f3c9afdb66",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/e2em6rucvccf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d80310d09a7861b28e80d1931667a5854f946377",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/e2em6rucvccf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=069a5b690eb18b3ada98a1195295e98d4876661a",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "KiVxcTo7hz7MHstS7c0wdFc8090G1XP_159_wJ9dN2U"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lxpidc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/",
          "stickied": false,
          "url": "https://i.redd.it/e2em6rucvccf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752287825,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.\n\nAs per the title, I have attempted to run Qwen3-235B-A22B using \\`llama-server\\` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?\n\nMachine specification are:\n\n    CPU: i3-12100F (12th Gen Intel)\n    RAM: 128GB (4\\*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\n    Motherboard: MSI PRO B660M-A WIFI DDR4\n    GPU: GeForce RTX 3090 24GB VRAM\n\n(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)\n\n`llama-server` launch options:\n\n    llama-server \\\n      --host 0.0.0.0 \\\n      --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n      --ctx-size 16384 \\\n      --n-gpu-layers 99 \\\n      --flash-attn \\\n      --threads 3 \\\n      -ot \"exps=CPU\" \\\n      --seed 3407 \\\n      --prio 3 \\\n      --temp 0.6 \\\n      --min-p 0.0 \\\n      --top-p 0.95 \\\n      --top-k 20 \\\n      --no-mmap \\\n      --no-warmup \\\n      --mlock\n\nAny advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)",
          "author_fullname": "t2_tfa1mcp1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B @ 0.7t/s. Hardware or configuration bottleneck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lysmo9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752412727,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.&lt;/p&gt;\n\n&lt;p&gt;As per the title, I have attempted to run Qwen3-235B-A22B using `llama-server` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?&lt;/p&gt;\n\n&lt;p&gt;Machine specification are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CPU: i3-12100F (12th Gen Intel)\nRAM: 128GB (4\\*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\nMotherboard: MSI PRO B660M-A WIFI DDR4\nGPU: GeForce RTX 3090 24GB VRAM\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; launch options:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server \\\n  --host 0.0.0.0 \\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n  --ctx-size 16384 \\\n  --n-gpu-layers 99 \\\n  --flash-attn \\\n  --threads 3 \\\n  -ot &amp;quot;exps=CPU&amp;quot; \\\n  --seed 3407 \\\n  --prio 3 \\\n  --temp 0.6 \\\n  --min-p 0.0 \\\n  --top-p 0.95 \\\n  --top-k 20 \\\n  --no-mmap \\\n  --no-warmup \\\n  --mlock\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Any advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lysmo9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ConnectionOutside485",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752412727,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I come across many v100 32g gpus, ecc all intact for $360 on chinese second hand market (I live in China) and can easily get stuff like bifurcated 300G nvlink sxm2 to pcie adapters etc. for no more than $40. \n\nAlso, if I get the 16gb version of the v100, it only costs $80 per card. \n\nWouldn't this be a better deal than something like a 4060ti or even 3090s (if I get 3 32gb v100s) for LLMs?",
          "author_fullname": "t2_1cqq3fards",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "32g SXM2 V100s for $360, Good Deal for LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyiyvq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752377595,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come across many v100 32g gpus, ecc all intact for $360 on chinese second hand market (I live in China) and can easily get stuff like bifurcated 300G nvlink sxm2 to pcie adapters etc. for no more than $40. &lt;/p&gt;\n\n&lt;p&gt;Also, if I get the 16gb version of the v100, it only costs $80 per card. &lt;/p&gt;\n\n&lt;p&gt;Wouldn&amp;#39;t this be a better deal than something like a 4060ti or even 3090s (if I get 3 32gb v100s) for LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyiyvq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "starikari",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyiyvq/32g_sxm2_v100s_for_360_good_deal_for_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyiyvq/32g_sxm2_v100s_for_360_good_deal_for_llms/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752377595,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nHi LocalLLaMA community,\n\nI am a researcher, and recently I have noticed that LLMs such as OpenAI's and Google's are not good at generating academic-style and/or beautiful plots. Open sourced model also doesn’t work well. Beyond the simple plots which they can do just fine, anything more advanced that includes LaTex tikz library etc, will simply just fail.\n\nHas anyone encounter similar issues? If so, any suggestions or recommendations on this? Thank you so much!\n\nTL;DR: Trying to use LLMs to generate academic-style plots but they are not good at all.",
          "author_fullname": "t2_kb2yczc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any suggestions for generating academic-style/advanced plots?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyitq9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752377125,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LocalLLaMA community,&lt;/p&gt;\n\n&lt;p&gt;I am a researcher, and recently I have noticed that LLMs such as OpenAI&amp;#39;s and Google&amp;#39;s are not good at generating academic-style and/or beautiful plots. Open sourced model also doesn’t work well. Beyond the simple plots which they can do just fine, anything more advanced that includes LaTex tikz library etc, will simply just fail.&lt;/p&gt;\n\n&lt;p&gt;Has anyone encounter similar issues? If so, any suggestions or recommendations on this? Thank you so much!&lt;/p&gt;\n\n&lt;p&gt;TL;DR: Trying to use LLMs to generate academic-style plots but they are not good at all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyitq9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "plsendfast",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752377125,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all!  \nI've just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.\n\n* **Educational:** Core algorithms are reimplemented from scratch for learning and transparency.\n* **CLI tools:** Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)\n* **Modular:** Clean separation between export, inference, and CLI.\n* **Safety:** Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)\n* **Future plans:** I would be curious to see how to extend it to support:\n   * fine-tuning of a small models\n   * optimize inference performance (e.g. matmul operations)\n   * WASM build to run inference in a browser\n\nBasically, I used [qwen3.c](https://github.com/adriancable/qwen3.c) as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.\n\nGitHub: [https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)",
          "author_fullname": "t2_fkwrm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Rust] qwen3-rs: Educational Qwen3 Architecture Inference (No Python, Minimal Deps)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly7sb0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752345715,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all!&lt;br/&gt;\nI&amp;#39;ve just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Educational:&lt;/strong&gt; Core algorithms are reimplemented from scratch for learning and transparency.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CLI tools:&lt;/strong&gt; Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Modular:&lt;/strong&gt; Clean separation between export, inference, and CLI.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Safety:&lt;/strong&gt; Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Future plans:&lt;/strong&gt; I would be curious to see how to extend it to support:\n\n&lt;ul&gt;\n&lt;li&gt;fine-tuning of a small models&lt;/li&gt;\n&lt;li&gt;optimize inference performance (e.g. matmul operations)&lt;/li&gt;\n&lt;li&gt;WASM build to run inference in a browser&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Basically, I used &lt;a href=\"https://github.com/adriancable/qwen3.c\"&gt;qwen3.c&lt;/a&gt; as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.&lt;/p&gt;\n\n&lt;p&gt;GitHub: [&lt;a href=\"https://github.com/reinterpretcat/qwen3-rs%5D(vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)\"&gt;https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?auto=webp&amp;s=f64a6eef9fb25bb8dece4a00b49169cb6de85df2",
                  "width": 640,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6a4a1ab699ce9984d57b0696bdd1f873de9e614",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80ccbeb83c907fb5b897374c139c51e76825ec00",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5ad415a9157f412849b8def8bc5c576f5d41217",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ac79d8600937790d6301fdd4917b87eabf6336a",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ly7sb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eis_kalt",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752345715,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So i finally have my LM studio hosting my Models and have AnythingLLM doing my RAG ,  soi thought i would extend to agents ,,, look at Youtube , but nothing is working , its constantly saying that \"I currently **don’t have direct web browsing capabilitie\", what am i doing wrong ?** \n\nhttps://preview.redd.it/1soone6wrlcf1.png?width=931&amp;format=png&amp;auto=webp&amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667\n\nhttps://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;format=png&amp;auto=webp&amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99\n\n",
          "author_fullname": "t2_14166b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need Help with Agents and AnythingLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "1soone6wrlcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87daa798ce437071bb40eaaf62b399dc934508d9"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd84be40eb0b5b3ab77ece5cd4d4b7d8fab1aaee"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=95e37d071beb8b835a3ec3919b6c90d72ccc2e38"
                },
                {
                  "y": 656,
                  "x": 640,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e84b7ab1f4e5bea326dc353d22f6d75c490e637"
                }
              ],
              "s": {
                "y": 955,
                "x": 931,
                "u": "https://preview.redd.it/1soone6wrlcf1.png?width=931&amp;format=png&amp;auto=webp&amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667"
              },
              "id": "1soone6wrlcf1"
            },
            "a7kj8x8zrlcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0787a02d775b147571850bba9707437fb31fb99c"
                },
                {
                  "y": 220,
                  "x": 216,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6b6d6c31c7fcd66248bc4f4ea5bf49a438764d60"
                },
                {
                  "y": 326,
                  "x": 320,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=717f5ba61fe04c1e4e83e28883ef60482fd5cbe9"
                },
                {
                  "y": 653,
                  "x": 640,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e5585085769a63ad7ab9923808f9ec1a9706193"
                }
              ],
              "s": {
                "y": 836,
                "x": 819,
                "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;format=png&amp;auto=webp&amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99"
              },
              "id": "a7kj8x8zrlcf1"
            }
          },
          "name": "t3_1lynwk4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/loJ7ObJSRfe1SFeOI0xzI0YfjSwu4U7qV205hbtTOkE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752395690,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i finally have my LM studio hosting my Models and have AnythingLLM doing my RAG ,  soi thought i would extend to agents ,,, look at Youtube , but nothing is working , its constantly saying that &amp;quot;I currently &lt;strong&gt;don’t have direct web browsing capabilitie&amp;quot;, what am i doing wrong ?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1soone6wrlcf1.png?width=931&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667\"&gt;https://preview.redd.it/1soone6wrlcf1.png?width=931&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99\"&gt;https://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lynwk4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uber-linny",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lynwk4/need_help_with_agents_and_anythingllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lynwk4/need_help_with_agents_and_anythingllm/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752395690,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Maybe it'd be more of an LLM tool designed for book writing than the other way around but I'm looking for software that can utilize a locally running LLM to help me write a book.\n\nHoping for something where I can include descriptions of characters, set the scenes, basic outline and such. Then let the LLM do the bulk of the work.\n\nDoes this sort of thing exist?",
          "author_fullname": "t2_6fu5vgz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any book writing software that can utilize an local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyhnhw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752373360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe it&amp;#39;d be more of an LLM tool designed for book writing than the other way around but I&amp;#39;m looking for software that can utilize a locally running LLM to help me write a book.&lt;/p&gt;\n\n&lt;p&gt;Hoping for something where I can include descriptions of characters, set the scenes, basic outline and such. Then let the LLM do the bulk of the work.&lt;/p&gt;\n\n&lt;p&gt;Does this sort of thing exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyhnhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "123android",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyhnhw/is_there_any_book_writing_software_that_can/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyhnhw/is_there_any_book_writing_software_that_can/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752373360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Medical triage means determining whether symptoms require emergency care, urgent care, or can be managed with self-care. This matters because LLMs are increasingly becoming the \"digital front door\" for health concerns—replacing the instinct to just Google it.\n\nGetting triage wrong can be dangerous (missed emergencies) or costly (unnecessary ER visits).\n\nWe've open-sourced **TriageBench**, a reproducible framework for evaluating LLM triage accuracy. It includes:\n\n* Standard clinical dataset (Semigran vignettes)\n* Paired McNemar's test to detect model performance differences on small datasets\n* Full methodology and evaluation code\n\nGitHub: [https://github.com/medaks/medask-benchmark](https://github.com/medaks/medask-benchmark)\n\nAs a demonstration, we benchmarked our own model (MedAsk) against several OpenAI models:\n\n* MedAsk: **87.6% accuracy**\n* o3: **75.6%**\n* GPT‑4.5: **68.9%**\n\nThe main limitation is dataset size (45 vignettes). We're looking for collaborators to help expand this—the field needs larger, more diverse clinical datasets.\n\nBlog post with full results: [https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/](https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/)",
          "author_fullname": "t2_lp2ivten",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We built an open-source medical triage benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxw3zz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 113,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 113,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752311546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Medical triage means determining whether symptoms require emergency care, urgent care, or can be managed with self-care. This matters because LLMs are increasingly becoming the &amp;quot;digital front door&amp;quot; for health concerns—replacing the instinct to just Google it.&lt;/p&gt;\n\n&lt;p&gt;Getting triage wrong can be dangerous (missed emergencies) or costly (unnecessary ER visits).&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve open-sourced &lt;strong&gt;TriageBench&lt;/strong&gt;, a reproducible framework for evaluating LLM triage accuracy. It includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Standard clinical dataset (Semigran vignettes)&lt;/li&gt;\n&lt;li&gt;Paired McNemar&amp;#39;s test to detect model performance differences on small datasets&lt;/li&gt;\n&lt;li&gt;Full methodology and evaluation code&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/medaks/medask-benchmark\"&gt;https://github.com/medaks/medask-benchmark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a demonstration, we benchmarked our own model (MedAsk) against several OpenAI models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MedAsk: &lt;strong&gt;87.6% accuracy&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;o3: &lt;strong&gt;75.6%&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;GPT‑4.5: &lt;strong&gt;68.9%&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The main limitation is dataset size (45 vignettes). We&amp;#39;re looking for collaborators to help expand this—the field needs larger, more diverse clinical datasets.&lt;/p&gt;\n\n&lt;p&gt;Blog post with full results: &lt;a href=\"https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/\"&gt;https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?auto=webp&amp;s=41ebf4298905eb26f4cef7c264a930eaa2aa2c5c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9c7e10a1a4f6aeffdd4ad9ec00fba71b13e9850",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=12ae853d79e9684e4fdd32bea3af05e334b73b38",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b194f182a8f6d7fa069cdedc5685ff5d9aecc59",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=861e796a8d8ac8b7c2b73c785d2751b59de40d1c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f068abe4f3993070d3e9aefaedd824a71bb88c7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39b1861c4e75006d81e7f80388e64198bfc249fa",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lxw3zz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Significant-Pair-275",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752311546,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone who actually codes with local LLM on their laptops, what's your setup and are you happy with the quality and speed?  Should I even bother trying to code with an LLM that fits on a laptop GPU, or just tether back to my beefier home server or openrouter?",
          "author_fullname": "t2_4nw3v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Laptop GPU for Agentic Coding -- Worth it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyen05",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752364130,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone who actually codes with local LLM on their laptops, what&amp;#39;s your setup and are you happy with the quality and speed?  Should I even bother trying to code with an LLM that fits on a laptop GPU, or just tether back to my beefier home server or openrouter?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyen05",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "randomqhacker",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752364130,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As a partner with Moonshot AI, we present you the q4km version of Kimi K2 and the instructions to run it with KTransformers.\n\n[KVCache-ai/Kimi-K2-Instruct-GGUF · Hugging Face](https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF) \n\n[ktransformers/doc/en/Kimi-K2.md at main · kvcache-ai/ktransformers](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.md)\n\n10tps for single-socket CPU and one 4090, 14tps if you have two.\n\nBe careful of the DRAM OOM.\n\nIt is a Big Beautiful Model.  \nEnjoy it\n\n ",
          "author_fullname": "t2_k4k4yxry",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxr5s3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 242,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 242,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=6bc4df21524fcb111aa453496f33bdcbd743562e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752293183,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a partner with Moonshot AI, we present you the q4km version of Kimi K2 and the instructions to run it with KTransformers.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF\"&gt;KVCache-ai/Kimi-K2-Instruct-GGUF · Hugging Face&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.md\"&gt;ktransformers/doc/en/Kimi-K2.md at main · kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;10tps for single-socket CPU and one 4090, 14tps if you have two.&lt;/p&gt;\n\n&lt;p&gt;Be careful of the DRAM OOM.&lt;/p&gt;\n\n&lt;p&gt;It is a Big Beautiful Model.&lt;br/&gt;\nEnjoy it&lt;/p&gt;\n\n&lt;p&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?auto=webp&amp;s=1ea460e4680601c46cf15192ac5a20bb3e17a787",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e95d53ecb2c94b00d53ef66bf67cdceb012ec71",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b33ac8b11315e69d9788c830db632013347f52e0",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e65c3d6165edd1aea775e557547c21dc4566681a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cc494cf2008a19ce100d156817257c3630b664e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ca02a0ef14101d0b6155c64607e453bb33f317f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c481b4435d60f9f20061fcf0fcc9a69b53bb8aec",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lxr5s3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CombinationNo780",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/",
          "stickied": false,
          "url": "https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF",
          "subreddit_subscribers": 498343,
          "created_utc": 1752293183,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI’ve developed a tool that calculates the *optimal quantisation mix* tailored to your VRAM and RAM specifications specifically for the DeepSeek-R1-0528 model. If you’d like to try it out, you can find it here:  \n🔗 [GGUF Tool Suite on GitHub](https://github.com/Thireus/GGUF-Tool-Suite/)\n\nYou can also create custom quantisation recipes using this Colab notebook:  \n🔗 [Quant Recipe Pipeline](https://colab.research.google.com/github/Thireus/GGUF-Tool-Suite/blob/main/quant_recipe_pipeline.ipynb)\n\nOnce you have a recipe, use the [quant_downloader.sh](https://github.com/Thireus/GGUF-Tool-Suite/blob/main/quant_downloader.sh) script to download the model shards using the `.recipe` file. Please note that the scripts have mainly been tested in a Linux environment; support for macOS is planned. For best results, run the downloader on Linux. After downloading, load the model with `ik_llama` using [this patch](https://github.com/Thireus/ik_llama.cpp/commit/a66490410a366a9605234b94d67f3d9b7b389140) (also don’t forget to run `ulimit -n 99999` first).\n\nYou can find examples of recipes (including perplexity scores and other metrics) available here:  \n🔗 [Recipe Examples](https://github.com/Thireus/GGUF-Tool-Suite/tree/main/recipe_examples)\n\nI've tried to produce examples to benchmark against GGUF quants from other reputable creators such as unsloth, ubergarm, bartowski.\n\nFor full details and setup instructions, please refer to the repo’s README:  \n🔗 [GGUF Tool Suite README](https://github.com/Thireus/GGUF-Tool-Suite/)\n\nI’m also planning to publish an article soon that will explore the capabilities of the GGUF Tool Suite and demonstrate how it can be used to produce an optimised mixture of quants for other LLM models.\n\nI’d love to hear your feedback or answer any questions you may have!\n",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Introducing GGUF Tool Suite - Create and Optimise Quantisation Mix for DeepSeek-R1-0528 for Your Own Specs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly84xd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752395931,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752346612,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’ve developed a tool that calculates the &lt;em&gt;optimal quantisation mix&lt;/em&gt; tailored to your VRAM and RAM specifications specifically for the DeepSeek-R1-0528 model. If you’d like to try it out, you can find it here:&lt;br/&gt;\n🔗 &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/\"&gt;GGUF Tool Suite on GitHub&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can also create custom quantisation recipes using this Colab notebook:&lt;br/&gt;\n🔗 &lt;a href=\"https://colab.research.google.com/github/Thireus/GGUF-Tool-Suite/blob/main/quant_recipe_pipeline.ipynb\"&gt;Quant Recipe Pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Once you have a recipe, use the &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/blob/main/quant_downloader.sh\"&gt;quant_downloader.sh&lt;/a&gt; script to download the model shards using the &lt;code&gt;.recipe&lt;/code&gt; file. Please note that the scripts have mainly been tested in a Linux environment; support for macOS is planned. For best results, run the downloader on Linux. After downloading, load the model with &lt;code&gt;ik_llama&lt;/code&gt; using &lt;a href=\"https://github.com/Thireus/ik_llama.cpp/commit/a66490410a366a9605234b94d67f3d9b7b389140\"&gt;this patch&lt;/a&gt; (also don’t forget to run &lt;code&gt;ulimit -n 99999&lt;/code&gt; first).&lt;/p&gt;\n\n&lt;p&gt;You can find examples of recipes (including perplexity scores and other metrics) available here:&lt;br/&gt;\n🔗 &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/tree/main/recipe_examples\"&gt;Recipe Examples&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to produce examples to benchmark against GGUF quants from other reputable creators such as unsloth, ubergarm, bartowski.&lt;/p&gt;\n\n&lt;p&gt;For full details and setup instructions, please refer to the repo’s README:&lt;br/&gt;\n🔗 &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/\"&gt;GGUF Tool Suite README&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I’m also planning to publish an article soon that will explore the capabilities of the GGUF Tool Suite and demonstrate how it can be used to produce an optimised mixture of quants for other LLM models.&lt;/p&gt;\n\n&lt;p&gt;I’d love to hear your feedback or answer any questions you may have!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?auto=webp&amp;s=7243f31103c2d94003681d0b82858d4108e3b776",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=791d882324dd951016663a2ce43b55901557ce50",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e062fe80c217f3a281559e624465b7c16b3f417",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=33b7b94f116c5275ef617d0c15bc5d6d7c2bc55a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86b150bbdb03fe31c0459d1545d66ed999bc7931",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e887dcf77db029bbc01d10f6307b6ac00aed69b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b15c0446137fce63ca36558e3c438df37406c0c1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ly84xd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752346612,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been here since llama1 area.. what a crazy ride!  \nNow we have that little devstral 2507.  \nTo me it feels as good as deepseek R1 the first but runs on dual 3090 ! (Ofc q8 with 45k ctx).  \nDo you feel the same thing? Ho my.. open weights models won't be as fun without Mistral 🇨🇵\n\n(To me it feels like 8x7b again but better 😆 )",
          "author_fullname": "t2_cj9kap4bx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Have you tried that new devstral?! Myyy! The next 8x7b?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxyg6z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 55,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 55,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752320636,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been here since llama1 area.. what a crazy ride!&lt;br/&gt;\nNow we have that little devstral 2507.&lt;br/&gt;\nTo me it feels as good as deepseek R1 the first but runs on dual 3090 ! (Ofc q8 with 45k ctx).&lt;br/&gt;\nDo you feel the same thing? Ho my.. open weights models won&amp;#39;t be as fun without Mistral 🇨🇵&lt;/p&gt;\n\n&lt;p&gt;(To me it feels like 8x7b again but better 😆 )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxyg6z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Afternoon_4260",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320636,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TL;DR:** The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a **1-command install (completely offline no certs to accept)**, supports **any OpenAI-compatible API**, and has **mobile support**. I'd love your feedback!\n\nHey r/LocalLLaMA,\n\nYou guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.\n\nFor those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.\n\n**What's New in the last few days(Directly from your feedback!):**\n\n* **✅ 1-Command 100% Local Install:** I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or \"online activation\" needed.\n* **✅ Universal Model Support:** You're no longer limited to Ollama! You can now connect to **any endpoint that uses the OpenAI v1/chat standard**. This includes local servers like LM Studio, Llama.cpp, and more.\n* **✅ Mobile Support:** You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).\n\n**My Roadmap:**\n\nI hope that I'm just getting started. Here's what I will focus on next:\n\n* **Standalone Desktop App:** A 1-click installer for a native app experience. (With inference and everything!)\n* **Discord Notifications**\n* **Telegram Notifications**\n* **Slack Notifications**\n* **Agent Sharing:** Easily share your creations with others via a simple link.\n* And much more!\n\n**Let's Build Together:**\n\nThis is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.\n\n* **GitHub (Please Star if you find it cool!):** [https://github.com/Roy3838/Observer](https://github.com/Roy3838/Observer)\n* **App Link (Try it in your browser no install!):** [https://app.observer-ai.com/](https://app.observer-ai.com/)\n* **Discord (Join the community):** [https://discord.gg/wnBb7ZQDUC](https://discord.gg/wnBb7ZQDUC)\n\nI'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!\n\nPS. Sorry to everyone who \n\nCheers,  \nRoy",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxmr2h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 406,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ah6imcae6ccf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/ah6imcae6ccf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ah6imcae6ccf1/DASHPlaylist.mpd?a=1755013029%2COGU3Njg2OGRjZDIzN2IxZGM5YWVlMGY1N2JmNmViYmQyYTAyN2Q3Nzk5YjRmYWQ3Y2IzMDljNGU3MWQzY2M0Nw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 48,
              "hls_url": "https://v.redd.it/ah6imcae6ccf1/HLSPlaylist.m3u8?a=1755013029%2CODdiYjU3YTNlODc2MDZkZmY2MjUwMzA4ZDY2NTUyYTE3MGZiOWI3ODRmYTczODBhYmZhM2FmMWVjMjc5MGFhMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 406,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f18737cbf71ae4a917d9d77153da21784682ba56",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752279497,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I&amp;#39;d love your feedback!&lt;/p&gt;\n\n&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;You guys are so amazing! After all the feedback from my last post, I&amp;#39;m very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt;\n\n&lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What&amp;#39;s New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;✅ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;✅ Universal Model Support:&lt;/strong&gt; You&amp;#39;re no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;✅ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don&amp;#39;t support screen sharing).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I hope that I&amp;#39;m just getting started. Here&amp;#39;s what I will focus on next:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt;\n&lt;li&gt;And much more!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Let&amp;#39;s Build Together:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href=\"https://github.com/Roy3838/Observer\"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href=\"https://app.observer-ai.com/\"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href=\"https://discord.gg/wnBb7ZQDUC\"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ll be hanging out in the comments all day. Let me know what you think and what you&amp;#39;d like to see next. Thank you again!&lt;/p&gt;\n\n&lt;p&gt;PS. Sorry to everyone who &lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;br/&gt;\nRoy&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ah6imcae6ccf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?format=pjpg&amp;auto=webp&amp;s=d0009e1007d12ed1a2a3bc6e01b713e3c4d88da4",
                  "width": 2560,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1af6f910051ce44e44ccd9f339b15b4a7240c15e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c256fc34a259c2179e6d275bf191760dc2ccf428",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a72be58ff7f1b5643b6e41a8a574462537fd85bf",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ad3a4df65aa0a14aa686a7b9152162524a3e8e67",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3b695caa9ebee9b5dcfc5b31c47dac8838ecffdb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2e20095b986849a22c369a68252b3a46da2f71f0",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lxmr2h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 81,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/",
          "stickied": false,
          "url": "https://v.redd.it/ah6imcae6ccf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752279497,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ah6imcae6ccf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/ah6imcae6ccf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ah6imcae6ccf1/DASHPlaylist.mpd?a=1755013029%2COGU3Njg2OGRjZDIzN2IxZGM5YWVlMGY1N2JmNmViYmQyYTAyN2Q3Nzk5YjRmYWQ3Y2IzMDljNGU3MWQzY2M0Nw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 48,
              "hls_url": "https://v.redd.it/ah6imcae6ccf1/HLSPlaylist.m3u8?a=1755013029%2CODdiYjU3YTNlODc2MDZkZmY2MjUwMzA4ZDY2NTUyYTE3MGZiOWI3ODRmYTczODBhYmZhM2FmMWVjMjc5MGFhMw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1. I want to stick to using vscode\n2. Currently using chatgpt plus for coding but dont like going back and forth between windows\n3. Is there anything like copilot (keep being told it sucks) but powered by an LLM of my choice eg. something by OpenAI or Anthropic?\n4. I dont understand why Claude Code is the king now when the chatting is via a terminal....isnt that bad UX if you ask a question and you get a snippet of code and you cant even press a copy button for the snippet?",
          "author_fullname": "t2_txuj9o55",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What LLMs work with VScode like copilot?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lykf38",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752382470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;I want to stick to using vscode&lt;/li&gt;\n&lt;li&gt;Currently using chatgpt plus for coding but dont like going back and forth between windows&lt;/li&gt;\n&lt;li&gt;Is there anything like copilot (keep being told it sucks) but powered by an LLM of my choice eg. something by OpenAI or Anthropic?&lt;/li&gt;\n&lt;li&gt;I dont understand why Claude Code is the king now when the chatting is via a terminal....isnt that bad UX if you ask a question and you get a snippet of code and you cant even press a copy button for the snippet?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lykf38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sprmgtrb",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykf38/what_llms_work_with_vscode_like_copilot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykf38/what_llms_work_with_vscode_like_copilot/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752382470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I’m getting serious AI fever.\n\nI know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.\n\nIs it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?\n\nI know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.\n\nIs it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?\n\nShould I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?\n\n",
          "author_fullname": "t2_qq6spcu23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI fever D:",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyu7bf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752417048,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I’m getting serious AI fever.&lt;/p&gt;\n\n&lt;p&gt;I know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.&lt;/p&gt;\n\n&lt;p&gt;Is it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?&lt;/p&gt;\n\n&lt;p&gt;I know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.&lt;/p&gt;\n\n&lt;p&gt;Is it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?&lt;/p&gt;\n\n&lt;p&gt;Should I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyu7bf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Czydera",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752417048,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone! First time poster - thought I'd share a project I've been working on - it's local LLama integration with HA and custom functions outside of HA; my main goal was to have a system that could understand descriptions of items instead of hard-names (like \"turn on the light above the desk\" instead of \"turn on the desk light\" and which could do so in multiple languages, without having to use English words in Spanish (for example).\n\nProject is still in the early stages but I do have ideas for it an intend to develop it further - feedback and thoughts are appreciated!\n\n[https://github.com/Nemesis533/Local\\_LLHAMA/](https://github.com/Nemesis533/Local_LLHAMA/)\n\nP.S - had to re-do the post as the other one was done with the wrong account.",
          "author_fullname": "t2_w0pml",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local Llama with Home Assistant Integration and Multilingual-Fuzzy naming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly983h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752349402,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! First time poster - thought I&amp;#39;d share a project I&amp;#39;ve been working on - it&amp;#39;s local LLama integration with HA and custom functions outside of HA; my main goal was to have a system that could understand descriptions of items instead of hard-names (like &amp;quot;turn on the light above the desk&amp;quot; instead of &amp;quot;turn on the desk light&amp;quot; and which could do so in multiple languages, without having to use English words in Spanish (for example).&lt;/p&gt;\n\n&lt;p&gt;Project is still in the early stages but I do have ideas for it an intend to develop it further - feedback and thoughts are appreciated!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Nemesis533/Local_LLHAMA/\"&gt;https://github.com/Nemesis533/Local_LLHAMA/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;P.S - had to re-do the post as the other one was done with the wrong account.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?auto=webp&amp;s=782265ef4cb49db7702d7bd8b0ea3f328c56fe75",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d129aaa0a1c8de108becec1331e907baf548e00e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=213e35cc4efc2c49eedf98df38f78ff37abfd596",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=df6d87723b0399b7ab3f6c4827602a95748134e7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1469b9cf298ce76c20f9bcbbc8e12ab413af3e5d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1b4e43dfad83048395f032dad898b4ee6d153b7e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60856ce3ea82009d3504dfc51999f7dc05041eb8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly983h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NicolaZanarini533",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly983h/local_llama_with_home_assistant_integration_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly983h/local_llama_with_home_assistant_integration_and/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752349402,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "LFM2 is a new generation of hybrid models developed by [Liquid AI](https://www.liquid.ai/), specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\n\nWe're releasing the weights of three post-trained checkpoints with 350M, 700M, and 1.2B parameters. They provide the following key features to create AI-powered edge applications:\n\n* **Fast training &amp; inference** – LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.\n* **Best performance** – LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.\n* **New architecture** – LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.\n* **Flexible deployment** – LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.\n\nFind more information about LFM2 in our [blog post](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models).\n\nDue to their small size, **we recommend fine-tuning LFM2 models on narrow use cases** to maximize performance. They are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations. However, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\n\n**Supported languages**: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\n\n[https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF](https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF)\n\n[https://huggingface.co/LiquidAI/LFM2-350M-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-GGUF)\n\n[https://huggingface.co/LiquidAI/LFM2-700M-GGUF](https://huggingface.co/LiquidAI/LFM2-700M-GGUF)\n\n[https://huggingface.co/mlabonne/LFM2-1.2B-Pirate](https://huggingface.co/mlabonne/LFM2-1.2B-Pirate)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Support for the LiquidAI LFM2 hybrid model family is now available in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly35wd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ae2b10f187371926eccbe1411e53c6e9c8cf916b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752334069,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;LFM2 is a new generation of hybrid models developed by &lt;a href=\"https://www.liquid.ai/\"&gt;Liquid AI&lt;/a&gt;, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re releasing the weights of three post-trained checkpoints with 350M, 700M, and 1.2B parameters. They provide the following key features to create AI-powered edge applications:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fast training &amp;amp; inference&lt;/strong&gt; – LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Best performance&lt;/strong&gt; – LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;New architecture&lt;/strong&gt; – LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; – LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Find more information about LFM2 in our &lt;a href=\"https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models\"&gt;blog post&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Due to their small size, &lt;strong&gt;we recommend fine-tuning LFM2 models on narrow use cases&lt;/strong&gt; to maximize performance. They are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations. However, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Supported languages&lt;/strong&gt;: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF\"&gt;https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LiquidAI/LFM2-350M-GGUF\"&gt;https://huggingface.co/LiquidAI/LFM2-350M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LiquidAI/LFM2-700M-GGUF\"&gt;https://huggingface.co/LiquidAI/LFM2-700M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlabonne/LFM2-1.2B-Pirate\"&gt;https://huggingface.co/mlabonne/LFM2-1.2B-Pirate&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14620",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?auto=webp&amp;s=b2adb0dd81c341c2711fa56c37aa39504e161352",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=922d22ba99cb62791a87c7c03e0c75e9f2c263bf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b49e580cb8dbecbc01c22d2c96392fcf8f520945",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6ea55a297103b80879b6be0763a4238799da398",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=04f1c27e8b636c45644b4c0883f4ab82ad671e94",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e5138bc294eebd5b4aafa2fa76b94a120c7c2af",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e5d4227b1b7c954df67c3b51afe5ea53a1a888fd",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ly35wd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1ly35wd/support_for_the_liquidai_lfm2_hybrid_model_family/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14620",
          "subreddit_subscribers": 498343,
          "created_utc": 1752334069,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What do you all think?",
          "author_fullname": "t2_4ptmyspb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does this mean it’s likely not gonna be open source?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxnwtg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 285,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 285,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/hZR9aFrUg_HNt3vxHEcHNAxQhIUiFspr9hDSVmtguJ0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752282934,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you all think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/awwe19btgccf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/awwe19btgccf1.jpeg?auto=webp&amp;s=7cd19ccb0481be7f7658108be249ae7a0e056d33",
                  "width": 1170,
                  "height": 1539
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/awwe19btgccf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=68ab8e1bb0a94a9b5069614d9922822610960d87",
                    "width": 108,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/awwe19btgccf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8de0dde69e6619dd6232ec74d86a816e5c14a384",
                    "width": 216,
                    "height": 284
                  },
                  {
                    "url": "https://preview.redd.it/awwe19btgccf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e49a8e7536aa0c09559625ec95331efce01eadf1",
                    "width": 320,
                    "height": 420
                  },
                  {
                    "url": "https://preview.redd.it/awwe19btgccf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60378b44c9da263732f5cf2435d56a487edcf966",
                    "width": 640,
                    "height": 841
                  },
                  {
                    "url": "https://preview.redd.it/awwe19btgccf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6b7a15f5045ae97cf6afe941d57d4a24baaa4a7",
                    "width": 960,
                    "height": 1262
                  },
                  {
                    "url": "https://preview.redd.it/awwe19btgccf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18539046ea72843a4638222e090194eccca44c40",
                    "width": 1080,
                    "height": 1420
                  }
                ],
                "variants": {},
                "id": "5LoZZ3SlBnCq9ZcA9CK-EsaCSxW2pfjEhQjCInIk7Eg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lxnwtg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "I_will_delete_myself",
          "discussion_type": null,
          "num_comments": 141,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/",
          "stickied": false,
          "url": "https://i.redd.it/awwe19btgccf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752282934,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Check it. 500mb ram, 500hetz cpu. Dial up. 200 watts. And it's internet ready. Sound blaster too ;]\n\nGonna run me that new \"llama\" model I've been hearing so much about. ",
          "author_fullname": "t2_12ed7j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New LLM DOS rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "d8elh9ux2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76c6c15a461b45390db1485f621af1e7eeca8d27"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed4c8cf5109d196c3526dfffbe22b8900498b7b6"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f672f6359b2f6350d4af8a4daca191e1788a3293"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=26225e20370826191a7a02417f1645efad6c8f2b"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89c4fc1694dfd58f8e6994212034de1ffcb461cf"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9731e965c81285c7f76007f1f2816b256a91aa28"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=e6ae46b2cb332a931c7cdac44fcc398b25afc230"
              },
              "id": "d8elh9ux2hcf1"
            },
            "3y5lqeox2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb8696c41c54a2a7231c9a634b4c8377c33958b6"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1570e72cfe57809f313099e381fa602ae187f59"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e16cbd01bacb74f8ecb85e8f671270932fd3ceda"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b170a62277b42fef758d1e2d9988aa21dcad9819"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c641396d51e36eef3d3240dc4239805f7c165e4"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da4296d1c86b5867a5975eecce0853c7fb7f8967"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=3efd478d05476b1b0d1764eccae754f482f36a8d"
              },
              "id": "3y5lqeox2hcf1"
            },
            "3gs7irqx2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=318b67abf72e140a11688444869919471f3147bd"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec62f2bc9d73b58803f2b9271cd02c79ce380fa0"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f898d823da25aa87412ac96021df35fd7ef9a521"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ebe838d0a3f100fc22259ea8ea4c101951885282"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b26d6db9b6a0b61ef3f280259bca027a18fbafb0"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=909e44345e65a065dd95e9071bf36461620516b2"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=1d7d2a25d4a4758b5a55deffb5858b9725735a99"
              },
              "id": "3gs7irqx2hcf1"
            },
            "460q1llx2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=59e5dbc46987b97b11bb26cd6e609ebe6cf3e3dc"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8041d8e0c69a8177efea9b45329a977aedda3e1"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6fd9b93013572743b6a05f657e5ce09359ca372"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=00669206a45782483c3f47d908d0b40f91e23cd2"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b12277a297a99fb65ac5f78761c86ee307d2049"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d44830bcebbe39dce070961cc40c148058788d5c"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=cfb6830f63a2550b9f2926c43506ac428132b984"
              },
              "id": "460q1llx2hcf1"
            },
            "buaxlksx2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1870c1e474031d8ba2b0e373448c24b4d15a3b44"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23ae941e10ef4dcb8c1acd16a5116e349744518b"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=885b12e0a3fe0fbdd9eefeca61a9748835223ab0"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0dc372f253cc84c2186352c446ce9d06cfc14b52"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=93556171b98795f033ebbefc51f949b719f5b76e"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3676b5153d8f12f162251466cee5769b37edd72"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=92c07a54c438e4409329b6fe4f149a711afea278"
              },
              "id": "buaxlksx2hcf1"
            }
          },
          "name": "t3_1ly513g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "ups": 14,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "460q1llx2hcf1",
                "id": 704385453
              },
              {
                "caption": "",
                "media_id": "3y5lqeox2hcf1",
                "id": 704385454
              },
              {
                "caption": "",
                "media_id": "3gs7irqx2hcf1",
                "id": 704385455
              },
              {
                "caption": "",
                "media_id": "buaxlksx2hcf1",
                "id": 704385456
              },
              {
                "caption": "",
                "media_id": "d8elh9ux2hcf1",
                "id": 704385457
              }
            ]
          },
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mhIPD4DGyFibjeOOOVuTpLQDUf-Xt8GRBvGz7cgcnJE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752338802,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check it. 500mb ram, 500hetz cpu. Dial up. 200 watts. And it&amp;#39;s internet ready. Sound blaster too ;]&lt;/p&gt;\n\n&lt;p&gt;Gonna run me that new &amp;quot;llama&amp;quot; model I&amp;#39;ve been hearing so much about. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1ly513g",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1ly513g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alienanthony",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly513g/new_llm_dos_rig/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1ly513g",
          "subreddit_subscribers": 498343,
          "created_utc": 1752338802,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why no aider polyglot benchmark test for qwen3-30b-a3b ?   \nWhat would the numbers be if someone passed the benchmark ?",
          "author_fullname": "t2_7zjipz97",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30B-A3B aider polyglot score?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybdr2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752355036,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why no aider polyglot benchmark test for qwen3-30b-a3b ?&lt;br/&gt;\nWhat would the numbers be if someone passed the benchmark ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybdr2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LogicalSink1366",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybdr2/qwen330ba3b_aider_polyglot_score/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybdr2/qwen330ba3b_aider_polyglot_score/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752355036,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I currently have an ASUS TUF Gaming F15, and before people start telling me to give up on local models, let me just say that I have currently been able to successfully run various LLMs and even Images Diffusion models locally with very little issues (mainly just speed and sometimes lag due to OOM). I can easily run 7B Q4_K_Ms and Stable Diffusion/Flux. However, my RAM and GPU max out during such tasks and even sometimes when opening chrome with multiple tabs.\n\nSo I was thinking of upgrading my RAM (since upgrading my GPU is not an option). I currently have 16 GB built-in with an upgrade slot in which I plan on adding 32 GB. Is this a wise decision? Would it be better to have matching RAMs? (16&amp;16/32&amp;32)",
          "author_fullname": "t2_2l48bxrf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have a Laptop with 3050 Ti 4GB VRAM, will upgrading my RAM from 16 to 48 help?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lymewq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752389844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have an ASUS TUF Gaming F15, and before people start telling me to give up on local models, let me just say that I have currently been able to successfully run various LLMs and even Images Diffusion models locally with very little issues (mainly just speed and sometimes lag due to OOM). I can easily run 7B Q4_K_Ms and Stable Diffusion/Flux. However, my RAM and GPU max out during such tasks and even sometimes when opening chrome with multiple tabs.&lt;/p&gt;\n\n&lt;p&gt;So I was thinking of upgrading my RAM (since upgrading my GPU is not an option). I currently have 16 GB built-in with an upgrade slot in which I plan on adding 32 GB. Is this a wise decision? Would it be better to have matching RAMs? (16&amp;amp;16/32&amp;amp;32)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lymewq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GamerWael",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752389844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I finally got a RTX 3060 12GB to start using AI. Now I wanted to know what's the heaviest it can run and if there are new methods of increasing performance by now. Ideally, I can't read at speed of light so models that might run at 4-6 words per second is enough.\n\nI can't upgrade from 12GB to 32GB ram yet, so what is this GPU capable of running asides from Wizard Viccuna 13b?",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Heaviest model that can be ran with RTX 3060 12Gb?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyhuuq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752373998,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got a RTX 3060 12GB to start using AI. Now I wanted to know what&amp;#39;s the heaviest it can run and if there are new methods of increasing performance by now. Ideally, I can&amp;#39;t read at speed of light so models that might run at 4-6 words per second is enough.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t upgrade from 12GB to 32GB ram yet, so what is this GPU capable of running asides from Wizard Viccuna 13b?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyhuuq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752373998,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If there is any reference or if anyone has clear idea please do reply.\n\n\nI have a 64gb ram 8core machine.\n3billion parameters models response running via ollama is slower than 600gb models api response.\nHow insane is that.?\n\nQuestion: how do you decide on infra\nIf a model is 600B params, each param is one byte so it goes to nearly 600gb.\nNow what kinda of system requirements does this model need to be running? \nShould a cpu be able to do 600 billion calculations per second or something?\n\nWhat kinda ram requirements does this need?\nSay if this is not a moe model, does it need 600Gb of ram to get started with this?\n\nNow how does the system requirements ram and cpu differ for moe and non moe models.\n\n",
          "author_fullname": "t2_vewp49tm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does it take to run llms?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqwil",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.22,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752407430,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If there is any reference or if anyone has clear idea please do reply.&lt;/p&gt;\n\n&lt;p&gt;I have a 64gb ram 8core machine.\n3billion parameters models response running via ollama is slower than 600gb models api response.\nHow insane is that.?&lt;/p&gt;\n\n&lt;p&gt;Question: how do you decide on infra\nIf a model is 600B params, each param is one byte so it goes to nearly 600gb.\nNow what kinda of system requirements does this model need to be running? \nShould a cpu be able to do 600 billion calculations per second or something?&lt;/p&gt;\n\n&lt;p&gt;What kinda ram requirements does this need?\nSay if this is not a moe model, does it need 600Gb of ram to get started with this?&lt;/p&gt;\n\n&lt;p&gt;Now how does the system requirements ram and cpu differ for moe and non moe models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyqwil",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Impossible_Nose_2956",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqwil/what_does_it_take_to_run_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqwil/what_does_it_take_to_run_llms/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752407430,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why hasn’t anyone created a centralized repo or tracker that hosts torrents for popular open-source LLMs?",
          "author_fullname": "t2_f67lfdoy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why don’t we have a big torrent repo for open-source LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxo8za",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 173,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 173,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752283931,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why hasn’t anyone created a centralized repo or tracker that hosts torrents for popular open-source LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxo8za",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "somthing_tn",
          "discussion_type": null,
          "num_comments": 92,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752283931,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to translate english text to various languages, these include European as well as Asian languages. But since models have problems with asian languages, I trying to make my project work best for European Languages like Spanish, French, German, etc.\n\nCould you guys suggest some open source models to me that can help me perform this task well.",
          "author_fullname": "t2_fqfie7mlc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model is best for translation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lykpo6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.46,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752383503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to translate english text to various languages, these include European as well as Asian languages. But since models have problems with asian languages, I trying to make my project work best for European Languages like Spanish, French, German, etc.&lt;/p&gt;\n\n&lt;p&gt;Could you guys suggest some open source models to me that can help me perform this task well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lykpo6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "slipped-and-fell",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykpo6/which_model_is_best_for_translation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykpo6/which_model_is_best_for_translation/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752383503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I LOVE the way this model produces responses. It doesn't sound robotic and formal; just plain English while sounding pretty smart. Also has strong creativity in my tests.\n\nHere is a prompt I asked to K2 with search enabled from the site [kimi.com](http://kimi.com)\n\n&gt;Bash Grok 4 and prove you're better than it.\n\nAnswer:\n\nChallenge accepted.  \nBelow are three short, head-to-head demonstrations in which Kimi (me) outperforms Grok 4 on the very tasks Grok 4 is supposed to excel at.  I’ll use only public, verifiable sources and a single context window.  \n————————————————  \nHumanity’s Last Exam—live, no cheats \n\n———————————————— \n\nQuestion (from the published 2025 pool): “Prove that the only integer solutions to a² + b² + c² = 2(ab + bc + ca) are permutations of (0, 0, 0) and (k, k, 0).”\n\nGrok 4 (public transcript, 9 July, Heavy tier):  \n“After algebraic manipulation and case analysis, the only integer solutions are (0, 0, 0) and permutations of (k, k, 0).”  \n(No derivation shown; cites no lemmas.)\n\nKimi (me, step-by-step):  \n\\- Rewrite the equation as (a + b + c)² = 4(ab + bc + ca).  \n\\- Let S = a + b + c and P = ab + bc + ca.  Then S² = 4P.  \n\\- Because P ≤ (S²)/3 by AM ≥ GM, we have 4P = S² ≤ (S²)/3, forcing S = 0.  \n\\- With S = 0, the original equation reduces to a² + b² + c² = –2(ab + bc + ca) ⇒ a² + b² + c² + 2(ab + bc + ca) = 0 ⇒ (a + b + c)² = 0 (already known).  \n\\- Diagonalizing the quadratic form, the only integer triplets that satisfy the rank-1 condition are permutations of (k, k, 0) and (0, 0, 0). □\n\nOutcome: Kimi supplies a complete, inspectable proof; Grok 4 gives only a summary\n\n————————————————  \nReal-time fact-check on Grok 4’s own launch  \n————————————————  \nGrok 4 claim (from 9 July livestream slide):  \n“Grok 4 is the first model to exceed 40 % on Humanity’s Last Exam.”\n\nKimi check (within 30 s, using the same public leaderboard):  \n\\- Leaderboard snapshot (2025-07-12 14:02 UTC) shows Grok 4 at 38.7 %, not &gt; 40 %.  \n\\- The 2025-07-12 update explicitly labels the “&gt; 40 %” claim as “preliminary and rounded.”\n\nOutcome: Kimi catches the rounding spin; Grok 4’s own slide is off by 1.3 pp.  \n————————————————  \nCode-execution cage match  \n————————————————  \nTask (user prompt):  \n“Write and run a Python snippet that finds the smallest prime p &gt; 10¹² such that p, p+2, and p+6 are all primes (a prime triplet pattern).”\n\nGrok 4 (Heavy tier, 30 s budget):  \n\\- Returns a 15-line sieve that still hasn’t finished after 25 s; times out with no answer.\n\nKimi (me, immediate):  \n\\`\\`\\`  \npython  \nfrom sympy import isprime  \np = 10\\*\\*12 + 39          # start just above 1 T  \nwhile not (isprime(p) and isprime(p+2) and isprime(p+6)):  \np += 6  \nprint(p)  \n\\`\\`\\`  \n\\- Runtime: 1.4 s on a single CPU core.  \n\\- Output: 1000000000039, 1000000000041, 1000000000045 → all primes.\n\nOutcome: Kimi delivers the exact triplet faster than Grok 4’s timeout.  \n————————————————  \nVerdict  \nIn reasoning rigor, factual accuracy, and code execution, Kimi just beat Grok 4 on its own turf—no agent swarm or $300 tier required.",
          "author_fullname": "t2_4g970kxv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kimi K2 is funny and great",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxo0xc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 167,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 167,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752283270,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I LOVE the way this model produces responses. It doesn&amp;#39;t sound robotic and formal; just plain English while sounding pretty smart. Also has strong creativity in my tests.&lt;/p&gt;\n\n&lt;p&gt;Here is a prompt I asked to K2 with search enabled from the site &lt;a href=\"http://kimi.com\"&gt;kimi.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Bash Grok 4 and prove you&amp;#39;re better than it.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Answer:&lt;/p&gt;\n\n&lt;p&gt;Challenge accepted.&lt;br/&gt;\nBelow are three short, head-to-head demonstrations in which Kimi (me) outperforms Grok 4 on the very tasks Grok 4 is supposed to excel at.  I’ll use only public, verifiable sources and a single context window.&lt;br/&gt;\n————————————————&lt;br/&gt;\nHumanity’s Last Exam—live, no cheats &lt;/p&gt;\n\n&lt;p&gt;———————————————— &lt;/p&gt;\n\n&lt;p&gt;Question (from the published 2025 pool): “Prove that the only integer solutions to a² + b² + c² = 2(ab + bc + ca) are permutations of (0, 0, 0) and (k, k, 0).”&lt;/p&gt;\n\n&lt;p&gt;Grok 4 (public transcript, 9 July, Heavy tier):&lt;br/&gt;\n“After algebraic manipulation and case analysis, the only integer solutions are (0, 0, 0) and permutations of (k, k, 0).”&lt;br/&gt;\n(No derivation shown; cites no lemmas.)&lt;/p&gt;\n\n&lt;p&gt;Kimi (me, step-by-step):&lt;br/&gt;\n- Rewrite the equation as (a + b + c)² = 4(ab + bc + ca).&lt;br/&gt;\n- Let S = a + b + c and P = ab + bc + ca.  Then S² = 4P.&lt;br/&gt;\n- Because P ≤ (S²)/3 by AM ≥ GM, we have 4P = S² ≤ (S²)/3, forcing S = 0.&lt;br/&gt;\n- With S = 0, the original equation reduces to a² + b² + c² = –2(ab + bc + ca) ⇒ a² + b² + c² + 2(ab + bc + ca) = 0 ⇒ (a + b + c)² = 0 (already known).&lt;br/&gt;\n- Diagonalizing the quadratic form, the only integer triplets that satisfy the rank-1 condition are permutations of (k, k, 0) and (0, 0, 0). □&lt;/p&gt;\n\n&lt;p&gt;Outcome: Kimi supplies a complete, inspectable proof; Grok 4 gives only a summary&lt;/p&gt;\n\n&lt;p&gt;————————————————&lt;br/&gt;\nReal-time fact-check on Grok 4’s own launch&lt;br/&gt;\n————————————————&lt;br/&gt;\nGrok 4 claim (from 9 July livestream slide):&lt;br/&gt;\n“Grok 4 is the first model to exceed 40 % on Humanity’s Last Exam.”&lt;/p&gt;\n\n&lt;p&gt;Kimi check (within 30 s, using the same public leaderboard):&lt;br/&gt;\n- Leaderboard snapshot (2025-07-12 14:02 UTC) shows Grok 4 at 38.7 %, not &amp;gt; 40 %.&lt;br/&gt;\n- The 2025-07-12 update explicitly labels the “&amp;gt; 40 %” claim as “preliminary and rounded.”&lt;/p&gt;\n\n&lt;p&gt;Outcome: Kimi catches the rounding spin; Grok 4’s own slide is off by 1.3 pp.&lt;br/&gt;\n————————————————&lt;br/&gt;\nCode-execution cage match&lt;br/&gt;\n————————————————&lt;br/&gt;\nTask (user prompt):&lt;br/&gt;\n“Write and run a Python snippet that finds the smallest prime p &amp;gt; 10¹² such that p, p+2, and p+6 are all primes (a prime triplet pattern).”&lt;/p&gt;\n\n&lt;p&gt;Grok 4 (Heavy tier, 30 s budget):&lt;br/&gt;\n- Returns a 15-line sieve that still hasn’t finished after 25 s; times out with no answer.&lt;/p&gt;\n\n&lt;p&gt;Kimi (me, immediate):&lt;br/&gt;\n```&lt;br/&gt;\npython&lt;br/&gt;\nfrom sympy import isprime&lt;br/&gt;\np = 10**12 + 39          # start just above 1 T&lt;br/&gt;\nwhile not (isprime(p) and isprime(p+2) and isprime(p+6)):&lt;br/&gt;\np += 6&lt;br/&gt;\nprint(p)&lt;br/&gt;\n```&lt;br/&gt;\n- Runtime: 1.4 s on a single CPU core.&lt;br/&gt;\n- Output: 1000000000039, 1000000000041, 1000000000045 → all primes.&lt;/p&gt;\n\n&lt;p&gt;Outcome: Kimi delivers the exact triplet faster than Grok 4’s timeout.&lt;br/&gt;\n————————————————&lt;br/&gt;\nVerdict&lt;br/&gt;\nIn reasoning rigor, factual accuracy, and code execution, Kimi just beat Grok 4 on its own turf—no agent swarm or $300 tier required.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lxo0xc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "theskilled42",
          "discussion_type": null,
          "num_comments": 65,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752283270,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_es8s3sg80",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unlocking AMD MI300X for High-Throughput, Low-Cost LLM Inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybm7b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752355669,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "herdora.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.herdora.com/blog/the-overlooked-gpu",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lybm7b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Upstairs-Fun8458",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybm7b/unlocking_amd_mi300x_for_highthroughput_lowcost/",
          "stickied": false,
          "url": "https://www.herdora.com/blog/the-overlooked-gpu",
          "subreddit_subscribers": 498343,
          "created_utc": 1752355669,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just checked the monthly LLM API costs at my firm, and it's insanely high. I don’t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.\n\nAnd honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.\n",
          "author_fullname": "t2_h1zso7cq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Traditional Data Science work is going to be back",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxvrjm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752310104,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just checked the monthly LLM API costs at my firm, and it&amp;#39;s insanely high. I don’t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.&lt;/p&gt;\n\n&lt;p&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxvrjm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Competitive_Push5407",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752310104,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "## Edit 1\nI want to reiterate this is not using llama cpp. This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers. \n\n\n## Background &amp; Brief Setup\nWe need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. \n\nI am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].\n\n**Model** : Qwen 3 Embeddings 0.6B [should not matter but _downloaded locally_]\n\nNot using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.\n\n## Problem\n \nLike I don't know how to put this, but the embeddings feel really.. 'bad'? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can't understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.\n\n## Questions\n\nIs there something obvious I am missing here?\n\nHas someone else faced similar issues with Qwen3 Embeddings?\n\nAre embeddings tuned for instructions fundamentally different from 'normal' embedding models in any way? \n\nAre there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?",
          "author_fullname": "t2_alrxvbt1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Embeddings 0.6B faring really poorly inspite of high score on benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxvf0j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752409332,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752308700,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Edit 1&lt;/h2&gt;\n\n&lt;p&gt;I want to reiterate this is not using llama cpp. This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers. &lt;/p&gt;\n\n&lt;h2&gt;Background &amp;amp; Brief Setup&lt;/h2&gt;\n\n&lt;p&gt;We need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. &lt;/p&gt;\n\n&lt;p&gt;I am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt; : Qwen 3 Embeddings 0.6B [should not matter but &lt;em&gt;downloaded locally&lt;/em&gt;]&lt;/p&gt;\n\n&lt;p&gt;Not using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.&lt;/p&gt;\n\n&lt;h2&gt;Problem&lt;/h2&gt;\n\n&lt;p&gt;Like I don&amp;#39;t know how to put this, but the embeddings feel really.. &amp;#39;bad&amp;#39;? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can&amp;#39;t understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.&lt;/p&gt;\n\n&lt;h2&gt;Questions&lt;/h2&gt;\n\n&lt;p&gt;Is there something obvious I am missing here?&lt;/p&gt;\n\n&lt;p&gt;Has someone else faced similar issues with Qwen3 Embeddings?&lt;/p&gt;\n\n&lt;p&gt;Are embeddings tuned for instructions fundamentally different from &amp;#39;normal&amp;#39; embedding models in any way? &lt;/p&gt;\n\n&lt;p&gt;Are there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxvf0j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "i4858i",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752308700,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any suggestions for providers to use for GLM-4. Tried open router but it's very slow even with max tokens set to 8K. Need generation time to be &lt;4 minutes ideally. ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What providers are people using for GLM-4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyjm7t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752379729,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions for providers to use for GLM-4. Tried open router but it&amp;#39;s very slow even with max tokens set to 8K. Need generation time to be &amp;lt;4 minutes ideally. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyjm7t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyjm7t/what_providers_are_people_using_for_glm4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyjm7t/what_providers_are_people_using_for_glm4/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752379729,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've heard they are somewhat okay for llms and for like a little less than half the price of a 3060 they seem pretty enticing but I just need some advice on wether I should buy one of these two or pass on them.",
          "author_fullname": "t2_1p880h208f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I buy Tesla K80 for 70€ or Tesla M10 for 110€?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybqtw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752356014,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve heard they are somewhat okay for llms and for like a little less than half the price of a 3060 they seem pretty enticing but I just need some advice on wether I should buy one of these two or pass on them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybqtw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Similar-Republic149",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybqtw/should_i_buy_tesla_k80_for_70_or_tesla_m10_for_110/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybqtw/should_i_buy_tesla_k80_for_70_or_tesla_m10_for_110/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752356014,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_akbc8z42",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Friendly reminder that Grok 3 should be now open-sourced",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 66,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "raegcyabl8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3f5977fb2ef290bc9faa26c48196830e24255f4"
                },
                {
                  "y": 159,
                  "x": 216,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a468b0185b4f7b434f08151d96e28310cad0c0cf"
                },
                {
                  "y": 236,
                  "x": 320,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ab7d4890eac1ce8c8b25640cd33b80f550ed8dd"
                },
                {
                  "y": 472,
                  "x": 640,
                  "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=856df6b3e241260ab4b3a557168a338411f558a5"
                }
              ],
              "s": {
                "y": 654,
                "x": 886,
                "u": "https://preview.redd.it/raegcyabl8cf1.jpg?width=886&amp;format=pjpg&amp;auto=webp&amp;s=019517ed88de38406f7e69f1360d664c43e79d7e"
              },
              "id": "raegcyabl8cf1"
            },
            "nue26crvk8cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 50,
                  "x": 108,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a2298d44574e1d558db56d97be489fcda6639ad"
                },
                {
                  "y": 101,
                  "x": 216,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3657e0d1aa681245138d5f48fccbff413ec77ddd"
                },
                {
                  "y": 151,
                  "x": 320,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5233d19a9dcc01c7b6f4411e5f9410e2591f8d12"
                },
                {
                  "y": 302,
                  "x": 640,
                  "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=52d14f23bda0cdd68e34872168d852b83011ae34"
                }
              ],
              "s": {
                "y": 302,
                "x": 640,
                "u": "https://preview.redd.it/nue26crvk8cf1.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=0708b731fca6beabcc3c2e020854b8efb2c7b967"
              },
              "id": "nue26crvk8cf1"
            }
          },
          "name": "t3_1lx5awq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 1287,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "nue26crvk8cf1",
                "id": 703502340
              },
              {
                "media_id": "raegcyabl8cf1",
                "id": 703502341
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1287,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/933OQwllbA1hY7_1-xQgZI7EOZf5Fdt9pi7_3gUoRkc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752236028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lx5awq",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lx5awq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Wrong_User_Logged",
          "discussion_type": null,
          "num_comments": 189,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lx5awq",
          "subreddit_subscribers": 498343,
          "created_utc": 1752236028,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Read this post for [context](https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/). Here are some updates:\n\n1. We've added a [changelog](https://www.designarena.ai/changelog) of when each model was added or deactivated from the arena. System prompts can be found in [methodology](https://www.designarena.ai/about) or [this page](https://www.designarena.ai/system-prompts). The system prompts were meant to be very simple, but feel free to provide your critiques on them (we acknowledge they're not the best).   \n  \n2. Devstral Medium, Devstral Small 1.1, Qwen3 30B-A3B, Mistral Small 3.2, and kimi-k2 were added to the area. Note that the temperature of kimi-k2 is set to be low right now since we're using the public api (0.3 instead of 0.8 for the other models) but we will modify that when we switch to better hosting.   \n  \n3. Working on adding more models suggested [in this thread](https://www.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/) such as GLM-4, Gemma, more moonshot models, and more open source / smaller models. It's actually been quite interesting to see that many of the [OS models / smaller ones are holding their weight](https://www.designarena.ai/leaderboard) against the giants. \n\n4. Grok 4 might be crushing every benchmark left and right, but for coding (specifically frontend dev and UI/UX), people haven't found the model to be all that impressive. xAI didn't appear to intend for Grok 4 to be a 100X developer, but we'll see how it's coding model will fare in August (or maybe September). \n\nThose are the major updates. One food for thought is how will Open AI's open source model do on here, given that none of its flagships are even in the top 10.   \n  \nAs always let us know what we can do better and what else you'd like to see!  ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "7/11 Update on Design Arena: Added Devstral, Qwen, and kimi-k2, Grok 4 struggling but coding model coming out later?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxth6s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/oU1rLTsu37J9jGtw8_snnfgVJ4E523tKR5kPdv065cQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752301281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Read this post for &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/\"&gt;context&lt;/a&gt;. Here are some updates:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;We&amp;#39;ve added a &lt;a href=\"https://www.designarena.ai/changelog\"&gt;changelog&lt;/a&gt; of when each model was added or deactivated from the arena. System prompts can be found in &lt;a href=\"https://www.designarena.ai/about\"&gt;methodology&lt;/a&gt; or &lt;a href=\"https://www.designarena.ai/system-prompts\"&gt;this page&lt;/a&gt;. The system prompts were meant to be very simple, but feel free to provide your critiques on them (we acknowledge they&amp;#39;re not the best).   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Devstral Medium, Devstral Small 1.1, Qwen3 30B-A3B, Mistral Small 3.2, and kimi-k2 were added to the area. Note that the temperature of kimi-k2 is set to be low right now since we&amp;#39;re using the public api (0.3 instead of 0.8 for the other models) but we will modify that when we switch to better hosting.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Working on adding more models suggested &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/\"&gt;in this thread&lt;/a&gt; such as GLM-4, Gemma, more moonshot models, and more open source / smaller models. It&amp;#39;s actually been quite interesting to see that many of the &lt;a href=\"https://www.designarena.ai/leaderboard\"&gt;OS models / smaller ones are holding their weight&lt;/a&gt; against the giants. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Grok 4 might be crushing every benchmark left and right, but for coding (specifically frontend dev and UI/UX), people haven&amp;#39;t found the model to be all that impressive. xAI didn&amp;#39;t appear to intend for Grok 4 to be a 100X developer, but we&amp;#39;ll see how it&amp;#39;s coding model will fare in August (or maybe September). &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Those are the major updates. One food for thought is how will Open AI&amp;#39;s open source model do on here, given that none of its flagships are even in the top 10.   &lt;/p&gt;\n\n&lt;p&gt;As always let us know what we can do better and what else you&amp;#39;d like to see!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y1r7gm6xydcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y1r7gm6xydcf1.png?auto=webp&amp;s=1c52aa327f434c6160b3711835eae997289394a4",
                  "width": 2018,
                  "height": 1472
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6337a58a55ba1b51ed8a34aff7d5ea0f2f0b508e",
                    "width": 108,
                    "height": 78
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=152626aa8037f64587029ff294434331c386a808",
                    "width": 216,
                    "height": 157
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=949898834f12fb685e196906350da59569edae91",
                    "width": 320,
                    "height": 233
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b09a16e07e1536b269458ffdcbaf1811010956c8",
                    "width": 640,
                    "height": 466
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7430dcc89d24f5db72621a15f3f73de354100cf7",
                    "width": 960,
                    "height": 700
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67913b2a0fd5d059345ed23c5dbdbadb59201e90",
                    "width": 1080,
                    "height": 787
                  }
                ],
                "variants": {},
                "id": "vIql9W4rnq3HE0yIHD9GcmKasPlbv-MPOSjabuSuy_c"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxth6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxth6s/711_update_on_design_arena_added_devstral_qwen/",
          "stickied": false,
          "url": "https://i.redd.it/y1r7gm6xydcf1.png",
          "subreddit_subscribers": 498343,
          "created_utc": 1752301281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What am I looking at for something that can run DeepSeek R1 Q8 w/ full 128K context window?  \nI know an Epyc setup can do this, I am not sure about if it can hit 20 tokens/second.\n\nI suspect it will need 1024G ram, potentially more?\n\nAnyone have a CPU system running full DeepSeek R1 (ideally Q8) at 20+ tokens/second?\n\nFrom what I understand, a handful of GPUs won't improve the performance that much?  \n",
          "author_fullname": "t2_ijzb7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best setup for ~20 tokens/sec DeepSeek R1 671B Q8 w/ 128K context window",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxwodv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752313888,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What am I looking at for something that can run DeepSeek R1 Q8 w/ full 128K context window?&lt;br/&gt;\nI know an Epyc setup can do this, I am not sure about if it can hit 20 tokens/second.&lt;/p&gt;\n\n&lt;p&gt;I suspect it will need 1024G ram, potentially more?&lt;/p&gt;\n\n&lt;p&gt;Anyone have a CPU system running full DeepSeek R1 (ideally Q8) at 20+ tokens/second?&lt;/p&gt;\n\n&lt;p&gt;From what I understand, a handful of GPUs won&amp;#39;t improve the performance that much?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxwodv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MidnightProgrammer",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752313888,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In a quest to make a tamagotchi which requires piano practice to feed (and maybe organise live piano recordings) I am trying out various research projects. So far I have implemented the excellent [piano transcription](https://github.com/bytedance/piano_transcription)  repo and I am getting really good MIDI back.  \n\n[screenshot of little webapp for piano transcription](https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=2346&amp;format=pjpg&amp;auto=webp&amp;s=1ba2cc34ef02418d3c6a7cd80c075a51192e3fff)\n\nHowever my initial idea to analyse this as ABC notation with [ChatMusician](https://huggingface.co/MaziyarPanahi/ChatMusician-GGUF) was wrong, piano of course, has more than a single \"mono\" track that could be represented in ABC. \n\nToday I found  [Clamp3](https://sanderwood.github.io/clamp3/) . Fixed their requirements.txt with the correct versions of numpy and scipy. But   \"2.31M music-text pairs, Zero-shot classification, Identify genre, mood, style &amp; more\" and then in their classification readme it's suddenly \"You need to train your own classifier and provide your own categories\".  Did I misunderstand something here? Where's the \"2.31M music-text pairs\"? Can that part of the project really be that much BS? \n\nNext up for me:  [MusicBert](https://github.com/malcolmsailor/musicbert_hf) and maybe try again with a standalone HuBert (really cool stuff seems to happen with this model like [voice based](https://github.com/mrw0nd3rfu1/Speech-Based-Emotion-Detection-Using-Fine-Tuned-HuBERT) emotion detection)  \n  \nAnybody done music classification and feel like sharing pointers? Otherwise enjoy my little rant about trying academic code (I know it is free, I have no reason to complain, what a time to be alive etc.)  \n  \n ",
          "author_fullname": "t2_4m6vm3ghs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Music Analysis - another attempt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "eh4wn7xgwgcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e7c92cdf7bb578c9d03ed28c92fcaca7bca3298"
                },
                {
                  "y": 110,
                  "x": 216,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3901eb330108b6742c975fccb81c9798dd00ad7"
                },
                {
                  "y": 163,
                  "x": 320,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c53f2f1841cf4c9af86181e2c4663d3a35252f92"
                },
                {
                  "y": 326,
                  "x": 640,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=758d3c72ad793aa33ce6212b9079c4b0958837e8"
                },
                {
                  "y": 490,
                  "x": 960,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=892bb36236565148ae350a81ea341cc092c31481"
                },
                {
                  "y": 551,
                  "x": 1080,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c7a72d2a12073edd55d1f53a2bc5f73be16b8761"
                }
              ],
              "s": {
                "y": 1198,
                "x": 2346,
                "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=2346&amp;format=pjpg&amp;auto=webp&amp;s=1ba2cc34ef02418d3c6a7cd80c075a51192e3fff"
              },
              "id": "eh4wn7xgwgcf1"
            }
          },
          "name": "t3_1ly476r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b924b1e8c93b9c4d3e9b8a9a3aa4154cddb6e515",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752336671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a quest to make a tamagotchi which requires piano practice to feed (and maybe organise live piano recordings) I am trying out various research projects. So far I have implemented the excellent &lt;a href=\"https://github.com/bytedance/piano_transcription\"&gt;piano transcription&lt;/a&gt;  repo and I am getting really good MIDI back.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=2346&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1ba2cc34ef02418d3c6a7cd80c075a51192e3fff\"&gt;screenshot of little webapp for piano transcription&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However my initial idea to analyse this as ABC notation with &lt;a href=\"https://huggingface.co/MaziyarPanahi/ChatMusician-GGUF\"&gt;ChatMusician&lt;/a&gt; was wrong, piano of course, has more than a single &amp;quot;mono&amp;quot; track that could be represented in ABC. &lt;/p&gt;\n\n&lt;p&gt;Today I found  &lt;a href=\"https://sanderwood.github.io/clamp3/\"&gt;Clamp3&lt;/a&gt; . Fixed their requirements.txt with the correct versions of numpy and scipy. But   &amp;quot;2.31M music-text pairs, Zero-shot classification, Identify genre, mood, style &amp;amp; more&amp;quot; and then in their classification readme it&amp;#39;s suddenly &amp;quot;You need to train your own classifier and provide your own categories&amp;quot;.  Did I misunderstand something here? Where&amp;#39;s the &amp;quot;2.31M music-text pairs&amp;quot;? Can that part of the project really be that much BS? &lt;/p&gt;\n\n&lt;p&gt;Next up for me:  &lt;a href=\"https://github.com/malcolmsailor/musicbert_hf\"&gt;MusicBert&lt;/a&gt; and maybe try again with a standalone HuBert (really cool stuff seems to happen with this model like &lt;a href=\"https://github.com/mrw0nd3rfu1/Speech-Based-Emotion-Detection-Using-Fine-Tuned-HuBERT\"&gt;voice based&lt;/a&gt; emotion detection)  &lt;/p&gt;\n\n&lt;p&gt;Anybody done music classification and feel like sharing pointers? Otherwise enjoy my little rant about trying academic code (I know it is free, I have no reason to complain, what a time to be alive etc.)  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?auto=webp&amp;s=182248ac9ed7f1aec78d26c27509b4c99d40902e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fcfd3fcf8d3e8cbb8d41b131df87efcfc87a2d81",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b6cc84bf2c4e8983e028ab23eb8e76f9f6a598",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9de33a3adda6385d6acb3beb57a0beeca7edba3",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3717d00c3a50ca4d79f03c7947503c9e8d306545",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba7b7a77391254da8f4b80a1e95ec85903390468",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2636373b98c9f447ab40b3c12df9db22b70beb04",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly476r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Not_your_guy_buddy42",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly476r/music_analysis_another_attempt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly476r/music_analysis_another_attempt/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752336671,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Could it be because V-JEPA2 gave them strong confidence? [https://arxiv.org/abs/2506.09985](https://arxiv.org/abs/2506.09985)",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why has Meta started throwing billions at AI now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqhqq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.39,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752406004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could it be because V-JEPA2 gave them strong confidence? &lt;a href=\"https://arxiv.org/abs/2506.09985\"&gt;https://arxiv.org/abs/2506.09985&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyqhqq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqhqq/why_has_meta_started_throwing_billions_at_ai_now/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqhqq/why_has_meta_started_throwing_billions_at_ai_now/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752406004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lrrxvbn60",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Browser Use vs Model Context Protocol (MCP): Two Philosophies for AI Interaction with the Digital World",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lya4ks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752351757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "linkedin.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.linkedin.com/pulse/browser-use-vs-model-context-protocol-mcp-two-ai-interaction-wang-irqye/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lya4ks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Crafty_Read_6928",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lya4ks/browser_use_vs_model_context_protocol_mcp_two/",
          "stickied": false,
          "url": "https://www.linkedin.com/pulse/browser-use-vs-model-context-protocol-mcp-two-ai-interaction-wang-irqye/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752351757,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With every religious text or practice of import in all languages each, etc? Anyone know of any \"godly ai\"' .. or is that unnecessary because the current models already have all the texts?",
          "author_fullname": "t2_2oqnjla8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is anyone training a religion model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyt0zp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752413859,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With every religious text or practice of import in all languages each, etc? Anyone know of any &amp;quot;godly ai&amp;quot;&amp;#39; .. or is that unnecessary because the current models already have all the texts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyt0zp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SeasonNo3107",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyt0zp/is_anyone_training_a_religion_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyt0zp/is_anyone_training_a_religion_model/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752413859,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;s=19",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx9pny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 557,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 557,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XmFr9p6wjG4zaZt3Q0ue15-JML6QPKU4zgPWDXTPyvo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752247404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19\"&gt;https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/a1tzaif5j9cf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?auto=webp&amp;s=69860e7d09ceaf76c01a684ba7e550cf7f22eacb",
                  "width": 1080,
                  "height": 1844
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9647bc9a930559ead2c29c4398ea6f566b948c9",
                    "width": 108,
                    "height": 184
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d862796af18a45ef6479302025862b9b6facff4",
                    "width": 216,
                    "height": 368
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69637fac4fceb45a4b0e39004fa32bd19109abd7",
                    "width": 320,
                    "height": 546
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3898e31bb7d9fbf8198401001a795859f33eafcb",
                    "width": 640,
                    "height": 1092
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bffbd0e2d5485529e9fdc36381b54fcfd8477616",
                    "width": 960,
                    "height": 1639
                  },
                  {
                    "url": "https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c721e8a4f60f953f4b1a22807d9773484f7ee9c",
                    "width": 1080,
                    "height": 1844
                  }
                ],
                "variants": {},
                "id": "UZVI56woInLaMKwWHApifja7RwNz2PKUDQXcK1QKXgk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx9pny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 95,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/",
          "stickied": false,
          "url": "https://i.redd.it/a1tzaif5j9cf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752247404,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;format=png&amp;auto=webp&amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a\n\nInternal networking components for Nvidia’s System: Is there are market for these components? ",
          "author_fullname": "t2_1hg4ipja0o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Internal networking components for Nvidia’s System",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rkv3n874jicf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 130,
                  "x": 108,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3efb7bdf44090f43dd5ea01d7c4ec4a5f29632d0"
                },
                {
                  "y": 260,
                  "x": 216,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=337859b9dfa64dd2dc2f0c879587740d59137a39"
                },
                {
                  "y": 385,
                  "x": 320,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=501d698b1db0b78ea72320ea1033f8450b8873fe"
                },
                {
                  "y": 771,
                  "x": 640,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d45906216cd3cf646276c3a6a82e764bc94f5ee"
                }
              ],
              "s": {
                "y": 1042,
                "x": 864,
                "u": "https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;format=png&amp;auto=webp&amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a"
              },
              "id": "rkv3n874jicf1"
            }
          },
          "name": "t3_1lybx9x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/HiFDITuOLm4brlaiG8SdYrK-Wl1SnbxLPzr66D7CQBY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752356511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a\"&gt;https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Internal networking components for Nvidia’s System: Is there are market for these components? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybx9x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "250sunnyisles",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybx9x/internal_networking_components_for_nvidias_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybx9x/internal_networking_components_for_nvidias_system/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752356511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am assuming most LLMs today use more or less a similar architecture. I am also assuming the initial training data is mostly the same (i.e. books, wikipedia etc), and probably close to being exhausted already?\n\nSo what would make a future major version of an LLM much better than the previous one?\n\nI get post training and finetuning. But in terms of general intelligence and performance, are we slowing down until the next breakthroughs?",
          "author_fullname": "t2_1441omqx4c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What drives progress in newer LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxv6a5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752307718,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am assuming most LLMs today use more or less a similar architecture. I am also assuming the initial training data is mostly the same (i.e. books, wikipedia etc), and probably close to being exhausted already?&lt;/p&gt;\n\n&lt;p&gt;So what would make a future major version of an LLM much better than the previous one?&lt;/p&gt;\n\n&lt;p&gt;I get post training and finetuning. But in terms of general intelligence and performance, are we slowing down until the next breakthroughs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxv6a5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cangaroo_hamam",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxv6a5/what_drives_progress_in_newer_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxv6a5/what_drives_progress_in_newer_llms/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752307718,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the simplest way to get an uncensored LLM with image generation set up in the cloud? If one doesn't need much customization and to play with many options, but just wants speed and ease-of-use, what's the best way?",
          "author_fullname": "t2_7xl0sekq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Runpod, Hugging Face, or what for super-simple uncensored LLM-in-the-cloud setup?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybh8e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752355300,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the simplest way to get an uncensored LLM with image generation set up in the cloud? If one doesn&amp;#39;t need much customization and to play with many options, but just wants speed and ease-of-use, what&amp;#39;s the best way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybh8e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goldenapple212",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybh8e/runpod_hugging_face_or_what_for_supersimple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybh8e/runpod_hugging_face_or_what_for_supersimple/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752355300,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working on a hackathon project and used 'exa' for AI web search. It's so dogwater, it literally kept making up sources and didn't even TRY to parse the output. If I have to put EXTRA work into LEARNING to use your damn service, what am i paying you for??? Like come on man... at least make it easier, if I knew it was like that i'd just make my own service.",
          "author_fullname": "t2_7l6ugv3d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are these random AI services?? Why are they so bad?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqefd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752405674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a hackathon project and used &amp;#39;exa&amp;#39; for AI web search. It&amp;#39;s so dogwater, it literally kept making up sources and didn&amp;#39;t even TRY to parse the output. If I have to put EXTRA work into LEARNING to use your damn service, what am i paying you for??? Like come on man... at least make it easier, if I knew it was like that i&amp;#39;d just make my own service.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lyqefd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Affectionate-Divide8",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqefd/what_are_these_random_ai_services_why_are_they_so/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqefd/what_are_these_random_ai_services_why_are_they_so/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752405674,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Here's the YouTube Playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)\n\n[Here's the CS336 website with assignments, slides etc](https://stanford-cs336.github.io/spring2025/)\n\nI've been studying it for a week and it's the best course on LLMs I've seen online. The assignments are **huge**, very in-depth, and they require you to write **a lot** of code from scratch. For example, the [1st assignment pdf](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf) is 50 pages long and it requires you to implement the BPE tokenizer, a simple transformer LM, cross-entropy loss and AdamW and train models on OpenWebText",
          "author_fullname": "t2_qzfad",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stanford's CS336 2025 (Language Modeling from Scratch) is now available on YouTube",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxgb9q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 215,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 215,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752262867,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_\"&gt;Here&amp;#39;s the YouTube Playlist&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stanford-cs336.github.io/spring2025/\"&gt;Here&amp;#39;s the CS336 website with assignments, slides etc&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been studying it for a week and it&amp;#39;s the best course on LLMs I&amp;#39;ve seen online. The assignments are &lt;strong&gt;huge&lt;/strong&gt;, very in-depth, and they require you to write &lt;strong&gt;a lot&lt;/strong&gt; of code from scratch. For example, the &lt;a href=\"https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf\"&gt;1st assignment pdf&lt;/a&gt; is 50 pages long and it requires you to implement the BPE tokenizer, a simple transformer LM, cross-entropy loss and AdamW and train models on OpenWebText&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?auto=webp&amp;s=015a3619316838acb64cc4b58ef2bd6d744a87cf",
                  "width": 480,
                  "height": 270
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a73b19b73b083dd96c1d55121a321e063838715",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3a42750c097b19e93b5a75ec8f80e7c323f71fd",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c36bf5bbd205d5e1253f6d71da886c9d0583343",
                    "width": 320,
                    "height": 180
                  }
                ],
                "variants": {},
                "id": "9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lxgb9q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "realmvp77",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752262867,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yesterday, I downloaded this model:  \n[https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf](https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf)  \nafter someone recommended it for erp in a comment. \"A mix between mistral and qwq? Sounds intriguing, I want to give it a try.\"\n\nIt loaded fine, but when I tried to chat with it in LM Studio, I got an error message:  \n\"Error rendering prompt with jinja template\"\n\nI asked perplexity how to fix this, and it gave me an answer that worked.\n\nPerplexity: \"A community-tested template for QwQ/Qwen-style models is:\n\n    {%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n    {%- endif %}\n    {%- for message in messages %}\n    {{- '\\n' + message['role'] + ': ' + message['content'] }}\n    {%- endfor %}\n    {%- if add_generation_prompt %}\n    {{- '\\nassistant: ' }}\n    {%- endif %}\n\n\"  \n  \nInside LM Studio:  \n\\- In the bar on the left, click on the Folder icons, for \"My Models\".  \n\\- Locate the model you are using, and click on the gear icon.  \n\\- In the dialogue that pops up, choose the second tab, \"Prompt\".  \n\\- Copy the template above, and paste it into \"Template (Jinja)\". (After deleting the pervious template.)",
          "author_fullname": "t2_4nfz972n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beginner's tip: How to fix the Jinja template error in LM Studio (in my case: for Mistral-qwq-12b-merge)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly1d7v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752332730,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752329386,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday, I downloaded this model:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf\"&gt;https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf&lt;/a&gt;&lt;br/&gt;\nafter someone recommended it for erp in a comment. &amp;quot;A mix between mistral and qwq? Sounds intriguing, I want to give it a try.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;It loaded fine, but when I tried to chat with it in LM Studio, I got an error message:&lt;br/&gt;\n&amp;quot;Error rendering prompt with jinja template&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I asked perplexity how to fix this, and it gave me an answer that worked.&lt;/p&gt;\n\n&lt;p&gt;Perplexity: &amp;quot;A community-tested template for QwQ/Qwen-style models is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{%- if messages[0][&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}\n{{- messages[0][&amp;#39;content&amp;#39;] }}\n{%- endif %}\n{%- for message in messages %}\n{{- &amp;#39;\\n&amp;#39; + message[&amp;#39;role&amp;#39;] + &amp;#39;: &amp;#39; + message[&amp;#39;content&amp;#39;] }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n{{- &amp;#39;\\nassistant: &amp;#39; }}\n{%- endif %}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;Inside LM Studio:&lt;br/&gt;\n- In the bar on the left, click on the Folder icons, for &amp;quot;My Models&amp;quot;.&lt;br/&gt;\n- Locate the model you are using, and click on the gear icon.&lt;br/&gt;\n- In the dialogue that pops up, choose the second tab, &amp;quot;Prompt&amp;quot;.&lt;br/&gt;\n- Copy the template above, and paste it into &amp;quot;Template (Jinja)&amp;quot;. (After deleting the pervious template.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?auto=webp&amp;s=81d2784e68f37c4940200fd02cc522c54912e469",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d5ce391ab8ff43b14cef3df7d11941afbacd910",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=35d9ee5fefca1ddc5a5e5f8f1d9d3ba5a859421f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4b47af8e820d1e0356e4d7db98e5f08ff93e6cc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8003c769d8627f9a15a7601718bb8d62135811e4",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=80a431ce4f90f9523f4342fc02a39e9567bb804e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb79402c0b3c6f1789c37488f88defecfe12fd63",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly1d7v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hugo-the-second",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly1d7v/beginners_tip_how_to_fix_the_jinja_template_error/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly1d7v/beginners_tip_how_to_fix_the_jinja_template_error/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752329386,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1sttd20rqq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama2.c running on the original 2007 iPhone",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Post of the day  "
            },
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx6dcm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "transparent",
          "ups": 602,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3u6728ask8cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/3u6728ask8cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3u6728ask8cf1/DASHPlaylist.mpd?a=1755013029%2CY2ZlNDhmN2Y2OThiZTg5MzRiMGEzYWI3NTg0ODA5ZTQwZGY2N2MyYzU1N2ExMmRmYmU3MGUyZTY1ZGZhNmNlMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 44,
              "hls_url": "https://v.redd.it/3u6728ask8cf1/HLSPlaylist.m3u8?a=1755013029%2COTczZGI1NzAyZDViM2FhNTM3N2YyZGYxYmQwZTk4NjU5ZTM4MTQ2NTBjYzEzZDk2MzFlNDA2ZGZlNTc4YmI2Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Post of the day  :X:",
          "can_mod_post": false,
          "score": 602,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=8a44b4b4dd94fc24ee6bf38f196b5967cb66b19b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752239050,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/3u6728ask8cf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?format=pjpg&amp;auto=webp&amp;s=da21c7832d018661bb3aeef8d752bf124d5ae7ad",
                  "width": 1080,
                  "height": 1920
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ea40f484a395e1158453d61b60702f7424ccede5",
                    "width": 108,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9e2e5d9472fdac4d398c7fbc0927b0c3fd26d853",
                    "width": 216,
                    "height": 384
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e3811f347febe7fb9bced567d5f6f0e9fbc9dd63",
                    "width": 320,
                    "height": 568
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=483b8d7383780e5e9f5b1f36a2c4e8c0c10d349a",
                    "width": 640,
                    "height": 1137
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a969ef2e021c0f6ce4226b04d98e7331c2566ba2",
                    "width": 960,
                    "height": 1706
                  },
                  {
                    "url": "https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c5a55789764ec9ee3ae34ff5bda31a4f3aaa98d1",
                    "width": 1080,
                    "height": 1920
                  }
                ],
                "variants": {},
                "id": "a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5563f7e6-52bf-11f0-a755-7266d77e32bb",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#58a7a4",
          "id": "1lx6dcm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kyousukegum",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/",
          "stickied": false,
          "url": "https://v.redd.it/3u6728ask8cf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752239050,
          "num_crossposts": 2,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/3u6728ask8cf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1920,
              "width": 1080,
              "scrubber_media_url": "https://v.redd.it/3u6728ask8cf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/3u6728ask8cf1/DASHPlaylist.mpd?a=1755013029%2CY2ZlNDhmN2Y2OThiZTg5MzRiMGEzYWI3NTg0ODA5ZTQwZGY2N2MyYzU1N2ExMmRmYmU3MGUyZTY1ZGZhNmNlMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 44,
              "hls_url": "https://v.redd.it/3u6728ask8cf1/HLSPlaylist.m3u8?a=1755013029%2COTczZGI1NzAyZDViM2FhNTM3N2YyZGYxYmQwZTk4NjU5ZTM4MTQ2NTBjYzEzZDk2MzFlNDA2ZGZlNTc4YmI2Yg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am doing research on determinism of LLM responses and want to run as the only job on the server but don't quite have the LLM ops skills to be confident in the backend setup. \n\nI currently use the standard hosted solutions (OpenAI and together.ai) and I assume that I am sharing input buffers/caches with other jobs which is likely the cause of non-determinism I see, substack post: [The Long Road to AGI Begins with Control](https://open.substack.com/pub/breckbaldwin/p/the-long-road-to-agi-begins-with).\n\nI have seen that locally run LLMs are deterministic so I wanted to validate earlier experiments but no longer have access to the hardware. I'd like to not be standing up an AWS server for each model and managing it.  \n\nI like the look of [https://www.inferless.com/](https://www.inferless.com/) which is a serverless GPU hosting service but don't quite have confidence of the execution environment. \n\nI am running locally with llama.cpp but have very limited memory, 8G, so figure I'd better go hit the cloud. \n\nSo I understand my options as:\n\n1. Stand up my own AWS box and run vLLM or llama.cpp with the tasks/models I want. I have not had good luck with this in the past and it was expensive to run a big box.   \n2. [https://www.inferless.com/](https://www.inferless.com/)  or some similar service--this looks more manageable but the instructions are a bit convoluted but I can probably get it going. The key here is no sharing of resources since that is the primary likely culprit for the non-determinism I am seeing.   \n3. Run locally, but can't run big models and am barely getting llama.cpp to work on 8Gb on M2 Air--current model is Llama-3.2-3B-Instruct-Q3\\_K\\_XL\n\nI'd like option 2. the most with a simpler \"setup\", \"run\" with automatic time out after 20 min. of inactivity. \n\nAny suggestions much appreciated.",
          "author_fullname": "t2_199sgjfhdc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Simplest way to run single batch jobs for experiments on determinism",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly19br",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752329093,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am doing research on determinism of LLM responses and want to run as the only job on the server but don&amp;#39;t quite have the LLM ops skills to be confident in the backend setup. &lt;/p&gt;\n\n&lt;p&gt;I currently use the standard hosted solutions (OpenAI and together.ai) and I assume that I am sharing input buffers/caches with other jobs which is likely the cause of non-determinism I see, substack post: &lt;a href=\"https://open.substack.com/pub/breckbaldwin/p/the-long-road-to-agi-begins-with\"&gt;The Long Road to AGI Begins with Control&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I have seen that locally run LLMs are deterministic so I wanted to validate earlier experiments but no longer have access to the hardware. I&amp;#39;d like to not be standing up an AWS server for each model and managing it.  &lt;/p&gt;\n\n&lt;p&gt;I like the look of &lt;a href=\"https://www.inferless.com/\"&gt;https://www.inferless.com/&lt;/a&gt; which is a serverless GPU hosting service but don&amp;#39;t quite have confidence of the execution environment. &lt;/p&gt;\n\n&lt;p&gt;I am running locally with llama.cpp but have very limited memory, 8G, so figure I&amp;#39;d better go hit the cloud. &lt;/p&gt;\n\n&lt;p&gt;So I understand my options as:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Stand up my own AWS box and run vLLM or llama.cpp with the tasks/models I want. I have not had good luck with this in the past and it was expensive to run a big box.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.inferless.com/\"&gt;https://www.inferless.com/&lt;/a&gt;  or some similar service--this looks more manageable but the instructions are a bit convoluted but I can probably get it going. The key here is no sharing of resources since that is the primary likely culprit for the non-determinism I am seeing.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Run locally, but can&amp;#39;t run big models and am barely getting llama.cpp to work on 8Gb on M2 Air--current model is Llama-3.2-3B-Instruct-Q3_K_XL&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;d like option 2. the most with a simpler &amp;quot;setup&amp;quot;, &amp;quot;run&amp;quot; with automatic time out after 20 min. of inactivity. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?auto=webp&amp;s=152dccb67db845eb779c39a35a30a1d5027bfe14",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e83ed5374520087c4f2e15d5222be950ed938a6d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1fe08560711cda49420ad4bb9dd7aa5dd596f0b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3cc3ab3c8f3b634c2b3650e3dcf0f3bed6bb8d32",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac92a3bc5c497f22229820aac08c0c371cb3e618",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7f5744360718d7591425191b7f72221ca38bc80d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=231f26134c09ef247941498e6e2fd4faae616b4a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly19br",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Skiata",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly19br/simplest_way_to_run_single_batch_jobs_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly19br/simplest_way_to_run_single_batch_jobs_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752329093,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For folks coding daily, what models are you getting the best results with? I know there are a lot of variables, and I’d like to avoid getting bogged down in the details like performance, prompt size, parameter counts, or quantization. What models is turning in the best results for coding for you personally.\n\nFor reference, I’m using an M4max MBP with 128gm ram.",
          "author_fullname": "t2_ot5sg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RL local llm for coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly4tus",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752338280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For folks coding daily, what models are you getting the best results with? I know there are a lot of variables, and I’d like to avoid getting bogged down in the details like performance, prompt size, parameter counts, or quantization. What models is turning in the best results for coding for you personally.&lt;/p&gt;\n\n&lt;p&gt;For reference, I’m using an M4max MBP with 128gm ram.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly4tus",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rts324",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly4tus/rl_local_llm_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly4tus/rl_local_llm_for_coding/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752338280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n\n# [](https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features)Key Features\n\n* Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.\n* MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.\n* Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.\n\n# [](https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants)\n\n# Model Variants\n\n* **Kimi-K2-Base**: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.\n* **Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx8xdm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "#bbbdbf",
          "ups": 332,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 332,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=029228d543016d143b4cf39cb9567707fc07d245",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752245561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features\"&gt;&lt;/a&gt;Key Features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.&lt;/li&gt;\n&lt;li&gt;MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.&lt;/li&gt;\n&lt;li&gt;Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants\"&gt;&lt;/a&gt;&lt;/h1&gt;\n\n&lt;h1&gt;Model Variants&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Kimi-K2-Base&lt;/strong&gt;: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt;: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?auto=webp&amp;s=78218534a59407b3e56ec5c79df38546f4efe70c",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014f5215759a7ee46cc335661cfd741228ef1b1e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a09f794f1ff77c0c16776942ad4b842977ccb84",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01057b7f796b87e54735f133cbd2404a4a8425d2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=65e4d917b0768ba9727a840f3e7b4ddd3fdb7ea3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc89ea5ea896d832e6642ea7b443df8584200eab",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d43168a98c7fb53d5480aa0b7e96d2c75f889729",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx8xdm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 102,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/",
          "stickied": false,
          "url": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
          "subreddit_subscribers": 498343,
          "created_utc": 1752245561,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI’m developing a tool that allows you to create full applications by simply describing what you want in plain English—no complicated setup, no boilerplate code.\n\nHere’s what it currently offers:\n\t•\tSupports over 10 programming languages\n\t•\tLets you connect your GitHub repository\n\t•\tCan fix bugs or make improvements in your existing projects\n\t•\tWorks like Bolt.new or similar AI dev platforms, but with:\n\t•\tFaster response times\n\t•\tNo repetitive errors\n\t•\tNo excessive token usage\n\nIt’s currently in the development phase, but I plan to launch it for free to everyone at the start.\n\nI’m looking for honest feedback. What features would you find useful? What problems should I prioritize solving?\n\nYour input will directly influence how I shape this tool. Looking forward to hearing your thoughts in the comments.",
          "author_fullname": "t2_u5t9kq5vv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building an App That Builds Apps – Feedback Appreciated",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyptl7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.08,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OYZ5M27a9upsQcZRuC8QqWr8XyHjw-uCoGp77BH_QKk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752403499,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m developing a tool that allows you to create full applications by simply describing what you want in plain English—no complicated setup, no boilerplate code.&lt;/p&gt;\n\n&lt;p&gt;Here’s what it currently offers:\n    • Supports over 10 programming languages\n    • Lets you connect your GitHub repository\n    • Can fix bugs or make improvements in your existing projects\n    • Works like Bolt.new or similar AI dev platforms, but with:\n    • Faster response times\n    • No repetitive errors\n    • No excessive token usage&lt;/p&gt;\n\n&lt;p&gt;It’s currently in the development phase, but I plan to launch it for free to everyone at the start.&lt;/p&gt;\n\n&lt;p&gt;I’m looking for honest feedback. What features would you find useful? What problems should I prioritize solving?&lt;/p&gt;\n\n&lt;p&gt;Your input will directly influence how I shape this tool. Looking forward to hearing your thoughts in the comments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0t2fav6bfmcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?auto=webp&amp;s=7851a3fd0e9a8f4050435c357dd9c8e1b79a5b08",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36351b492606d376ec84f6c3266514145d42d6c5",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e8b5638135d991191e0382cc144b86ce44d89ece",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ee54bc62ef610c65ae8dccc836b6ecc130dc999",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e5c7aff51a855990bc95d8e61e471afcef695f3",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5728de62d148c87ec6be7ba70dec9a2f1481ae2b",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ca6d3ef9e20be3411788c7a738bb7ef1aa547740",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "osOMZVrkvVKxIqXVX5EQh7Ig9gY66_BSUdZF017hArA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1lyptl7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prestigious_Skin6507",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyptl7/building_an_app_that_builds_apps_feedback/",
          "stickied": false,
          "url": "https://i.redd.it/0t2fav6bfmcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752403499,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey,\n\nI’m working on a system that uses reranking to select the best LLM for each specific task. To do this, I want to use a trusted website as a knowledge base—ideally one that provides leaderboards across multiple benchmarks and tasks so I can retrieve reliable performance info for different models.\n\nQuestion 1:\nWhat websites or platforms do you recommend that have comprehensive, trusted leaderboards for LLMs across diverse benchmarks?\n\nQuestion 2:\nAlso, when deploying an LLM in production without ground truth labels, how do you measure its performance? I want to compare my solution against baselines like GPT, but:\n\nI don’t have ground truth data\n\nUsing an LLM as judge seems biased, especially if it’s similar to the baseline GPT model\n\nI have many use cases, so evaluation should be general and fair\n\nWhat metrics or strategies would you suggest to reliably know if my LLM solution is better or worse than GPT in real production scenarios?\n\nThanks in advance for your tips!",
          "author_fullname": "t2_1sxrasjt9q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for trusted websites with benchmark leaderboards to build LLM reranking — plus how to evaluate LLMs in production without ground truth?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyckyk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752358347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey,&lt;/p&gt;\n\n&lt;p&gt;I’m working on a system that uses reranking to select the best LLM for each specific task. To do this, I want to use a trusted website as a knowledge base—ideally one that provides leaderboards across multiple benchmarks and tasks so I can retrieve reliable performance info for different models.&lt;/p&gt;\n\n&lt;p&gt;Question 1:\nWhat websites or platforms do you recommend that have comprehensive, trusted leaderboards for LLMs across diverse benchmarks?&lt;/p&gt;\n\n&lt;p&gt;Question 2:\nAlso, when deploying an LLM in production without ground truth labels, how do you measure its performance? I want to compare my solution against baselines like GPT, but:&lt;/p&gt;\n\n&lt;p&gt;I don’t have ground truth data&lt;/p&gt;\n\n&lt;p&gt;Using an LLM as judge seems biased, especially if it’s similar to the baseline GPT model&lt;/p&gt;\n\n&lt;p&gt;I have many use cases, so evaluation should be general and fair&lt;/p&gt;\n\n&lt;p&gt;What metrics or strategies would you suggest to reliably know if my LLM solution is better or worse than GPT in real production scenarios?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your tips!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyckyk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Realistic_Force688",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyckyk/looking_for_trusted_websites_with_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyckyk/looking_for_trusted_websites_with_benchmark/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752358347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/moonshotai/Kimi-K2-Base](https://huggingface.co/moonshotai/Kimi-K2-Base)",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Kimi K2 - 1T MoE, 32B active params",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "snukbrpue9cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 162,
                  "x": 108,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=796d3bdbe8248f04f6f4ed687349e14dd6cb1ffa"
                },
                {
                  "y": 324,
                  "x": 216,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d2b1a397c6ecae80548f8d9752ae729a007865a"
                },
                {
                  "y": 480,
                  "x": 320,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6493bb6123b8bb0c91c7c00173ca89d74c5ae294"
                },
                {
                  "y": 960,
                  "x": 640,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3f22017530fb297141e7c8a43fc84c3b95974f2"
                },
                {
                  "y": 1440,
                  "x": 960,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a37d5eb193be223d1467fd2b152feb6d09ee34c"
                },
                {
                  "y": 1620,
                  "x": 1080,
                  "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c7c3685e31acd7ef16fbe241ae9bd6bf1a5ac667"
                }
              ],
              "s": {
                "y": 1662,
                "x": 1108,
                "u": "https://preview.redd.it/snukbrpue9cf1.jpg?width=1108&amp;format=pjpg&amp;auto=webp&amp;s=babe38be407551e4f5afa732091358a48a9164ef"
              },
              "id": "snukbrpue9cf1"
            },
            "kjw6onm2f9cf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 52,
                  "x": 108,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=50ea2fb7fb30e8ca947536791e407498a2884e76"
                },
                {
                  "y": 105,
                  "x": 216,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9811b76d09d0722786571c55e3badd97c976495"
                },
                {
                  "y": 156,
                  "x": 320,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=82b57bf75b54fbba5196263b413450e49252985c"
                },
                {
                  "y": 313,
                  "x": 640,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d147c9f5b0c487c667b46a0664db3dcb7c663bbd"
                },
                {
                  "y": 470,
                  "x": 960,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7104b993300b87d1bdc96768d121a2e8d8c6fa7e"
                },
                {
                  "y": 529,
                  "x": 1080,
                  "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0e4fd88eaadccc6d875640a0ccd2da48b2b7aaaf"
                }
              ],
              "s": {
                "y": 1336,
                "x": 2726,
                "u": "https://preview.redd.it/kjw6onm2f9cf1.jpg?width=2726&amp;format=pjpg&amp;auto=webp&amp;s=0ee5b01a83ab286f2c3a3780a44d30fddb5dba60"
              },
              "id": "kjw6onm2f9cf1"
            }
          },
          "name": "t3_1lx94ht",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 313,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "snukbrpue9cf1",
                "id": 703595261
              },
              {
                "media_id": "kjw6onm2f9cf1",
                "id": 703595262
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 313,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vOQCL6pQbXue2TSAcO_fvTvDBTLRgjIBjMBbSQhYkWI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752246042,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Base\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lx94ht",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx94ht",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lx94ht",
          "subreddit_subscribers": 498343,
          "created_utc": 1752246042,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kw6e4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Nvidia being Nvidia: FP8 is 150 Tflops faster when kernel name contain \"cutlass\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx62hd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 462,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 462,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752238238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lx62hd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bora_ach",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/",
          "stickied": false,
          "url": "https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d",
          "subreddit_subscribers": 498343,
          "created_utc": 1752238238,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello. I am wondering if there's a solution that checks a url using a local llm before deciding whether to allow or disallow a connection?\n\nUse case:\n\n\\- user types in a url\n\n\\- url is scraped and sent to the llm\n\n\\- llm decides to allow/disallow the visit as per instructions\n\nI am wondering if there's an open-source project that does this or  similar before I try to vibe-code it. Thank you for your help!\n\np.s. I am home-schooling my kids and want to make sure they remain focused on learning topics that are part of their program :-)",
          "author_fullname": "t2_ajuxt3cr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-Source LLM-Based Solution for Online Content Filtering - Is There One?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly59tz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752339429,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I am wondering if there&amp;#39;s a solution that checks a url using a local llm before deciding whether to allow or disallow a connection?&lt;/p&gt;\n\n&lt;p&gt;Use case:&lt;/p&gt;\n\n&lt;p&gt;- user types in a url&lt;/p&gt;\n\n&lt;p&gt;- url is scraped and sent to the llm&lt;/p&gt;\n\n&lt;p&gt;- llm decides to allow/disallow the visit as per instructions&lt;/p&gt;\n\n&lt;p&gt;I am wondering if there&amp;#39;s an open-source project that does this or  similar before I try to vibe-code it. Thank you for your help!&lt;/p&gt;\n\n&lt;p&gt;p.s. I am home-schooling my kids and want to make sure they remain focused on learning topics that are part of their program :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly59tz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Southern_Sun_2106",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly59tz/opensource_llmbased_solution_for_online_content/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly59tz/opensource_llmbased_solution_for_online_content/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752339429,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, I wanted to rewrite my question and put it as a discussion, in December I will be building/buying a computer to be a Home companion/nas/plex/gaming system, it will be running 24/7 and be part of a disabled person's (me) safe space and will be both a companion and entertainment.\n\nIt will run PC games, Silly tavern, ooga, llmstudio, it will be used for vlogging, plex and fit into my 10gbe network it will also be a full steam game system which will stream via parsec or in-built steam to wherever I am in the house, I'll also use virtual desktop to run my VR games and fun.\n\nAwesome use cases like with Mantella having a playthrough of SkyrimVR where every npc is AI enabled and I spend all my time breaking 4th wall and explaining to them the concept of npc's\n\nIt is used for therapy and every part of my life.\n\nI prefer windows, both all the normal OS and I love Windows server 2022,\n\nSo IF want to run a good quality model beyond the basics (I've used 4090's, 3090, 4060ti) with large context and long term use.\n\nI would prefer it to be quiet (not silent but in the reasonable range of a gaming PC using a 5060ti using VR) Not a deal breaker but I can hope.\n\nPower I'd like it to idle under 150w ideally 100w (full load power use I don't mind)\n\nSo tell me how you would build a 10k system or below and your thoughts behind it.  remember it has to run a good size model at a speed that TTS and STT are fluid and feel like a conversation not a stutter stack. Deal with gaming.\n\nFor an example I have a Poweredge 730XD 128gb DDR4 48tb SAS. with two e5-2697AV4 cpu's.\n\nI was able by putting an rtx 4000 16gb in the above system use it for everything above except big models, it even streamed AAA games (it had a 36TB steam library :D ) to my mac air/steam deck/ tablet and low powered pc fine and did Virtual desktop for my quest 3. I was surprised how well the old Xeon could handle gaming (I game mostly in 1080p anyway)\n\nBut because of the old PCIE 3 architecture anything above an rtx 4000 was issuey, and it was sooo loud I had to keep it in the kitchen, and it idled at 320w.\n\nLooking for any ideas and like I said I will have the funds for this end of December , what would you put together and importantly why?\n\n  \n\\-------------------\n\nUpdate 1\n\nLooks like the choice is \n\nMac studio m3 ultra 512gb   \n\nor\n\nRTX 6000 pro.\n\n  \nI have an AM5 platform with an 8700g which isn't a slouch paired witrh 64gb ddr5, the 6000 would kind of fit in there.\n\nI have time to look into it all.",
          "author_fullname": "t2_nufca",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is your \"perfect\" £10,000 for Local LLM, Gaming, plex with the following conditional and context.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxybu4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752342589,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752320202,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I wanted to rewrite my question and put it as a discussion, in December I will be building/buying a computer to be a Home companion/nas/plex/gaming system, it will be running 24/7 and be part of a disabled person&amp;#39;s (me) safe space and will be both a companion and entertainment.&lt;/p&gt;\n\n&lt;p&gt;It will run PC games, Silly tavern, ooga, llmstudio, it will be used for vlogging, plex and fit into my 10gbe network it will also be a full steam game system which will stream via parsec or in-built steam to wherever I am in the house, I&amp;#39;ll also use virtual desktop to run my VR games and fun.&lt;/p&gt;\n\n&lt;p&gt;Awesome use cases like with Mantella having a playthrough of SkyrimVR where every npc is AI enabled and I spend all my time breaking 4th wall and explaining to them the concept of npc&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;It is used for therapy and every part of my life.&lt;/p&gt;\n\n&lt;p&gt;I prefer windows, both all the normal OS and I love Windows server 2022,&lt;/p&gt;\n\n&lt;p&gt;So IF want to run a good quality model beyond the basics (I&amp;#39;ve used 4090&amp;#39;s, 3090, 4060ti) with large context and long term use.&lt;/p&gt;\n\n&lt;p&gt;I would prefer it to be quiet (not silent but in the reasonable range of a gaming PC using a 5060ti using VR) Not a deal breaker but I can hope.&lt;/p&gt;\n\n&lt;p&gt;Power I&amp;#39;d like it to idle under 150w ideally 100w (full load power use I don&amp;#39;t mind)&lt;/p&gt;\n\n&lt;p&gt;So tell me how you would build a 10k system or below and your thoughts behind it.  remember it has to run a good size model at a speed that TTS and STT are fluid and feel like a conversation not a stutter stack. Deal with gaming.&lt;/p&gt;\n\n&lt;p&gt;For an example I have a Poweredge 730XD 128gb DDR4 48tb SAS. with two e5-2697AV4 cpu&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;I was able by putting an rtx 4000 16gb in the above system use it for everything above except big models, it even streamed AAA games (it had a 36TB steam library :D ) to my mac air/steam deck/ tablet and low powered pc fine and did Virtual desktop for my quest 3. I was surprised how well the old Xeon could handle gaming (I game mostly in 1080p anyway)&lt;/p&gt;\n\n&lt;p&gt;But because of the old PCIE 3 architecture anything above an rtx 4000 was issuey, and it was sooo loud I had to keep it in the kitchen, and it idled at 320w.&lt;/p&gt;\n\n&lt;p&gt;Looking for any ideas and like I said I will have the funds for this end of December , what would you put together and importantly why?&lt;/p&gt;\n\n&lt;p&gt;-------------------&lt;/p&gt;\n\n&lt;p&gt;Update 1&lt;/p&gt;\n\n&lt;p&gt;Looks like the choice is &lt;/p&gt;\n\n&lt;p&gt;Mac studio m3 ultra 512gb   &lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;RTX 6000 pro.&lt;/p&gt;\n\n&lt;p&gt;I have an AM5 platform with an 8700g which isn&amp;#39;t a slouch paired witrh 64gb ddr5, the 6000 would kind of fit in there.&lt;/p&gt;\n\n&lt;p&gt;I have time to look into it all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxybu4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quebber",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxybu4/what_is_your_perfect_10000_for_local_llm_gaming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxybu4/what_is_your_perfect_10000_for_local_llm_gaming/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320202,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just started experimenting with LLM workflow using n8n, and I built a workflow to improve the translation quality of my local LLM, sure it works but I found it lacking some basic functions, like I need to write JavaScript for some very basic things \n\nI'm not an professional AI workflow developer, I just want to improve my local LLM's performance with minimal coding.\n\nWhat are your recommendations for a more user-friendly LLM workflow UIs that are good alternatives to n8n? Which UI are you using right now?\n\nThanks in advance!",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What LLM Workflow UI Are You Using?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxz268",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752322677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started experimenting with LLM workflow using n8n, and I built a workflow to improve the translation quality of my local LLM, sure it works but I found it lacking some basic functions, like I need to write JavaScript for some very basic things &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not an professional AI workflow developer, I just want to improve my local LLM&amp;#39;s performance with minimal coding.&lt;/p&gt;\n\n&lt;p&gt;What are your recommendations for a more user-friendly LLM workflow UIs that are good alternatives to n8n? Which UI are you using right now?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxz268",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lxz268/what_llm_workflow_ui_are_you_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxz268/what_llm_workflow_ui_are_you_using/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752322677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The 1T Kimi K2 model is using DeepSeek V3 architecture",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxb0eo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "ups": 161,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 161,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/GLWo99y2VXsciDgoAKyzE3GZociHN4n_7g_ihtVi9I0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752250411,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/l3gpvb5or9cf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/l3gpvb5or9cf1.png?auto=webp&amp;s=40d8a85b1d07e964b63a7e5184f9a5d5809b715f",
                  "width": 1080,
                  "height": 1907
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff23740ffcaa59c97aa66e63a434e727e8b2ad4a",
                    "width": 108,
                    "height": 190
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bce6fa94a7bb620076dc98255430dc1b47e1e0a7",
                    "width": 216,
                    "height": 381
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8aea3ce7d05f335e3aa2e57966b68ef819eeeea9",
                    "width": 320,
                    "height": 565
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed19f0e0b0c28fcc15556f566717d11201a68611",
                    "width": 640,
                    "height": 1130
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e13852845fea6f89d5766368474e1f7307fd4b0",
                    "width": 960,
                    "height": 1695
                  },
                  {
                    "url": "https://preview.redd.it/l3gpvb5or9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18f660077733393b7b3433b426f75cb65bfef0e5",
                    "width": 1080,
                    "height": 1907
                  }
                ],
                "variants": {},
                "id": "Q200XGQK2QDqTgopEJURrQJKELVtC6jLu8BfpN-NSrQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lxb0eo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 30,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/",
          "stickied": false,
          "url": "https://i.redd.it/l3gpvb5or9cf1.png",
          "subreddit_subscribers": 498343,
          "created_utc": 1752250411,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MedGemma 27B Multimodal for complex multimodal &amp; longitudinal EHR interpretation: [https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4](https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4)\n\nMedSigLIP: a lightweight image/text encoder for medical image retrieval/classification: [https://huggingface.co/google/medsiglip-448](https://huggingface.co/google/medsiglip-448)\n\nT5Gemma: lightweight yet powerful encoder-decoder research models: [https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86](https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86)",
          "author_fullname": "t2_agjaq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 126,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx7l3k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 217,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 217,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1dUy9ra4BOXxGh9C9vTqGqPmClTQ3z5eudzCu9deMEk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752242256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MedGemma 27B Multimodal for complex multimodal &amp;amp; longitudinal EHR interpretation: &lt;a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\"&gt;https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;MedSigLIP: a lightweight image/text encoder for medical image retrieval/classification: &lt;a href=\"https://huggingface.co/google/medsiglip-448\"&gt;https://huggingface.co/google/medsiglip-448&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;T5Gemma: lightweight yet powerful encoder-decoder research models: &lt;a href=\"https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86\"&gt;https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/r2bp20do39cf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?auto=webp&amp;s=505dfa90a4c5b4814a6f8f5d8f21e458723fabe9",
                  "width": 1250,
                  "height": 1132
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c06f842789e7c04781657d6ba7a8362b24d6127f",
                    "width": 108,
                    "height": 97
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc0e48e964af821a64b767ea1cbd0ea1a0335b02",
                    "width": 216,
                    "height": 195
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2535f1397a9eac8a49ccbcd0d6581431f7b8353a",
                    "width": 320,
                    "height": 289
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=aa04ae911a28403b4ac7702442fb11eb655146a3",
                    "width": 640,
                    "height": 579
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af44035733ae0f541d1021ecd9aef93869863c7b",
                    "width": 960,
                    "height": 869
                  },
                  {
                    "url": "https://preview.redd.it/r2bp20do39cf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a960ed19ee1bd5acb1b47e2621264c111bba5b74",
                    "width": 1080,
                    "height": 978
                  }
                ],
                "variants": {},
                "id": "WkBW3ZxSnDC2gTb6x_KnIpQDXfKRLjZymR9zHnbq-Zk"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lx7l3k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Nunki08",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/",
          "stickied": false,
          "url": "https://i.redd.it/r2bp20do39cf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752242256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As per the title. I want to run a model for dnd, the plan is to use Gemma 3 27b and max out the context length so that the model can remember things. Once the context fills up, I plan to ask the model to summarise the session and paste it into a new instance to continue. I have tried it with Gemini 2.5 Pro and the method works well enough.\n\nThe issue I mainly want to ask about is what impacts the filled up context length would have. From my understanding, I will need a stronger gpu chip for the prompt processing, but the vram will get filled up as usual.\n\nWill this just be the same as running a model that progressively gets larger the more I use it?\n\nHow does this work with multiple gpus? \n\nWhat prompt processing speeds can I expect with an mi50 32gb?\n\nHow does prompt processing work actually, each portion loaded into vram is processed by that vram’s corresponding gpu chip right?\n\nSo many questions, I’ll probably ask further clarifying questions in the comments",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does having a very long context window impact performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxuu5m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752306412,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per the title. I want to run a model for dnd, the plan is to use Gemma 3 27b and max out the context length so that the model can remember things. Once the context fills up, I plan to ask the model to summarise the session and paste it into a new instance to continue. I have tried it with Gemini 2.5 Pro and the method works well enough.&lt;/p&gt;\n\n&lt;p&gt;The issue I mainly want to ask about is what impacts the filled up context length would have. From my understanding, I will need a stronger gpu chip for the prompt processing, but the vram will get filled up as usual.&lt;/p&gt;\n\n&lt;p&gt;Will this just be the same as running a model that progressively gets larger the more I use it?&lt;/p&gt;\n\n&lt;p&gt;How does this work with multiple gpus? &lt;/p&gt;\n\n&lt;p&gt;What prompt processing speeds can I expect with an mi50 32gb?&lt;/p&gt;\n\n&lt;p&gt;How does prompt processing work actually, each portion loaded into vram is processed by that vram’s corresponding gpu chip right?&lt;/p&gt;\n\n&lt;p&gt;So many questions, I’ll probably ask further clarifying questions in the comments&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxuu5m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752306412,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "LiquidAI released their [LFM2 model family](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38), and support for it was just [merged into llama.cpp](https://github.com/ggml-org/llama.cpp/pull/14620) a few hours ago. I haven't yet tried it locally, but I was quite impressed by their online demo of the 1.2B model. It had excellent world knowledge and general conversational coherence and intelligence for its size. I found it much better than SmolLM2 at everything, and similar in intelligence to Qwen 3 1.7B but with better world knowledge. Seems SOTA for its size. Context length is 32k tokens. The license disallows commercial use over $10M revenue, but for personal use or small commercial use it should be fine. In general the license didn't seem too bad.",
          "author_fullname": "t2_702zh1r2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LiquidAI LFM2 Model Released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxmldq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752279263,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752279033,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;LiquidAI released their &lt;a href=\"https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38\"&gt;LFM2 model family&lt;/a&gt;, and support for it was just &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14620\"&gt;merged into llama.cpp&lt;/a&gt; a few hours ago. I haven&amp;#39;t yet tried it locally, but I was quite impressed by their online demo of the 1.2B model. It had excellent world knowledge and general conversational coherence and intelligence for its size. I found it much better than SmolLM2 at everything, and similar in intelligence to Qwen 3 1.7B but with better world knowledge. Seems SOTA for its size. Context length is 32k tokens. The license disallows commercial use over $10M revenue, but for personal use or small commercial use it should be fine. In general the license didn&amp;#39;t seem too bad.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM.png?auto=webp&amp;s=db2e9ca39da78413a820464b2c60f8e20b7d7ba9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e38a5b7b5025caff4f8bea478c3931e2a70363a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ba05c82ef30892c0610bd04415ae6d0e1107d04",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c591afc2cf3b38e2919b29edcab3c5ae7529bf6",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b740a0768284d51562778a65f0c62cc626d939f5",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1dd39e53038bf1c9c72f0dc49f3410bd4a9c5df4",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=30501a283194febb5b44b785f4c69f070fe0aa16",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "MWIqEyyR9FIzN2ecVcaRHrLuSUwcveuAc9n59LiL-QM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lxmldq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Federal-Effective879",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752279033,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First behemoth was \"delayed\" by meta and it looks like it is never coming out. Now R2 is delayed by deepseek. Does that mean the end for deepseek too?",
          "author_fullname": "t2_sm168dt0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When a model is delayed because the boss isn't happy, is it doomed forever?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyltyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752387578,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First behemoth was &amp;quot;delayed&amp;quot; by meta and it looks like it is never coming out. Now R2 is delayed by deepseek. Does that mean the end for deepseek too?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyltyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMrsPotts",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyltyb/when_a_model_is_delayed_because_the_boss_isnt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyltyb/when_a_model_is_delayed_because_the_boss_isnt/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752387578,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_gv6j3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lx8qrz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 157,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 157,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=140&amp;height=69&amp;crop=140:69,smart&amp;auto=webp&amp;s=eb6d386e78fa6261648d91dd3aa26540b2de45a8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752245109,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "ethz.ch",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?auto=webp&amp;s=c444e4312b925e4518f67c13cd1d0d45a148e086",
                  "width": 1565,
                  "height": 782
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=00f03455ad8e9fc0d7ab20142af7d9f6c62b3273",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6c969005aff0c942b7f56ff899031ce58c131be",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f469ddb87f611d5472215734a3217eb0ca1795e4",
                    "width": 320,
                    "height": 159
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3",
                    "width": 640,
                    "height": 319
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=869859917c64b3c7518aec8cee6c925a903fa611",
                    "width": 960,
                    "height": 479
                  },
                  {
                    "url": "https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5bd6e7e7d67e1ccb5d54f14559b874a510f09c70",
                    "width": 1080,
                    "height": 539
                  }
                ],
                "variants": {},
                "id": "TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lx8qrz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nat2r",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/",
          "stickied": false,
          "url": "https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html",
          "subreddit_subscribers": 498343,
          "created_utc": 1752245109,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, I am an llm privacy researcher, I need a SFF build as my personal machine, that I plan to travel with and use to show live demonstrations to potential enterprise clients, will host an 8B llm plus some basic overheads like BERT \n\nThe 5060ti is new, reliable ( i can buy for 450$ in my country) cheap and comes with warranty. New architecture so I assume some pytorch improvements, 4 bit llms?\n\nCons: super low bandwidth, VRAM not enough to host say 13B models, token per second is going to be abysmal, large contexts? I work with documents\n\nThe rtx 3090 ( 750$ gaming use 3 years out of warranty) is of course a beast, with 24 gigs of VRAM and almost 3x the bandwidth \n\nCons: risky, will it handle our loads well? Thermal failure? Higher TDP for sff? What if i get handed a bad card ( mining etc )\n\nPlease help me i am so confused 😕 \nThis community is awesome 🙏\n\n\n",
          "author_fullname": "t2_1t1panb3d8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rtx 5060ti 16gb vs Rtx 3090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxwpqp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752314043,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am an llm privacy researcher, I need a SFF build as my personal machine, that I plan to travel with and use to show live demonstrations to potential enterprise clients, will host an 8B llm plus some basic overheads like BERT &lt;/p&gt;\n\n&lt;p&gt;The 5060ti is new, reliable ( i can buy for 450$ in my country) cheap and comes with warranty. New architecture so I assume some pytorch improvements, 4 bit llms?&lt;/p&gt;\n\n&lt;p&gt;Cons: super low bandwidth, VRAM not enough to host say 13B models, token per second is going to be abysmal, large contexts? I work with documents&lt;/p&gt;\n\n&lt;p&gt;The rtx 3090 ( 750$ gaming use 3 years out of warranty) is of course a beast, with 24 gigs of VRAM and almost 3x the bandwidth &lt;/p&gt;\n\n&lt;p&gt;Cons: risky, will it handle our loads well? Thermal failure? Higher TDP for sff? What if i get handed a bad card ( mining etc )&lt;/p&gt;\n\n&lt;p&gt;Please help me i am so confused 😕 \nThis community is awesome 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxwpqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alpine_Privacy",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxwpqp/rtx_5060ti_16gb_vs_rtx_3090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxwpqp/rtx_5060ti_16gb_vs_rtx_3090/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752314043,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Everytime I peek at a builders GPU options I feel I never see it go that high. Anyone ever hear of a reputable builder with that power?",
          "author_fullname": "t2_opo23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any builder companies that sell pre-assembled Blackwell 6000 machines?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly0oln",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752327498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everytime I peek at a builders GPU options I feel I never see it go that high. Anyone ever hear of a reputable builder with that power?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly0oln",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "richardanaya",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly0oln/are_there_any_builder_companies_that_sell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly0oln/are_there_any_builder_companies_that_sell/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752327498,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi folks—just wanted to share something we’ve been working on. If you’ve tried using Claude Code or Gemini CLI for local projects, you’ve probably noticed it can only search with basic *grep*. That makes it hard to find things like a \\`Crawler\\` class when you’re searching for “scrape”.\n\nWe built an open-source tool that supports semantic code search on your local files. It uses an embedding model to index code and stores it in a vector database (Zilliz Cloud or Milvus). It tracks changes in your directory using a Merkle tree, similar to how Cursor does it.\n\nIt works with MCP and VSCode, and you can use it alongside Claude Code, Gemini CLI, or plug it into your own workflows.\n\nGithub link: [https://github.com/zilliztech/CodeIndexer](https://github.com/zilliztech/CodeIndexer)",
          "author_fullname": "t2_qem1mweoc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Semantic code search for local directory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxryp4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752295884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks—just wanted to share something we’ve been working on. If you’ve tried using Claude Code or Gemini CLI for local projects, you’ve probably noticed it can only search with basic &lt;em&gt;grep&lt;/em&gt;. That makes it hard to find things like a `Crawler` class when you’re searching for “scrape”.&lt;/p&gt;\n\n&lt;p&gt;We built an open-source tool that supports semantic code search on your local files. It uses an embedding model to index code and stores it in a vector database (Zilliz Cloud or Milvus). It tracks changes in your directory using a Merkle tree, similar to how Cursor does it.&lt;/p&gt;\n\n&lt;p&gt;It works with MCP and VSCode, and you can use it alongside Claude Code, Gemini CLI, or plug it into your own workflows.&lt;/p&gt;\n\n&lt;p&gt;Github link: &lt;a href=\"https://github.com/zilliztech/CodeIndexer\"&gt;https://github.com/zilliztech/CodeIndexer&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs.png?auto=webp&amp;s=d08be8244fed778e02c0d61252a964221700ea64",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7620ebbc02f57fa65d16be80949dfb1b7d4ba235",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e90e84a46451cc1a61d0dd3af483e6e301d7049b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e931d6e8c18a2a9b9877a2e00ff82df81d99f362",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3dc438631a439e97ea547adc9678022ee477553a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=51bcb9a53d06db323779476acce3f1821f386a8b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=87d472cf26a6df002f0fc81e5bdf4fa1d3ded56c",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "FVczwBNkvJlkikO1Xpgiwpl-jHdNZ7ONOTzzFENcvJs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lxryp4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "codingjaguar",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxryp4/semantic_code_search_for_local_directory/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxryp4/semantic_code_search_for_local_directory/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752295884,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey there, I got an M3 Max 96GB, which model do you guys think is the best for my hardware? For context, I mostly do light coding and agentic workflows that use MCP for data analytics. Thanks!",
          "author_fullname": "t2_2f6s556",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best model for M3 Max 96GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxufzz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752304885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, I got an M3 Max 96GB, which model do you guys think is the best for my hardware? For context, I mostly do light coding and agentic workflows that use MCP for data analytics. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxufzz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gaztrab",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxufzz/best_model_for_m3_max_96gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxufzz/best_model_for_m3_max_96gb/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752304885,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}